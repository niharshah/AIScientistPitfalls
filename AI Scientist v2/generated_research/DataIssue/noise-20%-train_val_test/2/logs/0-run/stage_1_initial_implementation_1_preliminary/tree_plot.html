<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 5], [0, 7], [0, 6]], "layout": [[0.3333333333333333, 0.0], [0.5555555555555556, 0.0], [0.7777777777777778, 0.0], [1.0, 0.0], [0.0, 1.0], [0.2222222222222222, 1.0], [0.4444444444444444, 1.0], [0.6666666666666666, 1.0]], "plan": ["This baseline treats every symbol in a sequence as a discrete token, learns an\nembedding for each token, and leverages a lightweight Transformer encoder to\ncapture long-range dependencies. After mean-pooling the contextualized\nrepresentations (ignoring padding), a linear layer predicts the hidden rule\nclass. We build the vocabulary from the training set, encode the datasets with\nHuggingFace datasets, and use PyTorch DataLoader with a custom collator for\ndynamic padding. The model is trained for a few epochs with cross-entropy loss;\nafter every epoch we compute validation loss and macro-F1, print them, and save\nall metrics/predictions into ./working/experiment_data.npy for later analysis.\nThe script is self-contained, obeys GPU handling rules, and finishes within\nminutes on a modern GPU/CPU. This gives a solid, easily extensible baseline for\ncontextual-embedding reasoning on SPR_BENCH.", "We treat each symbolic sequence as a list of discrete tokens (characters or\nwhitespace-separated symbols), learn a small embedding for every token, feed the\nresulting vectors (preceded by a special CLS token and enriched with sinusoidal\npositional encoding) into a lightweight 2-layer Transformer encoder, and\nclassify the resulting CLS representation with a linear layer. A vocabulary is\nbuilt from the training split and unknown tokens are mapped to UNK; PAD is 0 so\nmasks are trivial. Data are loaded from the SPR_BENCH folder with the helper in\nSPR.py; if the folder is absent we synthesise a tiny mock dataset so the script\nalways runs. Training is performed for a few epochs with cross-entropy loss,\nAdam optimiser, batch-size 64, and we report validation loss plus macro-F1 after\nevery epoch. All tensors and the model are moved to GPU when available, strictly\nfollowing the requested device handling rules. Metrics, losses, predictions, and\nground-truth labels are accumulated in an experiment_data dictionary and saved\nas numpy arrays under ./working. Finally, test-set macro-F1 is printed and\nstored, giving a complete yet compact baseline that can later be improved.", "We will treat each symbolic sequence as a string of individual characters, build\na minimal vocabulary from the training split (including special tokens <pad>,\n<unk>, and <cls>), and encode sequences to integer IDs.   A small transformer-\nencoder (2 layers, 4 heads, d_model = 64) receives these IDs through an\nembedding layer plus learned positional embeddings; the [CLS] vector is fed to a\nlinear classifier.   We train for a few epochs with AdamW, monitor validation\nloss and macro-F1 each epoch, and keep the best model.   Data loaders pad\nbatches on-the-fly; everything is moved to GPU if available.   After training we\nevaluate on the test split, print the final macro-F1, and store losses, metrics,\npredictions, and ground-truth in experiment_data, saving it to\n./working/experiment_data.npy.   This script is fully self-contained, executes\non launch, follows the required GPU/CPU handling rules, and finishes within 30\nminutes on a single GPU.", "We convert each symbolic sequence to character-level tokens, build a vocabulary\nfrom the training split, and pad/clip all sequences to a fixed length.  A small\ntransformer encoder with learnable positional embeddings yields contextual\nrepresentations whose mean is passed to a linear classifier.  We train the model\nwith cross-entropy on the SPR_BENCH train set and monitor validation loss and\nmacro-F1 on the dev set after every epoch, printing the results and storing them\nin a structured experiment_data dictionary.  All tensors and the model are moved\nto GPU when available, following the required device handling guidelines.  The\nscript automatically loads the benchmark using the provided utility, constructs\nPyTorch datasets/dataloaders, trains for a few epochs (default 5), evaluates on\nthe held-out test split once training finishes, and saves metrics, predictions,\nand ground truth to \u201c./working/experiment_data.npy\u201d.  This serves as a clean,\nreproducible baseline that can be extended with larger models or richer\ntokenization strategies in later experiments.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport numpy as np\nfrom typing import Dict, List\nfrom collections import Counter\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ----------------------------------------------------\n# Device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------------------------------------------\n# Experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ----------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"Load SPR_BENCH splits into a DatasetDict.\"\"\"\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\n# ----------------------------------------------------\n# Paths (adjust if necessary)\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_bench = load_spr_bench(DATA_PATH)\n\nprint({split: len(ds) for split, ds in spr_bench.items()})\n\n\n# ----------------------------------------------------\n# Build vocabulary from training sequences (character-level)\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    chars = set(\"\".join(seqs))\n    vocab = {pad_token: idx for idx, pad_token in enumerate([\"<pad>\"])}\n    for ch in sorted(chars):\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr_bench[\"train\"][\"sequence\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n# ----------------------------------------------------\n# Label mapping\nlabels = sorted(set(spr_bench[\"train\"][\"label\"]))\nlabel2id = {lbl: idx for idx, lbl in enumerate(labels)}\nnum_classes = len(label2id)\nprint(f\"Number of classes: {num_classes}\")\n\n\ndef encode_example(example):\n    example[\"input_ids\"] = [vocab[ch] for ch in example[\"sequence\"]]\n    example[\"label_id\"] = label2id.get(example[\"label\"], -1)  # -1 for test\n    return example\n\n\nfor split in spr_bench:\n    spr_bench[split] = spr_bench[split].map(\n        encode_example, remove_columns=spr_bench[split].column_names\n    )\n\nspr_bench.set_format(type=\"python\", columns=[\"input_ids\", \"label_id\"])\n\n\n# ----------------------------------------------------\n# Collate function with dynamic padding\ndef collate_fn(batch):\n    seqs = [item[\"input_ids\"] for item in batch]\n    labels = torch.tensor([item[\"label_id\"] for item in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    attn_mask = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        attn_mask[i, : len(s)] = 1\n    return {\n        \"input_ids\": padded.to(device),\n        \"attention_mask\": attn_mask.to(device),\n        \"labels\": labels.to(device),\n    }\n\n\ntrain_loader = DataLoader(\n    spr_bench[\"train\"], batch_size=128, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    spr_bench[\"dev\"], batch_size=256, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ----------------------------------------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, num_classes, dim_ff=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x, attention_mask):\n        emb = self.embed(x)\n        key_padding_mask = ~attention_mask.bool()\n        enc_out = self.encoder(emb, src_key_padding_mask=key_padding_mask)\n        masked = enc_out * attention_mask.unsqueeze(-1)\n        pooled = masked.sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n        logits = self.fc(pooled)\n        return logits\n\n\nmodel = TransformerClassifier(\n    vocab_size=vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=num_classes\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ----------------------------------------------------\ndef evaluate(loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(dim=-1).cpu().numpy()\n            all_preds.extend(list(preds))\n            all_labels.extend(list(batch[\"labels\"].cpu().numpy()))\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    return avg_loss, macro_f1, all_preds, all_labels\n\n\n# ----------------------------------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n\n    train_loss = running_loss / len(train_loader.dataset)\n    val_loss, val_macro_f1, val_preds, val_labels = evaluate(val_loader)\n\n    train_macro_f1, _, _, _ = evaluate(train_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n        f\"train_macro_f1={train_macro_f1:.4f} | val_macro_f1={val_macro_f1:.4f}\"\n    )\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(val_labels)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ----------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, string, pathlib, math, time, sys\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, Dataset\n\n# --- working dir -----------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --- device ----------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------- utilities\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"Load SPR_BENCH as DatasetDict (expects train.csv/dev.csv/test.csv).\"\"\"\n\n    def _load(csv_name):\n        return Dataset.from_csv(str(root / csv_name))\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef build_vocab(data_iter, min_freq: int = 1):\n    freq = {}\n    for seq in data_iter:\n        tokens = tokenize(seq)\n        for tok in tokens:\n            freq[tok] = freq.get(tok, 0) + 1\n    vocab = {\"<PAD>\": 0, \"<CLS>\": 1, \"<UNK>\": 2}\n    for tok, c in sorted(freq.items()):\n        if c >= min_freq:\n            vocab.setdefault(tok, len(vocab))\n    return vocab\n\n\ndef tokenize(sequence: str):\n    # prefer whitespace split; fallback to char split\n    if \" \" in sequence.strip():\n        return sequence.strip().split()\n    return list(sequence.strip())\n\n\n# ------------------------------------------------------------------- Dataset wrap\nclass SPRDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.data = hf_ds\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        tokens = [self.vocab[\"<CLS>\"]] + [\n            self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokenize(row[\"sequence\"])\n        ]\n        return {\n            \"input_ids\": torch.tensor(tokens, dtype=torch.long),\n            \"label\": torch.tensor(row[\"label\"], dtype=torch.long),\n        }\n\n\ndef collate_fn(batch):\n    max_len = max(len(x[\"input_ids\"]) for x in batch)\n    input_ids = []\n    attention_mask = []\n    labels = []\n    for item in batch:\n        ids = item[\"input_ids\"]\n        pad_len = max_len - len(ids)\n        input_ids.append(torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)]))\n        attention_mask.append(\n            torch.cat(\n                [\n                    torch.ones(len(ids), dtype=torch.long),\n                    torch.zeros(pad_len, dtype=torch.long),\n                ]\n            )\n        )\n        labels.append(item[\"label\"])\n    return {\n        \"input_ids\": torch.stack(input_ids),\n        \"attention_mask\": torch.stack(attention_mask),\n        \"labels\": torch.stack(labels),\n    }\n\n\n# -------------------------------------------------------------- Model definition\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1,max_len,d_model)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1), :].to(x.device)\n        return x\n\n\nclass SPRTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_classes,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_feedforward=128,\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_enc = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.classifier = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        emb = self.embedding(input_ids)\n        emb = self.pos_enc(emb)\n        emb = self.transformer(emb, src_key_padding_mask=~attention_mask.bool())\n        cls_vec = emb[:, 0, :]\n        return self.classifier(cls_vec)\n\n\n# ------------------------------------------------------------- load / create data\nDATA_ROOT = pathlib.Path(\"./SPR_BENCH\")\nif DATA_ROOT.exists():\n    spr_sets = load_spr_bench(DATA_ROOT)\nelse:\n    # --- create minimal synthetic dataset to keep script runnable --------------\n    def synth_ds(n):\n        rows = []\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:6], k=random.randint(5, 10))\n            )\n            label = sum(ord(c) for c in seq) % 4\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return Dataset.from_list(rows)\n\n    spr_sets = DatasetDict(\n        {\"train\": synth_ds(500), \"dev\": synth_ds(100), \"test\": synth_ds(100)}\n    )\n    print(\"Synthetic data generated.\")\n\nnum_classes = len(set(spr_sets[\"train\"][\"label\"]))\nvocab = build_vocab(spr_sets[\"train\"][\"sequence\"])\nprint(f\"Vocab size: {len(vocab)}, Num classes: {num_classes}\")\n\ntrain_ds = SPRDataset(spr_sets[\"train\"], vocab)\ndev_ds = SPRDataset(spr_sets[\"dev\"], vocab)\ntest_ds = SPRDataset(spr_sets[\"test\"], vocab)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\n\n# --------------------------------------------------------------------- training\nmodel = SPRTransformerClassifier(len(vocab), num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\nEPOCHS = 3\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_losses = []\n    train_preds, train_labels = [], []\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n        train_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n        train_labels.extend(batch[\"labels\"].cpu().tolist())\n    train_f1 = f1_score(train_labels, train_preds, average=\"macro\")\n\n    # Validation\n    model.eval()\n    val_losses = []\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_losses.append(loss.item())\n            val_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n            val_labels.extend(batch[\"labels\"].cpu().tolist())\n    val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n    print(\n        f\"Epoch {epoch}: validation_loss = {np.mean(val_losses):.4f}, val_macro_f1 = {val_f1:.4f}\"\n    )\n\n    # store experiment data\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(np.mean(train_losses))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(np.mean(val_losses))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ------------------------------------------------------------ test evaluation\nmodel.eval()\ntest_preds, test_labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        test_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n        test_labels.extend(batch[\"labels\"].cpu().tolist())\ntest_f1 = f1_score(test_labels, test_preds, average=\"macro\")\nprint(f\"Test macro_f1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = np.array(test_preds)\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = np.array(test_labels)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ------------------ working dir ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ device -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------ experiment data --------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ------------------ dataset loader --------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded SPR_BENCH:\", {k: len(v) for k, v in spr.items()})\n\n\n# ------------------ tokenisation -----------------\ndef tokenize(seq: str):\n    return list(seq.strip())  # char-level\n\n\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor s in spr[\"train\"][\"sequence\"]:\n    for tok in tokenize(s):\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\npad_id, unk_id, cls_id = vocab[\"<pad>\"], vocab[\"<unk>\"], vocab[\"<cls>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in tokenize(seq)]\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    padded = []\n    for item in batch:\n        ids = item[\"input_ids\"]\n        pad_len = maxlen - len(ids)\n        padded.append(\n            torch.cat([ids, torch.full((pad_len,), pad_id, dtype=torch.long)])\n        )\n    input_ids = torch.stack(padded)\n    attention_mask = (input_ids != pad_id).long()\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------ model ------------------------\nclass MiniTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        n_classes=len(set(spr[\"train\"][\"label\"])),\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos = nn.Parameter(torch.randn(512, d_model))  # max 512 seq\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=128, dropout=0.1, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.cls = nn.Linear(d_model, n_classes)\n\n    def forward(self, input_ids, attention_mask=None):\n        x = self.embed(input_ids) + self.pos[: input_ids.size(1)]\n        if attention_mask is not None:\n            # Transformer expects bool mask: True for positions to ignore\n            key_padding_mask = ~(attention_mask.bool())\n        else:\n            key_padding_mask = None\n        h = self.enc(x, src_key_padding_mask=key_padding_mask)\n        cls_vec = h[:, 0]  # [CLS]\n        return self.cls(cls_vec)\n\n\nmodel = MiniTransformer(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n\n# ------------------ training loop ---------------\nEPOCHS = 5\nbest_val_f1 = 0.0\nfor epoch in range(1, EPOCHS + 1):\n    # ---- train ----\n    model.train()\n    running_loss = 0.0\n    preds, gts = [], []\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    train_loss = running_loss / len(train_loader.dataset)\n    train_f1 = f1_score(gts, preds, average=\"macro\")\n    # ---- validate ----\n    model.eval()\n    val_loss_tot = 0.0\n    v_preds, v_gts = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss_tot += loss.item() * batch[\"labels\"].size(0)\n            v_preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            v_gts.extend(batch[\"labels\"].cpu().tolist())\n    val_loss = val_loss_tot / len(val_loader.dataset)\n    val_f1 = f1_score(v_gts, v_preds, average=\"macro\")\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n    )\n    # store\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n    # keep best\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n\n# ------------------ test evaluation -------------\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best_model.pt\"), map_location=device)\n)\nmodel.eval()\nt_preds, t_gts = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        t_preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        t_gts.extend(batch[\"labels\"].cpu().tolist())\ntest_f1 = f1_score(t_gts, t_preds, average=\"macro\")\nprint(f\"Test macro_f1 = {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = t_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = t_gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- dataset loader utility (from prompt) --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        label = self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        length = len(ids)\n        if length < self.max_len:\n            ids += [self.pad_id] * (self.max_len - length)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=256\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1), :]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)  # (batch, seq, d_model)\n        x = x.mean(dim=1)\n        return self.cls(x)\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    total_loss, all_pred, all_true = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = out.argmax(dim=1).detach().cpu().numpy()\n        all_pred.extend(preds)\n        all_true.extend(batch[\"labels\"].cpu().numpy())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_true, all_pred, average=\"macro\")\n    return avg_loss, macro_f1\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    total_loss, all_pred, all_true = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = out.argmax(dim=1).cpu().numpy()\n        all_pred.extend(preds)\n        all_true.extend(batch[\"labels\"].cpu().numpy())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_true, all_pred, average=\"macro\")\n    return avg_loss, macro_f1, all_pred, all_true\n\n\n# -------------------- main routine --------------------\ndef main():\n    # Path to SPR_BENCH folder (adjust if needed)\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"Dataset path {DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # Build vocab from training sequences\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256)\n    test_loader = DataLoader(test_ds, batch_size=256)\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    epochs = 5\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n        )\n\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n    # Final test evaluation\n    test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n    print(f\"Test macro_f1 = {test_f1:.4f}\")\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\n\n    # Save experiment data\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# Execute immediately\nmain()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport numpy as np\nfrom typing import Dict, List\nfrom collections import Counter\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ----------------------------------------------------\n# Device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------------------------------------------\n# Experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ----------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"Load SPR_BENCH splits into a DatasetDict.\"\"\"\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\n# ----------------------------------------------------\n# Paths (adjust if necessary)\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_bench = load_spr_bench(DATA_PATH)\n\nprint({split: len(ds) for split, ds in spr_bench.items()})\n\n\n# ----------------------------------------------------\n# Build vocabulary from training sequences (character-level)\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    chars = set(\"\".join(seqs))\n    vocab = {pad_token: idx for idx, pad_token in enumerate([\"<pad>\"])}\n    for ch in sorted(chars):\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr_bench[\"train\"][\"sequence\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n# ----------------------------------------------------\n# Label mapping\nlabels = sorted(set(spr_bench[\"train\"][\"label\"]))\nlabel2id = {lbl: idx for idx, lbl in enumerate(labels)}\nnum_classes = len(label2id)\nprint(f\"Number of classes: {num_classes}\")\n\n\ndef encode_example(example):\n    example[\"input_ids\"] = [vocab[ch] for ch in example[\"sequence\"]]\n    example[\"label_id\"] = label2id.get(example[\"label\"], -1)  # -1 for test\n    return example\n\n\nfor split in spr_bench:\n    spr_bench[split] = spr_bench[split].map(\n        encode_example, remove_columns=spr_bench[split].column_names\n    )\n\nspr_bench.set_format(type=\"python\", columns=[\"input_ids\", \"label_id\"])\n\n\n# ----------------------------------------------------\n# Collate function with dynamic padding\ndef collate_fn(batch):\n    seqs = [item[\"input_ids\"] for item in batch]\n    labels = torch.tensor([item[\"label_id\"] for item in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    attn_mask = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        attn_mask[i, : len(s)] = 1\n    return {\n        \"input_ids\": padded.to(device),\n        \"attention_mask\": attn_mask.to(device),\n        \"labels\": labels.to(device),\n    }\n\n\ntrain_loader = DataLoader(\n    spr_bench[\"train\"], batch_size=128, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    spr_bench[\"dev\"], batch_size=256, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ----------------------------------------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, num_classes, dim_ff=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x, attention_mask):\n        emb = self.embed(x)\n        key_padding_mask = ~attention_mask.bool()\n        enc_out = self.encoder(emb, src_key_padding_mask=key_padding_mask)\n        masked = enc_out * attention_mask.unsqueeze(-1)\n        pooled = masked.sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n        logits = self.fc(pooled)\n        return logits\n\n\nmodel = TransformerClassifier(\n    vocab_size=vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=num_classes\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ----------------------------------------------------\ndef evaluate(loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(dim=-1).cpu().numpy()\n            all_preds.extend(list(preds))\n            all_labels.extend(list(batch[\"labels\"].cpu().numpy()))\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    return avg_loss, macro_f1, all_preds, all_labels\n\n\n# ----------------------------------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n\n    train_loss = running_loss / len(train_loader.dataset)\n    val_loss, val_macro_f1, val_preds, val_labels = evaluate(val_loader)\n\n    train_macro_f1, _, _, _ = evaluate(train_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n        f\"train_macro_f1={train_macro_f1:.4f} | val_macro_f1={val_macro_f1:.4f}\"\n    )\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(val_labels)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ----------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport numpy as np\nfrom typing import Dict, List\nfrom collections import Counter\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ----------------------------------------------------\n# Device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------------------------------------------\n# Experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ----------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"Load SPR_BENCH splits into a DatasetDict.\"\"\"\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\n# ----------------------------------------------------\n# Paths (adjust if necessary)\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_bench = load_spr_bench(DATA_PATH)\n\nprint({split: len(ds) for split, ds in spr_bench.items()})\n\n\n# ----------------------------------------------------\n# Build vocabulary from training sequences (character-level)\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    chars = set(\"\".join(seqs))\n    vocab = {pad_token: idx for idx, pad_token in enumerate([\"<pad>\"])}\n    for ch in sorted(chars):\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr_bench[\"train\"][\"sequence\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n# ----------------------------------------------------\n# Label mapping\nlabels = sorted(set(spr_bench[\"train\"][\"label\"]))\nlabel2id = {lbl: idx for idx, lbl in enumerate(labels)}\nnum_classes = len(label2id)\nprint(f\"Number of classes: {num_classes}\")\n\n\ndef encode_example(example):\n    example[\"input_ids\"] = [vocab[ch] for ch in example[\"sequence\"]]\n    example[\"label_id\"] = label2id.get(example[\"label\"], -1)  # -1 for test\n    return example\n\n\nfor split in spr_bench:\n    spr_bench[split] = spr_bench[split].map(\n        encode_example, remove_columns=spr_bench[split].column_names\n    )\n\nspr_bench.set_format(type=\"python\", columns=[\"input_ids\", \"label_id\"])\n\n\n# ----------------------------------------------------\n# Collate function with dynamic padding\ndef collate_fn(batch):\n    seqs = [item[\"input_ids\"] for item in batch]\n    labels = torch.tensor([item[\"label_id\"] for item in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    attn_mask = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        attn_mask[i, : len(s)] = 1\n    return {\n        \"input_ids\": padded.to(device),\n        \"attention_mask\": attn_mask.to(device),\n        \"labels\": labels.to(device),\n    }\n\n\ntrain_loader = DataLoader(\n    spr_bench[\"train\"], batch_size=128, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    spr_bench[\"dev\"], batch_size=256, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ----------------------------------------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, num_classes, dim_ff=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x, attention_mask):\n        emb = self.embed(x)\n        key_padding_mask = ~attention_mask.bool()\n        enc_out = self.encoder(emb, src_key_padding_mask=key_padding_mask)\n        masked = enc_out * attention_mask.unsqueeze(-1)\n        pooled = masked.sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n        logits = self.fc(pooled)\n        return logits\n\n\nmodel = TransformerClassifier(\n    vocab_size=vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=num_classes\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ----------------------------------------------------\ndef evaluate(loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(dim=-1).cpu().numpy()\n            all_preds.extend(list(preds))\n            all_labels.extend(list(batch[\"labels\"].cpu().numpy()))\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    return avg_loss, macro_f1, all_preds, all_labels\n\n\n# ----------------------------------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n\n    train_loss = running_loss / len(train_loader.dataset)\n    val_loss, val_macro_f1, val_preds, val_labels = evaluate(val_loader)\n\n    train_macro_f1, _, _, _ = evaluate(train_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n        f\"train_macro_f1={train_macro_f1:.4f} | val_macro_f1={val_macro_f1:.4f}\"\n    )\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(val_labels)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ----------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport numpy as np\nfrom typing import Dict, List\nfrom collections import Counter\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ----------------------------------------------------\n# Device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------------------------------------------\n# Experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ----------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"Load SPR_BENCH splits into a DatasetDict.\"\"\"\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\n# ----------------------------------------------------\n# Paths (adjust if necessary)\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_bench = load_spr_bench(DATA_PATH)\n\nprint({split: len(ds) for split, ds in spr_bench.items()})\n\n\n# ----------------------------------------------------\n# Build vocabulary from training sequences (character-level)\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    chars = set(\"\".join(seqs))\n    vocab = {pad_token: idx for idx, pad_token in enumerate([\"<pad>\"])}\n    for ch in sorted(chars):\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr_bench[\"train\"][\"sequence\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n# ----------------------------------------------------\n# Label mapping\nlabels = sorted(set(spr_bench[\"train\"][\"label\"]))\nlabel2id = {lbl: idx for idx, lbl in enumerate(labels)}\nnum_classes = len(label2id)\nprint(f\"Number of classes: {num_classes}\")\n\n\ndef encode_example(example):\n    example[\"input_ids\"] = [vocab[ch] for ch in example[\"sequence\"]]\n    example[\"label_id\"] = label2id.get(example[\"label\"], -1)  # -1 for test\n    return example\n\n\nfor split in spr_bench:\n    spr_bench[split] = spr_bench[split].map(\n        encode_example, remove_columns=spr_bench[split].column_names\n    )\n\nspr_bench.set_format(type=\"python\", columns=[\"input_ids\", \"label_id\"])\n\n\n# ----------------------------------------------------\n# Collate function with dynamic padding\ndef collate_fn(batch):\n    seqs = [item[\"input_ids\"] for item in batch]\n    labels = torch.tensor([item[\"label_id\"] for item in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    attn_mask = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        attn_mask[i, : len(s)] = 1\n    return {\n        \"input_ids\": padded.to(device),\n        \"attention_mask\": attn_mask.to(device),\n        \"labels\": labels.to(device),\n    }\n\n\ntrain_loader = DataLoader(\n    spr_bench[\"train\"], batch_size=128, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    spr_bench[\"dev\"], batch_size=256, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ----------------------------------------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, num_classes, dim_ff=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x, attention_mask):\n        emb = self.embed(x)\n        key_padding_mask = ~attention_mask.bool()\n        enc_out = self.encoder(emb, src_key_padding_mask=key_padding_mask)\n        masked = enc_out * attention_mask.unsqueeze(-1)\n        pooled = masked.sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1)\n        logits = self.fc(pooled)\n        return logits\n\n\nmodel = TransformerClassifier(\n    vocab_size=vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=num_classes\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ----------------------------------------------------\ndef evaluate(loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(dim=-1).cpu().numpy()\n            all_preds.extend(list(preds))\n            all_labels.extend(list(batch[\"labels\"].cpu().numpy()))\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    return avg_loss, macro_f1, all_preds, all_labels\n\n\n# ----------------------------------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n\n    train_loss = running_loss / len(train_loader.dataset)\n    val_loss, val_macro_f1, val_preds, val_labels = evaluate(val_loader)\n\n    train_macro_f1, _, _, _ = evaluate(train_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n        f\"train_macro_f1={train_macro_f1:.4f} | val_macro_f1={val_macro_f1:.4f}\"\n    )\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_macro_f1)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(val_labels)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ----------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 116413.05\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 87908.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 138494.44\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize: 10', '\\n', 'Number of classes: 2', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n21203.54 examples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 22790.42\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]',\n'', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 24193.77 examples/s]',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6959 |\nval_loss=0.6322 | train_macro_f1=0.6296 | val_macro_f1=0.7719', '\\n', 'Epoch 2:\ntrain_loss=0.5832 | val_loss=0.5528 | train_macro_f1=0.5324 |\nval_macro_f1=0.7579', '\\n', 'Epoch 3: train_loss=0.5295 | val_loss=0.5511 |\ntrain_macro_f1=0.5421 | val_macro_f1=0.7940', '\\n', 'Epoch 4: train_loss=0.5314\n| val_loss=0.5388 | train_macro_f1=0.5276 | val_macro_f1=0.7860', '\\n', 'Epoch\n5: train_loss=0.5280 | val_loss=0.5361 | train_macro_f1=0.5189 |\nval_macro_f1=0.7860', '\\n', 'Experiment data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n1/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Synthetic data generated.', '\\n', 'Vocab size: 9,\nNum classes: 4', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 1.3944,\nval_macro_f1 = 0.1075', '\\n', 'Epoch 2: validation_loss = 1.4521, val_macro_f1 =\n0.1000', '\\n', 'Epoch 3: validation_loss = 1.3932, val_macro_f1 = 0.2097', '\\n',\n'Test macro_f1: 0.1875', '\\n', 'Execution time: 2 seconds seconds (time limit is\n30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 176933.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 94080.66\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 146536.14\nexamples/s]', '\\n', 'Loaded SPR_BENCH:', ' ', \"{'train': 2000, 'dev': 500,\n'test': 1000}\", '\\n', 'Vocab size:', ' ', '12', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6337,\nval_macro_f1 = 0.7679', '\\n', 'Epoch 2: validation_loss = 0.5248, val_macro_f1 =\n0.7918', '\\n', 'Epoch 3: validation_loss = 0.5331, val_macro_f1 = 0.7840', '\\n',\n'Epoch 4: validation_loss = 0.5349, val_macro_f1 = 0.7800', '\\n', 'Epoch 5:\nvalidation_loss = 0.5521, val_macro_f1 = 0.7431', '\\n', 'Test macro_f1 =\n0.7920', '\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 51003.26\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 90660.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 178162.60\nexamples/s]', '\\n', 'Epoch 1: validation_loss = 0.5978, val_macro_f1 = 0.7161',\n'\\n', 'Epoch 2: validation_loss = 0.5617, val_macro_f1 = 0.7599', '\\n', 'Epoch\n3: validation_loss = 0.5421, val_macro_f1 = 0.7840', '\\n', 'Epoch 4:\nvalidation_loss = 0.5328, val_macro_f1 = 0.7920', '\\n', 'Epoch 5:\nvalidation_loss = 0.5362, val_macro_f1 = 0.7860', '\\n', 'Test macro_f1 =\n0.7949', '\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 126441.09\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 92279.86\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 153222.18\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize: 10', '\\n', 'Number of classes: 2', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n21750.35 examples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 23037.02\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]',\n'', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 23932.99 examples/s]',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6447 |\nval_loss=0.5619 | train_macro_f1=0.5496 | val_macro_f1=0.7639', '\\n', 'Epoch 2:\ntrain_loss=0.5374 | val_loss=0.5408 | train_macro_f1=0.5322 |\nval_macro_f1=0.7920', '\\n', 'Epoch 3: train_loss=0.5288 | val_loss=0.5344 |\ntrain_macro_f1=0.5196 | val_macro_f1=0.7860', '\\n', 'Epoch 4: train_loss=0.5252\n| val_loss=0.5272 | train_macro_f1=0.5186 | val_macro_f1=0.7959', '\\n', 'Epoch\n5: train_loss=0.5259 | val_loss=0.5324 | train_macro_f1=0.5150 |\nval_macro_f1=0.7860', '\\n', 'Experiment data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n2/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 10', '\\n', 'Number of classes: 2', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 20283.99 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n21645.12 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 22904.17\nexamples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6933 |\nval_loss=0.6425 | train_macro_f1=0.6439 | val_macro_f1=0.4879', '\\n', 'Epoch 2:\ntrain_loss=0.5686 | val_loss=0.5509 | train_macro_f1=0.5396 |\nval_macro_f1=0.7880', '\\n', 'Epoch 3: train_loss=0.5340 | val_loss=0.5330 |\ntrain_macro_f1=0.5202 | val_macro_f1=0.7860', '\\n', 'Epoch 4: train_loss=0.5271\n| val_loss=0.5328 | train_macro_f1=0.5236 | val_macro_f1=0.7880', '\\n', 'Epoch\n5: train_loss=0.5254 | val_loss=0.5642 | train_macro_f1=0.5372 |\nval_macro_f1=0.7599', '\\n', 'Experiment data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n3/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 10', '\\n', 'Number of classes: 2', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 22777.80 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n22473.66 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 23524.65\nexamples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6759 |\nval_loss=0.6210 | train_macro_f1=0.6229 | val_macro_f1=0.6021', '\\n', 'Epoch 2:\ntrain_loss=0.5654 | val_loss=0.5413 | train_macro_f1=0.5299 |\nval_macro_f1=0.7880', '\\n', 'Epoch 3: train_loss=0.5307 | val_loss=0.5468 |\ntrain_macro_f1=0.5257 | val_macro_f1=0.7639', '\\n', 'Epoch 4: train_loss=0.5204\n| val_loss=0.5360 | train_macro_f1=0.5291 | val_macro_f1=0.7959', '\\n', 'Epoch\n5: train_loss=0.5172 | val_loss=0.5280 | train_macro_f1=0.5136 |\nval_macro_f1=0.7860', '\\n', 'Experiment data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n4/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["The code executed successfully without any errors or bugs. The training and\nevaluation process for the Synthetic PolyRule Reasoning (SPR) task was\ncompleted, and the results were saved to a file. Metrics like train and\nvalidation losses, as well as macro F1 scores, were logged for each epoch. The\nimplementation appears to be working as intended.", "The model's performance on the validation and test sets is poor, as indicated by\nthe low macro F1 scores (validation: 0.1075 to 0.2097, test: 0.1875). This\nsuggests that the model is not learning effectively. Possible reasons could\ninclude insufficient model capacity, inappropriate hyperparameters, or issues\nwith the synthetic dataset being too simplistic or not representative of the\nproblem. To address this, consider: 1. Increasing the model complexity (e.g.,\nmore transformer layers, larger embedding size). 2. Adjusting hyperparameters\nsuch as learning rate or batch size. 3. Using a more realistic and challenging\ndataset to better simulate the SPR_BENCH benchmark. 4. Ensuring that the\nsynthetic dataset labels are meaningful and aligned with the task.", "The training script executed successfully without any bugs. The dataset was\nloaded correctly, the vocabulary was built, and the model training and\nevaluation processes were completed. The validation and test macro F1 scores\nwere reasonable for a preliminary experiment, with the best validation macro F1\nreaching 0.7918 and the test macro F1 achieving 0.7920. This indicates that the\nimplementation is functionally correct for the initial stage.", "", "", "The execution of the training script was successful without any bugs. The model\ntrained over 5 epochs and showed improvements in validation macro F1 scores\ninitially, but there was a slight drop in the last epoch. The experiment data\nwas saved successfully. The implementation appears to be functioning correctly.", "", ""], "exc_type": [null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score, which is the harmonic mean of precision and recall, calculated across all classes equally.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, which indicates how well the model is performing. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.528, "best_value": 0.528}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, which indicates how well the model is generalizing. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5361, "best_value": 0.5361}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.386, "best_value": 1.386}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.3932, "best_value": 1.3932}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2417, "best_value": 0.2417}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2097, "best_value": 0.2097}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1875, "best_value": 0.1875}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score, a measure of a model's accuracy considering both precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error between the predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5521, "best_value": 0.5253}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5092, "best_value": 0.5092}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5328, "best_value": 0.5328}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5496, "best_value": 0.5496}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5259, "best_value": 0.5259}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5324, "best_value": 0.5324}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the macro-averaged F1 score, which evaluates the balance between precision and recall across multiple classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6439, "best_value": 0.788}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Represents the loss value during training, indicating how well the model fits the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5254, "best_value": 0.5254}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Represents the loss value during validation, indicating how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5642, "best_value": 0.5642}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score, which considers the F1 score for each class and then averages them.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6229, "best_value": 0.7959}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the error rate of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5172, "best_value": 0.5172}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, indicating the error rate of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.528, "best_value": 0.528}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_28dac03619b145faac2359822b5bdcc8/SPR_BENCH_agg_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_28dac03619b145faac2359822b5bdcc8/SPR_BENCH_agg_macro_f1_curve.png"]], "plot_paths": [["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_28dac03619b145faac2359822b5bdcc8/SPR_BENCH_agg_loss_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_28dac03619b145faac2359822b5bdcc8/SPR_BENCH_agg_macro_f1_curve.png"]], "plot_analyses": [[{"analysis": "The plot shows the train and validation loss over epochs. Both losses decrease consistently, indicating that the model is learning effectively. The gap between the train and validation loss is minimal, suggesting that the model is generalizing well and there is no significant overfitting or underfitting observed. The validation loss stabilizes towards the end, which is a positive sign for convergence.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_loss_curve.png"}, {"analysis": "The plot shows the train and validation Macro-F1 scores over epochs. The validation Macro-F1 score starts high and stabilizes around 0.8, which is a good performance indicator. However, the train Macro-F1 score decreases slightly over epochs, which might indicate underfitting on the training data. This could be due to insufficient training or overly strong regularization.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The confusion matrix for the development dataset at the last epoch shows that the model performs reasonably well, with a high number of true positives and true negatives. However, there are notable false positives (62) and false negatives (45). This indicates that while the model is effective, there is room for improvement in reducing misclassifications, potentially through better hyperparameter tuning or additional data augmentation.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c252788b9d7c4ad99d47b0048203d3f6_proc_3154573/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The cross-entropy loss curves show a steady decrease in training loss over the first three epochs, indicating that the model is learning effectively. However, the validation loss initially decreases but starts to increase slightly after epoch 3, suggesting some overfitting. The gap between training and validation loss also widens after epoch 3, further supporting this observation. Regularization techniques or early stopping might be considered to mitigate overfitting.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves indicate strong initial improvement in both training and validation performance, with validation performance peaking around epoch 3. Beyond this point, the validation Macro-F1 score starts to decline slightly, while the training score plateaus. This trend aligns with the overfitting observed in the loss curves, as the model's generalization performance on unseen data begins to degrade.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "The confusion matrix on the test set shows a reasonable balance between true positives and true negatives, with 399 true negatives and 393 true positives. However, there are 99 false positives and 109 false negatives, indicating some room for improvement in classification accuracy. The model appears to have a slight bias towards predicting the negative class, as evidenced by the higher number of false negatives compared to false positives. Adjusting the decision threshold or using class-weighted loss could help address this imbalance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_937a6db9a5764873bff0d0c0f85ec8f6_proc_3154575/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The cross-entropy loss curves for both training and validation sets demonstrate a steady decline over the epochs. The training loss decreases more sharply compared to the validation loss, which indicates effective learning on the training data. However, the validation loss plateaus after a few epochs, suggesting that the model's generalization capability might be reaching its limit. There is no indication of overfitting as the validation loss does not increase after the initial epochs.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_loss_curves.png"}, {"analysis": "The macro-F1 curves for both training and validation sets show consistent improvement over the epochs. The validation macro-F1 score closely follows the training macro-F1 score, indicating that the model is generalizing well to unseen data. The scores plateau after about four epochs, suggesting that the model has reached its peak performance on this metric.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "The confusion matrix for the test set shows a strong diagonal dominance, indicating that the model performs well in correctly classifying the majority of the test samples. However, there is some misclassification evident in the off-diagonal cells, which could be further analyzed to identify specific patterns or classes that are more prone to errors.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss over 5 epochs. The training loss decreases sharply initially and then stabilizes, indicating that the model is learning from the data. The validation loss follows a similar trend but begins to slightly increase after epoch 4, suggesting potential overfitting. The gap between the training and validation loss is minimal, which implies that the model generalizes reasonably well to the validation set.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot displays the Macro-F1 score for the training and validation sets over 5 epochs. The validation Macro-F1 score starts high and remains relatively stable, fluctuating slightly around 0.78 to 0.80. In contrast, the training Macro-F1 score decreases steadily, which could indicate that the model is focusing on minimizing loss rather than optimizing for F1 score, or it might suggest some challenges in learning balanced representations for all classes.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The confusion matrix for the development set at the last epoch shows that the model has a reasonably balanced performance across the two classes. However, it misclassifies 62 instances of class 0 as class 1 and 45 instances of class 1 as class 0. This indicates a slight bias toward predicting class 1, but the overall performance is fairly balanced.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over 5 epochs. Both losses initially decrease, indicating that the model is learning effectively. However, the validation loss starts to increase slightly after epoch 3, suggesting potential overfitting. The training loss continues to decrease, which further supports this observation. To mitigate overfitting, regularization techniques such as dropout or early stopping could be considered.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot illustrates the Macro-F1 scores for training and validation datasets across 5 epochs. The validation Macro-F1 score increases sharply at the beginning and stabilizes at a high value, while the training Macro-F1 score decreases initially before slightly improving. This discrepancy suggests that the model generalizes well on the validation set but struggles to achieve consistent performance on the training set, possibly due to underfitting or noisy data.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The confusion matrix for the development set at the last epoch shows a balanced performance across the two classes. The model correctly classifies 185 and 195 instances for classes 0 and 1, respectively, while misclassifying 75 and 45 instances. The higher number of false negatives for class 0 compared to class 1 indicates a slight bias towards predicting class 1. This could be addressed by adjusting the class weights or using a more balanced dataset.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot depicts the cross-entropy loss for both training and validation sets over five epochs. The training loss consistently decreases, indicating that the model is learning effectively during training. Similarly, the validation loss also decreases, which suggests that the model is not overfitting and generalizes well to unseen data. The convergence of both curves toward a lower loss value shows promising progress in optimizing the model for the SPR task.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot shows the macro-F1 scores for both training and validation sets over five epochs. While the training macro-F1 score decreases slightly after the first epoch, the validation macro-F1 score improves significantly in the initial epochs and stabilizes at a high value. This indicates that the model performs well on the validation set, achieving good balance across different classes. However, the decline in training macro-F1 may suggest potential issues with the model's ability to generalize across all classes during training.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The confusion matrix for the development set at the last epoch reveals the model's classification performance. The true positive and true negative counts are relatively high, indicating that the model performs well in correctly classifying both classes. However, there is a noticeable number of false positives and false negatives, which suggests room for improvement in the model's precision and recall. Overall, the confusion matrix shows reasonable performance but highlights areas where the model could be further optimized to reduce misclassifications.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model is learning effectively with consistent loss\nreduction and a stable validation Macro-F1 score. However, there is a slight\nunderfitting observed in the training data, and the confusion matrix highlights\nareas for improvement in reducing misclassifications.", "[]", "The plots provide valuable insights into the model's performance, highlighting\noverfitting after epoch 3 and a need to improve generalization and balance in\nclassification performance. Regularization, early stopping, and threshold\nadjustment are potential avenues for improvement.", "The results indicate that the model is learning effectively and generalizing\nwell, as evidenced by the consistent decline in loss and improvement in macro-F1\nscores. The confusion matrix highlights strong classification performance with\nsome room for improvement in reducing misclassifications.", "The plots indicate that the model is learning effectively, with a reasonable\nbalance between training and validation performance. However, there is a slight\nindication of overfitting in the loss plot and a potential bias in the confusion\nmatrix. Further tuning of hyperparameters or regularization techniques might\nimprove these aspects.", "The plots indicate that the model is learning effectively but faces challenges\nwith overfitting and slight class imbalance. Validation performance is strong,\nbut training performance shows some inconsistencies. Further refinement of the\nmodel and training process is recommended to address these issues.", "The plots demonstrate that the model is learning effectively, with decreasing\nloss values and high validation macro-F1 scores. The confusion matrix suggests\ngood classification performance, though there is room for improvement in\nreducing misclassifications.", "[]"], "exec_time": [4.613096714019775, 2.630110263824463, 3.346054792404175, 3.4055440425872803, 5.105106830596924, 5.1193859577178955, 5.202953815460205, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# ------------------------------- #\n# Paths and data loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"SPR_BENCH\", {})\nmetrics = data.get(\"metrics\", {})\nlosses = data.get(\"losses\", {})\nepochs = data.get(\"epochs\", [])\n\n# ------------------------------- #\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train Loss\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 2: Macro-F1 curves\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_macro_f1\", []), label=\"Train Macro-F1\")\n    plt.plot(epochs, metrics.get(\"val_macro_f1\", []), label=\"Validation Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Train vs Validation Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 3: Confusion matrix (last epoch)\ntry:\n    preds_list = data.get(\"predictions\", [])\n    gts_list = data.get(\"ground_truth\", [])\n    if preds_list and gts_list:\n        preds = preds_list[-1]\n        gts = gts_list[-1]\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        plt.title(\"SPR_BENCH: Confusion Matrix (Dev, Last Epoch)\")\n        plt.colorbar()\n        classes = range(cm.shape[0])\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=8,\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Print best validation Macro-F1\ntry:\n    val_f1s = metrics.get(\"val_macro_f1\", [])\n    if val_f1s:\n        best_epoch = int(np.argmax(val_f1s)) + 1\n        print(f\"Best Validation Macro-F1: {max(val_f1s):.4f} at epoch {best_epoch}\")\nexcept Exception as e:\n    print(f\"Error computing best metric: {e}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nds = \"SPR_BENCH\"\nif ds in experiment_data:\n    data = experiment_data[ds]\n    epochs = data[\"epochs\"]\n    tr_loss = data[\"losses\"][\"train\"]\n    va_loss = data[\"losses\"][\"val\"]\n    tr_f1 = data[\"metrics\"][\"train_macro_f1\"]\n    va_f1 = data[\"metrics\"][\"val_macro_f1\"]\n    preds = np.array(data.get(\"predictions\", []))\n    gts = np.array(data.get(\"ground_truth\", []))\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, va_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- 2. macro-F1 curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train\")\n        plt.plot(epochs, va_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 Curves\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix ----------\n    try:\n        if preds.size and gts.size:\n            cm = confusion_matrix(gts, preds)\n            disp = ConfusionMatrixDisplay(cm)\n            disp.plot(values_format=\"d\", cmap=\"Blues\")\n            plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n            plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ---------- print evaluation metric ----------\n    if preds.size and gts.size:\n        test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n        print(f\"Test macro-F1 (recomputed) = {test_macro_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nds_key = \"SPR_BENCH\"\nif ds_key not in experiment_data:\n    print(f\"Dataset {ds_key} not found in experiment data.\")\n    exit()\n\nds = experiment_data[ds_key]\n\n# -------------------- 1. Loss curves --------------------\ntry:\n    epochs = ds[\"epochs\"]\n    tr_loss = ds[\"losses\"][\"train\"]\n    val_loss = ds[\"losses\"][\"val\"]\n    plt.figure()\n    plt.plot(epochs, tr_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# -------------------- 2. Macro-F1 curves --------------------\ntry:\n    tr_f1 = ds[\"metrics\"][\"train_macro_f1\"]\n    val_f1 = ds[\"metrics\"][\"val_macro_f1\"]\n    plt.figure()\n    plt.plot(epochs, tr_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# -------------------- 3. Confusion matrix --------------------\ntry:\n    preds = np.array(ds.get(\"predictions\", []))\n    gts = np.array(ds.get(\"ground_truth\", []))\n    if preds.size > 0 and gts.size > 0:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH Confusion Matrix\\nDataset: Test Set\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    else:\n        print(\"Predictions / ground-truth empty; skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# -------------------- print evaluation metric --------------------\nif preds.size > 0 and gts.size > 0:\n    test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n    print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# ------------------------------- #\n# Paths and data loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"SPR_BENCH\", {})\nmetrics = data.get(\"metrics\", {})\nlosses = data.get(\"losses\", {})\nepochs = data.get(\"epochs\", [])\n\n# ------------------------------- #\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train Loss\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 2: Macro-F1 curves\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_macro_f1\", []), label=\"Train Macro-F1\")\n    plt.plot(epochs, metrics.get(\"val_macro_f1\", []), label=\"Validation Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Train vs Validation Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 3: Confusion matrix (last epoch)\ntry:\n    preds_list = data.get(\"predictions\", [])\n    gts_list = data.get(\"ground_truth\", [])\n    if preds_list and gts_list:\n        preds = preds_list[-1]\n        gts = gts_list[-1]\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        plt.title(\"SPR_BENCH: Confusion Matrix (Dev, Last Epoch)\")\n        plt.colorbar()\n        classes = range(cm.shape[0])\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=8,\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Print best validation Macro-F1\ntry:\n    val_f1s = metrics.get(\"val_macro_f1\", [])\n    if val_f1s:\n        best_epoch = int(np.argmax(val_f1s)) + 1\n        print(f\"Best Validation Macro-F1: {max(val_f1s):.4f} at epoch {best_epoch}\")\nexcept Exception as e:\n    print(f\"Error computing best metric: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# ------------------------------- #\n# Paths and data loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"SPR_BENCH\", {})\nmetrics = data.get(\"metrics\", {})\nlosses = data.get(\"losses\", {})\nepochs = data.get(\"epochs\", [])\n\n# ------------------------------- #\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train Loss\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 2: Macro-F1 curves\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_macro_f1\", []), label=\"Train Macro-F1\")\n    plt.plot(epochs, metrics.get(\"val_macro_f1\", []), label=\"Validation Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Train vs Validation Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 3: Confusion matrix (last epoch)\ntry:\n    preds_list = data.get(\"predictions\", [])\n    gts_list = data.get(\"ground_truth\", [])\n    if preds_list and gts_list:\n        preds = preds_list[-1]\n        gts = gts_list[-1]\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        plt.title(\"SPR_BENCH: Confusion Matrix (Dev, Last Epoch)\")\n        plt.colorbar()\n        classes = range(cm.shape[0])\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=8,\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Print best validation Macro-F1\ntry:\n    val_f1s = metrics.get(\"val_macro_f1\", [])\n    if val_f1s:\n        best_epoch = int(np.argmax(val_f1s)) + 1\n        print(f\"Best Validation Macro-F1: {max(val_f1s):.4f} at epoch {best_epoch}\")\nexcept Exception as e:\n    print(f\"Error computing best metric: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# ------------------------------- #\n# Paths and data loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"SPR_BENCH\", {})\nmetrics = data.get(\"metrics\", {})\nlosses = data.get(\"losses\", {})\nepochs = data.get(\"epochs\", [])\n\n# ------------------------------- #\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train Loss\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 2: Macro-F1 curves\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_macro_f1\", []), label=\"Train Macro-F1\")\n    plt.plot(epochs, metrics.get(\"val_macro_f1\", []), label=\"Validation Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Train vs Validation Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Plot 3: Confusion matrix (last epoch)\ntry:\n    preds_list = data.get(\"predictions\", [])\n    gts_list = data.get(\"ground_truth\", [])\n    if preds_list and gts_list:\n        preds = preds_list[-1]\n        gts = gts_list[-1]\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        plt.title(\"SPR_BENCH: Confusion Matrix (Dev, Last Epoch)\")\n        plt.colorbar()\n        classes = range(cm.shape[0])\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=8,\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------- #\n# Print best validation Macro-F1\ntry:\n    val_f1s = metrics.get(\"val_macro_f1\", [])\n    if val_f1s:\n        best_epoch = int(np.argmax(val_f1s)) + 1\n        print(f\"Best Validation Macro-F1: {max(val_f1s):.4f} at epoch {best_epoch}\")\nexcept Exception as e:\n    print(f\"Error computing best metric: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------ #\n# Load every run listed in the prompt\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d0670b7bc8ae43658fc047ce6dee1519_proc_3154574/experiment_data.npy\",\n    \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e7ed40c2afe242c3a282c49c82b0ede2_proc_3154575/experiment_data.npy\",\n    \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8a46dc6bee2a47149fb29438fae5e881_proc_3154576/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\n# ------------------------------------------------------------------ #\n# Aggregate per dataset\ndatasets = set()\nfor run_dict in all_experiment_data:\n    datasets.update(run_dict.keys())\n\nfor dname in datasets:\n    # --- collect runs that actually contain this dataset\n    runs_for_ds = [run[dname] for run in all_experiment_data if dname in run]\n    if not runs_for_ds:\n        continue\n\n    # ----- helper to stack metric arrays across runs -----\n    def stack_metric(metric_path):\n        \"\"\"metric_path: tuple of nested keys, e.g. ('losses','train')\"\"\"\n        collected = []\n        for r in runs_for_ds:\n            ref = r\n            try:\n                for k in metric_path:\n                    ref = ref[k]\n                collected.append(np.asarray(ref))\n            except Exception:\n                pass\n        if not collected:\n            return None\n        # Trim to common length\n        min_len = min([len(arr) for arr in collected])\n        collected = np.stack([arr[:min_len] for arr in collected], axis=0)\n        return collected\n\n    # epoch axis\n    epoch_arr = None\n    try:\n        epoch_candidates = [\n            r.get(\"epochs\", []) for r in runs_for_ds if r.get(\"epochs\", [])\n        ]\n        if epoch_candidates:\n            min_len = min(len(e) for e in epoch_candidates)\n            epoch_arr = np.asarray(epoch_candidates[0][:min_len])\n    except Exception:\n        pass\n    if epoch_arr is None:\n        print(f\"No epoch information for {dname}, skipping dataset\")\n        continue\n\n    # -------------- Plot 1: Loss curves with SE -----------------\n    try:\n        train_loss = stack_metric((\"losses\", \"train\"))\n        val_loss = stack_metric((\"losses\", \"val\"))\n        if train_loss is not None and val_loss is not None:\n            plt.figure()\n            # train\n            mean_tr = train_loss.mean(0)\n            se_tr = train_loss.std(0, ddof=1) / np.sqrt(train_loss.shape[0])\n            plt.plot(epoch_arr, mean_tr, label=\"Train Loss (mean)\", color=\"tab:blue\")\n            plt.fill_between(\n                epoch_arr,\n                mean_tr - se_tr,\n                mean_tr + se_tr,\n                alpha=0.3,\n                color=\"tab:blue\",\n                label=\"Train Loss (\u00b1SE)\",\n            )\n            # val\n            mean_val = val_loss.mean(0)\n            se_val = val_loss.std(0, ddof=1) / np.sqrt(val_loss.shape[0])\n            plt.plot(epoch_arr, mean_val, label=\"Val Loss (mean)\", color=\"tab:orange\")\n            plt.fill_between(\n                epoch_arr,\n                mean_val - se_val,\n                mean_val + se_val,\n                alpha=0.3,\n                color=\"tab:orange\",\n                label=\"Val Loss (\u00b1SE)\",\n            )\n\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Train vs Validation Loss (Mean \u00b1 SE)\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_agg_loss_curve.png\")\n            plt.savefig(fname)\n            plt.close()\n        else:\n            print(f\"{dname}: missing loss information, skipping loss plot\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss curve for {dname}: {e}\")\n        plt.close()\n\n    # -------------- Plot 2: Macro-F1 curves with SE -------------\n    try:\n        train_f1 = stack_metric((\"metrics\", \"train_macro_f1\"))\n        val_f1 = stack_metric((\"metrics\", \"val_macro_f1\"))\n        if train_f1 is not None and val_f1 is not None:\n            plt.figure()\n            # train\n            mean_tr = train_f1.mean(0)\n            se_tr = train_f1.std(0, ddof=1) / np.sqrt(train_f1.shape[0])\n            plt.plot(\n                epoch_arr, mean_tr, label=\"Train Macro-F1 (mean)\", color=\"tab:green\"\n            )\n            plt.fill_between(\n                epoch_arr,\n                mean_tr - se_tr,\n                mean_tr + se_tr,\n                alpha=0.3,\n                color=\"tab:green\",\n                label=\"Train Macro-F1 (\u00b1SE)\",\n            )\n            # val\n            mean_val = val_f1.mean(0)\n            se_val = val_f1.std(0, ddof=1) / np.sqrt(val_f1.shape[0])\n            plt.plot(epoch_arr, mean_val, label=\"Val Macro-F1 (mean)\", color=\"tab:red\")\n            plt.fill_between(\n                epoch_arr,\n                mean_val - se_val,\n                mean_val + se_val,\n                alpha=0.3,\n                color=\"tab:red\",\n                label=\"Val Macro-F1 (\u00b1SE)\",\n            )\n\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Train vs Validation Macro-F1 (Mean \u00b1 SE)\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_agg_macro_f1_curve.png\")\n            plt.savefig(fname)\n            plt.close()\n\n            # Print best mean val F1\n            best_idx = int(np.argmax(mean_val))\n            print(\n                f\"{dname} | Best mean Val Macro-F1: {mean_val[best_idx]:.4f} \u00b1 {se_val[best_idx]:.4f} at epoch {epoch_arr[best_idx]}\"\n            )\n        else:\n            print(f\"{dname}: missing macro-F1 information, skipping F1 plot\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated Macro-F1 curve for {dname}: {e}\")\n        plt.close()\n"], "plot_plan": ["We will first load the serialized experiment_data from working_dir and extract the SPR_BENCH metrics. Two line plots will be produced: (1) train-vs-validation loss and (2) train-vs-validation macro-F1 across epochs. Each plot is drawn in its own try-except block, titled clearly, saved with descriptive filenames, and the figures are closed afterwards. A third (optional) plot will visualise the confusion matrix on the development set for the final epoch; this is also wrapped in a try-except block and only executed if data are present. We respect the limit of \u22645 figures, so only these three plots are attempted. After plotting, the script prints the best validation macro-F1 and its corresponding epoch. All plotting uses basic matplotlib, and every file is saved into the working directory created earlier. The code follows the required imports, creates the directory if needed, and adheres to the provided guidelines.", null, "We will first load experiment_data.npy and extract losses, macro-F1 scores, predictions and ground-truth for the single dataset SPR_BENCH.  Three plots are produced, each wrapped in its own try-except: (1) training vs validation loss over epochs, (2) training vs validation macro-F1 over epochs, and (3) a confusion matrix of best-model test predictions.  Each plot is saved in working_dir with informative names that include the dataset and plot type.  Titles are added and figures are always closed, even on error.  We only plot what is present in the NumPy file, and because epochs are few we plot every epoch (<<5).  After plotting, we compute and print the final test macro-F1 that was stored in the experiment data to give a quick quantitative summary.  The code follows all guidelines: required imports up top, working directory creation, basic matplotlib usage, and separate try-except blocks for each figure.  No synthetic data, no extra styles.", "We will first load experiment_data.npy from the working directory and verify that the expected keys exist. Then, for the single dataset (SPR_BENCH), we extract epoch indices, training/validation losses, and training/validation macro-F1 scores. Three figures are created in independent try\u2013except blocks: (1) a line chart of training vs. validation loss, (2) a line chart of training vs. validation macro-F1, and (3) a confusion-matrix heat-map built from stored test predictions and ground-truth labels (plotted only if those arrays are non-empty). Each plot is titled clearly, saved with descriptive file names in working_dir, and the figure is always closed afterward. Finally, we compute and print the overall test macro-F1 so the user sees a quantitative summary alongside the visualizations.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate `experiment_data.npy` within the automatically-created\n`working` directory, load it as a Python dict, then iterate over every dataset\nentry (e.g., \u201cSPR_BENCH\u201d).   For each dataset it will:   \u2022 find the highest\nobserved \u201ctrain macro F1 score\u201d and \u201cvalidation macro F1 score\u201d;   \u2022 take the\nlast recorded \u201ctraining loss\u201d and \u201cvalidation loss\u201d as the final losses;   \u2022\nprint these four values with explicit, descriptive labels.   No plotting, file\nwriting, or `if __name__ == \"__main__\":` guard is used, so the code runs\nimmediately when executed.", "The script will load experiment_data.npy, iterate through each stored dataset\n(e.g., \u201cSPR_BENCH\u201d), and print the final training loss, final validation loss,\nfinal training macro-F1, best validation macro-F1, and test macro-F1 (re-\ncomputed from stored predictions) with explicit metric names. Everything\nexecutes at import time\u2014no special entry point is used.", "The script loads the stored numpy dictionary, identifies each dataset contained\nwithin, and then computes or retrieves the final/best values for every recorded\nmetric. It chooses the maximum value for F1 scores (best) and the last logged\nvalue for losses (final). When ground-truth and prediction arrays are present,\nit additionally recomputes the test macro-F1 score. Everything is printed with\nexplicit, descriptive metric labels exactly once per dataset.", "We will load the saved NumPy dictionary, iterate over every dataset entry, and\nfor each one inspect the \u201cmetrics\u201d and \u201closses\u201d sub-dictionaries.   For\nF1-scores we report the best (maximum) value; for losses we report the best\n(minimum) value.   Each line is printed with an explicit, self-descriptive label\nsuch as \u201cbest training macro F1 score\u201d or \u201cbest validation loss\u201d so that the\noutput is unambiguous.   The script contains no `if __name__ == \"__main__\":`\nguard; everything executes immediately when the file is run.", "The script will locate `experiment_data.npy` within the automatically-created\n`working` directory, load it as a Python dict, then iterate over every dataset\nentry (e.g., \u201cSPR_BENCH\u201d).   For each dataset it will:   \u2022 find the highest\nobserved \u201ctrain macro F1 score\u201d and \u201cvalidation macro F1 score\u201d;   \u2022 take the\nlast recorded \u201ctraining loss\u201d and \u201cvalidation loss\u201d as the final losses;   \u2022\nprint these four values with explicit, descriptive labels.   No plotting, file\nwriting, or `if __name__ == \"__main__\":` guard is used, so the code runs\nimmediately when executed.", "The script will locate `experiment_data.npy` within the automatically-created\n`working` directory, load it as a Python dict, then iterate over every dataset\nentry (e.g., \u201cSPR_BENCH\u201d).   For each dataset it will:   \u2022 find the highest\nobserved \u201ctrain macro F1 score\u201d and \u201cvalidation macro F1 score\u201d;   \u2022 take the\nlast recorded \u201ctraining loss\u201d and \u201cvalidation loss\u201d as the final losses;   \u2022\nprint these four values with explicit, descriptive labels.   No plotting, file\nwriting, or `if __name__ == \"__main__\":` guard is used, so the code runs\nimmediately when executed.", "The script will locate `experiment_data.npy` within the automatically-created\n`working` directory, load it as a Python dict, then iterate over every dataset\nentry (e.g., \u201cSPR_BENCH\u201d).   For each dataset it will:   \u2022 find the highest\nobserved \u201ctrain macro F1 score\u201d and \u201cvalidation macro F1 score\u201d;   \u2022 take the\nlast recorded \u201ctraining loss\u201d and \u201cvalidation loss\u201d as the final losses;   \u2022\nprint these four values with explicit, descriptive labels.   No plotting, file\nwriting, or `if __name__ == \"__main__\":` guard is used, so the code runs\nimmediately when executed.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ----------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------------\n# Helper to print a metric nicely\ndef _print_metric(metric_name: str, value: float):\n    print(f\"  {metric_name}: {value:.4f}\")\n\n\n# ----------------------------------------------------\n# Iterate over datasets and report metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract metric and loss lists\n    train_f1_scores = data[\"metrics\"][\"train_macro_f1\"]\n    val_f1_scores = data[\"metrics\"][\"val_macro_f1\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n\n    # Compute required values\n    best_train_f1 = max(train_f1_scores) if train_f1_scores else float(\"nan\")\n    best_val_f1 = max(val_f1_scores) if val_f1_scores else float(\"nan\")\n    final_train_loss = train_losses[-1] if train_losses else float(\"nan\")\n    final_val_loss = val_losses[-1] if val_losses else float(\"nan\")\n\n    # Print metrics with precise names\n    _print_metric(\"Best train macro F1 score\", best_train_f1)\n    _print_metric(\"Best validation macro F1 score\", best_val_f1)\n    _print_metric(\"Final training loss\", final_train_loss)\n    _print_metric(\"Final validation loss\", final_val_loss)\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# -------------------------------------------------------------------------\n# 0. Locate and load the experiment data\n# -------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# -------------------------------------------------------------------------\n# 1. Iterate through datasets and print requested metrics\n# -------------------------------------------------------------------------\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # Dataset header\n\n    # ----- losses ---------------------------------------------------------\n    train_losses = dataset_dict[\"losses\"][\"train\"]\n    val_losses = dataset_dict[\"losses\"][\"val\"]\n\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n\n    if val_losses:\n        final_val_loss = val_losses[-1]\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n    # ----- F1 scores ------------------------------------------------------\n    train_f1_history = dataset_dict[\"metrics\"][\"train_macro_f1\"]\n    val_f1_history = dataset_dict[\"metrics\"][\"val_macro_f1\"]\n\n    if train_f1_history:\n        final_train_f1 = train_f1_history[-1]\n        print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n\n    if val_f1_history:\n        best_val_f1 = max(val_f1_history)\n        print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ----- test metrics ---------------------------------------------------\n    preds = np.asarray(dataset_dict.get(\"predictions\", []))\n    gts = np.asarray(dataset_dict.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n        print(f\"Test macro F1 score: {test_macro_f1:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# --------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# --------- helper to print metric nicely ------------\ndef _print_metric(name: str, value: float):\n    print(f\"{name}: {value:.4f}\")\n\n\n# --------- iterate through each dataset -------------\nfor dset_name, dset_dict in experiment_data.items():\n    print(f\"\\nDataset: {dset_name}\")\n\n    # ---- training / validation metrics ----\n    train_f1_values = dset_dict.get(\"metrics\", {}).get(\"train_macro_f1\", [])\n    val_f1_values = dset_dict.get(\"metrics\", {}).get(\"val_macro_f1\", [])\n    train_loss_values = dset_dict.get(\"losses\", {}).get(\"train\", [])\n    val_loss_values = dset_dict.get(\"losses\", {}).get(\"val\", [])\n\n    if train_f1_values:\n        _print_metric(\"Best training macro F1 score\", max(train_f1_values))\n    if val_f1_values:\n        _print_metric(\"Best validation macro F1 score\", max(val_f1_values))\n    if train_loss_values:\n        _print_metric(\"Final training loss\", train_loss_values[-1])\n    if val_loss_values:\n        _print_metric(\"Final validation loss\", val_loss_values[-1])\n\n    # ---- test metric (re-compute from stored preds) ----\n    preds = dset_dict.get(\"predictions\", [])\n    gts = dset_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        _print_metric(\"Test macro F1 score\", test_f1)\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"Could not find file at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# helper to decide whether to maximise or minimise\n# -------------------------------------------------\ndef best_value(values, higher_is_better=True):\n    if not values:  # empty list safety-check\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# -------------------------------------------------\n# iterate through every stored dataset\n# -------------------------------------------------\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset heading\n\n    # ----- F1 metrics -----\n    metrics = content.get(\"metrics\", {})\n    if \"train_macro_f1\" in metrics:\n        best_train_f1 = best_value(metrics[\"train_macro_f1\"], higher_is_better=True)\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n    if \"val_macro_f1\" in metrics:\n        best_val_f1 = best_value(metrics[\"val_macro_f1\"], higher_is_better=True)\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ----- Loss values -----\n    losses = content.get(\"losses\", {})\n    if \"train\" in losses:\n        best_train_loss = best_value(losses[\"train\"], higher_is_better=False)\n        print(f\"best training loss: {best_train_loss:.4f}\")\n    if \"val\" in losses:\n        best_val_loss = best_value(losses[\"val\"], higher_is_better=False)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------------\n# Helper to print a metric nicely\ndef _print_metric(metric_name: str, value: float):\n    print(f\"  {metric_name}: {value:.4f}\")\n\n\n# ----------------------------------------------------\n# Iterate over datasets and report metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract metric and loss lists\n    train_f1_scores = data[\"metrics\"][\"train_macro_f1\"]\n    val_f1_scores = data[\"metrics\"][\"val_macro_f1\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n\n    # Compute required values\n    best_train_f1 = max(train_f1_scores) if train_f1_scores else float(\"nan\")\n    best_val_f1 = max(val_f1_scores) if val_f1_scores else float(\"nan\")\n    final_train_loss = train_losses[-1] if train_losses else float(\"nan\")\n    final_val_loss = val_losses[-1] if val_losses else float(\"nan\")\n\n    # Print metrics with precise names\n    _print_metric(\"Best train macro F1 score\", best_train_f1)\n    _print_metric(\"Best validation macro F1 score\", best_val_f1)\n    _print_metric(\"Final training loss\", final_train_loss)\n    _print_metric(\"Final validation loss\", final_val_loss)\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------------\n# Helper to print a metric nicely\ndef _print_metric(metric_name: str, value: float):\n    print(f\"  {metric_name}: {value:.4f}\")\n\n\n# ----------------------------------------------------\n# Iterate over datasets and report metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract metric and loss lists\n    train_f1_scores = data[\"metrics\"][\"train_macro_f1\"]\n    val_f1_scores = data[\"metrics\"][\"val_macro_f1\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n\n    # Compute required values\n    best_train_f1 = max(train_f1_scores) if train_f1_scores else float(\"nan\")\n    best_val_f1 = max(val_f1_scores) if val_f1_scores else float(\"nan\")\n    final_train_loss = train_losses[-1] if train_losses else float(\"nan\")\n    final_val_loss = val_losses[-1] if val_losses else float(\"nan\")\n\n    # Print metrics with precise names\n    _print_metric(\"Best train macro F1 score\", best_train_f1)\n    _print_metric(\"Best validation macro F1 score\", best_val_f1)\n    _print_metric(\"Final training loss\", final_train_loss)\n    _print_metric(\"Final validation loss\", final_val_loss)\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------------\n# Helper to print a metric nicely\ndef _print_metric(metric_name: str, value: float):\n    print(f\"  {metric_name}: {value:.4f}\")\n\n\n# ----------------------------------------------------\n# Iterate over datasets and report metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract metric and loss lists\n    train_f1_scores = data[\"metrics\"][\"train_macro_f1\"]\n    val_f1_scores = data[\"metrics\"][\"val_macro_f1\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n\n    # Compute required values\n    best_train_f1 = max(train_f1_scores) if train_f1_scores else float(\"nan\")\n    best_val_f1 = max(val_f1_scores) if val_f1_scores else float(\"nan\")\n    final_train_loss = train_losses[-1] if train_losses else float(\"nan\")\n    final_val_loss = val_losses[-1] if val_losses else float(\"nan\")\n\n    # Print metrics with precise names\n    _print_metric(\"Best train macro F1 score\", best_train_f1)\n    _print_metric(\"Best validation macro F1 score\", best_val_f1)\n    _print_metric(\"Final training loss\", final_train_loss)\n    _print_metric(\"Final validation loss\", final_val_loss)\n", ""], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', '  Best train macro F1 score: 0.6296', '\\n', '\nBest validation macro F1 score: 0.7940', '\\n', '  Final training loss: 0.5280',\n'\\n', '  Final validation loss: 0.5361', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'Final training loss: 1.3860', '\\n', 'Final validation\nloss: 1.3932', '\\n', 'Final training macro F1 score: 0.2417', '\\n', 'Best\nvalidation macro F1 score: 0.2097', '\\n', 'Test macro F1 score: 0.1875', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training macro F1 score: 0.7915', '\\n',\n'Best validation macro F1 score: 0.7918', '\\n', 'Final training loss: 0.5253',\n'\\n', 'Final validation loss: 0.5521', '\\n', 'Test macro F1 score: 0.7920',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'best training macro F1 score: 0.7955', '\\n', 'best\nvalidation macro F1 score: 0.7920', '\\n', 'best training loss: 0.5092', '\\n',\n'best validation loss: 0.5328', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Best train macro F1 score: 0.5496', '\\n', '\nBest validation macro F1 score: 0.7959', '\\n', '  Final training loss: 0.5259',\n'\\n', '  Final validation loss: 0.5324', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Best train macro F1 score: 0.6439', '\\n', '\nBest validation macro F1 score: 0.7880', '\\n', '  Final training loss: 0.5254',\n'\\n', '  Final validation loss: 0.5642', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Best train macro F1 score: 0.6229', '\\n', '\nBest validation macro F1 score: 0.7959', '\\n', '  Final training loss: 0.5172',\n'\\n', '  Final validation loss: 0.5280', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
