
% This paper introduces the Transformer architecture, which is foundational to the use of attention mechanisms and contextual embeddings in deep learning. It is directly relevant as the experiments in this research rely on transformer-based models for Synthetic PolyRule Reasoning tasks. This citation should be included when discussing the model architecture and methodological foundations of the study.
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}

% This paper analyzes the contextualized word representations produced by models such as BERT, ELMo, and GPT-2, highlighting their ability to produce context-specific embeddings. It is relevant to the discussion of the methodological foundation of using contextual embeddings in NLP tasks and their potential adaptation for symbolic reasoning. This citation will be used when summarizing research and discussing the role of embeddings in capturing context within sequences.
@article{ethayarajh2019howca,
 author = {Kawin Ethayarajh},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 pages = {55-65},
 title = {How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
 year = {2019}
}

% Paper 0 discusses the integration of neural networks with symbolic reasoning, which is directly relevant to the study's objective of combining contextual embeddings with symbolic reasoning for Synthetic PolyRule Reasoning tasks. It will be cited when highlighting the research gap and situating the study within the broader field of neuro-symbolic AI. Paper 3 delves into optimization techniques for neuro-symbolic reasoning, offering insights into challenges and methodologies for training models under constraints. It will be cited when discussing the methodological challenges and potential solutions in adapting contextual embeddings for symbolic reasoning tasks.
@article{pulicharla2025neurosymbolicab,
 author = {Mohan Raja Pulicharla},
 booktitle = {World Journal of Advanced Research and Reviews},
 journal = {World Journal of Advanced Research and Reviews},
 title = {Neurosymbolic AI: Bridging neural networks and symbolic reasoning},
 year = {2025}
}

@article{lu2021trainingln,
 author = {Songtao Lu and Naweed Khan and I. Akhalwaya and Ryan Riegel and L. Horesh and Alexander G. Gray},
 booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
 journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 pages = {5559-5563},
 title = {Training Logical Neural Networks by Primalâ€“Dual Methods for Neuro-Symbolic Reasoning},
 year = {2021}
}

% The paper 'Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars' introduces a framework for generating synthetic logical reasoning datasets with flexible context-sensitive rules. It aligns closely with the SPR_BENCH dataset used in this study, which involves synthetic symbolic reasoning tasks. This citation will be used to credit the authors of the dataset and provide readers with a reference to its structure and purpose, particularly in the section discussing datasets and experimental setup.
@article{sileo2024scalingsl,
 author = {Damien Sileo},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 journal = {ArXiv},
 title = {Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars},
 volume = {abs/2406.11035},
 year = {2024}
}

% The paper 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction' discusses reasoning tasks, emphasizing symbolic, scientific, and logic reasoning through contextually-grounded methods. It is relevant to this study as it aligns with the challenges of applying contextual embeddings to symbolic reasoning tasks, supporting the methodological foundation and highlighting gaps in adapting NLP advancements for symbolic reasoning. This citation should be included when discussing related work and challenges in adapting embeddings for reasoning tasks.
@article{li2025codeiocr,
 author = {Junlong Li and Daya Guo and Dejian Yang and Runxin Xu and Yu Wu and Junxian He},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction},
 volume = {abs/2502.07316},
 year = {2025}
}
