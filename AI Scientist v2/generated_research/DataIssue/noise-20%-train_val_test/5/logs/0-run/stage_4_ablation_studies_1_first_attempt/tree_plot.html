<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 8], [0, 1], [0, 5], [0, 3], [0, 4], [0, 2], [0, 7], [0, 6]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: nhead (number_of_attention_heads). The solution sweeps\nover the candidate numbers of attention heads (2, 4, 8, 16), trains a separate\nTransformer for each value, records per-epoch train/validation loss & accuracy,\nstores the final test predictions, and saves everything in the required\nexperiment_data.npy file.", "Ablation name: RemovePositionalEmbeddings. The script below replicates the\nbaseline pipeline but realises the RemovePositionalEmbeddings ablation: the\ntransformer encoder works without any positional\u2010encoding term (word-order\ninformation is suppressed). All other hyper-parameters, data handling, training\nloops, and result logging are kept unchanged. The outputs needed for later\nplotting (metrics, losses, predictions, ground-truth) are stored in the required\nexperiment_data.npy file.", "Ablation name: MultiSyntheticDatasets. We create three synthetic datasets\n(EvenParity, MajoritySymbol, CyclicShift), run the same n-head sweep on each\nwith the existing transformer classifier, collect train/val curves and test\npredictions, and store everything in a single experiment_data dict under the key\n\u201cMultiSyntheticDatasets\u201d. All per-dataset vocabs, loaders and classifiers are\nbuilt on the fly; accuracy is used as evaluation for every task. Finally\neverything is saved to \u201cexperiment_data.npy\u201d.", "Ablation name: NoPaddingMask. We reproduce the original training script but\nmodify the forward pass so that the Transformer encoder no longer receives\n`src_key_padding_mask`; everything else (including pooling that still ignores\npadding for the classifier) is kept intact. Results for every `nhead` value are\nstored under the ablation name `NoPaddingMask` and finally saved to\n`experiment_data.npy`.", "Ablation name: FreezeTokenEmbeddings. We reuse the baseline pipeline but insert\nthe \u201cFreezeTokenEmbeddings\u201d ablation: right after building the model we set\n`requires_grad=False` for `model.embed.weight` and only pass the remaining\ntrainable parameters to the optimizer. Everything else (data preparation,\ntraining loop, evaluation, result storage) stays unchanged, so we can directly\ncompare with the original run. Results for every n-head value are stored under\nthe new key `\"FreezeTokenEmbeddings\"` and finally dumped to the mandatory\n`experiment_data.npy` file.", "Ablation name: UseCLSClassificationToken. We extend the baseline by adding a\nlearnable \u201c[CLS]\u201d token variant and train/evaluate both pooling strategies under\nidentical hyper-parameter sweeps.  A boolean flag chooses between masked-mean\npooling (baseline) and CLS token pooling; results for every n-head setting are\nstored separately in experiment_data and saved to disk for later analysis.", "Ablation name: SingleTransformerLayer. Here is the ablation in which we collapse\nthe encoder depth from two layers to a single Transformer layer while leaving\nevery other hyper-parameter untouched.  The script mirrors the original pipeline\n(data handling, training loop, evaluation, logging) but sets num_layers = 1 and\nstores all outputs under the ablation key \u201cSingleTransformerLayer\u201d.", "Ablation name: RemoveFeedForwardNetwork. We keep the entire training/evaluation\npipeline but introduce a custom TransformerEncoderLayerNoFFN that drops the\nposition-wise FFN and keeps only the self-attention sub-layer with its\nresidual/normalisation.   SimpleTransformerClassifier now takes a flag\nremove_ffn; when true it builds the encoder from this new layer.   We run the\nsame n-head sweep as before, store per-head metrics/losses/predictions for the\n\u201cRemoveFeedForwardNetwork\u201d ablation, and finally save everything to\nexperiment_data.npy.", "Ablation name: NoTransformerEncoder. The script introduces a new Bag-of-\nEmbeddings classifier (NoTransformerEncoder) that skips the TransformerEncoder\nentirely. After computing token + positional embeddings it directly mean-pools\nover non-padding tokens and feeds the result to a linear classifier. The rest of\nthe pipeline\u2014data loading, training, evaluation, and result saving\u2014remains\nidentical to the baseline so that performance can be compared fairly."], "code": ["# n-head hyper-parameter tuning \u2013 single-file script\nimport os, pathlib, random, time, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# -------------------- EXPERIMENT DATA STRUCTURE --------------------- #\nexperiment_data = {\n    \"nhead_tuning\": {  # hyperparameter-tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"results\": {}  # will be filled with one entry per nhead\n        }\n    }\n}\n\n# ---------------------- MISC SETUP ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- DATA UTILS ----------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef build_vocab(dataset: Dataset, seq_field: str = \"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for s in dataset[seq_field]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq, vocab, max_len=None):\n    tokens = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n    if max_len is not None:\n        tokens = tokens[:max_len]\n    return tokens\n\n\n# ----------------------- SYNTHETIC DATA ----------------------------- #\ndef build_synthetic(num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def gen_split(n):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            label = 1 if seq.count(\"A\") % 2 == 0 else 0\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\" \".join(seq))\n            data[\"label\"].append(label)\n        return Dataset.from_dict(data)\n\n    return DatasetDict(\n        train=gen_split(num_train), dev=gen_split(num_dev), test=gen_split(num_test)\n    )\n\n\n# ----------------------------- MODEL -------------------------------- #\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, nhead, num_layers, num_classes, pad_idx):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos_embed = nn.Embedding(512, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            dropout=0.1,\n            activation=\"relu\",\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, mask):\n        pos = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos_embed(pos)\n        h = self.encoder(h, src_key_padding_mask=mask)\n        mask_flt = (~mask).unsqueeze(-1)\n        h_sum = (h * mask_flt).sum(1)\n        lengths = mask_flt.sum(1).clamp(min=1)\n        pooled = h_sum / lengths\n        return self.classifier(pooled)\n\n\n# ---------------------- DATALOADER UTILS ---------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len_batch = max(len(s) for s in seqs)\n    padded = [s + [vocab[\"<pad>\"]] * (max_len_batch - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == vocab[\"<pad>\"]\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss, correct, count = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            count += batch[\"labels\"].size(0)\n    return total_loss / count, correct / count\n\n\n# --------------------------- LOAD DATA ------------------------------ #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Could not load real dataset, using synthetic:\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)}, num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# --------------------- HYPERPARAMETER SWEEP ------------------------- #\nnhead_values = [2, 4, 8, 16]\nepochs = 5\nembed_dim = 128\n\nfor nhead in nhead_values:\n    if embed_dim % nhead != 0:\n        print(f\"Skipping nhead={nhead} because embed_dim {embed_dim} not divisible.\")\n        continue\n    print(f\"\\n=== Training with nhead={nhead} ===\")\n    model = SimpleTransformerClassifier(\n        vocab_size=len(vocab),\n        embed_dim=embed_dim,\n        nhead=nhead,\n        num_layers=2,\n        num_classes=num_classes,\n        pad_idx=vocab[\"<pad>\"],\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss, correct, total = 0.0, 0, 0\n        for batch in train_dl:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n        train_loss = epoch_loss / total\n        train_acc = correct / total\n        val_loss, val_acc = evaluate(model, dev_dl, criterion)\n        print(\n            f\"Epoch {epoch}/{epochs} | nhead={nhead} | \"\n            f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\"\n        )\n\n        metrics[\"train_acc\"].append(train_acc)\n        metrics[\"val_acc\"].append(val_acc)\n        losses[\"train_loss\"].append(train_loss)\n        losses[\"val_loss\"].append(val_loss)\n\n    # ------------------ TEST EVALUATION ----------------------------- #\n    test_loss, test_acc = evaluate(model, test_dl, criterion)\n    print(f\"nhead={nhead} | Test accuracy: {test_acc:.4f}\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_dl:\n            batch_gpu = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n            preds_all.extend(logits.argmax(-1).cpu().tolist())\n            gts_all.extend(batch[\"labels\"].tolist())\n\n    # --------------- SAVE RESULTS TO EXPERIMENT_DATA ---------------- #\n    experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][\"results\"][str(nhead)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"test_acc\": test_acc,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n    }\n\n# ---------------- SAVE ALL EXPERIMENT DATA -------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# RemovePositionalEmbeddings \u2013 single-file ablation study\n# ======================================================\nimport os, pathlib, random, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# ---------------- EXPERIMENT DATA STRUCT ---------------- #\nexperiment_data = {\n    \"RemovePositionalEmbeddings\": {\n        \"SPR_BENCH\": {  # will also be used for synthetic when real data missing\n            \"results\": {}\n        }\n    }\n}\n\n# ---------------------- SETUP --------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------- DATA UTILITIES -------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\ndef build_vocab(dataset: Dataset, seq_field=\"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for s in dataset[seq_field]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq: str, vocab, max_len=None):\n    toks = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n    return toks[:max_len] if max_len else toks\n\n\n# ------------------ SYNTHETIC BACK-UP ------------------- #\ndef build_synthetic(n_train=500, n_dev=100, n_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def gen(n):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\" \".join(seq))\n            data[\"label\"].append(1 if seq.count(\"A\") % 2 == 0 else 0)\n        return Dataset.from_dict(data)\n\n    return DatasetDict(train=gen(n_train), dev=gen(n_dev), test=gen(n_test))\n\n\n# -------------------- MODEL W/O POS ---------------------- #\nclass TransformerNoPos(nn.Module):\n    \"\"\"Transformer encoder classifier without positional embeddings.\"\"\"\n\n    def __init__(self, vocab_sz, embed_dim, nhead, nlayers, n_classes, pad_idx):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, embed_dim, padding_idx=pad_idx)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            dropout=0.1,\n            activation=\"relu\",\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.cls = nn.Linear(embed_dim, n_classes)\n\n    def forward(self, x, pad_mask):\n        h = self.embed(x)  # << NO positional encoding here\n        h = self.encoder(h, src_key_padding_mask=pad_mask)\n        not_pad = (~pad_mask).unsqueeze(-1).float()\n        h_sum = (h * not_pad).sum(1)\n        lengths = not_pad.sum(1).clamp(min=1)\n        pooled = h_sum / lengths\n        return self.cls(pooled)\n\n\n# -------------------- DATALOADER ------------------------ #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len_b = max(len(s) for s in seqs)\n    pad_id = vocab[\"<pad>\"]\n    padded = [s + [pad_id] * (max_len_b - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == pad_id\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    tot_loss, correct, n_samples = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            n_samples += batch[\"labels\"].size(0)\n    return tot_loss / n_samples, correct / n_samples\n\n\n# -------------------- LOAD DATA ------------------------- #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Failed to load real dataset, falling back to synthetic.\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(\"Vocab size:\", len(vocab), \"| num_classes:\", num_classes)\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# ------------------ HYPERPARAM SWEEP -------------------- #\nembed_dim = 128\nepochs = 5\nnhead_values = [2, 4, 8, 16]\n\nfor nhead in nhead_values:\n    if embed_dim % nhead != 0:\n        print(f\"Skip nhead={nhead}, embed_dim not divisible.\")\n        continue\n    print(f\"\\n=== Ablation: training w/o positional encodings | nhead={nhead} ===\")\n    model = TransformerNoPos(\n        vocab_sz=len(vocab),\n        embed_dim=embed_dim,\n        nhead=nhead,\n        nlayers=2,\n        n_classes=num_classes,\n        pad_idx=vocab[\"<pad>\"],\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics, losses = {\"train_acc\": [], \"val_acc\": []}, {\n        \"train_loss\": [],\n        \"val_loss\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        for batch in train_dl:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n        train_loss = running_loss / total\n        train_acc = correct / total\n        val_loss, val_acc = evaluate(model, dev_dl, criterion)\n        print(\n            f\"Epoch {epoch}/{epochs} | train_acc={train_acc:.4f} val_acc={val_acc:.4f}\"\n        )\n        losses[\"train_loss\"].append(train_loss)\n        losses[\"val_loss\"].append(val_loss)\n        metrics[\"train_acc\"].append(train_acc)\n        metrics[\"val_acc\"].append(val_acc)\n\n    # --------------- Test evaluation -------------------- #\n    test_loss, test_acc = evaluate(model, test_dl, criterion)\n    print(f\"nhead={nhead} | Test accuracy: {test_acc:.4f}\")\n\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_dl:\n            batch_gpu = {\n                k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n            }\n            logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n            preds_all.extend(logits.argmax(-1).cpu().tolist())\n            gts_all.extend(batch[\"labels\"].tolist())\n\n    # -------------- log results ------------------------- #\n    experiment_data[\"RemovePositionalEmbeddings\"][\"SPR_BENCH\"][\"results\"][\n        str(nhead)\n    ] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"test_acc\": test_acc,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n    }\n\n# -------------------- SAVE RESULTS ---------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# MultiSyntheticDatasets ablation \u2013 single-file script\nimport os, random, math, pathlib, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict\n\n# ---------------------- GLOBAL CONFIG ------------------------------ #\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"MultiSyntheticDatasets\": {}}  # root container\n\n\n# ------------------------ DATA BUILDERS ---------------------------- #\ndef _symbols(vocab_sz=12):\n    return [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n\ndef build_even_parity(num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12):\n    syms = _symbols(vocab_sz)\n\n    def _split(n):\n        d = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(syms) for _ in range(seqlen)]\n            label = 1 if seq.count(\"A\") % 2 == 0 else 0\n            d[\"id\"].append(str(i))\n            d[\"sequence\"].append(\" \".join(seq))\n            d[\"label\"].append(label)\n        return Dataset.from_dict(d)\n\n    return DatasetDict(\n        train=_split(num_train), dev=_split(num_dev), test=_split(num_test)\n    )\n\n\ndef build_majority_symbol(\n    num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12\n):\n    syms = _symbols(vocab_sz)\n\n    def _split(n):\n        d = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(syms) for _ in range(seqlen)]\n            counts = {s: seq.count(s) for s in syms}\n            max_ct = max(counts.values())\n            majors = [s for s, c in counts.items() if c == max_ct]\n            label_sym = random.choice(majors)\n            label = syms.index(label_sym)\n            d[\"id\"].append(str(i))\n            d[\"sequence\"].append(\" \".join(seq))\n            d[\"label\"].append(label)\n        return Dataset.from_dict(d)\n\n    return DatasetDict(\n        train=_split(num_train), dev=_split(num_dev), test=_split(num_test)\n    )\n\n\ndef is_cyclic_rotation(seq, pattern):\n    \"\"\"seq, pattern are lists of tokens\"\"\"\n    if len(seq) != len(pattern):\n        return False\n    joined = pattern * 2\n    for i in range(len(pattern)):\n        if joined[i : i + len(pattern)] == seq:\n            return True\n    return False\n\n\ndef build_cyclic_shift(\n    num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12\n):\n    syms = _symbols(vocab_sz)\n    base_pattern = syms[:seqlen]  # \"A B C ... \"\n\n    def rotate(lst, k):\n        k %= len(lst)\n        return lst[k:] + lst[:k]\n\n    def _split(n):\n        d = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            label = random.randint(0, 1)\n            if label == 1:\n                shift = random.randint(0, seqlen - 1)\n                seq = rotate(base_pattern, shift)\n            else:\n                while True:\n                    seq = [random.choice(syms) for _ in range(seqlen)]\n                    if not is_cyclic_rotation(seq, base_pattern):\n                        break\n            d[\"id\"].append(str(i))\n            d[\"sequence\"].append(\" \".join(seq))\n            d[\"label\"].append(label)\n        return Dataset.from_dict(d)\n\n    return DatasetDict(\n        train=_split(num_train), dev=_split(num_dev), test=_split(num_test)\n    )\n\n\n# --------------------------- VOCAB --------------------------------- #\ndef build_vocab(dataset: Dataset, field=\"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for sent in dataset[field]:\n        for tok in sent.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq, vocab, max_len=None):\n    toks = [vocab.get(t, vocab[\"<unk>\"]) for t in seq.split()]\n    return toks[:max_len] if max_len else toks\n\n\n# --------------------------- MODEL --------------------------------- #\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, nhead, num_layers, num_classes, pad_idx):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos = nn.Embedding(512, embed_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            batch_first=True,\n            activation=\"relu\",\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.cls = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, mask):\n        pos_ids = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos(pos_ids)\n        h = self.encoder(h, src_key_padding_mask=mask)\n        mask_inv = (~mask).unsqueeze(-1).float()\n        pooled = (h * mask_inv).sum(1) / mask_inv.sum(1).clamp(min=1)\n        return self.cls(pooled)\n\n\n# ---------------------- TRAIN / EVAL UTILS ------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    L = max(len(s) for s in seqs)\n    pad_val = vocab[\"<pad>\"]\n    padded = [s + [pad_val] * (L - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == pad_val\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    tot_loss = 0\n    correct = 0\n    cnt = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            cnt += batch[\"labels\"].size(0)\n    return tot_loss / cnt, correct / cnt\n\n\n# ------------------------- MAIN RUN -------------------------------- #\ntasks = {\n    \"EvenParity\": build_even_parity,\n    \"MajoritySymbol\": build_majority_symbol,\n    \"CyclicShift\": build_cyclic_shift,\n}\nnhead_values = [2, 4, 8, 16]\nepochs = 5\nembed_dim = 128\nbatch_size = 64\n\nfor task_name, builder in tasks.items():\n    print(f\"\\n########## DATASET: {task_name} ##########\")\n    dsets = builder()\n    vocab = build_vocab(dsets[\"train\"])\n    pad_idx = vocab[\"<pad>\"]\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n    print(f\"Vocab={len(vocab)} | classes={num_classes}\")\n    loaders = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        loaders[split] = DataLoader(\n            dsets[split],\n            batch_size=batch_size,\n            shuffle=(split == \"train\"),\n            collate_fn=lambda b, v=vocab: collate_fn(b, v),\n        )\n    experiment_data[\"MultiSyntheticDatasets\"][task_name] = {}\n    for nhead in nhead_values:\n        if embed_dim % nhead != 0:\n            print(f\"Skip nhead={nhead}\")\n            continue\n        print(f\"\\n--- Training nhead={nhead} ---\")\n        model = SimpleTransformerClassifier(\n            len(vocab), embed_dim, nhead, 2, num_classes, pad_idx\n        ).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n        crit = nn.CrossEntropyLoss()\n        log = {\"metrics\": {\"train\": [], \"val\": []}, \"losses\": {\"train\": [], \"val\": []}}\n        for epoch in range(1, epochs + 1):\n            model.train()\n            eloss = 0\n            correct = 0\n            tot = 0\n            for batch in loaders[\"train\"]:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                opt.zero_grad()\n                logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n                loss = crit(logits, batch[\"labels\"])\n                loss.backward()\n                opt.step()\n                eloss += loss.item() * batch[\"labels\"].size(0)\n                correct += (logits.argmax(-1) == batch[\"labels\"]).sum().item()\n                tot += batch[\"labels\"].size(0)\n            tr_acc = correct / tot\n            tr_loss = eloss / tot\n            val_loss, val_acc = evaluate(model, loaders[\"dev\"], crit)\n            log[\"metrics\"][\"train\"].append(tr_acc)\n            log[\"metrics\"][\"val\"].append(val_acc)\n            log[\"losses\"][\"train\"].append(tr_loss)\n            log[\"losses\"][\"val\"].append(val_loss)\n            print(f\"Ep{epoch}/{epochs} acc:{tr_acc:.3f}/{val_acc:.3f}\")\n        test_loss, test_acc = evaluate(model, loaders[\"test\"], crit)\n        preds, gts = [], []\n        model.eval()\n        with torch.no_grad():\n            for batch in loaders[\"test\"]:\n                gbatch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(gbatch[\"input_ids\"], gbatch[\"attention_mask\"])\n                preds.extend(logits.argmax(-1).cpu().tolist())\n                gts.extend(batch[\"labels\"].tolist())\n        log[\"test_acc\"] = test_acc\n        log[\"predictions\"] = preds\n        log[\"ground_truth\"] = gts\n        experiment_data[\"MultiSyntheticDatasets\"][task_name][str(nhead)] = log\n        print(f\"nhead={nhead} test_acc={test_acc:.3f}\")\n\n# ---------------------- SAVE RESULTS ------------------------------- #\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data)\nprint(\"\\nSaved experiment data to\", save_path)\n", "# ------------------  NoPaddingMask ablation \u2013 single-file script ------------------ #\nimport os, pathlib, random, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# -------------------- EXPERIMENT DATA STRUCTURE --------------------- #\nexperiment_data = {\n    \"NoPaddingMask\": {\"SPR_BENCH\": {\"results\": {}}}  # will store one entry per nhead\n}\n\n# ---------------------- MISC SETUP ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- DATA UTILS ----------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\ndef build_vocab(dataset: Dataset, seq_field: str = \"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for s in dataset[seq_field]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq, vocab, max_len=None):\n    tokens = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n    if max_len is not None:\n        tokens = tokens[:max_len]\n    return tokens\n\n\n# ----------------------- SYNTHETIC DATA ----------------------------- #\ndef build_synthetic(num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def gen_split(n):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            label = 1 if seq.count(\"A\") % 2 == 0 else 0\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\" \".join(seq))\n            data[\"label\"].append(label)\n        return Dataset.from_dict(data)\n\n    return DatasetDict(\n        train=gen_split(num_train), dev=gen_split(num_dev), test=gen_split(num_test)\n    )\n\n\n# ----------------------------- MODEL -------------------------------- #\nclass NoPadMaskTransformerClassifier(nn.Module):\n    \"\"\"\n    Identical to baseline SimpleTransformerClassifier except we DO NOT supply\n    src_key_padding_mask to the encoder (ablation).\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_dim, nhead, num_layers, num_classes, pad_idx):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos_embed = nn.Embedding(512, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            dropout=0.1,\n            activation=\"relu\",\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, mask):\n        # x : [B,T] ; mask : [B,T]  (True where PAD)\n        pos = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos_embed(pos)\n        # ---------------- No padding mask passed here ---------------- #\n        h = self.encoder(h)  # <- ablation\n        mask_flt = (~mask).unsqueeze(-1)  # False for PAD\n        h_sum = (h * mask_flt).sum(1)  # masked mean pool\n        lengths = mask_flt.sum(1).clamp(min=1)\n        pooled = h_sum / lengths\n        return self.classifier(pooled)\n\n\n# ---------------------- DATALOADER UTILS ---------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len_batch = max(len(s) for s in seqs)\n    padded = [s + [vocab[\"<pad>\"]] * (max_len_batch - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == vocab[\"<pad>\"]\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss, correct, count = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            count += batch[\"labels\"].size(0)\n    return total_loss / count, correct / count\n\n\n# --------------------------- LOAD DATA ------------------------------ #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Could not load real dataset, using synthetic:\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)}, num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# --------------------- HYPERPARAMETER SWEEP ------------------------- #\nnhead_values = [2, 4, 8, 16]\nepochs = 5\nembed_dim = 128\n\nfor nhead in nhead_values:\n    if embed_dim % nhead != 0:\n        print(f\"Skipping nhead={nhead} because embed_dim {embed_dim} not divisible.\")\n        continue\n    print(f\"\\n=== Training with nhead={nhead} (NoPaddingMask) ===\")\n    model = NoPadMaskTransformerClassifier(\n        vocab_size=len(vocab),\n        embed_dim=embed_dim,\n        nhead=nhead,\n        num_layers=2,\n        num_classes=num_classes,\n        pad_idx=vocab[\"<pad>\"],\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss, correct, total = 0.0, 0, 0\n        for batch in train_dl:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n\n        train_loss = epoch_loss / total\n        train_acc = correct / total\n        val_loss, val_acc = evaluate(model, dev_dl, criterion)\n\n        print(\n            f\"Epoch {epoch}/{epochs} | nhead={nhead} | train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\"\n        )\n\n        metrics[\"train_acc\"].append(train_acc)\n        metrics[\"val_acc\"].append(val_acc)\n        losses[\"train_loss\"].append(train_loss)\n        losses[\"val_loss\"].append(val_loss)\n\n    # ------------------ TEST EVALUATION ----------------------------- #\n    test_loss, test_acc = evaluate(model, test_dl, criterion)\n    print(f\"nhead={nhead} | Test accuracy: {test_acc:.4f}\")\n\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_dl:\n            batch_gpu = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n            preds_all.extend(logits.argmax(-1).cpu().tolist())\n            gts_all.extend(batch[\"labels\"].tolist())\n\n    # --------------- SAVE RESULTS TO EXPERIMENT_DATA ---------------- #\n    experiment_data[\"NoPaddingMask\"][\"SPR_BENCH\"][\"results\"][str(nhead)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"test_acc\": test_acc,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n    }\n\n# ---------------- SAVE ALL EXPERIMENT DATA -------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# -------------------- EXPERIMENT DATA STRUCTURE --------------------- #\nexperiment_data = {\n    \"FreezeTokenEmbeddings\": {  # ablation name\n        \"SPR_BENCH\": {\"results\": {}}  # dataset name  # one entry per nhead\n    }\n}\n\n# ---------------------- MISC SETUP ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- DATA UTILS ----------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef build_vocab(dataset: Dataset, seq_field: str = \"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for s in dataset[seq_field]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq, vocab, max_len=None):\n    tokens = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n    if max_len is not None:\n        tokens = tokens[:max_len]\n    return tokens\n\n\n# ----------------------- SYNTHETIC DATA ----------------------------- #\ndef build_synthetic(num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def gen_split(n):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            label = 1 if seq.count(\"A\") % 2 == 0 else 0\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\" \".join(seq))\n            data[\"label\"].append(label)\n        return Dataset.from_dict(data)\n\n    return DatasetDict(\n        train=gen_split(num_train), dev=gen_split(num_dev), test=gen_split(num_test)\n    )\n\n\n# ----------------------------- MODEL -------------------------------- #\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, nhead, num_layers, num_classes, pad_idx):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos_embed = nn.Embedding(512, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            dropout=0.1,\n            activation=\"relu\",\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, mask):\n        pos = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos_embed(pos)\n        h = self.encoder(h, src_key_padding_mask=mask)\n        mask_flt = (~mask).unsqueeze(-1)\n        h_sum = (h * mask_flt).sum(1)\n        lengths = mask_flt.sum(1).clamp(min=1)\n        pooled = h_sum / lengths\n        return self.classifier(pooled)\n\n\n# ---------------------- DATALOADER UTILS ---------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len_batch = max(len(s) for s in seqs)\n    padded = [s + [vocab[\"<pad>\"]] * (max_len_batch - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == vocab[\"<pad>\"]\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    tot_loss, correct, cnt = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            cnt += batch[\"labels\"].size(0)\n    return tot_loss / cnt, correct / cnt\n\n\n# --------------------------- LOAD DATA ------------------------------ #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Could not load real dataset, using synthetic:\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)}, num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# --------------------- HYPERPARAMETER SWEEP ------------------------- #\nnhead_values = [2, 4, 8, 16]\nepochs = 5\nembed_dim = 128\n\nfor nhead in nhead_values:\n    if embed_dim % nhead != 0:\n        print(f\"Skipping nhead={nhead} because embed_dim {embed_dim} not divisible.\")\n        continue\n    print(f\"\\n=== FreezeTokenEmbeddings | Training with nhead={nhead} ===\")\n    model = SimpleTransformerClassifier(\n        vocab_size=len(vocab),\n        embed_dim=embed_dim,\n        nhead=nhead,\n        num_layers=2,\n        num_classes=num_classes,\n        pad_idx=vocab[\"<pad>\"],\n    ).to(device)\n\n    # -------------------- FREEZE TOKEN EMBEDDINGS ------------------- #\n    for param in model.embed.parameters():\n        param.requires_grad = False\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3\n    )\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        ep_loss, correct, total = 0.0, 0, 0\n        for batch in train_dl:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            ep_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n        train_loss = ep_loss / total\n        train_acc = correct / total\n        val_loss, val_acc = evaluate(model, dev_dl, criterion)\n        print(\n            f\"Epoch {epoch}/{epochs} | nhead={nhead} | train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\"\n        )\n        metrics[\"train_acc\"].append(train_acc)\n        metrics[\"val_acc\"].append(val_acc)\n        losses[\"train_loss\"].append(train_loss)\n        losses[\"val_loss\"].append(val_loss)\n\n    # ------------------ TEST EVALUATION ----------------------------- #\n    test_loss, test_acc = evaluate(model, test_dl, criterion)\n    print(f\"nhead={nhead} | Test accuracy: {test_acc:.4f}\")\n\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_dl:\n            batch_gpu = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n            preds_all.extend(logits.argmax(-1).cpu().tolist())\n            gts_all.extend(batch[\"labels\"].tolist())\n\n    # --------------- SAVE RESULTS TO EXPERIMENT_DATA ---------------- #\n    experiment_data[\"FreezeTokenEmbeddings\"][\"SPR_BENCH\"][\"results\"][str(nhead)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"test_acc\": test_acc,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n    }\n\n# ---------------- SAVE ALL EXPERIMENT DATA -------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# UseCLS token ablation vs. mean-pooling \u2013 single-file script\nimport os, pathlib, random, numpy as np, torch, math, time, json\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# ------------------ EXPERIMENT DATA STRUCTURE ---------------------- #\nexperiment_data = {\n    \"mean_pooling\": {\"SPR_BENCH\": {\"results\": {}}},\n    \"cls_token\": {\"SPR_BENCH\": {\"results\": {}}},\n}\n\n# ----------------------- MISC SETUP -------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------- DATA UTILS ------------------------------ #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef build_vocab(dataset: DatasetDict, seq_field: str = \"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for s in dataset[seq_field]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    vocab[\"<cls>\"] = idx  # reserve slot for CLS token (not used in sequences)\n    return vocab\n\n\ndef encode_sequence(seq: str, vocab, max_len=None):\n    toks = [vocab.get(t, vocab[\"<unk>\"]) for t in seq.strip().split()]\n    return toks[:max_len] if max_len else toks\n\n\n# --------------------- SYNTHETIC PARITY DATA ----------------------- #\ndef build_synthetic(n_train=500, n_dev=100, n_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def _gen(n):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            lbl = 1 if seq.count(\"A\") % 2 == 0 else 0\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\" \".join(seq))\n            data[\"label\"].append(lbl)\n        return Dataset.from_dict(data)\n\n    return DatasetDict(train=_gen(n_train), dev=_gen(n_dev), test=_gen(n_test))\n\n\n# --------------------------- MODEL --------------------------------- #\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim,\n        nhead,\n        num_layers,\n        num_classes,\n        pad_idx,\n        use_cls: bool = False,\n    ):\n        super().__init__()\n        self.use_cls = use_cls\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos_embed = nn.Embedding(512, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            dropout=0.1,\n            activation=\"relu\",\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        if self.use_cls:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            nn.init.trunc_normal_(self.cls_token, std=0.02)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n        bs, seq_len = x.size()\n        pos = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos_embed(pos)\n        if self.use_cls:\n            cls_tok = self.cls_token.expand(bs, -1, -1)  # (B,1,D)\n            h = torch.cat([cls_tok, h], dim=1)\n            zeros = torch.zeros(bs, 1, dtype=torch.bool, device=mask.device)\n            mask = torch.cat([zeros, mask], dim=1)  # cls never masked\n            pos = torch.arange(0, h.size(1), device=x.device).unsqueeze(0)\n            h[:, 1:] += self.pos_embed(pos[:, 1:])  # add pos to non-cls part\n        h = self.encoder(h, src_key_padding_mask=mask)\n        if self.use_cls:\n            pooled = h[:, 0, :]\n        else:\n            mask_flt = (~mask).unsqueeze(-1)  # (B,L,1)  True for valid tokens\n            h_sum = (h * mask_flt).sum(1)\n            lengths = mask_flt.sum(1).clamp(min=1)\n            pooled = h_sum / lengths\n        return self.classifier(pooled)\n\n\n# ----------------------- DATALOADER -------------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len_b = max(len(s) for s in seqs)\n    pad_id = vocab[\"<pad>\"]\n    padded = [s + [pad_id] * (max_len_b - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == pad_id  # True where padding\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, loader, crit):\n    model.eval()\n    loss_tot, correct, count = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = crit(logits, batch[\"labels\"])\n            loss_tot += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            count += batch[\"labels\"].size(0)\n    return loss_tot / count, correct / count\n\n\n# ----------------------- LOAD DATASET ------------------------------ #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Could not load real dataset, using synthetic parity data:\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)}, num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# ------------------- HYPER-PARAMETER SWEEP ------------------------- #\nnhead_values = [2, 4, 8, 16]\nepochs, embed_dim = 5, 128\n\nfor variant in [\"mean_pooling\", \"cls_token\"]:\n    print(f\"\\n##### Variant: {variant} #####\")\n    use_cls_flag = variant == \"cls_token\"\n    for nhead in nhead_values:\n        if embed_dim % nhead:\n            print(f\"Skip nhead={nhead} (embed_dim not divisible).\")\n            continue\n        print(f\"\\n=== Training ({variant}) with nhead={nhead} ===\")\n        model = SimpleTransformerClassifier(\n            vocab_size=len(vocab),\n            embed_dim=embed_dim,\n            nhead=nhead,\n            num_layers=2,\n            num_classes=num_classes,\n            pad_idx=vocab[\"<pad>\"],\n            use_cls=use_cls_flag,\n        ).to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        metrics = {\"train_acc\": [], \"val_acc\": []}\n        losses = {\"train_loss\": [], \"val_loss\": []}\n\n        for ep in range(1, epochs + 1):\n            model.train()\n            ep_loss, correct, total = 0.0, 0, 0\n            for batch in train_dl:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n                loss = criterion(logits, batch[\"labels\"])\n                loss.backward()\n                optimizer.step()\n                ep_loss += loss.item() * batch[\"labels\"].size(0)\n                preds = logits.argmax(-1)\n                correct += (preds == batch[\"labels\"]).sum().item()\n                total += batch[\"labels\"].size(0)\n            train_loss, train_acc = ep_loss / total, correct / total\n            val_loss, val_acc = evaluate(model, dev_dl, criterion)\n            print(\n                f\"Epoch {ep}/{epochs} | nhead={nhead} | train_acc={train_acc:.4f} \"\n                f\"val_acc={val_acc:.4f}\"\n            )\n            metrics[\"train_acc\"].append(train_acc)\n            metrics[\"val_acc\"].append(val_acc)\n            losses[\"train_loss\"].append(train_loss)\n            losses[\"val_loss\"].append(val_loss)\n\n        # -------------------- TEST EVAL ------------------------------ #\n        test_loss, test_acc = evaluate(model, test_dl, criterion)\n        print(f\"Finished nhead={nhead} | Test accuracy: {test_acc:.4f}\")\n\n        # gather predictions\n        preds_all, gts_all = [], []\n        model.eval()\n        with torch.no_grad():\n            for batch in test_dl:\n                gts_all.extend(batch[\"labels\"].tolist())\n                batch_gpu = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n                preds_all.extend(logits.argmax(-1).cpu().tolist())\n\n        # store results\n        experiment_data[variant][\"SPR_BENCH\"][\"results\"][str(nhead)] = {\n            \"metrics\": metrics,\n            \"losses\": losses,\n            \"test_acc\": test_acc,\n            \"predictions\": preds_all,\n            \"ground_truth\": gts_all,\n        }\n\n# -------------------- SAVE EXPERIMENT DATA ------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# ------------------------------------------------------------\n# Single-layer Transformer ablation study \u2013 self-contained file\n# ------------------------------------------------------------\nimport os, pathlib, random, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# ---------------- EXPERIMENT DATA DICT ---------------------- #\nexperiment_data = {\n    \"SingleTransformerLayer\": {\"SPR_BENCH\": {\"results\": {}}}  # one entry per nhead\n}\n\n# -------------------- BASIC SETUP --------------------------- #\nwork_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(work_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------ DATA UTILS ------------------------ #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef build_vocab(dataset: Dataset, seq_field=\"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for seq in dataset[seq_field]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq, vocab, max_len=None):\n    toks = [vocab.get(t, vocab[\"<unk>\"]) for t in seq.strip().split()]\n    return toks[:max_len] if max_len else toks\n\n\n# -------------- SYNTHETIC DATA FALLBACK --------------------- #\ndef build_synthetic(n_train=500, n_dev=100, n_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def _gen(n):\n        d = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            lab = 1 if seq.count(\"A\") % 2 == 0 else 0\n            d[\"id\"].append(str(i))\n            d[\"sequence\"].append(\" \".join(seq))\n            d[\"label\"].append(lab)\n        return Dataset.from_dict(d)\n\n    return DatasetDict(train=_gen(n_train), dev=_gen(n_dev), test=_gen(n_test))\n\n\n# ---------------------- MODEL ------------------------------- #\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, nhead, num_layers, num_classes, pad_idx):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos = nn.Embedding(512, embed_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=nhead,\n            dim_feedforward=embed_dim * 4,\n            dropout=0.1,\n            activation=\"relu\",\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.cls = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, mask):\n        pos_ids = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos(pos_ids)\n        h = self.encoder(h, src_key_padding_mask=mask)\n        mask_inv = (~mask).unsqueeze(-1)\n        pooled = (h * mask_inv).sum(1) / mask_inv.sum(1).clamp(min=1)\n        return self.cls(pooled)\n\n\n# ------------------- BATCH COLLATOR ------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    maxlen = max(len(s) for s in seqs)\n    pad_idx = vocab[\"<pad>\"]\n    padded = [s + [pad_idx] * (maxlen - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == pad_idx\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\n# -------------------- LOAD DATA ----------------------------- #\nDATA_ROOT = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_ROOT)\n    print(\"Loaded SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Could not load SPR_BENCH; using synthetic. Err:\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)}, num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n\n# --------------------- TRAIN / EVAL ------------------------- #\ndef evaluate(model, dl, crit):\n    model.eval()\n    tot_loss = 0\n    correct = 0\n    n = 0\n    with torch.no_grad():\n        for bt in dl:\n            bt = {k: v.to(device) for k, v in bt.items() if isinstance(v, torch.Tensor)}\n            out = model(bt[\"input_ids\"], bt[\"attention_mask\"])\n            loss = crit(out, bt[\"labels\"])\n            preds = out.argmax(-1)\n            tot_loss += loss.item() * bt[\"labels\"].size(0)\n            correct += (preds == bt[\"labels\"]).sum().item()\n            n += bt[\"labels\"].size(0)\n    return tot_loss / n, correct / n\n\n\n# -------------------- ABLATION RUN -------------------------- #\nembed_dim = 128\nnhead_values = [2, 4, 8, 16]\nepochs = 5\n\nfor nhead in nhead_values:\n    if embed_dim % nhead != 0:\n        print(f\"Skipping nhead={nhead}; embed_dim not divisible.\")\n        continue\n    print(f\"\\n=== SingleLayer Ablation | nhead={nhead} ===\")\n    model = SimpleTransformerClassifier(\n        vocab_size=len(vocab),\n        embed_dim=embed_dim,\n        nhead=nhead,\n        num_layers=1,  # <-- key ablation change\n        num_classes=num_classes,\n        pad_idx=vocab[\"<pad>\"],\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train\": [], \"val\": []}\n    losses = {\"train\": [], \"val\": []}\n\n    for ep in range(1, epochs + 1):\n        model.train()\n        ep_loss = 0\n        correct = 0\n        total = 0\n        for bt in train_dl:\n            bt = {k: v.to(device) for k, v in bt.items() if isinstance(v, torch.Tensor)}\n            optim.zero_grad()\n            out = model(bt[\"input_ids\"], bt[\"attention_mask\"])\n            loss = criterion(out, bt[\"labels\"])\n            loss.backward()\n            optim.step()\n            ep_loss += loss.item() * bt[\"labels\"].size(0)\n            preds = out.argmax(-1)\n            correct += (preds == bt[\"labels\"]).sum().item()\n            total += bt[\"labels\"].size(0)\n        train_loss = ep_loss / total\n        train_acc = correct / total\n        val_loss, val_acc = evaluate(model, dev_dl, criterion)\n        metrics[\"train\"].append(train_acc)\n        metrics[\"val\"].append(val_acc)\n        losses[\"train\"].append(train_loss)\n        losses[\"val\"].append(val_loss)\n        print(f\"Epoch {ep}/{epochs} | train_acc={train_acc:.4f} val_acc={val_acc:.4f}\")\n\n    # -------------- TEST & LOGGING -------------------------- #\n    test_loss, test_acc = evaluate(model, test_dl, criterion)\n    print(f\"nhead={nhead} | TEST ACCURACY: {test_acc:.4f}\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for bt in test_dl:\n            bt_gpu = {\n                k: v.to(device) for k, v in bt.items() if isinstance(v, torch.Tensor)\n            }\n            out = model(bt_gpu[\"input_ids\"], bt_gpu[\"attention_mask\"])\n            preds_all.extend(out.argmax(-1).cpu().tolist())\n            gts_all.extend(bt[\"labels\"].tolist())\n\n    experiment_data[\"SingleTransformerLayer\"][\"SPR_BENCH\"][\"results\"][str(nhead)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"test_acc\": test_acc,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n    }\n\n# ------------------ SAVE EVERYTHING ------------------------- #\nsave_path = os.path.join(work_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data)\nprint(\"\\nSaved experiment data to\", save_path)\n", "# Remove-FFN ablation \u2013 single-file runnable script\nimport os, pathlib, random, math, json, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# ----------------- EXPERIMENT DATA STRUCTURE ----------------- #\nexperiment_data = {\n    \"RemoveFeedForwardNetwork\": {\n        \"SPR_BENCH\": {\"results\": {}}  # one entry per nhead value\n    }\n}\n\n# ------------------ MISC / DEVICE --------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------- DATASET HELPERS ------------------------ #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndef build_vocab(dataset: Dataset, seq_field=\"sequence\"):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[seq_field]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\ndef encode_sequence(seq, vocab, max_len=None):\n    toks = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n    return toks[:max_len] if max_len else toks\n\n\ndef build_synthetic(n_train=500, n_dev=100, n_test=200, seqlen=10, vocab_sz=12):\n    syms = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def _gen(n):\n        d = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(syms) for _ in range(seqlen)]\n            d[\"id\"].append(str(i))\n            d[\"sequence\"].append(\" \".join(seq))\n            d[\"label\"].append(1 if seq.count(\"A\") % 2 == 0 else 0)\n        return Dataset.from_dict(d)\n\n    return DatasetDict(train=_gen(n_train), dev=_gen(n_dev), test=_gen(n_test))\n\n\n# ----------------- MODEL COMPONENTS ------------------------ #\nclass TransformerEncoderLayerNoFFN(nn.Module):\n    \"\"\"Encoder layer without the position-wise FFN (identity mapping).\"\"\"\n\n    def __init__(self, d_model, nhead, dropout=0.1, batch_first=True):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(\n            d_model, nhead, dropout=dropout, batch_first=batch_first\n        )\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\n    def forward(self, src, src_key_padding_mask=None):\n        attn_output, _ = self.self_attn(\n            src, src, src, key_padding_mask=src_key_padding_mask\n        )\n        src = self.norm1(src + self.dropout1(attn_output))\n        return src\n\n\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim,\n        nhead,\n        num_layers,\n        num_classes,\n        pad_idx,\n        remove_ffn=False,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos_embed = nn.Embedding(512, embed_dim)\n        if remove_ffn:\n            layer_cls = lambda: TransformerEncoderLayerNoFFN(\n                embed_dim, nhead, dropout=0.1, batch_first=True\n            )\n        else:\n            layer_cls = lambda: nn.TransformerEncoderLayer(\n                embed_dim,\n                nhead,\n                dim_feedforward=embed_dim * 4,\n                dropout=0.1,\n                activation=\"relu\",\n                batch_first=True,\n            )\n        self.encoder = nn.TransformerEncoder(layer_cls(), num_layers=num_layers)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, mask):\n        pos = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        h = self.embed(x) + self.pos_embed(pos)\n        h = self.encoder(h, src_key_padding_mask=mask)\n        mask_inv = (~mask).unsqueeze(-1)  # 1 for real tokens\n        summed = (h * mask_inv).sum(1)\n        lengths = mask_inv.sum(1).clamp(min=1)\n        pooled = summed / lengths\n        return self.classifier(pooled)\n\n\n# ------------------- COLLATE & EVAL ------------------------- #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    maxlen = max(len(s) for s in seqs)\n    pad_id = vocab[\"<pad>\"]\n    padded = [s + [pad_id] * (maxlen - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    mask = x == pad_id\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return {\"input_ids\": x, \"attention_mask\": mask, \"labels\": labels}\n\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    tot_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            pred = logits.argmax(-1)\n            correct += (pred == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return tot_loss / total, correct / total\n\n\n# ------------------- LOAD DATA ------------------------------------ #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    dsets = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH.\")\nexcept Exception as e:\n    print(\"Falling back to synthetic:\", e)\n    dsets = build_synthetic()\n\nvocab = build_vocab(dsets[\"train\"])\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)} | num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    dsets[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    dsets[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    dsets[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# ------------------- TRAINING SWEEP ------------------------------- #\nembed_dim, epochs = 128, 5\nnhead_values = [2, 4, 8, 16]\n\nfor nhead in nhead_values:\n    if embed_dim % nhead != 0:\n        print(f\"Skip nhead={nhead} \u2013 embed_dim not divisible.\")\n        continue\n    print(f\"\\n=== nhead {nhead} (FFN removed) ===\")\n    model = SimpleTransformerClassifier(\n        vocab_size=len(vocab),\n        embed_dim=embed_dim,\n        nhead=nhead,\n        num_layers=2,\n        num_classes=num_classes,\n        pad_idx=vocab[\"<pad>\"],\n        remove_ffn=True,\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics, losses = {\"train_acc\": [], \"val_acc\": []}, {\n        \"train_loss\": [],\n        \"val_loss\": [],\n    }\n\n    for ep in range(1, epochs + 1):\n        model.train()\n        tot_loss, correct, total = 0.0, 0, 0\n        for batch in train_dl:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optim.zero_grad()\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optim.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            pred = logits.argmax(-1)\n            correct += (pred == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n        train_loss, train_acc = tot_loss / total, correct / total\n        val_loss, val_acc = evaluate(model, dev_dl, criterion)\n        print(f\"Epoch {ep}/{epochs} | train_acc={train_acc:.4f} val_acc={val_acc:.4f}\")\n        metrics[\"train_acc\"].append(train_acc)\n        metrics[\"val_acc\"].append(val_acc)\n        losses[\"train_loss\"].append(train_loss)\n        losses[\"val_loss\"].append(val_loss)\n\n    test_loss, test_acc = evaluate(model, test_dl, criterion)\n    print(f\"nhead={nhead} | Test accuracy: {test_acc:.4f}\")\n\n    # Predictions / ground truth for test set\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_dl:\n            batch_gpu = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n            preds_all.extend(logits.argmax(-1).cpu().tolist())\n            gts_all.extend(batch[\"labels\"].tolist())\n\n    experiment_data[\"RemoveFeedForwardNetwork\"][\"SPR_BENCH\"][\"results\"][str(nhead)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"test_acc\": test_acc,\n        \"predictions\": np.array(preds_all),\n        \"ground_truth\": np.array(gts_all),\n    }\n\n# ------------------- SAVE ALL DATA -------------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# NoTransformerEncoder ablation \u2013 single-file script\nimport os, pathlib, random, math, json, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, DatasetDict, load_dataset\n\n# -------------------- EXPERIMENT DATA STRUCTURE -------------------- #\nexperiment_data = {\n    \"NoTransformerEncoder\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"test_acc\": None,\n        }\n    }\n}\n\n# --------------------------- MISC SETUP ---------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------------- DATA UTILS -------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef build_vocab(dataset: Dataset, seq_field: str = \"sequence\"):\n    vocab, idx = {\"<pad>\": 0, \"<unk>\": 1}, 2\n    for s in dataset[seq_field]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\ndef encode_sequence(seq: str, vocab: dict, max_len=None):\n    tok_ids = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n    return tok_ids[:max_len] if max_len else tok_ids\n\n\n# ----------------------- SYNTHETIC DATA ---------------------------- #\ndef build_synthetic(num_train=500, num_dev=100, num_test=200, seqlen=10, vocab_sz=12):\n    symbols = [chr(ord(\"A\") + i) for i in range(vocab_sz)]\n\n    def _gen(n):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        for i in range(n):\n            seq = [random.choice(symbols) for _ in range(seqlen)]\n            label = 1 if seq.count(\"A\") % 2 == 0 else 0\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\" \".join(seq))\n            data[\"label\"].append(label)\n        return Dataset.from_dict(data)\n\n    return DatasetDict(train=_gen(num_train), dev=_gen(num_dev), test=_gen(num_test))\n\n\n# ----------------------------- MODEL ------------------------------- #\nclass BagOfEmbedsClassifier(nn.Module):\n    \"\"\"NoTransformerEncoder ablation model: EMBED -> MEAN POOL -> FC.\"\"\"\n\n    def __init__(self, vocab_size, embed_dim, num_classes, pad_idx):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        self.pos_embed = nn.Embedding(512, embed_dim)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x, pad_mask):\n        # x: [B, T], pad_mask True where PAD\n        pos = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        h = self.token_embed(x) + self.pos_embed(pos)  # [B,T,E]\n        mask_flt = (~pad_mask).unsqueeze(-1).float()  # [B,T,1]\n        h_sum = (h * mask_flt).sum(1)  # [B,E]\n        lengths = mask_flt.sum(1).clamp(min=1)  # [B,1]\n        pooled = h_sum / lengths  # [B,E]\n        return self.classifier(pooled)  # [B,C]\n\n\n# ------------------------- DATALOADER UTILS ------------------------ #\ndef collate_fn(batch, vocab, max_len=128):\n    seqs = [encode_sequence(b[\"sequence\"], vocab, max_len) for b in batch]\n    lbls = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len_b = max(len(s) for s in seqs)\n    padded = [s + [vocab[\"<pad>\"]] * (max_len_b - len(s)) for s in seqs]\n    x = torch.tensor(padded, dtype=torch.long)\n    pad_mask = x == vocab[\"<pad>\"]\n    return {\"input_ids\": x, \"attention_mask\": pad_mask, \"labels\": lbls}\n\n\n@torch.no_grad()\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    tot_loss, correct, count = 0.0, 0, 0\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = logits.argmax(-1)\n        correct += (preds == batch[\"labels\"]).sum().item()\n        count += batch[\"labels\"].size(0)\n    return tot_loss / count, correct / count\n\n\n# --------------------------- LOAD DATA ----------------------------- #\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    datasets_dict = load_spr_bench(DATA_PATH)\n    print(\"Loaded real SPR_BENCH dataset.\")\nexcept Exception as e:\n    print(\"Could not load real dataset, using synthetic:\", e)\n    datasets_dict = build_synthetic()\n\nvocab = build_vocab(datasets_dict[\"train\"])\nnum_classes = len(set(datasets_dict[\"train\"][\"label\"]))\nprint(f\"Vocab size: {len(vocab)}, num_classes: {num_classes}\")\n\nbatch_size = 64\ntrain_dl = DataLoader(\n    datasets_dict[\"train\"],\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ndev_dl = DataLoader(\n    datasets_dict[\"dev\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\ntest_dl = DataLoader(\n    datasets_dict[\"test\"],\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab),\n)\n\n# ----------------------------- TRAIN ------------------------------- #\nembed_dim = 128\nepochs = 5\nmodel = BagOfEmbedsClassifier(\n    vocab_size=len(vocab),\n    embed_dim=embed_dim,\n    num_classes=num_classes,\n    pad_idx=vocab[\"<pad>\"],\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nfor ep in range(1, epochs + 1):\n    model.train()\n    ep_loss, correct, total = 0.0, 0, 0\n    for batch in train_dl:\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        ep_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = logits.argmax(-1)\n        correct += (preds == batch[\"labels\"]).sum().item()\n        total += batch[\"labels\"].size(0)\n    train_loss = ep_loss / total\n    train_acc = correct / total\n    val_loss, val_acc = evaluate(model, dev_dl, criterion)\n    print(f\"Epoch {ep}/{epochs} | train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n    experiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        train_acc\n    )\n    experiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        val_acc\n    )\n    experiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n    experiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        val_loss\n    )\n\n# -------------------------- FINAL TEST ----------------------------- #\ntest_loss, test_acc = evaluate(model, test_dl, criterion)\nexperiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"test_acc\"] = test_acc\nprint(f\"Test accuracy (NoTransformerEncoder): {test_acc:.4f}\")\n\n# store predictions / ground truth\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_dl:\n        batch_gpu = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        logits = model(batch_gpu[\"input_ids\"], batch_gpu[\"attention_mask\"])\n        experiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"predictions\"].extend(\n            logits.argmax(-1).cpu().tolist()\n        )\n        experiment_data[\"NoTransformerEncoder\"][\"SPR_BENCH\"][\"ground_truth\"].extend(\n            batch[\"labels\"].tolist()\n        )\n\n# --------------------------- SAVE DATA ----------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n"], "term_out": ["['Using device: cuda', '\\n', 'Loaded real SPR_BENCH dataset.', '\\n', 'Vocab\nsize: 18, num_classes: 2', '\\n', '\\n=== Training with nhead=2 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/5 | nhead=2 |\ntrain_acc=0.6600, val_acc=0.7240', '\\n', 'Epoch 2/5 | nhead=2 |\ntrain_acc=0.7790, val_acc=0.7880', '\\n', 'Epoch 3/5 | nhead=2 |\ntrain_acc=0.7935, val_acc=0.7020', '\\n', 'Epoch 4/5 | nhead=2 |\ntrain_acc=0.7895, val_acc=0.7900', '\\n', 'Epoch 5/5 | nhead=2 |\ntrain_acc=0.8005, val_acc=0.7820', '\\n', 'nhead=2 | Test accuracy: 0.8030',\n'\\n', '\\n=== Training with nhead=4 ===', '\\n', 'Epoch 1/5 | nhead=4 |\ntrain_acc=0.6105, val_acc=0.7040', '\\n', 'Epoch 2/5 | nhead=4 |\ntrain_acc=0.7555, val_acc=0.7940', '\\n', 'Epoch 3/5 | nhead=4 |\ntrain_acc=0.7860, val_acc=0.7880', '\\n', 'Epoch 4/5 | nhead=4 |\ntrain_acc=0.7965, val_acc=0.7480', '\\n', 'Epoch 5/5 | nhead=4 |\ntrain_acc=0.7930, val_acc=0.7740', '\\n', 'nhead=4 | Test accuracy: 0.7890',\n'\\n', '\\n=== Training with nhead=8 ===', '\\n', 'Epoch 1/5 | nhead=8 |\ntrain_acc=0.6530, val_acc=0.7640', '\\n', 'Epoch 2/5 | nhead=8 |\ntrain_acc=0.7860, val_acc=0.7740', '\\n', 'Epoch 3/5 | nhead=8 |\ntrain_acc=0.7925, val_acc=0.7580', '\\n', 'Epoch 4/5 | nhead=8 |\ntrain_acc=0.7950, val_acc=0.7820', '\\n', 'Epoch 5/5 | nhead=8 |\ntrain_acc=0.8015, val_acc=0.7800', '\\n', 'nhead=8 | Test accuracy: 0.7850',\n'\\n', '\\n=== Training with nhead=16 ===', '\\n', 'Epoch 1/5 | nhead=16 |\ntrain_acc=0.6215, val_acc=0.7360', '\\n', 'Epoch 2/5 | nhead=16 |\ntrain_acc=0.7845, val_acc=0.7620', '\\n', 'Epoch 3/5 | nhead=16 |\ntrain_acc=0.7870, val_acc=0.7740', '\\n', 'Epoch 4/5 | nhead=16 |\ntrain_acc=0.7975, val_acc=0.7840', '\\n', 'Epoch 5/5 | nhead=16 |\ntrain_acc=0.8030, val_acc=0.7800', '\\n', 'nhead=16 | Test accuracy: 0.7810',\n'\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-10/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n70158.22 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 111806.37\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 187320.98\nexamples/s]', '\\n', 'Loaded real SPR_BENCH dataset.', '\\n', 'Vocab size:', ' ',\n'18', ' ', '| num_classes:', ' ', '2', '\\n', '\\n=== Ablation: training w/o\npositional encodings | nhead=2 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/5 | train_acc=0.7070\nval_acc=0.7760', '\\n', 'Epoch 2/5 | train_acc=0.7725 val_acc=0.7720', '\\n',\n'Epoch 3/5 | train_acc=0.7800 val_acc=0.7580', '\\n', 'Epoch 4/5 |\ntrain_acc=0.7770 val_acc=0.7820', '\\n', 'Epoch 5/5 | train_acc=0.7785\nval_acc=0.7620', '\\n', 'nhead=2 | Test accuracy: 0.7520', '\\n', '\\n=== Ablation:\ntraining w/o positional encodings | nhead=4 ===', '\\n', 'Epoch 1/5 |\ntrain_acc=0.6715 val_acc=0.7540', '\\n', 'Epoch 2/5 | train_acc=0.7675\nval_acc=0.7680', '\\n', 'Epoch 3/5 | train_acc=0.7770 val_acc=0.7780', '\\n',\n'Epoch 4/5 | train_acc=0.7850 val_acc=0.7800', '\\n', 'Epoch 5/5 |\ntrain_acc=0.7830 val_acc=0.7780', '\\n', 'nhead=4 | Test accuracy: 0.7860', '\\n',\n'\\n=== Ablation: training w/o positional encodings | nhead=8 ===', '\\n', 'Epoch\n1/5 | train_acc=0.6805 val_acc=0.7560', '\\n', 'Epoch 2/5 | train_acc=0.7730\nval_acc=0.7940', '\\n', 'Epoch 3/5 | train_acc=0.7810 val_acc=0.7880', '\\n',\n'Epoch 4/5 | train_acc=0.7895 val_acc=0.7820', '\\n', 'Epoch 5/5 |\ntrain_acc=0.7875 val_acc=0.7740', '\\n', 'nhead=8 | Test accuracy: 0.7830', '\\n',\n'\\n=== Ablation: training w/o positional encodings | nhead=16 ===', '\\n', 'Epoch\n1/5 | train_acc=0.7140 val_acc=0.7880', '\\n', 'Epoch 2/5 | train_acc=0.7780\nval_acc=0.7900', '\\n', 'Epoch 3/5 | train_acc=0.7860 val_acc=0.7920', '\\n',\n'Epoch 4/5 | train_acc=0.7685 val_acc=0.7900', '\\n', 'Epoch 5/5 |\ntrain_acc=0.7940 val_acc=0.7860', '\\n', 'nhead=16 | Test accuracy: 0.7890',\n'\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-20/working/experiment_data.npy', '\\n', 'Execution time:\n9 seconds seconds (time limit is 30 minutes).']", "['\\n########## DATASET: EvenParity ##########', '\\n', 'Vocab=14 | classes=2',\n'\\n', '\\n--- Training nhead=2 ---', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Ep1/5 acc:0.528/0.580', '\\n',\n'Ep2/5 acc:0.610/0.860', '\\n', 'Ep3/5 acc:0.842/0.820', '\\n', 'Ep4/5\nacc:0.840/0.870', '\\n', 'Ep5/5 acc:0.842/0.850', '\\n', 'nhead=2 test_acc=0.815',\n'\\n', '\\n--- Training nhead=4 ---', '\\n', 'Ep1/5 acc:0.548/0.510', '\\n', 'Ep2/5\nacc:0.720/0.660', '\\n', 'Ep3/5 acc:0.786/0.830', '\\n', 'Ep4/5 acc:0.830/0.850',\n'\\n', 'Ep5/5 acc:0.864/0.840', '\\n', 'nhead=4 test_acc=0.815', '\\n', '\\n---\nTraining nhead=8 ---', '\\n', 'Ep1/5 acc:0.564/0.580', '\\n', 'Ep2/5\nacc:0.672/0.590', '\\n', 'Ep3/5 acc:0.818/0.640', '\\n', 'Ep4/5 acc:0.766/0.860',\n'\\n', 'Ep5/5 acc:0.842/0.830', '\\n', 'nhead=8 test_acc=0.810', '\\n', '\\n---\nTraining nhead=16 ---', '\\n', 'Ep1/5 acc:0.584/0.490', '\\n', 'Ep2/5\nacc:0.720/0.670', '\\n', 'Ep3/5 acc:0.776/0.850', '\\n', 'Ep4/5 acc:0.846/0.830',\n'\\n', 'Ep5/5 acc:0.842/0.860', '\\n', 'nhead=16 test_acc=0.835', '\\n',\n'\\n########## DATASET: MajoritySymbol ##########', '\\n', 'Vocab=14 |\nclasses=12', '\\n', '\\n--- Training nhead=2 ---', '\\n', 'Ep1/5 acc:0.232/0.380',\n'\\n', 'Ep2/5 acc:0.530/0.540', '\\n', 'Ep3/5 acc:0.638/0.510', '\\n', 'Ep4/5\nacc:0.728/0.600', '\\n', 'Ep5/5 acc:0.778/0.600', '\\n', 'nhead=2 test_acc=0.650',\n'\\n', '\\n--- Training nhead=4 ---', '\\n', 'Ep1/5 acc:0.264/0.360', '\\n', 'Ep2/5\nacc:0.560/0.540', '\\n', 'Ep3/5 acc:0.676/0.530', '\\n', 'Ep4/5 acc:0.724/0.570',\n'\\n', 'Ep5/5 acc:0.760/0.590', '\\n', 'nhead=4 test_acc=0.610', '\\n', '\\n---\nTraining nhead=8 ---', '\\n', 'Ep1/5 acc:0.292/0.480', '\\n', 'Ep2/5\nacc:0.594/0.580', '\\n', 'Ep3/5 acc:0.662/0.570', '\\n', 'Ep4/5 acc:0.738/0.590',\n'\\n', 'Ep5/5 acc:0.784/0.650', '\\n', 'nhead=8 test_acc=0.660', '\\n', '\\n---\nTraining nhead=16 ---', '\\n', 'Ep1/5 acc:0.206/0.470', '\\n', 'Ep2/5\nacc:0.580/0.530', '\\n', 'Ep3/5 acc:0.662/0.650', '\\n', 'Ep4/5 acc:0.738/0.630',\n'\\n', 'Ep5/5 acc:0.804/0.610', '\\n', 'nhead=16 test_acc=0.620', '\\n',\n'\\n########## DATASET: CyclicShift ##########', '\\n', 'Vocab=14 | classes=2',\n'\\n', '\\n--- Training nhead=2 ---', '\\n', 'Ep1/5 acc:0.578/0.540', '\\n', 'Ep2/5\nacc:0.834/0.920', '\\n', 'Ep3/5 acc:0.942/0.920', '\\n', 'Ep4/5 acc:0.966/0.940',\n'\\n', 'Ep5/5 acc:0.986/0.940', '\\n', 'nhead=2 test_acc=0.950', '\\n', '\\n---\nTraining nhead=4 ---', '\\n', 'Ep1/5 acc:0.706/0.910', '\\n', 'Ep2/5\nacc:0.926/0.910', '\\n', 'Ep3/5 acc:0.960/0.940', '\\n', 'Ep4/5 acc:0.976/0.950',\n'\\n', 'Ep5/5 acc:0.994/0.950', '\\n', 'nhead=4 test_acc=0.950', '\\n', '\\n---\nTraining nhead=8 ---', '\\n', 'Ep1/5 acc:0.550/0.530', '\\n', 'Ep2/5\nacc:0.770/0.920', '\\n', 'Ep3/5 acc:0.944/0.930', '\\n', 'Ep4/5 acc:0.962/0.930',\n'\\n', 'Ep5/5 acc:0.982/0.970', '\\n', 'nhead=8 test_acc=0.945', '\\n', '\\n---\nTraining nhead=16 ---', '\\n', 'Ep1/5 acc:0.532/0.530', '\\n', 'Ep2/5\nacc:0.744/0.910', '\\n', 'Ep3/5 acc:0.944/0.910', '\\n', 'Ep4/5 acc:0.960/0.950',\n'\\n', 'Ep5/5 acc:0.990/0.970', '\\n', 'nhead=16 test_acc=0.985', '\\n', '\\nSaved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_00-45-19_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n21/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 44188.94\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 122784.07\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 196041.32\nexamples/s]', '\\n', 'Loaded real SPR_BENCH dataset.', '\\n', 'Vocab size: 18,\nnum_classes: 2', '\\n', '\\n=== Training with nhead=2 (NoPaddingMask) ===', '\\n',\n'Epoch 1/5 | nhead=2 | train_acc=0.6630, val_acc=0.7200', '\\n', 'Epoch 2/5 |\nnhead=2 | train_acc=0.7765, val_acc=0.7880', '\\n', 'Epoch 3/5 | nhead=2 |\ntrain_acc=0.7925, val_acc=0.7200', '\\n', 'Epoch 4/5 | nhead=2 |\ntrain_acc=0.7895, val_acc=0.7820', '\\n', 'Epoch 5/5 | nhead=2 |\ntrain_acc=0.7955, val_acc=0.7780', '\\n', 'nhead=2 | Test accuracy: 0.8040',\n'\\n', '\\n=== Training with nhead=4 (NoPaddingMask) ===', '\\n', 'Epoch 1/5 |\nnhead=4 | train_acc=0.6050, val_acc=0.7060', '\\n', 'Epoch 2/5 | nhead=4 |\ntrain_acc=0.7545, val_acc=0.7920', '\\n', 'Epoch 3/5 | nhead=4 |\ntrain_acc=0.7885, val_acc=0.7800', '\\n', 'Epoch 4/5 | nhead=4 |\ntrain_acc=0.7950, val_acc=0.7520', '\\n', 'Epoch 5/5 | nhead=4 |\ntrain_acc=0.7995, val_acc=0.7720', '\\n', 'nhead=4 | Test accuracy: 0.7940',\n'\\n', '\\n=== Training with nhead=8 (NoPaddingMask) ===', '\\n', 'Epoch 1/5 |\nnhead=8 | train_acc=0.6505, val_acc=0.7620', '\\n', 'Epoch 2/5 | nhead=8 |\ntrain_acc=0.7840, val_acc=0.7700', '\\n', 'Epoch 3/5 | nhead=8 |\ntrain_acc=0.7940, val_acc=0.7580', '\\n', 'Epoch 4/5 | nhead=8 |\ntrain_acc=0.7950, val_acc=0.7800', '\\n', 'Epoch 5/5 | nhead=8 |\ntrain_acc=0.8035, val_acc=0.7860', '\\n', 'nhead=8 | Test accuracy: 0.7870',\n'\\n', '\\n=== Training with nhead=16 (NoPaddingMask) ===', '\\n', 'Epoch 1/5 |\nnhead=16 | train_acc=0.6200, val_acc=0.7360', '\\n', 'Epoch 2/5 | nhead=16 |\ntrain_acc=0.7860, val_acc=0.7600', '\\n', 'Epoch 3/5 | nhead=16 |\ntrain_acc=0.7880, val_acc=0.7720', '\\n', 'Epoch 4/5 | nhead=16 |\ntrain_acc=0.7975, val_acc=0.7820', '\\n', 'Epoch 5/5 | nhead=16 |\ntrain_acc=0.7995, val_acc=0.7840', '\\n', 'nhead=16 | Test accuracy: 0.7820',\n'\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-22/working/experiment_data.npy', '\\n', 'Execution time:\n8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 98637.29\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 115941.62\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 196399.33\nexamples/s]', '\\n', 'Loaded real SPR_BENCH dataset.', '\\n', 'Vocab size: 18,\nnum_classes: 2', '\\n', '\\n=== FreezeTokenEmbeddings | Training with nhead=2\n===', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/5 | nhead=2 |\ntrain_acc=0.6645, val_acc=0.7260', '\\n', 'Epoch 2/5 | nhead=2 |\ntrain_acc=0.7815, val_acc=0.7900', '\\n', 'Epoch 3/5 | nhead=2 |\ntrain_acc=0.7930, val_acc=0.7280', '\\n', 'Epoch 4/5 | nhead=2 |\ntrain_acc=0.7905, val_acc=0.7900', '\\n', 'Epoch 5/5 | nhead=2 |\ntrain_acc=0.7960, val_acc=0.7840', '\\n', 'nhead=2 | Test accuracy: 0.8000',\n'\\n', '\\n=== FreezeTokenEmbeddings | Training with nhead=4 ===', '\\n', 'Epoch\n1/5 | nhead=4 | train_acc=0.6065, val_acc=0.6980', '\\n', 'Epoch 2/5 | nhead=4 |\ntrain_acc=0.7540, val_acc=0.7940', '\\n', 'Epoch 3/5 | nhead=4 |\ntrain_acc=0.7875, val_acc=0.7840', '\\n', 'Epoch 4/5 | nhead=4 |\ntrain_acc=0.7980, val_acc=0.7480', '\\n', 'Epoch 5/5 | nhead=4 |\ntrain_acc=0.7975, val_acc=0.7740', '\\n', 'nhead=4 | Test accuracy: 0.7880',\n'\\n', '\\n=== FreezeTokenEmbeddings | Training with nhead=8 ===', '\\n', 'Epoch\n1/5 | nhead=8 | train_acc=0.6560, val_acc=0.7640', '\\n', 'Epoch 2/5 | nhead=8 |\ntrain_acc=0.7860, val_acc=0.7700', '\\n', 'Epoch 3/5 | nhead=8 |\ntrain_acc=0.7935, val_acc=0.7620', '\\n', 'Epoch 4/5 | nhead=8 |\ntrain_acc=0.7940, val_acc=0.7780', '\\n', 'Epoch 5/5 | nhead=8 |\ntrain_acc=0.8010, val_acc=0.7800', '\\n', 'nhead=8 | Test accuracy: 0.7850',\n'\\n', '\\n=== FreezeTokenEmbeddings | Training with nhead=16 ===', '\\n', 'Epoch\n1/5 | nhead=16 | train_acc=0.6200, val_acc=0.7380', '\\n', 'Epoch 2/5 | nhead=16\n| train_acc=0.7835, val_acc=0.7600', '\\n', 'Epoch 3/5 | nhead=16 |\ntrain_acc=0.7880, val_acc=0.7720', '\\n', 'Epoch 4/5 | nhead=16 |\ntrain_acc=0.7970, val_acc=0.7840', '\\n', 'Epoch 5/5 | nhead=16 |\ntrain_acc=0.8025, val_acc=0.7760', '\\n', 'nhead=16 | Test accuracy: 0.7810',\n'\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-23/working/experiment_data.npy', '\\n', 'Execution time:\n8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded real SPR_BENCH dataset.', '\\n', 'Vocab\nsize: 19, num_classes: 2', '\\n', '\\n##### Variant: mean_pooling #####', '\\n',\n'\\n=== Training (mean_pooling) with nhead=2 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/5 | nhead=2 |\ntrain_acc=0.6380 val_acc=0.7600', '\\n', 'Epoch 2/5 | nhead=2 | train_acc=0.7825\nval_acc=0.7820', '\\n', 'Epoch 3/5 | nhead=2 | train_acc=0.7880 val_acc=0.7740',\n'\\n', 'Epoch 4/5 | nhead=2 | train_acc=0.7945 val_acc=0.7720', '\\n', 'Epoch 5/5\n| nhead=2 | train_acc=0.7955 val_acc=0.7720', '\\n', 'Finished nhead=2 | Test\naccuracy: 0.7850', '\\n', '\\n=== Training (mean_pooling) with nhead=4 ===', '\\n',\n'Epoch 1/5 | nhead=4 | train_acc=0.6045 val_acc=0.7840', '\\n', 'Epoch 2/5 |\nnhead=4 | train_acc=0.7765 val_acc=0.7780', '\\n', 'Epoch 3/5 | nhead=4 |\ntrain_acc=0.7840 val_acc=0.7820', '\\n', 'Epoch 4/5 | nhead=4 | train_acc=0.7900\nval_acc=0.7460', '\\n', 'Epoch 5/5 | nhead=4 | train_acc=0.7920 val_acc=0.7760',\n'\\n', 'Finished nhead=4 | Test accuracy: 0.7860', '\\n', '\\n=== Training\n(mean_pooling) with nhead=8 ===', '\\n', 'Epoch 1/5 | nhead=8 | train_acc=0.6730\nval_acc=0.7760', '\\n', 'Epoch 2/5 | nhead=8 | train_acc=0.7870 val_acc=0.7620',\n'\\n', 'Epoch 3/5 | nhead=8 | train_acc=0.7895 val_acc=0.7780', '\\n', 'Epoch 4/5\n| nhead=8 | train_acc=0.7945 val_acc=0.7840', '\\n', 'Epoch 5/5 | nhead=8 |\ntrain_acc=0.8045 val_acc=0.7720', '\\n', 'Finished nhead=8 | Test accuracy:\n0.7860', '\\n', '\\n=== Training (mean_pooling) with nhead=16 ===', '\\n', 'Epoch\n1/5 | nhead=16 | train_acc=0.6875 val_acc=0.7640', '\\n', 'Epoch 2/5 | nhead=16 |\ntrain_acc=0.7815 val_acc=0.7720', '\\n', 'Epoch 3/5 | nhead=16 | train_acc=0.7925\nval_acc=0.7800', '\\n', 'Epoch 4/5 | nhead=16 | train_acc=0.7950 val_acc=0.7740',\n'\\n', 'Epoch 5/5 | nhead=16 | train_acc=0.7990 val_acc=0.7740', '\\n', 'Finished\nnhead=16 | Test accuracy: 0.7850', '\\n', '\\n##### Variant: cls_token #####',\n'\\n', '\\n=== Training (cls_token) with nhead=2 ===', '\\n', 'Epoch 1/5 | nhead=2\n| train_acc=0.6435 val_acc=0.7700', '\\n', 'Epoch 2/5 | nhead=2 |\ntrain_acc=0.7700 val_acc=0.8000', '\\n', 'Epoch 3/5 | nhead=2 | train_acc=0.7830\nval_acc=0.7940', '\\n', 'Epoch 4/5 | nhead=2 | train_acc=0.7985 val_acc=0.7680',\n'\\n', 'Epoch 5/5 | nhead=2 | train_acc=0.7935 val_acc=0.7720', '\\n', 'Finished\nnhead=2 | Test accuracy: 0.7920', '\\n', '\\n=== Training (cls_token) with nhead=4\n===', '\\n', 'Epoch 1/5 | nhead=4 | train_acc=0.5780 val_acc=0.6000', '\\n',\n'Epoch 2/5 | nhead=4 | train_acc=0.7585 val_acc=0.7800', '\\n', 'Epoch 3/5 |\nnhead=4 | train_acc=0.7800 val_acc=0.7820', '\\n', 'Epoch 4/5 | nhead=4 |\ntrain_acc=0.7855 val_acc=0.7760', '\\n', 'Epoch 5/5 | nhead=4 | train_acc=0.7945\nval_acc=0.7800', '\\n', 'Finished nhead=4 | Test accuracy: 0.7970', '\\n', '\\n===\nTraining (cls_token) with nhead=8 ===', '\\n', 'Epoch 1/5 | nhead=8 |\ntrain_acc=0.6655 val_acc=0.7780', '\\n', 'Epoch 2/5 | nhead=8 | train_acc=0.7775\nval_acc=0.7780', '\\n', 'Epoch 3/5 | nhead=8 | train_acc=0.7855 val_acc=0.7780',\n'\\n', 'Epoch 4/5 | nhead=8 | train_acc=0.7995 val_acc=0.7740', '\\n', 'Epoch 5/5\n| nhead=8 | train_acc=0.7965 val_acc=0.7620', '\\n', 'Finished nhead=8 | Test\naccuracy: 0.7730', '\\n', '\\n=== Training (cls_token) with nhead=16 ===', '\\n',\n'Epoch 1/5 | nhead=16 | train_acc=0.6765 val_acc=0.7560', '\\n', 'Epoch 2/5 |\nnhead=16 | train_acc=0.7730 val_acc=0.7000', '\\n', 'Epoch 3/5 | nhead=16 |\ntrain_acc=0.7755 val_acc=0.7500', '\\n', 'Epoch 4/5 | nhead=16 | train_acc=0.7885\nval_acc=0.7800', '\\n', 'Epoch 5/5 | nhead=16 | train_acc=0.7985 val_acc=0.7780',\n'\\n', 'Finished nhead=16 | Test accuracy: 0.7710', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-45-\n19_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n20/working/experiment_data.npy', '\\n', 'Execution time: 14 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH dataset.', '\\n', 'Vocab size: 18,\nnum_classes: 2', '\\n', '\\n=== SingleLayer Ablation | nhead=2 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/5 | train_acc=0.6880\nval_acc=0.7200', '\\n', 'Epoch 2/5 | train_acc=0.7740 val_acc=0.7760', '\\n',\n'Epoch 3/5 | train_acc=0.7905 val_acc=0.7500', '\\n', 'Epoch 4/5 |\ntrain_acc=0.7895 val_acc=0.7840', '\\n', 'Epoch 5/5 | train_acc=0.7970\nval_acc=0.7780', '\\n', 'nhead=2 | TEST ACCURACY: 0.7910', '\\n', '\\n===\nSingleLayer Ablation | nhead=4 ===', '\\n', 'Epoch 1/5 | train_acc=0.6570\nval_acc=0.7520', '\\n', 'Epoch 2/5 | train_acc=0.7635 val_acc=0.7740', '\\n',\n'Epoch 3/5 | train_acc=0.7870 val_acc=0.7680', '\\n', 'Epoch 4/5 |\ntrain_acc=0.7925 val_acc=0.7720', '\\n', 'Epoch 5/5 | train_acc=0.7835\nval_acc=0.7680', '\\n', 'nhead=4 | TEST ACCURACY: 0.7920', '\\n', '\\n===\nSingleLayer Ablation | nhead=8 ===', '\\n', 'Epoch 1/5 | train_acc=0.6885\nval_acc=0.7600', '\\n', 'Epoch 2/5 | train_acc=0.7890 val_acc=0.7620', '\\n',\n'Epoch 3/5 | train_acc=0.7915 val_acc=0.7560', '\\n', 'Epoch 4/5 |\ntrain_acc=0.7905 val_acc=0.7840', '\\n', 'Epoch 5/5 | train_acc=0.8025\nval_acc=0.7840', '\\n', 'nhead=8 | TEST ACCURACY: 0.7870', '\\n', '\\n===\nSingleLayer Ablation | nhead=16 ===', '\\n', 'Epoch 1/5 | train_acc=0.6545\nval_acc=0.7240', '\\n', 'Epoch 2/5 | train_acc=0.7820 val_acc=0.7580', '\\n',\n'Epoch 3/5 | train_acc=0.7815 val_acc=0.7680', '\\n', 'Epoch 4/5 |\ntrain_acc=0.7915 val_acc=0.7820', '\\n', 'Epoch 5/5 | train_acc=0.8000\nval_acc=0.7740', '\\n', 'nhead=16 | TEST ACCURACY: 0.7820', '\\n', '\\nSaved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_00-45-19_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n22/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n51167.83 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 94292.16\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 157054.74\nexamples/s]', '\\n', 'Loaded real SPR_BENCH.', '\\n', 'Vocab size: 18 |\nnum_classes: 2', '\\n', '\\n=== nhead 2 (FFN removed) ===', '\\n', 'Traceback (most\nrecent call last):\\n  File \"runfile.py\", line 223, in <module>\\n    logits =\nmodel(batch[\"input_ids\"], batch[\"attention_mask\"])\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 122, in forward\\n    h = self.encoder(h,\nsrc_key_padding_mask=mask)\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py\", line 511, in forward\\n    output =\nmod(\\n             ^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: TransformerEncoderLayerNoFFN.forward()\ngot an unexpected keyword argument \\'src_mask\\'\\n', 'Execution time: 2 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded real SPR_BENCH dataset.', '\\n', 'Vocab\nsize: 18, num_classes: 2', '\\n', 'Epoch 1/5 | train_acc=0.6115, val_acc=0.7160',\n'\\n', 'Epoch 2/5 | train_acc=0.7525, val_acc=0.7300', '\\n', 'Epoch 3/5 |\ntrain_acc=0.7595, val_acc=0.7320', '\\n', 'Epoch 4/5 | train_acc=0.7650,\nval_acc=0.7420', '\\n', 'Epoch 5/5 | train_acc=0.7665, val_acc=0.7460', '\\n',\n'Test accuracy (NoTransformerEncoder): 0.7640', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-45-\n19_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n23/working/experiment_data.npy', '\\n', 'Execution time: 2 seconds seconds (time\nlimit is 30 minutes).']"], "analysis": ["", "", "", "The execution of the training script completed successfully without any errors\nor bugs. The model was trained and evaluated on the SPR_BENCH dataset with\ndifferent values of 'nhead', and the results were saved successfully. Test\naccuracies were reported for all configurations, demonstrating that the ablation\nstudy was conducted as planned.", "", "", "", "The execution failed due to a TypeError in the TransformerEncoderLayerNoFFN\nclass. The error occurs because the forward method does not accept the\n'src_mask' keyword argument, which is passed by the nn.TransformerEncoder during\nits forward call. To fix this, update the forward method of\nTransformerEncoderLayerNoFFN to include an additional parameter 'src_mask=None'\nand ensure it is compatible with the nn.TransformerEncoder's expectations.", "The execution of the training script completed successfully without any bugs.\nThe model was trained on the real SPR_BENCH dataset, achieving a final test\naccuracy of 76.4% for the NoTransformerEncoder ablation. All results were saved\ncorrectly, and the script ran efficiently within the time limit."], "exc_type": [null, null, null, null, null, null, null, "TypeError", null], "exc_info": [null, null, null, null, null, null, null, {"args": ["TransformerEncoderLayerNoFFN.forward() got an unexpected keyword argument 'src_mask'"]}, null], "exc_stack": [null, null, null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 223, "<module>", "logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 122, "forward", "h = self.encoder(h, src_key_padding_mask=mask)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/transformer.py", 511, "forward", "output = mod("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"]], null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.803, "best_value": 0.803}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4702, "best_value": 0.4702}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5292, "best_value": 0.5292}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.803, "best_value": 0.803}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.786, "best_value": 0.786}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5159, "best_value": 0.5159}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5181, "best_value": 0.5181}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.789, "best_value": 0.789}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training data.", "data": [{"dataset_name": "EvenParity", "final_value": 0.842, "best_value": 0.864}, {"dataset_name": "MajoritySymbol", "final_value": 0.804, "best_value": 0.804}, {"dataset_name": "CyclicShift", "final_value": 0.99, "best_value": 0.994}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation data.", "data": [{"dataset_name": "EvenParity", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "MajoritySymbol", "final_value": 0.61, "best_value": 0.65}, {"dataset_name": "CyclicShift", "final_value": 0.97, "best_value": 0.97}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training data.", "data": [{"dataset_name": "EvenParity", "final_value": 0.3278, "best_value": 0.2747}, {"dataset_name": "MajoritySymbol", "final_value": 0.757, "best_value": 0.757}, {"dataset_name": "CyclicShift", "final_value": 0.0311, "best_value": 0.0215}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation data.", "data": [{"dataset_name": "EvenParity", "final_value": 0.3273, "best_value": 0.2737}, {"dataset_name": "MajoritySymbol", "final_value": 0.9857, "best_value": 0.9079}, {"dataset_name": "CyclicShift", "final_value": 0.0808, "best_value": 0.0808}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test data.", "data": [{"dataset_name": "EvenParity", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "MajoritySymbol", "final_value": 0.62, "best_value": 0.66}, {"dataset_name": "CyclicShift", "final_value": 0.985, "best_value": 0.985}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7995, "best_value": 0.8035}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.792}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4793, "best_value": 0.4691}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5285, "best_value": 0.5285}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.804, "best_value": 0.804}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8025, "best_value": 0.8025}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.784}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4699, "best_value": 0.4699}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5389, "best_value": 0.5389}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8, "best_value": 0.8}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions made by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.797, "best_value": 0.8045}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error in the model's predictions. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5224, "best_value": 0.4619}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH | nhead=2", "final_value": 0.797, "best_value": 0.797}, {"dataset_name": "SPR_BENCH | nhead=4", "final_value": 0.7835, "best_value": 0.7835}, {"dataset_name": "SPR_BENCH | nhead=8", "final_value": 0.8025, "best_value": 0.8025}, {"dataset_name": "SPR_BENCH | nhead=16", "final_value": 0.8, "best_value": 0.8}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH | nhead=2", "final_value": 0.778, "best_value": 0.778}, {"dataset_name": "SPR_BENCH | nhead=4", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "SPR_BENCH | nhead=8", "final_value": 0.784, "best_value": 0.784}, {"dataset_name": "SPR_BENCH | nhead=16", "final_value": 0.774, "best_value": 0.774}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH | nhead=2", "final_value": 0.4704, "best_value": 0.4704}, {"dataset_name": "SPR_BENCH | nhead=4", "final_value": 0.4915, "best_value": 0.4915}, {"dataset_name": "SPR_BENCH | nhead=8", "final_value": 0.4653, "best_value": 0.4653}, {"dataset_name": "SPR_BENCH | nhead=16", "final_value": 0.4811, "best_value": 0.4811}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH | nhead=2", "final_value": 0.5726, "best_value": 0.5726}, {"dataset_name": "SPR_BENCH | nhead=4", "final_value": 0.5525, "best_value": 0.5525}, {"dataset_name": "SPR_BENCH | nhead=8", "final_value": 0.5608, "best_value": 0.5608}, {"dataset_name": "SPR_BENCH | nhead=16", "final_value": 0.5582, "best_value": 0.5582}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH | nhead=2", "final_value": 0.791, "best_value": 0.791}, {"dataset_name": "SPR_BENCH | nhead=4", "final_value": 0.792, "best_value": 0.792}, {"dataset_name": "SPR_BENCH | nhead=8", "final_value": 0.787, "best_value": 0.787}, {"dataset_name": "SPR_BENCH | nhead=16", "final_value": 0.782, "best_value": 0.782}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7665, "best_value": 0.7665}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.746, "best_value": 0.746}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Represents the loss value of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.557622, "best_value": 0.557622}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Represents the loss value of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.569559, "best_value": 0.569559}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.764, "best_value": 0.764}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_confusion_matrix_best.png"], ["../../logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_accuracy_vs_heads.png", "../../logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead2.png", "../../logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead4.png", "../../logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead8.png", "../../logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead16.png"], ["../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_test_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead2_train_val_acc.png", "../../logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead4_train_val_acc.png", "../../logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead8_train_val_acc.png", "../../logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead16_train_val_acc.png", "../../logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_test_acc_barplot.png"], ["../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_test_acc_vs_nhead.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_acc_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_acc_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_acc_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead16_acc_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead16_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead16_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_mean_pooling_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_cls_token_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_test_accuracy_vs_nhead.png"], ["../../logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_test_accuracy_bar.png"], [], ["../../logs/0-run/experiment_results/experiment_e46f3c6a839048e584bf3541709fdb79_proc_3173699/SPR_BENCH_accuracy_curve_NoTransformerEncoder.png", "../../logs/0-run/experiment_results/experiment_e46f3c6a839048e584bf3541709fdb79_proc_3173699/SPR_BENCH_loss_curve_NoTransformerEncoder.png"]], "plot_paths": [["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_test_accuracy.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_confusion_matrix_best.png"], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_accuracy_vs_heads.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead2.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead4.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead8.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead16.png"], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_loss_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_test_accuracy.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_loss_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_test_accuracy.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_loss_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_test_accuracy.png"], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead2_train_val_acc.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead4_train_val_acc.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead8_train_val_acc.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead16_train_val_acc.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_test_acc_barplot.png"], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_test_acc_vs_nhead.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_acc_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_loss_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_confusion_matrix.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_acc_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_loss_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_confusion_matrix.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_acc_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_loss_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_confusion_matrix.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead16_acc_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead16_loss_curve.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead16_confusion_matrix.png"], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_mean_pooling_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_cls_token_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_test_accuracy_vs_nhead.png"], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_test_accuracy_bar.png"], [], ["experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e46f3c6a839048e584bf3541709fdb79_proc_3173699/SPR_BENCH_accuracy_curve_NoTransformerEncoder.png", "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e46f3c6a839048e584bf3541709fdb79_proc_3173699/SPR_BENCH_loss_curve_NoTransformerEncoder.png"]], "plot_analyses": [[{"analysis": "The accuracy curves for different n-head values show that a lower n-head value (e.g., n-head=2) results in higher training and validation accuracy. The trends indicate that models with higher n-head values (e.g., n-head=16) tend to have slightly lower performance, particularly in the earlier epochs. This suggests that smaller n-head values may be better suited for this task, likely due to better generalization or reduced overfitting. The model with n-head=2 achieves the highest accuracy across both training and validation sets.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The loss curves reinforce the findings from the accuracy curves. Models with lower n-head values (e.g., n-head=2 and n-head=4) show lower loss values throughout the training process, indicating better convergence. Higher n-head values (e.g., n-head=16) exhibit higher loss values and less stable convergence patterns, particularly in the validation set. This further supports the observation that smaller n-head values are more effective for this task.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_loss_curves.png"}, {"analysis": "The test accuracy bar chart reveals that all n-head values achieve similar test accuracy, with n-head=2 slightly outperforming the others. This consistency across n-head values suggests that while training and validation performance vary, the final test performance is robust to changes in n-head within the tested range. However, n-head=2 remains the optimal choice due to its marginally better performance.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_test_accuracy.png"}, {"analysis": "The confusion matrix for n-head=2 demonstrates good classification performance, with a high number of correct predictions in both classes. However, there are still a notable number of misclassifications, particularly in the upper right and lower left quadrants. This indicates room for improvement in distinguishing between the two classes, possibly through further hyperparameter tuning or model adjustments.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1a788b2273c9434aa4a5f57864d9af39_proc_3161120/SPR_BENCH_confusion_matrix_best.png"}], [{"analysis": "The plot shows the relationship between the number of attention heads and test accuracy when positional embeddings are removed. Initially, the accuracy increases significantly as the number of heads increases from 2 to 4, indicating that the model benefits from additional attention heads. However, beyond 4 heads, the improvement becomes marginal, with a slight dip at 8 heads, followed by an increase up to 16 heads. This suggests diminishing returns with more attention heads and possibly some instability in performance at intermediate values like 8 heads. The removal of positional embeddings may be influencing the model's ability to fully leverage additional attention heads.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_accuracy_vs_heads.png"}, {"analysis": "With 2 attention heads, the training accuracy improves steadily over epochs, while validation accuracy shows a peak at epoch 4 before declining. This suggests potential overfitting as the model learns more. The training loss decreases consistently, but the validation loss starts increasing after epoch 3, further indicating overfitting.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead2.png"}, {"analysis": "With 4 attention heads, both training and validation accuracies improve steadily over epochs and converge closely, indicating better generalization compared to 2 heads. The loss curves for both training and validation decrease consistently, suggesting that the model is learning effectively without significant overfitting.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead4.png"}, {"analysis": "With 8 attention heads, the training accuracy continues to improve steadily, but validation accuracy peaks at epoch 3 and then declines slightly, suggesting mild overfitting. The training loss decreases consistently, while validation loss starts to plateau after epoch 3, indicating a potential limit to generalization performance with this configuration.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead8.png"}, {"analysis": "With 16 attention heads, the training accuracy shows a slight dip at epoch 3 before increasing again, while validation accuracy remains relatively stable, showing a slight decline towards the end. Training loss decreases consistently, but validation loss fluctuates, suggesting some instability in generalization performance. The model may be struggling to balance complexity and generalization at this configuration.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8ff134f7504443ea954ca8ff3dcc3aca_proc_3173696/SPR_BENCH_training_curves_nhead16.png"}], [{"analysis": "The accuracy curves for the EvenParity task show that increasing the number of heads (nhead) generally leads to better validation performance, with nhead=16 achieving the highest validation accuracy. Training accuracy consistently increases for all configurations, indicating effective learning. However, the gap between training and validation accuracy for nhead=2 and nhead=4 suggests potential overfitting. The results imply that higher nhead values capture sequence dependencies more effectively, leading to better generalization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_accuracy_curves.png"}, {"analysis": "The loss curves for the EvenParity task demonstrate a steady decrease in cross-entropy loss for both training and validation sets across epochs. Higher nhead values show lower validation loss, particularly for nhead=16, which aligns with its superior accuracy. The consistent reduction in loss across configurations indicates effective model optimization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_loss_curves.png"}, {"analysis": "The test accuracy bar chart for EvenParity reveals that nhead=16 achieves the highest test accuracy (0.83), marginally outperforming other configurations. This suggests that increasing nhead improves generalization performance, albeit with diminishing returns beyond nhead=8.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/EvenParity_test_accuracy.png"}, {"analysis": "The accuracy curves for the MajoritySymbol task indicate that increasing nhead improves training accuracy, with nhead=16 achieving the highest performance. However, the validation accuracy for nhead=2 and nhead=4 stagnates after initial improvement, suggesting limited generalization. The separation between training and validation accuracy for lower nhead values indicates potential overfitting.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_accuracy_curves.png"}, {"analysis": "The loss curves for the MajoritySymbol task show a consistent decrease in cross-entropy loss for all configurations, with nhead=16 achieving the lowest validation loss. The results indicate that higher nhead values enable better optimization, though the validation loss plateau for nhead=2 and nhead=4 suggests limited generalization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_loss_curves.png"}, {"analysis": "The test accuracy bar chart for MajoritySymbol shows that nhead=8 achieves the highest test accuracy (0.66), slightly outperforming other configurations. The relatively lower test accuracy compared to training performance suggests that the task may be inherently more challenging or that the model struggles to generalize effectively.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/MajoritySymbol_test_accuracy.png"}, {"analysis": "The accuracy curves for the CyclicShift task show that all configurations achieve high training and validation accuracy, with nhead=4 and nhead=16 performing particularly well. The rapid convergence of validation accuracy for nhead=4 suggests that it effectively captures sequence dependencies for this task.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_accuracy_curves.png"}, {"analysis": "The loss curves for the CyclicShift task demonstrate a consistent reduction in cross-entropy loss across epochs, with nhead=4 achieving the lowest validation loss. The results indicate that the model is well-optimized for this task, with minimal overfitting observed.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_loss_curves.png"}, {"analysis": "The test accuracy bar chart for CyclicShift shows uniformly high performance across configurations, with nhead=16 achieving the highest accuracy (0.98). This indicates that the model generalizes well for this task, and increasing nhead provides marginal benefits.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4211a431bb0b4a979d8603c36575fa48_proc_3173697/CyclicShift_test_accuracy.png"}], [{"analysis": "The training accuracy steadily increases over the epochs, stabilizing around 0.8. However, the validation accuracy shows fluctuations, reaching a peak at epoch 2, dropping at epoch 3, and recovering afterward. The gap between train and validation accuracy suggests potential overfitting, especially as the training accuracy stabilizes while validation accuracy fluctuates.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead2_train_val_acc.png"}, {"analysis": "Both training and validation accuracies improve steadily in the initial epochs, with training accuracy reaching a stable level around 0.8. Validation accuracy peaks at epoch 2 and then slightly decreases but remains relatively stable. The smaller gap between train and validation accuracies compared to the previous configuration indicates improved generalization with this setting.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead4_train_val_acc.png"}, {"analysis": "Training accuracy increases rapidly to stabilize at around 0.8, while validation accuracy shows a slight dip at epoch 3 before recovering. The gap between training and validation accuracies is slightly more pronounced than in the previous configuration, indicating a minor trade-off in generalization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead8_train_val_acc.png"}, {"analysis": "Training accuracy stabilizes around 0.8, while validation accuracy steadily improves over the epochs, showing no significant dips. The gap between training and validation accuracies is narrow, suggesting improved generalization performance with this configuration.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_nhead16_train_val_acc.png"}, {"analysis": "Test accuracy shows marginal differences across the different nhead configurations, ranging between 0.78 and 0.80. The highest accuracy is achieved with nhead=2, while the lowest is with nhead=16. This suggests that increasing the number of heads beyond a certain point does not yield significant improvements and might even slightly degrade performance.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_32c926219b2d4f6bb241f545df0707bb_proc_3173698/SPR_BENCH_test_acc_barplot.png"}], [{"analysis": "This plot demonstrates the relationship between the number of attention heads (nhead) and test accuracy. The accuracy decreases as the number of attention heads increases, suggesting that higher nhead values might introduce excessive complexity or noise that hinders performance. The best performance is observed at nhead=2, with a steady decline as nhead increases.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_test_acc_vs_nhead.png"}, {"analysis": "This plot compares training and validation accuracy across epochs for nhead=2. The training accuracy increases steadily, indicating effective learning. Validation accuracy fluctuates, initially improving but later showing signs of instability. This suggests potential overfitting or sensitivity to hyperparameters.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_acc_curve.png"}, {"analysis": "This plot shows the training and validation loss across epochs for nhead=2. Both losses decrease over time, with training loss reducing more consistently. The validation loss fluctuates, indicating potential overfitting or model instability despite overall improvement.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_loss_curve.png"}, {"analysis": "The confusion matrix provides insight into the model's performance for nhead=2. It shows a relatively balanced distribution of correct predictions for both classes, with some misclassifications. This suggests room for improvement in capturing symbolic rules.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead2_confusion_matrix.png"}, {"analysis": "This plot compares training and validation accuracy across epochs for nhead=4. Both training and validation accuracy improve initially, but validation accuracy starts to diverge from training accuracy in later epochs, indicating overfitting. The model performs slightly worse than for nhead=2.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_acc_curve.png"}, {"analysis": "This plot shows training and validation loss across epochs for nhead=4. Both losses decrease over time, but the validation loss plateaus and fluctuates in later epochs, further suggesting overfitting or difficulty in generalizing to unseen data.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_loss_curve.png"}, {"analysis": "This plot compares training and validation accuracy across epochs for nhead=8. Training accuracy increases steadily and stabilizes, while validation accuracy improves but remains consistently lower than training accuracy. The gap between training and validation accuracy reflects overfitting.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead4_confusion_matrix.png"}, {"analysis": "This plot shows training and validation loss across epochs for nhead=8. Training loss decreases steadily, while validation loss fluctuates and remains higher than training loss, indicating challenges in generalization and potential overfitting.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_acc_curve.png"}, {"analysis": "This plot compares training and validation accuracy across epochs for nhead=16. Training accuracy continues to improve, while validation accuracy shows a more pronounced divergence, indicating significant overfitting. The model's performance on unseen data is likely to be poor.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_loss_curve.png"}, {"analysis": "This plot shows training and validation loss across epochs for nhead=16. Training loss decreases steadily, while validation loss shows an increasing trend in later epochs, further confirming overfitting and poor generalization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3458a72cf7104fd895bcfb1abe0dd588_proc_3173699/SPR_BENCH_FreezeTokenEmbeddings_nhead8_confusion_matrix.png"}], [{"analysis": "The first set of plots compares train and validation accuracy across different numbers of attention heads (nhead) for the mean pooling method. The training accuracy increases steadily for all configurations, converging near 0.80. However, validation accuracy shows some variability, with nhead=4 experiencing a dip at epoch 4. This suggests that while the model learns effectively during training, generalization to unseen data may be inconsistent for certain configurations.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_mean_pooling_accuracy_curves.png"}, {"analysis": "The second set of plots evaluates train and validation accuracy for the cls_token method. Training accuracy trends are similar to the first set, with all configurations converging near 0.80. Validation accuracy, however, exhibits more pronounced fluctuations, especially for nhead=16, which shows instability in early epochs before improving. This indicates that cls_token may be more sensitive to the choice of nhead, impacting generalization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_cls_token_accuracy_curves.png"}, {"analysis": "The final plot compares test accuracy for mean pooling and cls_token methods across different nhead values. Mean pooling maintains relatively stable performance around 0.785 across all configurations. In contrast, cls_token shows a sharp decline in accuracy as nhead increases beyond 4, suggesting that this method is less robust to higher values of nhead. This highlights a potential limitation of cls_token in handling increased model complexity.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_178a4120d23a4e8d8d8793111b9f39a2_proc_3173696/SPR_BENCH_test_accuracy_vs_nhead.png"}], [{"analysis": "This plot compares the training and validation accuracy across different numbers of attention heads (2, 4, 8, 16) over 5 epochs. As the number of attention heads increases, the training accuracy generally improves slightly, with 16-heads achieving the highest training accuracy. However, the validation accuracy does not consistently improve with more attention heads and starts to diverge after epoch 3, especially for configurations with higher numbers of heads. This suggests that larger models may overfit the training data, leading to reduced generalization.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot displays the cross-entropy loss for training and validation data for different numbers of attention heads. The training loss decreases steadily for all configurations, with 16-heads achieving the lowest final loss. However, validation loss shows irregular trends, with some configurations (e.g., 2-heads and 4-heads) experiencing increases after epoch 3. This indicates potential overfitting for more complex models and suggests that simpler configurations may generalize better.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the final test accuracy for different numbers of attention heads. All configurations achieve similar test accuracy, with only marginal differences between them. This suggests that increasing the number of attention heads beyond 2 does not significantly impact the model's ability to generalize to the test data, highlighting diminishing returns for more complex models in this task.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2a41055a408c4fd09a30dc44e5fc15e7_proc_3173698/SPR_BENCH_test_accuracy_bar.png"}], [], [{"analysis": "The plot shows the training and validation accuracy over 5 epochs for a model without a Transformer encoder on the SPR_BENCH dataset. The training accuracy increases steadily, reaching approximately 0.76 by epoch 5, while the validation accuracy plateaus around 0.74 after epoch 3. This indicates that the model is learning effectively during training but has limited generalization capacity as the validation accuracy does not improve significantly beyond epoch 3. The small gap between training and validation accuracy suggests that overfitting is not a major issue at this stage, but the model's performance might be constrained by its architecture or lack of certain features, such as a Transformer encoder.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e46f3c6a839048e584bf3541709fdb79_proc_3173699/SPR_BENCH_accuracy_curve_NoTransformerEncoder.png"}, {"analysis": "This plot illustrates the training and validation loss over 5 epochs for the same model. Both losses decrease steadily, with the training loss dropping slightly faster than the validation loss. By epoch 5, the training loss is around 0.56, while the validation loss is slightly higher, indicating that the model is learning but may have room for improvement in terms of generalization. The consistent reduction in loss indicates that the optimization process is effective, but the relatively small gap between training and validation loss suggests that the model might be underutilizing its capacity for capturing complex patterns in the data, potentially due to the absence of a Transformer encoder.", "plot_path": "experiments/2025-08-17_00-45-19_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e46f3c6a839048e584bf3541709fdb79_proc_3173699/SPR_BENCH_loss_curve_NoTransformerEncoder.png"}]], "vlm_feedback_summary": ["The analysis highlights that smaller n-head values, particularly n-head=2, lead\nto better training, validation, and test performance. The results suggest that\nthe model's ability to generalize and converge effectively is enhanced with\nfewer attention heads. However, there is still room for improvement in reducing\nmisclassifications, as shown in the confusion matrix.", "The plots provide insights into the effect of varying attention heads on model\nperformance and training dynamics. Increasing attention heads generally improves\naccuracy, but with diminishing returns and potential overfitting or instability\nat higher values. The training and validation curves highlight the importance of\nbalancing model complexity and regularization to achieve optimal generalization.", "The provided plots demonstrate the impact of varying the number of attention\nheads (nhead) on model performance across different tasks. Higher nhead values\ngenerally improve both training and validation performance, though the benefits\ndiminish beyond a certain point. The results suggest that the proposed model\neffectively captures sequence dependencies, particularly for tasks like\nCyclicShift, while tasks like MajoritySymbol pose greater challenges for\ngeneralization.", "The experiments reveal that training accuracy consistently stabilizes at around\n0.8 across all configurations, while validation and test accuracies show\nvariations. Lower nhead values (e.g., 2) tend to generalize slightly better,\nachieving the highest test accuracy. Increasing nhead does not significantly\nimprove performance and may slightly degrade it. The results highlight the\nimportance of balancing model complexity with generalization capabilities.", "The analysis highlights the impact of varying the number of attention heads\n(nhead) on model performance. Lower nhead values (e.g., nhead=2) yield better\ntest accuracy, while higher values result in overfitting and reduced\ngeneralization. Validation metrics indicate instability and overfitting across\nall configurations, emphasizing the need for regularization and hyperparameter\ntuning.", "The analysis highlights the strengths and weaknesses of the two methods (mean\npooling and cls_token) in handling different numbers of attention heads. Mean\npooling demonstrates stable performance across configurations, while cls_token\nexhibits sensitivity to nhead, particularly at higher values. The results\nsuggest that mean pooling may be more reliable for the SPR task, whereas\ncls_token requires careful tuning of nhead to avoid performance degradation.", "The plots reveal that while increasing the number of attention heads improves\ntraining performance, it does not consistently enhance validation or test\naccuracy. Overfitting appears to be a concern for higher-head configurations, as\nindicated by diverging validation accuracy and loss trends. Simpler\nconfigurations may offer a better balance between performance and\ngeneralization.", "[]", "The results suggest that the model is learning effectively, with steady\nimprovements in both accuracy and loss. However, the performance plateaus early,\nindicating potential limitations in the architecture. Incorporating a\nTransformer encoder or other enhancements might improve generalization and\noverall performance on the SPR_BENCH dataset."], "exec_time": [7.311305522918701, 9.236622095108032, 6.240814447402954, 8.154168844223022, 8.266341924667358, 14.230510950088501, 6.708912134170532, 2.7345664501190186, 2.940399169921875], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['EvenParity'", "'CyclicShift']"], ["[\"SPR_BENCH\"]"], ["['nhead=2']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], [], ["['SPR_BENCH']"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------ #\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    results = experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][\"results\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    results = {}\n\n# Helper to get best nhead by highest test accuracy\nbest_nhead = None\nbest_test_acc = -1\nfor nhead, data in results.items():\n    if data[\"test_acc\"] > best_test_acc:\n        best_test_acc = data[\"test_acc\"]\n        best_nhead = nhead\n\n# ------------------------------------------------------------------ #\n# 1. Accuracy curves\ntry:\n    plt.figure()\n    for nhead, data in results.items():\n        epochs = np.arange(1, len(data[\"metrics\"][\"train_acc\"]) + 1)\n        plt.plot(\n            epochs,\n            data[\"metrics\"][\"train_acc\"],\n            marker=\"o\",\n            label=f\"train nhead={nhead}\",\n        )\n        plt.plot(\n            epochs, data[\"metrics\"][\"val_acc\"], marker=\"x\", label=f\"val nhead={nhead}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH Accuracy Curves (n-head tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(\"Saved\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------ #\n# 2. Loss curves\ntry:\n    plt.figure()\n    for nhead, data in results.items():\n        epochs = np.arange(1, len(data[\"losses\"][\"train_loss\"]) + 1)\n        plt.plot(\n            epochs,\n            data[\"losses\"][\"train_loss\"],\n            marker=\"o\",\n            label=f\"train nhead={nhead}\",\n        )\n        plt.plot(\n            epochs, data[\"losses\"][\"val_loss\"], marker=\"x\", label=f\"val nhead={nhead}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (n-head tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    print(\"Saved\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------ #\n# 3. Test accuracy bar chart\ntry:\n    plt.figure()\n    nheads = list(results.keys())\n    test_accs = [results[n][\"test_acc\"] for n in nheads]\n    plt.bar(nheads, test_accs, color=\"skyblue\")\n    plt.xlabel(\"n-head\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"SPR_BENCH Test Accuracy by n-head\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy.png\")\n    plt.savefig(fname)\n    print(\"Saved\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------ #\n# 4. Confusion matrix for best model\ntry:\n    if best_nhead is not None:\n        preds = np.array(results[best_nhead][\"predictions\"])\n        gts = np.array(results[best_nhead][\"ground_truth\"])\n        num_classes = len(np.unique(gts))\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for p, g in zip(preds, gts):\n            cm[g, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"SPR_BENCH Confusion Matrix (best nhead={best_nhead})\")\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"red\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix_best.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------- load experiment data ------------------ #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch nested dict safely\ndef get(d, *keys, default=None):\n    for k in keys:\n        if isinstance(d, dict) and k in d:\n            d = d[k]\n        else:\n            return default\n    return d\n\n\nresults = get(\n    experiment_data, \"RemovePositionalEmbeddings\", \"SPR_BENCH\", \"results\", default={}\n)\nif not results:\n    print(\"No results found for SPR_BENCH. Exiting plotting script.\")\nelse:\n    # ----------- aggregate test accuracy plot ----------- #\n    try:\n        heads, test_accs = [], []\n        for head, res in sorted(results.items(), key=lambda x: int(x[0])):\n            acc = res.get(\"test_acc\")\n            if acc is not None:\n                heads.append(int(head))\n                test_accs.append(acc)\n        if heads:\n            plt.figure()\n            plt.plot(heads, test_accs, marker=\"o\")\n            plt.xlabel(\"Number of Heads\")\n            plt.ylabel(\"Test Accuracy\")\n            plt.title(\n                \"SPR_BENCH: Test Accuracy vs Number of Attention Heads\\n(RemovePositionalEmbeddings)\"\n            )\n            fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_vs_heads.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy-vs-heads plot: {e}\")\n        plt.close()\n\n    # ----------- per-head training curves (<=4) --------- #\n    plotted = 0\n    for head, res in sorted(results.items(), key=lambda x: int(x[0])):\n        if plotted >= 4:  # at most 5 total plots including the aggregate above\n            break\n        try:\n            metrics = res.get(\"metrics\", {})\n            losses = res.get(\"losses\", {})\n            epochs = range(1, len(metrics.get(\"train_acc\", [])) + 1)\n            if not epochs:\n                continue\n            plt.figure(figsize=(8, 4))\n            # subplot 1: accuracy\n            plt.subplot(1, 2, 1)\n            plt.plot(epochs, metrics.get(\"train_acc\", []), label=\"Train\")\n            plt.plot(epochs, metrics.get(\"val_acc\", []), label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(\"Accuracy\")\n            plt.legend()\n            # subplot 2: loss\n            plt.subplot(1, 2, 2)\n            plt.plot(epochs, losses.get(\"train_loss\", []), label=\"Train\")\n            plt.plot(epochs, losses.get(\"val_loss\", []), label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss\")\n            plt.legend()\n            plt.suptitle(\n                f\"SPR_BENCH Training Curves | nhead={head}\\nLeft: Accuracy, Right: Loss\"\n            )\n            fname = os.path.join(\n                working_dir, f\"SPR_BENCH_training_curves_nhead{head}.png\"\n            )\n            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plotted += 1\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating training curve for nhead={head}: {e}\")\n            plt.close()\n\n    # -------------- print final metrics ----------------- #\n    for head, res in sorted(results.items(), key=lambda x: int(x[0])):\n        print(f\"nhead={head}: test_acc={res.get('test_acc', 'N/A')}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nroot = experiment_data.get(\"MultiSyntheticDatasets\", {})\nfor task_name, task_data in root.items():\n    # Gather nhead keys sorted as int\n    nheads = sorted(task_data.keys(), key=lambda x: int(x))\n    # ---------- Accuracy Curves ----------\n    try:\n        plt.figure()\n        for nh in nheads:\n            log = task_data[nh]\n            epochs = np.arange(1, len(log[\"metrics\"][\"train\"]) + 1)\n            plt.plot(\n                epochs, log[\"metrics\"][\"train\"], label=f\"nhead={nh} train\", marker=\"o\"\n            )\n            plt.plot(\n                epochs,\n                log[\"metrics\"][\"val\"],\n                label=f\"nhead={nh} val\",\n                linestyle=\"--\",\n                marker=\"x\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"MultiSyntheticDatasets \u2013 {task_name} \u2013 Accuracy Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{task_name}_accuracy_curves.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {task_name}: {e}\")\n        plt.close()\n\n    # ---------- Loss Curves ----------\n    try:\n        plt.figure()\n        for nh in nheads:\n            log = task_data[nh]\n            epochs = np.arange(1, len(log[\"losses\"][\"train\"]) + 1)\n            plt.plot(\n                epochs, log[\"losses\"][\"train\"], label=f\"nhead={nh} train\", marker=\"o\"\n            )\n            plt.plot(\n                epochs,\n                log[\"losses\"][\"val\"],\n                label=f\"nhead={nh} val\",\n                linestyle=\"--\",\n                marker=\"x\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"MultiSyntheticDatasets \u2013 {task_name} \u2013 Loss Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{task_name}_loss_curves.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {task_name}: {e}\")\n        plt.close()\n\n    # ---------- Test Accuracy Bar ----------\n    try:\n        plt.figure()\n        test_accs = [task_data[nh][\"test_acc\"] for nh in nheads]\n        plt.bar([str(nh) for nh in nheads], test_accs, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.xlabel(\"nhead\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(f\"MultiSyntheticDatasets \u2013 {task_name} \u2013 Test Accuracy\")\n        for idx, acc in enumerate(test_accs):\n            plt.text(idx, acc + 0.01, f\"{acc:.2f}\", ha=\"center\")\n        fname = os.path.join(working_dir, f\"{task_name}_test_accuracy.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test-accuracy plot for {task_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ LOAD EXPERIMENT DATA ------------------ #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Quick exit if nothing to plot\nif not experiment_data:\n    exit()\n\nresults_dict = (\n    experiment_data.get(\"NoPaddingMask\", {}).get(\"SPR_BENCH\", {}).get(\"results\", {})\n)\nnheads = sorted(results_dict.keys(), key=int)\n\n# ------------------ PER-NHEAD ACCURACY CURVES --------------- #\nfor nh in nheads:\n    try:\n        metrics = results_dict[nh][\"metrics\"]\n        train_acc = metrics[\"train_acc\"]\n        val_acc = metrics[\"val_acc\"]\n        epochs = np.arange(1, len(train_acc) + 1)\n\n        plt.figure()\n        plt.plot(epochs, train_acc, marker=\"o\", label=\"Train Acc\")\n        plt.plot(epochs, val_acc, marker=\"s\", label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"SPR_BENCH \u2013 NoPaddingMask / Train vs Val Acc\\nnhead={nh}\")\n        plt.legend()\n        fname = f\"SPR_BENCH_nhead{nh}_train_val_acc.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating acc curve for nhead={nh}: {e}\")\n        plt.close()\n\n# ------------------ TEST ACCURACY BARPLOT ------------------ #\ntry:\n    test_accs = [results_dict[nh][\"test_acc\"] for nh in nheads]\n    plt.figure()\n    plt.bar(range(len(nheads)), test_accs, tick_label=nheads)\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"SPR_BENCH \u2013 NoPaddingMask / Test Acc vs nhead\")\n    for idx, acc in enumerate(test_accs):\n        plt.text(idx, acc + 0.01, f\"{acc:.2f}\", ha=\"center\")\n    fname = \"SPR_BENCH_test_acc_barplot.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar plot: {e}\")\n    plt.close()\n\nprint(\"Finished plotting experiment results.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------ #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ablation, dsets in experiment_data.items():\n    for dset_name, dset_blob in dsets.items():\n        results = dset_blob.get(\"results\", {})\n        # ----------------- 1. Test accuracy vs nhead ---------------- #\n        try:\n            nheads, test_accs = [], []\n            for nhead, blob in results.items():\n                nheads.append(int(nhead))\n                test_accs.append(blob.get(\"test_acc\", np.nan))\n            if nheads:\n                idx = np.argsort(nheads)\n                nheads = np.array(nheads)[idx]\n                test_accs = np.array(test_accs)[idx]\n                plt.figure()\n                plt.plot(nheads, test_accs, marker=\"o\")\n                plt.xlabel(\"nhead\")\n                plt.ylabel(\"Test Accuracy\")\n                plt.title(f\"{dset_name} | {ablation} | Test Accuracy vs nhead\")\n                save_path = os.path.join(\n                    working_dir, f\"{dset_name}_{ablation}_test_acc_vs_nhead.png\"\n                )\n                plt.savefig(save_path)\n                plt.close()\n        except Exception as e:\n            print(f\"Error plotting test-accuracy curve: {e}\")\n            plt.close()\n\n        # ----------- 2. Per-head training / validation curves ------- #\n        for i, (nhead, blob) in enumerate(results.items()):\n            metrics = blob.get(\"metrics\", {})\n            losses = blob.get(\"losses\", {})\n            epochs = range(1, 1 + len(metrics.get(\"train_acc\", [])))\n\n            # Accuracy plot\n            try:\n                plt.figure()\n                plt.plot(epochs, metrics.get(\"train_acc\", []), label=\"train\")\n                plt.plot(epochs, metrics.get(\"val_acc\", []), label=\"val\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Accuracy\")\n                plt.title(\n                    f\"{dset_name} | {ablation} | nhead={nhead}\\nTraining vs Validation Accuracy\"\n                )\n                plt.legend()\n                fname = f\"{dset_name}_{ablation}_nhead{nhead}_acc_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating acc plot for nhead={nhead}: {e}\")\n                plt.close()\n\n            # Loss plot\n            try:\n                plt.figure()\n                plt.plot(epochs, losses.get(\"train_loss\", []), label=\"train\")\n                plt.plot(epochs, losses.get(\"val_loss\", []), label=\"val\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.title(\n                    f\"{dset_name} | {ablation} | nhead={nhead}\\nTraining vs Validation Loss\"\n                )\n                plt.legend()\n                fname = f\"{dset_name}_{ablation}_nhead{nhead}_loss_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating loss plot for nhead={nhead}: {e}\")\n                plt.close()\n\n            # Confusion matrix (limit to first 4 heads)\n            if i < 4:\n                try:\n                    preds = np.array(blob.get(\"predictions\", []), dtype=int)\n                    gts = np.array(blob.get(\"ground_truth\", []), dtype=int)\n                    if preds.size and gts.size:\n                        num_classes = max(preds.max(), gts.max()) + 1\n                        cm = np.zeros((num_classes, num_classes), dtype=int)\n                        for gt, pr in zip(gts, preds):\n                            cm[gt, pr] += 1\n                        plt.figure()\n                        plt.imshow(cm, cmap=\"Blues\")\n                        plt.colorbar()\n                        plt.xlabel(\"Predicted\")\n                        plt.ylabel(\"Ground Truth\")\n                        plt.title(\n                            f\"{dset_name} | {ablation} | nhead={nhead}\\nConfusion Matrix\"\n                        )\n                        fname = (\n                            f\"{dset_name}_{ablation}_nhead{nhead}_confusion_matrix.png\"\n                        )\n                        plt.savefig(os.path.join(working_dir, fname))\n                        plt.close()\n                except Exception as e:\n                    print(f\"Error creating confusion matrix for nhead={nhead}: {e}\")\n                    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- LOAD EXPERIMENT DATA ------------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nvariants = [\"mean_pooling\", \"cls_token\"]\ndataset_name = \"SPR_BENCH\"\n\n# ----------------------- PLOT ACCURACY CURVES ---------------------- #\nfor v in variants:\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n        for nhead, res in (\n            experiment_data.get(v, {}).get(dataset_name, {}).get(\"results\", {}).items()\n        ):\n            epochs = range(1, len(res[\"metrics\"][\"train_acc\"]) + 1)\n            axes[0].plot(epochs, res[\"metrics\"][\"train_acc\"], label=f\"nhead={nhead}\")\n            axes[1].plot(epochs, res[\"metrics\"][\"val_acc\"], label=f\"nhead={nhead}\")\n        axes[0].set_title(\"Train Accuracy\")\n        axes[1].set_title(\"Validation Accuracy\")\n        for ax in axes:\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Accuracy\")\n            ax.legend()\n        fig.suptitle(f\"{dataset_name}: {v} (Left: Train, Right: Val)\")\n        save_path = os.path.join(working_dir, f\"{dataset_name}_{v}_accuracy_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy curves for {v}: {e}\")\n        plt.close()\n\n# --------------- PLOT TEST ACCURACY VS NHEAD COMPARISON ------------ #\ntry:\n    fig = plt.figure(figsize=(6, 4))\n    for v in variants:\n        nheads, test_accs = [], []\n        for nhead, res in (\n            experiment_data.get(v, {}).get(dataset_name, {}).get(\"results\", {}).items()\n        ):\n            nheads.append(int(nhead))\n            test_accs.append(res[\"test_acc\"])\n        idx = np.argsort(nheads)\n        plt.plot(np.array(nheads)[idx], np.array(test_accs)[idx], marker=\"o\", label=v)\n    plt.title(f\"{dataset_name}: Test Accuracy vs nhead\")\n    plt.xlabel(\"Number of Attention Heads\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.legend()\n    save_path = os.path.join(working_dir, f\"{dataset_name}_test_accuracy_vs_nhead.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting test accuracy comparison: {e}\")\n    plt.close()\n\n# ---------------------- PRINT SUMMARY METRICS ---------------------- #\nfor v in variants:\n    res_dict = experiment_data.get(v, {}).get(dataset_name, {}).get(\"results\", {})\n    best = max(res_dict.items(), key=lambda x: x[1][\"test_acc\"]) if res_dict else None\n    if best:\n        print(f\"{v:15s} | best test acc={best[1]['test_acc']:.4f} @ nhead={best[0]}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    results = experiment_data[\"SingleTransformerLayer\"][\"SPR_BENCH\"][\"results\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    results = {}\n\nnheads, test_accs = [], []\ntrain_acc_curves, val_acc_curves = {}, {}\ntrain_loss_curves, val_loss_curves = {}, {}\n\n# Extract metrics\nfor nh, res in results.items():\n    nheads.append(int(nh))\n    test_accs.append(res[\"test_acc\"])\n    train_acc_curves[nh] = res[\"metrics\"][\"train\"]\n    val_acc_curves[nh] = res[\"metrics\"][\"val\"]\n    train_loss_curves[nh] = res[\"losses\"][\"train\"]\n    val_loss_curves[nh] = res[\"losses\"][\"val\"]\n    print(f\"nhead={nh}: test_acc={res['test_acc']:.4f}\")\n\n# Plot 1: Accuracy curves\ntry:\n    plt.figure()\n    for nh in sorted(train_acc_curves, key=int):\n        epochs = range(1, len(train_acc_curves[nh]) + 1)\n        plt.plot(epochs, train_acc_curves[nh], label=f\"{nh}-head Train\")\n        plt.plot(epochs, val_acc_curves[nh], linestyle=\"--\", label=f\"{nh}-head Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy Curves - SPR_BENCH\\nTrain vs Validation for each nhead\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot 2: Loss curves\ntry:\n    plt.figure()\n    for nh in sorted(train_loss_curves, key=int):\n        epochs = range(1, len(train_loss_curves[nh]) + 1)\n        plt.plot(epochs, train_loss_curves[nh], label=f\"{nh}-head Train\")\n        plt.plot(epochs, val_loss_curves[nh], linestyle=\"--\", label=f\"{nh}-head Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Loss Curves - SPR_BENCH\\nTrain vs Validation for each nhead\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot 3: Test accuracy bar chart\ntry:\n    plt.figure()\n    plt.bar([str(nh) for nh in nheads], test_accs, color=\"skyblue\")\n    plt.xlabel(\"Number of Attention Heads\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.ylim(0, 1)\n    plt.title(\"Final Test Accuracy - SPR_BENCH\\nSingle-Layer Transformer Ablation\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test-accuracy bar plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- Load experiment data ------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ------------------- Helper for safe fetch ------------------ #\ndef _safe_get(d, *keys, default=None):\n    cur = d\n    for k in keys:\n        if k not in cur:\n            return default\n        cur = cur[k]\n    return cur if cur is not None else default\n\n\n# ------------------- Iterate and plot ----------------------- #\nfor model_name, datasets in experiment_data.items():\n    for ds_name, records in datasets.items():\n        # Fetch arrays\n        train_acc = _safe_get(records, \"metrics\", \"train\", default=[])\n        val_acc = _safe_get(records, \"metrics\", \"val\", default=[])\n        train_loss = _safe_get(records, \"losses\", \"train\", default=[])\n        val_loss = _safe_get(records, \"losses\", \"val\", default=[])\n        test_acc = _safe_get(records, \"test_acc\", default=None)\n\n        epochs = range(1, len(train_acc) + 1)\n\n        # --------------- Accuracy curve ---------------- #\n        try:\n            plt.figure()\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{model_name} on {ds_name}\\nTraining vs Validation Accuracy\")\n            plt.legend()\n            save_name = f\"{ds_name}_accuracy_curve_{model_name}.png\"\n            plt.savefig(os.path.join(working_dir, save_name))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n            plt.close()\n\n        # --------------- Loss curve -------------------- #\n        try:\n            plt.figure()\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{model_name} on {ds_name}\\nTraining vs Validation Loss\")\n            plt.legend()\n            save_name = f\"{ds_name}_loss_curve_{model_name}.png\"\n            plt.savefig(os.path.join(working_dir, save_name))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds_name}: {e}\")\n            plt.close()\n\n        # --------------- Print final metrics ----------- #\n        final_train = train_acc[-1] if train_acc else None\n        final_val = val_acc[-1] if val_acc else None\n        if final_train is not None:\n            print(\n                f\"{model_name} | {ds_name}: Train Acc={final_train:.4f}, Val Acc={final_val:.4f}, Test Acc={test_acc:.4f}\"\n                if test_acc is not None\n                else f\"{model_name} | {ds_name}: Train Acc={final_train:.4f}, Val Acc={final_val:.4f}\"\n            )\n"], "plot_plan": ["We load experiment_data.npy, locate the \u201cnhead_tuning \u2192 SPR_BENCH \u2192 results\u201d dictionary, and collect per-epoch \u2018train_acc\u2019, \u2018val_acc\u2019, \u2018train_loss\u2019, \u2018val_loss\u2019, plus the final \u2018test_acc\u2019, predictions and ground-truth. We draw at most four figures: (1) one multi-line figure with train/val accuracy curves for all n-head values, (2) one similar figure for losses, (3) a bar chart of test accuracies, and (4) a confusion-matrix heat-map for the model with the highest test accuracy. Each figure is produced inside its own try-except block, saved in working_dir with descriptive filenames, titled clearly, and closed afterward. We sample every epoch (there are \u22645) so no further sub-sampling is needed. Only matplotlib and numpy are used, complying with the \u201cbasic matplotlib\u201d rule. Finally, the script prints the path of each saved plot so downstream tasks can pick them up.", "We load experiment_data.npy, inspect the dictionary to extract the \u201cSPR_BENCH\u201d results produced by the ablation study, and collect per-head metrics (train/val loss and accuracy for each epoch plus final test accuracy). A first figure shows the overall test accuracy as a function of nhead to highlight any performance trend induced by removing positional embeddings. For the remaining heads (max four) we draw one figure per head containing two subplots\u2014train/val accuracy and train/val loss versus epoch\u2014so that we do not exceed the five-figure limit. Each figure lives in its own try-except block, is saved with a descriptive filename that mentions the dataset and plot type, and is closed afterwards. All plots use only existing values; if a metric is missing the corresponding plot is skipped gracefully. After saving, the script prints the test accuracies so users can verify the numbers programmatically. The code adheres strictly to the given matplotlib guidelines, uses the provided working_dir, and avoids any custom styling or fabricated data.", "We will load the saved experiment_data.npy, iterate over the three synthetic datasets, and for each dataset gather (1) per-epoch train/validation accuracy, (2) per-epoch train/validation loss, and (3) final test accuracy for every recorded nhead configuration. Within three separate try-except blocks per dataset we will:  \n\u2022 plot combined accuracy curves (one line per nhead, solid=train, dashed=val);  \n\u2022 plot combined loss curves (same legend convention);  \n\u2022 plot a bar chart of test accuracies vs. nhead.  \nEach figure is titled descriptively (e.g. \u201cMultiSyntheticDatasets \u2013 EvenParity \u2013 Accuracy Curves\u201d) and saved to working_dir with an informative filename. We ensure at most five figures of each type are produced by limiting to the available three datasets. All figures are closed after saving, and any error during plotting is caught and printed without stopping the rest of the script. Finally, we print the file locations of the generated plots for quick verification.", "We first load the saved experiment_data.npy, identify every nhead setting tested, and for each of them fetch the per-epoch train/val accuracy arrays plus the final test accuracy.  \nA loop then creates one figure per nhead that overlays train and validation accuracy versus epoch; these plots help diagnose over/under-fitting and are saved with descriptive filenames.  \nTo keep the figure count reasonable (<5) we only plot accuracy curves (loss curves are similar and can be inspected numerically if needed).  \nAfter producing the four nhead-specific curves, we make a single bar plot that compares the final test accuracy across all heads\u2014this gives a quick \u201con one page\u201d view of model quality.  \nEvery plotting section is wrapped in its own try-except so that a failure on one figure does not stop the rest; in the except branch we still close any open figure.  \nEach figure is explicitly closed after saving, has a clear title and subtitle (e.g. \u201cSPR_BENCH \u2013 NoPaddingMask / Train vs Val Acc, nhead=4\u201d), and is written into the working directory.  \nThe script adheres to the import/working_dir requirements and prints a short confirmation once the plots have been generated.", "We will load the stored experiment_data.npy, iterate over every ablation\u2013dataset pair, and gather the per-head metrics already recorded by the training script.  For each head we will: (1) plot training/validation accuracy across epochs, (2) plot training/validation loss across epochs, and (3) draw a confusion-matrix heat-map from ground-truth versus predictions on the test set.  After that, we will aggregate all heads into a single line plot showing test accuracy versus nhead.  Each individual figure is wrapped in its own try-except so that one failure does not stop subsequent plots; figures are always closed.  File names contain ablation, dataset, head (when relevant) and the plotted metric.  We keep the number of confusion-matrix plots below the 5-figure cap by limiting them to the first four heads found.  All plots are saved into working_dir created at runtime.  Only data that exist inside experiment_data.npy are used\u2014no made-up values or external datasets.", "We will load the stored experiment_data.npy, iterate through the two model variants and their n-head settings, and extract the per-epoch training/validation accuracy and loss as well as final test accuracy. Three separate figures will be created: (1) accuracy curves for the mean-pooling model, (2) accuracy curves for the CLS-token model, and (3) a comparison of test accuracy versus the number of heads for the two variants. Each figure is wrapped in its own try-except so that a failure in one plot does not stop the rest. All plots are saved into working_dir with descriptive filenames that include the dataset (SPR_BENCH) and plot type. After plotting, we print a small summary table of the best test accuracies for quick inspection.", "We will load the saved experiment_data.npy, iterate over the four n-head settings, and extract the stored \u201cmetrics\u201d, \u201closses\u201d, and \u201ctest_acc\u201d.  \nThree figures will be produced (each in its own try\u2013except):  \n1) Training vs validation accuracy curves for every n-head on the same axes.  \n2) Training vs validation loss curves, again overlaying the four heads.  \n3) A bar chart of final test accuracy for each n-head.  \nAll plots will be saved in the working directory with descriptive file names that contain the dataset name (SPR_BENCH) and plot type.  \nTitles will mention the dataset and include a clear subtitle describing the comparison (e.g. \u201cTrain vs Val\u201d).  \nWe respect the \u201c\u22645 figures\u201d rule and close every figure.  \nThe script also prints the loaded test accuracies so the user immediately sees the quantitative results.", null, "The solution loads the saved experiment dictionary, safely extracts available arrays, and produces up-to-date visualizations without fabricating information. Two plots are generated for each dataset\u2013model pair: (1) training/validation accuracy across epochs and (2) training/validation loss across epochs. Each figure is created inside its own try-except block, closed after saving, and titled descriptively with subtitles clarifying the left/right content where relevant. Filenames always include the dataset name and plot type to remain self-explanatory. When optional data (e.g., predictions) exist, additional summary metrics such as final test accuracy are printed, but no extra plots are made because they would duplicate information. Only the first five epochs (or fewer if the run is shorter) are plotted to respect the \u201cat most 5 figures\u201d rule. All files are stored in the predefined working directory, which the script ensures exists before plotting."], "ablation_name": [null, "RemovePositionalEmbeddings", "MultiSyntheticDatasets", "NoPaddingMask", "FreezeTokenEmbeddings", "UseCLSClassificationToken", "SingleTransformerLayer", "RemoveFeedForwardNetwork", "NoTransformerEncoder"], "hyperparam_name": ["nhead (number_of_attention_heads)", null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["The script will load the saved numpy file from the `working` directory, navigate\nthe nested dictionary (`nhead_tuning \u2192 dataset \u2192 results`), and for each dataset\ncollect the best values of all recorded metrics across the different `nhead`\nruns. \u201cBest\u201d is defined as highest accuracy and lowest loss. After computing\nthese maxima/minima it prints the dataset name followed by the clearly-labelled\nmetric/value pairs. No plots are generated, and the code runs immediately at\nimport time.", "This solution loads the saved NumPy dictionary, drills down to the results\nrecorded for each experiment configuration (the number of attention heads), and\nprints the final-epoch metrics stored in the lists together with the single test\naccuracy value. The script respects the required directory layout, clearly\nlabels each dataset and metric, and runs immediately on execution (no `if\n__name__ == \"__main__\":` guard). All printing is done in a readable, explicit\nmanner without generating any plots.", "The script loads the saved NumPy dictionary, iterates over each synthetic\ndataset, and for every attention-head configuration prints the final (last-\nepoch) train accuracy, validation accuracy, train loss, validation loss, and the\ntest accuracy. Metric names are spelled out explicitly so there\u2019s no ambiguity,\nand the code runs immediately on execution without relying on any `__main__`\nguard.", "The script will locate the working directory, load the serialized experiment\ndictionary, and iterate through every recorded model variant.   For each dataset\n(e.g., \u201cSPR_BENCH\u201d) it will compute the best value for every stored metric list\n(maximum for accuracies, minimum for losses) and report those along with the\nfinal test accuracy.   Results are printed in a clear, prefixed format such as\n\u201cbest train accuracy:\u201d or \u201cfinal test accuracy:\u201d so that each value is\nunambiguous.   All code is written at global scope and runs immediately when the\nfile is executed.", "The script loads the saved NumPy file from the working directory, traverses the\nnested dictionary to reach each dataset (e.g., SPR_BENCH) and its individual\nexperiment settings (different `nhead` values). For every setting it prints the\ndataset name first, then the metric names followed by their best/final values:\nthe last epoch\u2019s train/validation accuracy and loss, along with the test\naccuracy that was stored separately. The code is flat (no `if __name__ ==\n\"__main__\":`) so it runs immediately when executed.", "The script will load the saved NumPy file from the working directory, traverse\nevery variant (mean-pooling vs. cls-token) and every attention-head setting,\ncollect the best value (max for accuracies, min for losses) across all runs,\naggregate those per-dataset, and finally print a concise summary of the best\ntrain/validation accuracies, test accuracy, and the lowest train/validation\nlosses for each dataset.", "The script will load the experiment_data.npy file from the \u201cworking\u201d directory,\nwalk through every stored run (keyed by the number of attention heads) inside\nthe SPR_BENCH entry, and print the final-epoch train accuracy, validation\naccuracy, train loss, validation loss, and the held-out test accuracy. Each\nresult block is preceded by a descriptive header that clearly identifies the\ndataset and configuration (e.g., \u201cSPR_BENCH | nhead=4\u201d).", "", "The script will locate the working directory, load the experiment_data.npy file\ninto a Python dictionary, and iterate through every model and dataset saved\ninside.   For each dataset it prints the dataset name, then the final (i.e.,\nlast-epoch) training accuracy, validation accuracy, training loss, validation\nloss, and the stored test accuracy, each preceded by a clear descriptive label.\nNo plotting or special entry point is used; everything executes immediately upon\nrunning the file."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -----------------------------------------------------------\n# Locate and load the experiment data\n# -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Cannot find experiment data at {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------\n# Helper to update best metric (max for acc / min for loss)\n# -----------------------------------------------------------\ndef update_best(current_best, candidate, higher_is_better=True):\n    if candidate is None:\n        return current_best\n    if current_best is None:\n        return candidate\n    if higher_is_better:\n        return candidate if candidate > current_best else current_best\n    return candidate if candidate < current_best else current_best\n\n\n# -----------------------------------------------------------\n# Traverse structure and print best/final metrics\n# -----------------------------------------------------------\nfor hp_name, datasets in experiment_data.items():  # e.g. 'nhead_tuning'\n    for dataset_name, dataset_blob in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        best_train_acc = None\n        best_val_acc = None\n        best_test_acc = None\n        best_train_loss = None\n        best_val_loss = None\n\n        for nhead_key, run_dict in dataset_blob.get(\"results\", {}).items():\n            # accuracy lists\n            train_acc_list = run_dict.get(\"metrics\", {}).get(\"train_acc\", [])\n            val_acc_list = run_dict.get(\"metrics\", {}).get(\"val_acc\", [])\n            # loss lists\n            train_loss_list = run_dict.get(\"losses\", {}).get(\"train_loss\", [])\n            val_loss_list = run_dict.get(\"losses\", {}).get(\"val_loss\", [])\n\n            # final / best values for this run\n            train_acc_final = train_acc_list[-1] if train_acc_list else None\n            val_acc_best = max(val_acc_list) if val_acc_list else None\n            train_loss_final = train_loss_list[-1] if train_loss_list else None\n            val_loss_best = min(val_loss_list) if val_loss_list else None\n            test_acc_value = run_dict.get(\"test_acc\")\n\n            # aggregate best across runs\n            best_train_acc = update_best(best_train_acc, train_acc_final, True)\n            best_val_acc = update_best(best_val_acc, val_acc_best, True)\n            best_test_acc = update_best(best_test_acc, test_acc_value, True)\n            best_train_loss = update_best(best_train_loss, train_loss_final, False)\n            best_val_loss = update_best(best_val_loss, val_loss_best, False)\n\n        # Print results with clear labels\n        if best_train_acc is not None:\n            print(f\"Best train accuracy: {best_train_acc:.4f}\")\n        if best_val_acc is not None:\n            print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n        if best_train_loss is not None:\n            print(f\"Best train loss: {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"Best validation loss: {best_val_loss:.4f}\")\n        if best_test_acc is not None:\n            print(f\"Best test accuracy: {best_test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------- LOAD SAVED EXPERIMENT DATA ----------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not locate experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------------- PARSE & DISPLAY METRICS -------------------- #\nfor experiment_name, experiment in experiment_data.items():\n    # e.g. experiment_name = \"RemovePositionalEmbeddings\"\n    for dataset_name, dataset_blob in experiment.items():\n        print(f\"Dataset: {dataset_name}\")\n        results = dataset_blob.get(\"results\", {})\n        if not results:\n            print(\"  No results found.\")\n            continue\n\n        # Loop over each hyper-parameter configuration (e.g., different nhead values)\n        for config_key, run in results.items():\n            metrics = run.get(\"metrics\", {})\n            losses = run.get(\"losses\", {})\n            test_acc = run.get(\"test_acc\")\n\n            # Retrieve final epoch values (last entry in each list)\n            train_acc_final = metrics.get(\"train_acc\", [None])[-1]\n            val_acc_final = metrics.get(\"val_acc\", [None])[-1]\n            train_loss_final = losses.get(\"train_loss\", [None])[-1]\n            val_loss_final = losses.get(\"val_loss\", [None])[-1]\n\n            print(f\"  Configuration: nhead = {config_key}\")\n            if train_acc_final is not None:\n                print(f\"    train accuracy: {train_acc_final:.4f}\")\n            if val_acc_final is not None:\n                print(f\"    validation accuracy: {val_acc_final:.4f}\")\n            if train_loss_final is not None:\n                print(f\"    train loss: {train_loss_final:.4f}\")\n            if val_loss_final is not None:\n                print(f\"    validation loss: {val_loss_final:.4f}\")\n            if test_acc is not None:\n                print(f\"    test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------\n# 1. Locate and load the experiment data\n# -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------\n# 2. Parse and print metrics\n# -----------------------------------------------------------\nroot = experiment_data.get(\"MultiSyntheticDatasets\", {})\n\nfor dataset_name, cfg_dict in root.items():\n    print(f\"\\n===== DATASET: {dataset_name} =====\")\n    # Sort nheads numerically for nicer output\n    for nhead_str in sorted(cfg_dict.keys(), key=lambda x: int(x)):\n        log = cfg_dict[nhead_str]\n        # Final epoch metrics\n        final_train_acc = log[\"metrics\"][\"train\"][-1]\n        final_val_acc = log[\"metrics\"][\"val\"][-1]\n        final_train_loss = log[\"losses\"][\"train\"][-1]\n        final_val_loss = log[\"losses\"][\"val\"][-1]\n        test_acc = log[\"test_acc\"]\n\n        print(f\"\\n  Attention heads = {nhead_str}\")\n        print(f\"    train accuracy: {final_train_acc:.4f}\")\n        print(f\"    validation accuracy: {final_val_acc:.4f}\")\n        print(f\"    train loss: {final_train_loss:.4f}\")\n        print(f\"    validation loss: {final_val_loss:.4f}\")\n        print(f\"    test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate the working directory and load the experiment data file\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(npy_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {npy_path}\")\n\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# 1. Helper to pick best values depending on metric type\n# ------------------------------------------------------------------\ndef best_value(metric_name, values):\n    \"\"\"\n    For accuracy-type metrics we want the maximum; for loss-type metrics the minimum.\n    \"\"\"\n    if \"acc\" in metric_name.lower():\n        return max(values)\n    if \"loss\" in metric_name.lower():\n        return min(values)\n    # Fallback \u2192 return last value\n    return values[-1]\n\n\n# ------------------------------------------------------------------\n# 2. Iterate through all recorded results and print metrics\n# ------------------------------------------------------------------\nfor method_name, datasets in experiment_data.items():  # e.g. \"NoPaddingMask\"\n    for dataset_name, dataset_dict in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\nDataset: {dataset_name}\")\n        results = dataset_dict.get(\"results\", {})\n        if not results:\n            print(\"  (No results found.)\")\n            continue\n\n        # Each key corresponds to one hyper-parameter configuration (e.g. nhead)\n        for config_id, res in results.items():\n            print(f\"  Configuration: nhead={config_id}\")\n\n            # Extract best or final values for every stored metric list\n            for mname, mvals in res.get(\"metrics\", {}).items():\n                value = best_value(mname, mvals)\n                # Construct a clearer name\n                pretty_name = mname.replace(\"_\", \" \")\n                print(f\"    best {pretty_name}: {value:.4f}\")\n\n            for lname, lvals in res.get(\"losses\", {}).items():\n                value = best_value(lname, lvals)\n                pretty_name = lname.replace(\"_\", \" \")\n                print(f\"    best {pretty_name}: {value:.4f}\")\n\n            # Test accuracy is a single scalar\n            if \"test_acc\" in res:\n                print(f\"    final test accuracy: {res['test_acc']:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Locate and load experiment data\n# -------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------------------\n# Helper to pick the best/final metric value\n# -------------------------------------------------------------------\ndef final_value(values, maximize=True):\n    \"\"\"\n    Return the last value in the list (final epoch).\n    Could easily be changed to max/min, but spec allows final.\n    \"\"\"\n    return values[-1] if values else None\n\n\n# -------------------------------------------------------------------\n# Traverse and print\n# -------------------------------------------------------------------\nfor ablation_name, datasets in experiment_data.items():\n    # datasets is e.g. {\"SPR_BENCH\": {...}}\n    for dataset_name, dataset_content in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        results = dataset_content.get(\"results\", {})\n        # Iterate over each hyper-parameter configuration (e.g., nhead)\n        for cfg_name, cfg_data in results.items():\n            metrics = cfg_data.get(\"metrics\", {})\n            losses = cfg_data.get(\"losses\", {})\n            test_acc = cfg_data.get(\"test_acc\")\n\n            print(f\"  Configuration (nhead={cfg_name}):\")\n\n            # -- Accuracy metrics --\n            train_acc_final = final_value(metrics.get(\"train_acc\", []))\n            val_acc_final = final_value(metrics.get(\"val_acc\", []))\n            if train_acc_final is not None:\n                print(f\"    train accuracy: {train_acc_final:.4f}\")\n            if val_acc_final is not None:\n                print(f\"    validation accuracy: {val_acc_final:.4f}\")\n\n            # -- Loss metrics --\n            train_loss_final = final_value(losses.get(\"train_loss\", []), maximize=False)\n            val_loss_final = final_value(losses.get(\"val_loss\", []), maximize=False)\n            if train_loss_final is not None:\n                print(f\"    train loss: {train_loss_final:.4f}\")\n            if val_loss_final is not None:\n                print(f\"    validation loss: {val_loss_final:.4f}\")\n\n            # -- Test accuracy --\n            if test_acc is not None:\n                print(f\"    test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0-1. Load experiment data saved by the training script\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 2-5. Aggregate best / final metrics per dataset\n# ------------------------------------------------------------------\ndataset_best = {}\n\nfor variant, datasets in experiment_data.items():  # mean_pooling / cls_token\n    for dataset_name, dataset_info in datasets.items():  # SPR_BENCH (only one here)\n        results = dataset_info.get(\"results\", {})\n        # initialise per-dataset aggregator on first encounter\n        agg = dataset_best.setdefault(\n            dataset_name,\n            {\n                \"train accuracy\": -1.0,\n                \"validation accuracy\": -1.0,\n                \"test accuracy\": -1.0,\n                \"train loss\": float(\"inf\"),\n                \"validation loss\": float(\"inf\"),\n            },\n        )\n\n        for nhead, run in results.items():\n            metrics = run.get(\"metrics\", {})\n            losses = run.get(\"losses\", {})\n            test_acc = run.get(\"test_acc\", None)\n\n            # best / worst for this particular run\n            best_train_acc = max(metrics.get(\"train_acc\", []), default=None)\n            best_val_acc = max(metrics.get(\"val_acc\", []), default=None)\n            lowest_train_loss = min(losses.get(\"train_loss\", []), default=None)\n            lowest_val_loss = min(losses.get(\"val_loss\", []), default=None)\n\n            # update dataset-level aggregate\n            if best_train_acc is not None and best_train_acc > agg[\"train accuracy\"]:\n                agg[\"train accuracy\"] = best_train_acc\n            if best_val_acc is not None and best_val_acc > agg[\"validation accuracy\"]:\n                agg[\"validation accuracy\"] = best_val_acc\n            if test_acc is not None and test_acc > agg[\"test accuracy\"]:\n                agg[\"test accuracy\"] = test_acc\n            if lowest_train_loss is not None and lowest_train_loss < agg[\"train loss\"]:\n                agg[\"train loss\"] = lowest_train_loss\n            if lowest_val_loss is not None and lowest_val_loss < agg[\"validation loss\"]:\n                agg[\"validation loss\"] = lowest_val_loss\n\n# ------------------------------------------------------------------\n# 3-4. Print the consolidated metrics per dataset\n# ------------------------------------------------------------------\nfor dataset_name, stats in dataset_best.items():\n    print(dataset_name)\n    for metric_name, value in stats.items():\n        # guard against infinities if something was never updated\n        if np.isfinite(value):\n            print(f\"{metric_name}: {value:.4f}\")\n        else:\n            print(f\"{metric_name}: N/A\")\n    print()\n", "import os\nimport numpy as np\n\n# ---------- LOAD SAVED EXPERIMENT DATA ----------\nwork_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(work_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not locate experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- EXTRACT AND PRINT METRICS ----------\nfor model_name, datasets in experiment_data.items():  # e.g., \"SingleTransformerLayer\"\n    for dataset_name, dataset_info in datasets.items():  # e.g., \"SPR_BENCH\"\n        results = dataset_info.get(\"results\", {})\n        for run_key, run_data in results.items():  # e.g., run_key == \"4\" for nhead=4\n            header = f\"{dataset_name} | nhead={run_key}\"\n            print(header)\n\n            # Retrieve sequences of metrics/losses collected during training\n            train_acc_history = run_data[\"metrics\"][\"train\"]\n            val_acc_history = run_data[\"metrics\"][\"val\"]\n            train_loss_history = run_data[\"losses\"][\"train\"]\n            val_loss_history = run_data[\"losses\"][\"val\"]\n\n            # Final epoch values (could also take best with max/min)\n            final_train_acc = train_acc_history[-1]\n            final_val_acc = val_acc_history[-1]\n            final_train_loss = train_loss_history[-1]\n            final_val_loss = val_loss_history[-1]\n            test_accuracy = run_data[\"test_acc\"]\n\n            # ---- Printing with explicit metric names ----\n            print(f\"  train accuracy: {final_train_acc:.4f}\")\n            print(f\"  validation accuracy: {final_val_acc:.4f}\")\n            print(f\"  train loss: {final_train_loss:.4f}\")\n            print(f\"  validation loss: {final_val_loss:.4f}\")\n            print(f\"  test accuracy: {test_accuracy:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(npy_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {npy_path}\")\n\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to safely extract the last value of a list (if list exists)\n# ------------------------------------------------------------------\ndef last_or_none(lst):\n    return lst[-1] if isinstance(lst, (list, tuple)) and lst else None\n\n\n# ------------------------------------------------------------------\n# Iterate and print metrics\n# ------------------------------------------------------------------\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # Accuracy metrics\n        final_train_acc = last_or_none(results.get(\"metrics\", {}).get(\"train\", []))\n        final_val_acc = last_or_none(results.get(\"metrics\", {}).get(\"val\", []))\n\n        # Loss metrics\n        final_train_loss = last_or_none(results.get(\"losses\", {}).get(\"train\", []))\n        final_val_loss = last_or_none(results.get(\"losses\", {}).get(\"val\", []))\n\n        # Test metric\n        test_acc = results.get(\"test_acc\", None)\n\n        # Print with explicit labels\n        if final_train_acc is not None:\n            print(f\"Final train accuracy:     {final_train_acc:.4f}\")\n        if final_val_acc is not None:\n            print(f\"Final validation accuracy:{final_val_acc:.4f}\")\n        if final_train_loss is not None:\n            print(f\"Final train loss:         {final_train_loss:.6f}\")\n        if final_val_loss is not None:\n            print(f\"Final validation loss:    {final_val_loss:.6f}\")\n        if test_acc is not None:\n            print(f\"Test accuracy:            {test_acc:.4f}\")\n"], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', 'Best train accuracy: 0.8030', '\\n', 'Best\nvalidation accuracy: 0.7940', '\\n', 'Best train loss: 0.4702', '\\n', 'Best\nvalidation loss: 0.5292', '\\n', 'Best test accuracy: 0.8030', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Configuration: nhead = 2', '\\n', '    train\naccuracy: 0.7785', '\\n', '    validation accuracy: 0.7620', '\\n', '    train\nloss: 0.5276', '\\n', '    validation loss: 0.5688', '\\n', '    test accuracy:\n0.7520', '\\n', '  Configuration: nhead = 4', '\\n', '    train accuracy: 0.7830',\n'\\n', '    validation accuracy: 0.7780', '\\n', '    train loss: 0.5276', '\\n', '\nvalidation loss: 0.5421', '\\n', '    test accuracy: 0.7860', '\\n', '\nConfiguration: nhead = 8', '\\n', '    train accuracy: 0.7875', '\\n', '\nvalidation accuracy: 0.7740', '\\n', '    train loss: 0.5223', '\\n', '\nvalidation loss: 0.5351', '\\n', '    test accuracy: 0.7830', '\\n', '\nConfiguration: nhead = 16', '\\n', '    train accuracy: 0.7940', '\\n', '\nvalidation accuracy: 0.7860', '\\n', '    train loss: 0.5159', '\\n', '\nvalidation loss: 0.5181', '\\n', '    test accuracy: 0.7890', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\n===== DATASET: EvenParity =====', '\\n', '\\n  Attention heads = 2', '\\n', '\ntrain accuracy: 0.8420', '\\n', '    validation accuracy: 0.8500', '\\n', '\ntrain loss: 0.2747', '\\n', '    validation loss: 0.2737', '\\n', '    test\naccuracy: 0.8150', '\\n', '\\n  Attention heads = 4', '\\n', '    train accuracy:\n0.8640', '\\n', '    validation accuracy: 0.8400', '\\n', '    train loss:\n0.2801', '\\n', '    validation loss: 0.3125', '\\n', '    test accuracy: 0.8150',\n'\\n', '\\n  Attention heads = 8', '\\n', '    train accuracy: 0.8420', '\\n', '\nvalidation accuracy: 0.8300', '\\n', '    train loss: 0.2991', '\\n', '\nvalidation loss: 0.2996', '\\n', '    test accuracy: 0.8100', '\\n', '\\n\nAttention heads = 16', '\\n', '    train accuracy: 0.8420', '\\n', '    validation\naccuracy: 0.8600', '\\n', '    train loss: 0.3278', '\\n', '    validation loss:\n0.3273', '\\n', '    test accuracy: 0.8350', '\\n', '\\n===== DATASET:\nMajoritySymbol =====', '\\n', '\\n  Attention heads = 2', '\\n', '    train\naccuracy: 0.7780', '\\n', '    validation accuracy: 0.6000', '\\n', '    train\nloss: 0.7976', '\\n', '    validation loss: 1.0041', '\\n', '    test accuracy:\n0.6500', '\\n', '\\n  Attention heads = 4', '\\n', '    train accuracy: 0.7600',\n'\\n', '    validation accuracy: 0.5900', '\\n', '    train loss: 0.7971', '\\n', '\nvalidation loss: 1.0787', '\\n', '    test accuracy: 0.6100', '\\n', '\\n\nAttention heads = 8', '\\n', '    train accuracy: 0.7840', '\\n', '    validation\naccuracy: 0.6500', '\\n', '    train loss: 0.7877', '\\n', '    validation loss:\n0.9079', '\\n', '    test accuracy: 0.6600', '\\n', '\\n  Attention heads = 16',\n'\\n', '    train accuracy: 0.8040', '\\n', '    validation accuracy: 0.6100',\n'\\n', '    train loss: 0.7570', '\\n', '    validation loss: 0.9857', '\\n', '\ntest accuracy: 0.6200', '\\n', '\\n===== DATASET: CyclicShift =====', '\\n', '\\n\nAttention heads = 2', '\\n', '    train accuracy: 0.9860', '\\n', '    validation\naccuracy: 0.9400', '\\n', '    train loss: 0.0540', '\\n', '    validation loss:\n0.2157', '\\n', '    test accuracy: 0.9500', '\\n', '\\n  Attention heads = 4',\n'\\n', '    train accuracy: 0.9940', '\\n', '    validation accuracy: 0.9500',\n'\\n', '    train loss: 0.0215', '\\n', '    validation loss: 0.1470', '\\n', '\ntest accuracy: 0.9500', '\\n', '\\n  Attention heads = 8', '\\n', '    train\naccuracy: 0.9820', '\\n', '    validation accuracy: 0.9700', '\\n', '    train\nloss: 0.0569', '\\n', '    validation loss: 0.0931', '\\n', '    test accuracy:\n0.9450', '\\n', '\\n  Attention heads = 16', '\\n', '    train accuracy: 0.9900',\n'\\n', '    validation accuracy: 0.9700', '\\n', '    train loss: 0.0311', '\\n', '\nvalidation loss: 0.0808', '\\n', '    test accuracy: 0.9850', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: nhead=2', '\\n', '    best train\nacc: 0.7955', '\\n', '    best val acc: 0.7880', '\\n', '    best train loss:\n0.4834', '\\n', '    best val loss: 0.5522', '\\n', '    final test accuracy:\n0.8040', '\\n', '  Configuration: nhead=4', '\\n', '    best train acc: 0.7995',\n'\\n', '    best val acc: 0.7920', '\\n', '    best train loss: 0.4840', '\\n', '\nbest val loss: 0.5332', '\\n', '    final test accuracy: 0.7940', '\\n', '\nConfiguration: nhead=8', '\\n', '    best train acc: 0.8035', '\\n', '    best val\nacc: 0.7860', '\\n', '    best train loss: 0.4691', '\\n', '    best val loss:\n0.5397', '\\n', '    final test accuracy: 0.7870', '\\n', '  Configuration:\nnhead=16', '\\n', '    best train acc: 0.7995', '\\n', '    best val acc: 0.7840',\n'\\n', '    best train loss: 0.4793', '\\n', '    best val loss: 0.5285', '\\n', '\nfinal test accuracy: 0.7820', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration (nhead=2):', '\\n', '    train\naccuracy: 0.7960', '\\n', '    validation accuracy: 0.7840', '\\n', '    train\nloss: 0.4825', '\\n', '    validation loss: 0.5559', '\\n', '    test accuracy:\n0.8000', '\\n', '  Configuration (nhead=4):', '\\n', '    train accuracy: 0.7975',\n'\\n', '    validation accuracy: 0.7740', '\\n', '    train loss: 0.4840', '\\n', '\nvalidation loss: 0.5389', '\\n', '    test accuracy: 0.7880', '\\n', '\nConfiguration (nhead=8):', '\\n', '    train accuracy: 0.8010', '\\n', '\nvalidation accuracy: 0.7800', '\\n', '    train loss: 0.4699', '\\n', '\nvalidation loss: 0.5591', '\\n', '    test accuracy: 0.7850', '\\n', '\nConfiguration (nhead=16):', '\\n', '    train accuracy: 0.8025', '\\n', '\nvalidation accuracy: 0.7760', '\\n', '    train loss: 0.4807', '\\n', '\nvalidation loss: 0.5440', '\\n', '    test accuracy: 0.7810', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.8045', '\\n', 'validation accuracy:\n0.8000', '\\n', 'test accuracy: 0.7970', '\\n', 'train loss: 0.4619', '\\n',\n'validation loss: 0.5224', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH | nhead=2', '\\n', '  train accuracy: 0.7970', '\\n', '  validation\naccuracy: 0.7780', '\\n', '  train loss: 0.4704', '\\n', '  validation loss:\n0.5726', '\\n', '  test accuracy: 0.7910\\n', '\\n', 'SPR_BENCH | nhead=4', '\\n', '\ntrain accuracy: 0.7835', '\\n', '  validation accuracy: 0.7680', '\\n', '  train\nloss: 0.4915', '\\n', '  validation loss: 0.5525', '\\n', '  test accuracy:\n0.7920\\n', '\\n', 'SPR_BENCH | nhead=8', '\\n', '  train accuracy: 0.8025', '\\n',\n'  validation accuracy: 0.7840', '\\n', '  train loss: 0.4653', '\\n', '\nvalidation loss: 0.5608', '\\n', '  test accuracy: 0.7870\\n', '\\n', 'SPR_BENCH |\nnhead=16', '\\n', '  train accuracy: 0.8000', '\\n', '  validation accuracy:\n0.7740', '\\n', '  train loss: 0.4811', '\\n', '  validation loss: 0.5582', '\\n',\n'  test accuracy: 0.7820\\n', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "", "['\\nDataset: SPR_BENCH', '\\n', 'Final train accuracy:     0.7665', '\\n', 'Final\nvalidation accuracy:0.7460', '\\n', 'Final train loss:         0.557622', '\\n',\n'Final validation loss:    0.569559', '\\n', 'Test accuracy:            0.7640',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
