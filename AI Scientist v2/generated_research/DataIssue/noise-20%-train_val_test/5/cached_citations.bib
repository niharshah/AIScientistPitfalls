
% The foundational paper that introduced the transformer architecture, which is a key component of the proposed model. This should be cited when discussing the methodology and the use of transformer-based models with contextual embeddings for symbolic reasoning tasks.
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}

% The foundational paper introducing BERT, which provides the theoretical and methodological basis for using contextual embeddings in the proposed model for symbolic reasoning tasks. This paper should be cited when explaining the adaptation of BERT-inspired contextual embeddings to capture intricate dependencies within symbolic sequences.
@article{devlin2019bertpo,
 author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 pages = {4171-4186},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

% This paper provides a mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task. It identifies interpretable mechanisms within transformers and validates them with causal evidence, which is relevant to understanding the internal workings of transformers in symbolic reasoning. It should be cited when discussing related work on symbolic reasoning with transformers and highlighting research gaps in understanding their internal mechanisms.
@article{brinkmann2024ama,
 author = {Jannik Brinkmann and A. Sheshadri and Victor Levoso and Paul Swoboda and Christian Bartelt},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 pages = {4082-4102},
 title = {A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task},
 year = {2024}
}

% The paper 'Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars' discusses advancements in synthetic logical reasoning datasets, using context-sensitive declarative grammars. This aligns with the SPR_BENCH dataset's focus on synthetic symbolic reasoning tasks. It should be cited when introducing the SPR_BENCH dataset and discussing its relevance and advancements in the field of synthetic reasoning datasets.
@article{sileo2024scalingsl,
 author = {Damien Sileo},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 journal = {ArXiv},
 title = {Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars},
 volume = {abs/2406.11035},
 year = {2024}
}
