{"edges": [[0, 3], [0, 8], [0, 2], [0, 4], [0, 1], [0, 5], [0, 7], [0, 6], [8, 12], [8, 10], [8, 9], [8, 11]], "layout": [[0.4117647058823529, 0.0], [0.0, 0.5], [0.11764705882352941, 0.5], [0.23529411764705882, 0.5], [0.35294117647058826, 0.5], [0.47058823529411764, 0.5], [0.5882352941176471, 0.5], [0.7058823529411765, 0.5], [0.8235294117647058, 0.5], [0.6470588235294118, 1.0], [0.7647058823529411, 1.0], [0.8823529411764706, 1.0], [1.0, 1.0]], "plan": ["Our first baseline treats each symbol sequence as a string of characters, builds\na simple vocabulary, and feeds the resulting integer sequence into a lightweight\nRNN classifier (Embedding \u2192 single-layer Bi-LSTM \u2192 max-pool \u2192 Linear).  Padding\nis handled batch-wise so the network can be trained efficiently on GPU when\navailable.  We load the benchmark splits with the helper in SPR.py; if the\ndirectory is missing, we synthesise a tiny random dataset so the script always\nruns.  Training proceeds for a handful of epochs while tracking cross-entropy\nloss and the required Macro-F1 on the dev set after every epoch.  Both metrics,\ntogether with per-epoch predictions and ground truth, are stored in a structured\nexperiment_data dictionary and saved as a NumPy file in ./working.  The model,\ndata, and every batch tensor are consistently moved to the correct device.  At\nthe end, the script prints the best dev Macro-F1 and evaluates once on the\nhidden test split, giving us a clean, reproducible baseline to improve later.", "Hyperparam tuning name: num_epochs. We test four training-length settings (5,\n10, 15, 20 epochs) and add optional early stopping (patience = 3) to avoid\nwasted computation.   For every epoch budget we re-initialize the model, train\nwhile monitoring dev macro-F1, keep the parameters that give the best dev score,\nthen evaluate on the held-out test split.   All per-run curves (losses,\nmetrics), best predictions and scores are saved under\nexperiment_data['num_epochs']['SPR_BENCH']['epochs_<N>'] so results can be\ncompared later.", "Hyperparam tuning name: learning_rate. We load SPR-BENCH (or fallback synthetic)\nonce, build the vocabulary, and prepare dataloaders.   A list of learning-rates\n(5e-4, 1e-3, 2e-3, 5e-3) is swept: for every value we re-instantiate the model\nand Adam optimizer, train for five epochs, track losses/F1 on train & dev, keep\nthe best-dev snapshot, and finally evaluate on the test set.   All results are\nstored in a hierarchical experiment_data dictionary under the top-level key\n'learning_rate', with one sub-entry per learning\u2013rate, then saved to\nexperiment_data.npy.   The code below is fully self-contained and executable.", "Hyperparam tuning name: batch_size. We iterate over a small grid of batch sizes\n(32, 64, 128).   For each value we rebuild data-loaders, re-instantiate the\nmodel, and train for a fixed number of epochs, logging per-epoch losses and\nmacro-F1 on train/dev splits.   After training we keep the dev set predictions\nof the best epoch and evaluate on the test set; all relevant data are stored\nunder experiment_data['batch_size']['SPR_BENCH'][str(bs)].   Finally the whole\nexperiment_data dictionary is saved to experiment_data.npy.", "Hyperparam tuning name: hidden_size. The solution runs a small sweep over LSTM\nhidden sizes (64 / 128 / 256 / 512).   For every value it (1) re-initialises the\nmodel, (2) trains for 5 epochs with the unchanged optimisation set-up, (3)\ntracks per-epoch train/validation losses and macro-F1, and (4) keeps the best-\nvalidation parameters to score on the test set.   All information is stored in a\nsingle nested experiment_data dictionary under the top-level key \"hidden_size\"\nand finally saved to experiment_data.npy.", "Hyperparam tuning name: dropout_prob. We iterate over dropout probabilities\n[0.0, 0.2, 0.4, 0.6], inserting `nn.Dropout` between the LSTM max-pool and the\nfinal linear layer.   For each probability we train the model for a few epochs,\ntrack losses/F1 on train- and dev-sets, and keep the model achieving the best\ndev macro-F1.   All results are recorded in an `experiment_data` dictionary\n(nested by hyper-parameter value) and finally saved to `experiment_data.npy`;\nthe best configuration is evaluated on the test set.", "Hyperparam tuning name: embed_dim. We keep the original data-loading, vocabulary\nbuilding, dataloader and training code, but wrap the training routine inside a\nloop that instantiates a fresh model for each candidate embedding dimension (32,\n64, 128, 256).   For every embed_dim we train for 5 epochs, track losses and\nmacro-F1 on both train and dev sets, retain the best dev epoch\u2019s predictions,\nand store everything under `experiment_data['embed_dim'][embed_dim]`.   After\nall candidates are evaluated we report the best dev configuration and its test\nscore, then save the full `experiment_data` structure to `experiment_data.npy`.", "Hyperparam tuning name: num_lstm_layers. The solution repeatedly trains the\nBiLSTM classifier while varying the number of stacked LSTM layers (1, 2, 3).\nFor every setting it builds a fresh model (`num_layers` is passed to `nn.LSTM`),\ntrains for five epochs, records train/validation losses and macro-F1, and keeps\nthe weights that yield the best validation F1.   All results are saved in the\nrequired `experiment_data` structure under the key `\"num_lstm_layers\"` and\nfinally written to `experiment_data.npy`.   The configuration with the highest\nvalidation macro-F1 is additionally evaluated on the test set and reported.", "Hyperparam tuning name: weight_decay. We add a simple sweep over Adam\u2019s\nweight_decay: for every value in [0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3] we re-\ninstantiate the model/optimizer with that decay, train for 5 epochs exactly like\nthe baseline, evaluate on dev and test, and log all per-epoch metrics/losses\ntogether with best-epoch predictions. Logged results are stored in a\nhierarchical experiment_data dictionary under the key 'weight_decay' \u2192\n'SPR_BENCH' \u2192 str(weight_decay). Everything is finally saved to\nexperiment_data.npy. The rest of the pipeline (data loading, model, training\nloop) remains unchanged.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# --------------- device handling ------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------- load SPR_BENCH -------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:\n    # fallback synthetic data (very small, just to guarantee runnable script)\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            lbl = int(seq.count(\"#\") % 2 == 0)  # arbitrary rule\n            seqs.append(seq)\n            labels.append(lbl)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"] = synth_split(512)\n    dsets[\"dev\"] = synth_split(128)\n    dsets[\"test\"] = synth_split(128)\nprint({k: len(v) for k, v in dsets.items()})\n\n# --------------- vocab & encoding -----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# --------------- Dataset / Dataloader -------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# --------------- Model ----------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        packed_out, _ = self.lstm(em)\n        # simple max-pool over time\n        pooled, _ = torch.max(packed_out, dim=1)\n        return self.fc(pooled)\n\n\nmodel = SPRClassifier(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------- experiment_data dict -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# --------------- training loop --------------------\nepochs = 5\nbest_f1 = 0.0\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_losses, train_preds, train_gts = [], [], []\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n        train_preds.extend(logits.argmax(1).cpu().numpy())\n        train_gts.extend(batch[\"labels\"].cpu().numpy())\n    train_f1 = f1_score(train_gts, train_preds, average=\"macro\")\n    # ---- eval on dev ----\n    model.eval()\n    dev_losses, dev_preds, dev_gts = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            dev_losses.append(loss.item())\n            dev_preds.extend(logits.argmax(1).cpu().numpy())\n            dev_gts.extend(batch[\"labels\"].cpu().numpy())\n    dev_f1 = f1_score(dev_gts, dev_preds, average=\"macro\")\n    print(\n        f\"Epoch {epoch}: train_loss={np.mean(train_losses):.4f}, val_loss={np.mean(dev_losses):.4f}, val_macroF1={dev_f1:.4f}\"\n    )\n    # store experiment data\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(dev_f1)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(np.mean(train_losses))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(np.mean(dev_losses))\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    # save best preds\n    if dev_f1 > best_f1:\n        best_f1 = dev_f1\n        experiment_data[\"SPR_BENCH\"][\"predictions\"] = dev_preds\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = dev_gts\n\n# --------------- final test evaluation ------------\nmodel.eval()\ntest_preds, test_gts = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"])\n        test_preds.extend(logits.argmax(1).cpu().numpy())\n        test_gts.extend(batch[\"labels\"].cpu().numpy())\ntest_f1 = f1_score(test_gts, test_preds, average=\"macro\")\nprint(f\"Best Dev Macro-F1 = {best_f1:.4f} | Test Macro-F1 = {test_f1:.4f}\")\n\n# --------------- save experiment data -------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ---------- experiment data skeleton ----------\nexperiment_data = {\n    \"num_epochs\": {  # hyperparam tuning type\n        \"SPR_BENCH\": {\"configurations\": {}}  # will be filled with epochs_5 / 10 / ...\n    }\n}\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load data ----------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # fallback synthetic data\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"], dsets[\"dev\"], dsets[\"test\"] = (\n        synth_split(512),\n        synth_split(128),\n        synth_split(128),\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------- vocab ----------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ---------- dataloaders ----------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(map(len, ids))\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = lambda: DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---------- model ----------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, 1)\n        return self.fc(pooled)\n\n\n# ---------- training routine ----------\ndef run_training(max_epochs: int, patience: int = 3):\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    tr_metrics, dv_metrics, tr_losses, dv_losses = [], [], [], []\n    best_state, best_dev_f1, epochs_no_improve = None, 0.0, 0\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        t_losses, t_preds, t_gts = [], [], []\n        for batch in train_loader():\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n            t_preds.extend(logits.argmax(1).cpu().numpy())\n            t_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(t_gts, t_preds, average=\"macro\")\n\n        model.eval()\n        d_losses, d_preds, d_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device) if torch.is_tensor(v) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                d_losses.append(criterion(logits, batch[\"labels\"]).item())\n                d_preds.extend(logits.argmax(1).cpu().numpy())\n                d_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(d_gts, d_preds, average=\"macro\")\n\n        tr_metrics.append(train_f1)\n        dv_metrics.append(dev_f1)\n        tr_losses.append(np.mean(t_losses))\n        dv_losses.append(np.mean(d_losses))\n        print(\n            f\"[epochs={max_epochs}] Epoch {epoch}: train_loss={tr_losses[-1]:.4f} dev_loss={dv_losses[-1]:.4f} dev_F1={dev_f1:.4f}\"\n        )\n\n        if dev_f1 > best_dev_f1:\n            best_dev_f1, best_state = dev_f1, model.state_dict()\n            best_preds, best_gts = d_preds.copy(), d_gts.copy()\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        if patience and epochs_no_improve >= patience:\n            print(\"Early stopping triggered\")\n            break\n\n    # load best state for final test\n    model.load_state_dict(best_state)\n    model.eval()\n    t_preds, t_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            t_preds.extend(logits.argmax(1).cpu().numpy())\n            t_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(t_gts, t_preds, average=\"macro\")\n\n    return {\n        \"metrics\": {\"train\": tr_metrics, \"val\": dv_metrics},\n        \"losses\": {\"train\": tr_losses, \"val\": dv_losses},\n        \"predictions\": best_preds,\n        \"ground_truth\": best_gts,\n        \"best_dev_f1\": best_dev_f1,\n        \"test_f1\": test_f1,\n        \"epochs\": list(range(1, len(tr_metrics) + 1)),\n    }\n\n\n# ---------- hyperparameter sweep ----------\nepoch_options = [5, 10, 15, 20]\nfor n_ep in epoch_options:\n    result = run_training(n_ep, patience=3)\n    exp_key = f\"epochs_{n_ep}\"\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"configurations\"][exp_key] = result\n    print(\n        f\"Finished setting {exp_key}: best_dev_F1={result['best_dev_f1']:.4f}, test_F1={result['test_f1']:.4f}\"\n    )\n\n# ---------- save ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"All results saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ---------------- filesystem / save dir -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device --------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR_BENCH ------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:\n    # fallback very small synthetic dataset\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"] = synth_split(512)\n    dsets[\"dev\"] = synth_split(128)\n    dsets[\"test\"] = synth_split(128)\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------------- vocab / encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ---------------- collator / loaders --------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------- model definition ----------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        lstm_out, _ = self.lstm(em)\n        pooled, _ = torch.max(lstm_out, dim=1)\n        return self.fc(pooled)\n\n\n# ---------------- hyperparameter sweep ------------\nlearning_rates = [5e-4, 1e-3, 2e-3, 5e-3]\nepochs = 5\n\nexperiment_data = {\"learning_rate\": {}}\n\nfor lr in learning_rates:\n    print(f\"\\n=== Training with learning rate {lr} ===\")\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    run_metrics = {\"train\": [], \"val\": []}\n    run_losses = {\"train\": [], \"val\": []}\n    best_f1, best_preds, best_gts = 0.0, [], []\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- dev ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                dv_losses.append(criterion(logits, batch[\"labels\"]).item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n\n        run_metrics[\"train\"].append(train_f1)\n        run_metrics[\"val\"].append(dev_f1)\n        run_losses[\"train\"].append(np.mean(tr_losses))\n        run_losses[\"val\"].append(np.mean(dv_losses))\n\n        if dev_f1 > best_f1:\n            best_f1 = dev_f1\n            best_preds, best_gts = dv_preds.copy(), dv_gts.copy()\n\n        print(\n            f\"Epoch {epoch}: lr={lr} train_loss={np.mean(tr_losses):.4f} \"\n            f\"val_loss={np.mean(dv_losses):.4f} val_macroF1={dev_f1:.4f}\"\n        )\n\n    # ---- final test evaluation ----\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    print(f\"LR {lr}: Best Dev F1 = {best_f1:.4f} | Test F1 = {test_f1:.4f}\")\n\n    # ---- store experiment data ----\n    exp_key = f\"lr_{lr}\"\n    experiment_data[\"learning_rate\"][exp_key] = {\n        \"metrics\": run_metrics,\n        \"losses\": run_losses,\n        \"predictions\": best_preds,\n        \"ground_truth\": best_gts,\n        \"best_dev_f1\": best_f1,\n        \"test_f1\": test_f1,\n        \"epochs\": list(range(1, epochs + 1)),\n    }\n\n# ---------------- save all experiment data --------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ------------------ reproducibility -----------------\ntorch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n# ------------------ device --------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------ load SPR_BENCH ------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            lbl = int(seq.count(\"#\") % 2 == 0)\n            seqs.append(seq)\n            labels.append(lbl)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"] = synth_split(512)\n    dsets[\"dev\"] = synth_split(128)\n    dsets[\"test\"] = synth_split(128)\nprint({k: len(v) for k, v in dsets.items()})\n\n# ------------------ vocab & encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ------------------ collate fn ----------------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\n# ------------------ model ---------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ------------------ training routine ---------------\ndef train_with_bs(batch_size, epochs=5, lr=1e-3):\n    # loaders\n    train_loader = DataLoader(\n        dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n    )\n    dev_loader = DataLoader(\n        dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n    )\n    test_loader = DataLoader(\n        dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n    )\n    # model / optim\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"test_f1\": None,\n    }\n    best_f1 = 0.0\n    best_preds, best_gts = None, None\n\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        tr_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # dev\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dv_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n\n        print(\n            f\"[bs={batch_size}] Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_macroF1={dv_f1:.4f}\"\n        )\n\n        # log\n        log[\"metrics\"][\"train\"].append(tr_f1)\n        log[\"metrics\"][\"val\"].append(dv_f1)\n        log[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        log[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        log[\"epochs\"].append(epoch)\n\n        if dv_f1 > best_f1:\n            best_f1 = dv_f1\n            best_preds, best_gts = dv_preds, dv_gts\n\n    # test eval\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    log[\"predictions\"] = best_preds\n    log[\"ground_truth\"] = best_gts\n    log[\"test_f1\"] = test_f1\n    print(\n        f\"[bs={batch_size}] Best Dev Macro-F1={best_f1:.4f} | Test Macro-F1={test_f1:.4f}\"\n    )\n    return log\n\n\n# ------------------ hyperparameter sweep -----------\nbatch_sizes = [32, 64, 128]\nexperiment_data = {\"batch_size\": {\"SPR_BENCH\": {}}}\n\nfor bs in batch_sizes:\n    log = train_with_bs(bs)\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][str(bs)] = log\n\n# ------------------ save ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ---------------------- reproducibility -------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ---------------------- device & dir ----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ---------------------- load SPR_BENCH --------------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # ------- tiny synthetic fallback -------------\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"], dsets[\"dev\"], dsets[\"test\"] = (\n        synth_split(512),\n        synth_split(128),\n        synth_split(128),\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------------------- vocab & encoding ------------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ---------------------- DataLoader ------------------------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------------- Model definition ------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ---------------------- experiment data dict --------------------\nexperiment_data = {\"hidden_size\": {\"SPR_BENCH\": {}}}  # Filled per hidden size\n\n\n# ---------------------- training / evaluation -------------------\ndef evaluate(model, loader, criterion):\n    model.eval()\n    losses, preds, gts = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            losses.append(criterion(logits, batch[\"labels\"]).item())\n            preds.extend(logits.argmax(1).cpu().numpy())\n            gts.extend(batch[\"labels\"].cpu().numpy())\n    return np.mean(losses), f1_score(gts, preds, average=\"macro\"), preds, gts\n\n\n# ---------------------- hyperparameter sweep --------------------\nhidden_sizes = [64, 128, 256, 512]\nepochs = 5\nfor hs in hidden_sizes:\n    run_key = f\"hs_{hs}\"\n    print(f\"\\n=== Training with hidden size {hs} ===\")\n    exp_run = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"best_dev_f1\": 0.0,\n        \"test_f1\": 0.0,\n    }\n    model = SPRClassifier(vocab_size, hidden=hs).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_state = None\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        train_losses, train_preds, train_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n            train_preds.extend(logits.argmax(1).cpu().numpy())\n            train_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(train_gts, train_preds, average=\"macro\")\n\n        # ---- validation ----\n        val_loss, val_f1, val_preds, val_gts = evaluate(model, dev_loader, criterion)\n\n        # ---- logging ----\n        print(\n            f\"Epoch {epoch}: train_loss={np.mean(train_losses):.4f}, val_loss={val_loss:.4f}, val_F1={val_f1:.4f}\"\n        )\n        exp_run[\"metrics\"][\"train\"].append(train_f1)\n        exp_run[\"metrics\"][\"val\"].append(val_f1)\n        exp_run[\"losses\"][\"train\"].append(np.mean(train_losses))\n        exp_run[\"losses\"][\"val\"].append(val_loss)\n        exp_run[\"epochs\"].append(epoch)\n\n        # ---- save best ----\n        if val_f1 > exp_run[\"best_dev_f1\"]:\n            exp_run[\"best_dev_f1\"] = val_f1\n            exp_run[\"predictions\"] = val_preds\n            exp_run[\"ground_truth\"] = val_gts\n            best_state = model.state_dict()\n\n    # ---- reload best and evaluate on test ----\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    _, test_f1, _, _ = evaluate(model, test_loader, criterion)\n    exp_run[\"test_f1\"] = test_f1\n    print(\n        f\"Finished hidden size {hs}: Best Dev F1 = {exp_run['best_dev_f1']:.4f} | Test F1 = {test_f1:.4f}\"\n    )\n\n    # store\n    experiment_data[\"hidden_size\"][\"SPR_BENCH\"][run_key] = exp_run\n\n# ---------------------- save everything -------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ------------ experiment_data dict -------------\nexperiment_data = {\n    \"dropout_prob\": {\"SPR_BENCH\": {}}  # will be filled with one sub-dict per p\n}\n\n# ------------ device ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------ small helpers --------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed()\n\n\n# ------------ load SPR_BENCH or synth ----------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for i in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"], dsets[\"dev\"], dsets[\"test\"] = (\n        synth_split(512),\n        synth_split(128),\n        synth_split(128),\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ------------ vocab / encoding -----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        vocab.setdefault(ch, len(vocab))\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ------------- collator ------------------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ------------- model definition ----------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128, dropout_p=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(p=dropout_p)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        pooled = self.drop(pooled)\n        return self.fc(pooled)\n\n\n# ------------- training & tuning ---------------\ndropout_vals = [0.0, 0.2, 0.4, 0.6]\nepochs = 5\nbest_overall_f1, best_overall_p, best_state = 0.0, None, None\n\nfor p in dropout_vals:\n    print(f\"\\n=== training with dropout_p={p} ===\")\n    exp_dict = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    experiment_data[\"dropout_prob\"][\"SPR_BENCH\"][str(p)] = exp_dict\n\n    model = SPRClassifier(len(vocab), dropout_p=p).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    best_f1 = 0.0\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n        # ---- eval ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n        print(\n            f\"  Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_macroF1={dev_f1:.4f}\"\n        )\n        # record\n        exp_dict[\"metrics\"][\"train\"].append(train_f1)\n        exp_dict[\"metrics\"][\"val\"].append(dev_f1)\n        exp_dict[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        exp_dict[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        exp_dict[\"epochs\"].append(epoch)\n        # keep best within this p\n        if dev_f1 > best_f1:\n            best_f1 = dev_f1\n            exp_dict[\"predictions\"] = dv_preds\n            exp_dict[\"ground_truth\"] = dv_gts\n            # also check global best\n            if dev_f1 > best_overall_f1:\n                best_overall_f1, best_overall_p = dev_f1, p\n                best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n    # cleanup\n    del model\n    torch.cuda.empty_cache()\n\nprint(\n    f\"\\nBest dev macro-F1 = {best_overall_f1:.4f} achieved with dropout_p={best_overall_p}\"\n)\n\n# ------------- test evaluation of best ----------\nbest_model = SPRClassifier(len(vocab), dropout_p=best_overall_p).to(device)\nbest_model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\nbest_model.eval()\ntest_preds, test_gts = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = best_model(batch[\"input_ids\"])\n        test_preds.extend(logits.argmax(1).cpu().numpy())\n        test_gts.extend(batch[\"labels\"].cpu().numpy())\ntest_f1 = f1_score(test_gts, test_preds, average=\"macro\")\nprint(f\"Test macro-F1 with best dropout_p={best_overall_p}: {test_f1:.4f}\")\n\n# ------------- save experiment data -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ---------------- paths & dirs --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device --------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- dataset -------------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # ------------ synthetic fallback -------------\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"], dsets[\"dev\"], dsets[\"test\"] = (\n        synth_split(512),\n        synth_split(128),\n        synth_split(128),\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------------- vocab & encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ---------------- collate fn ----------------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------- model def -----------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ---------------- experiment store ----------------\nexperiment_data = {\"embed_dim\": {}}\n\n# ---------------- hyper-parameter grid ------------\nembed_dims = [32, 64, 128, 256]\noverall_best_f1, overall_best_cfg, overall_best_preds, overall_best_gts = (\n    -1,\n    None,\n    None,\n    None,\n)\n\n# ---------------- training loop per embed_dim -----\nepochs = 5\nfor dim in embed_dims:\n    print(f\"\\n===== Training with embed_dim={dim} =====\")\n    model = SPRClassifier(vocab_size, embed_dim=dim).to(device)\n    crit = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    edata = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    best_dev_f1, best_dev_preds, best_dev_gts = -1, None, None\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            opt.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"labels\"])\n            loss.backward()\n            opt.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- dev ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = crit(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n\n        print(\n            f\"Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}  \"\n            f\"val_loss={np.mean(dv_losses):.4f}  val_macroF1={dev_f1:.4f}\"\n        )\n\n        # store epoch data\n        edata[\"metrics\"][\"train\"].append(train_f1)\n        edata[\"metrics\"][\"val\"].append(dev_f1)\n        edata[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        edata[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        edata[\"epochs\"].append(epoch)\n\n        # keep best epoch per dim\n        if dev_f1 > best_dev_f1:\n            best_dev_f1, best_dev_preds, best_dev_gts = (\n                dev_f1,\n                dv_preds.copy(),\n                dv_gts.copy(),\n            )\n\n    # save best preds/gts for this dim\n    edata[\"predictions\"] = best_dev_preds\n    edata[\"ground_truth\"] = best_dev_gts\n    experiment_data[\"embed_dim\"][dim] = edata\n\n    # track best overall cfg\n    if best_dev_f1 > overall_best_f1:\n        overall_best_f1 = best_dev_f1\n        overall_best_cfg = (dim, model)  # keep model ref for test\n        overall_best_preds, overall_best_gts = best_dev_preds, best_dev_gts\n\n# ---------------- final test on best cfg ----------\nbest_dim, best_model = overall_best_cfg\nprint(f\"\\n*** Best embed_dim={best_dim} with Dev Macro-F1={overall_best_f1:.4f} ***\")\nbest_model.eval()\ntest_preds, test_gts = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = best_model(batch[\"input_ids\"])\n        test_preds.extend(logits.argmax(1).cpu().numpy())\n        test_gts.extend(batch[\"labels\"].cpu().numpy())\ntest_f1 = f1_score(test_gts, test_preds, average=\"macro\")\nprint(f\"Test Macro-F1 with best embed_dim={best_dim}: {test_f1:.4f}\")\n\n# ---------------- save experiment data ------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ---------- utils / dirs ------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:\n    # ---------- synthetic fallback ----\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            lbl = int(seq.count(\"#\") % 2 == 0)\n            seqs.append(seq)\n            labels.append(lbl)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict()\n    dsets[\"train\"], dsets[\"dev\"], dsets[\"test\"] = (\n        synth_split(512),\n        synth_split(128),\n        synth_split(128),\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------- vocab & encoding ---------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):  # char -> ids\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ---------- dataloaders --------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---------- model --------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128, num_layers=1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim,\n            hidden,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n        )\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ---------- experiment_data ----------\nexperiment_data = {\"num_lstm_layers\": {\"SPR_BENCH\": {}}}\n\n\n# ---------- training / evaluation ----\ndef train_eval(num_layers, epochs=5, lr=1e-3):\n    model = SPRClassifier(vocab_size, num_layers=num_layers).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    best_f1, best_preds, best_gts = 0.0, None, None\n\n    for ep in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            optim.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optim.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        tr_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- dev ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: (v.to(device) if torch.is_tensor(v) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dv_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n\n        print(\n            f\"[layers={num_layers}] Epoch {ep}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_macroF1={dv_f1:.4f}\"\n        )\n\n        # logging\n        logs[\"metrics\"][\"train\"].append(tr_f1)\n        logs[\"metrics\"][\"val\"].append(dv_f1)\n        logs[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        logs[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        logs[\"epochs\"].append(ep)\n        if dv_f1 > best_f1:\n            best_f1, best_preds, best_gts = dv_f1, dv_preds.copy(), dv_gts.copy()\n\n    # ---- test with best model (simple retraining not saved; evaluate last weights) ----\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    tst_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    print(f\"[layers={num_layers}] Best Dev F1={best_f1:.4f} | Test F1={tst_f1:.4f}\")\n\n    logs[\"predictions\"], logs[\"ground_truth\"] = best_preds, best_gts\n    logs[\"best_dev_f1\"], logs[\"test_f1\"] = best_f1, tst_f1\n    return logs, best_f1\n\n\n# ---------- hyperparameter tuning ----\nlayer_options = [1, 2, 3]\nbest_overall = -1.0\nbest_setting = None\nfor n_layers in layer_options:\n    result, dev_f1 = train_eval(n_layers)\n    experiment_data[\"num_lstm_layers\"][\"SPR_BENCH\"][str(n_layers)] = result\n    if dev_f1 > best_overall:\n        best_overall, best_setting = dev_f1, n_layers\n\nprint(f\"\\nBest setting: num_layers={best_setting} with Dev F1={best_overall:.4f}\")\n\n# ---------- save experiment data ----\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, random, string, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ----------------- misc setup ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ----------------- load SPR_BENCH ------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # fallback synthetic data\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict(\n        {\"train\": synth_split(512), \"dev\": synth_split(128), \"test\": synth_split(128)}\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ----------------- vocab & encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ----------------- dataloader helpers --------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------- model ---------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ----------------- hyper-parameter sweep -----------\nweight_decays = [0.0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\nepochs = 5\n\nexperiment_data = {\"weight_decay\": {\"SPR_BENCH\": {}}}\n\nfor wd in weight_decays:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"test_f1\": None,\n    }\n\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    best_f1, best_preds, best_gts = 0.0, [], []\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- eval ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n        print(\n            f\"[wd={wd:.0e}] Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_F1={dev_f1:.4f}\"\n        )\n\n        # ---- log ----\n        dstore = experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key]\n        dstore[\"metrics\"][\"train\"].append(train_f1)\n        dstore[\"metrics\"][\"val\"].append(dev_f1)\n        dstore[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        dstore[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        dstore[\"epochs\"].append(epoch)\n\n        # best preds\n        if dev_f1 > best_f1:\n            best_f1 = dev_f1\n            best_preds, best_gts = dv_preds, dv_gts\n\n    # ---- final test evaluation ----\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    print(f\"[wd={wd:.0e}] Best Dev F1={best_f1:.4f} | Test F1={test_f1:.4f}\")\n\n    # ---- save best preds ----\n    dstore[\"predictions\"] = best_preds\n    dstore[\"ground_truth\"] = best_gts\n    dstore[\"test_f1\"] = test_f1\n\n# ----------------- save ----------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"All experiment data saved.\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ----------------- misc setup ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ----------------- load SPR_BENCH ------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # fallback synthetic data\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict(\n        {\"train\": synth_split(512), \"dev\": synth_split(128), \"test\": synth_split(128)}\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ----------------- vocab & encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ----------------- dataloader helpers --------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------- model ---------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ----------------- hyper-parameter sweep -----------\nweight_decays = [0.0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\nepochs = 5\n\nexperiment_data = {\"weight_decay\": {\"SPR_BENCH\": {}}}\n\nfor wd in weight_decays:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"test_f1\": None,\n    }\n\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    best_f1, best_preds, best_gts = 0.0, [], []\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- eval ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n        print(\n            f\"[wd={wd:.0e}] Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_F1={dev_f1:.4f}\"\n        )\n\n        # ---- log ----\n        dstore = experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key]\n        dstore[\"metrics\"][\"train\"].append(train_f1)\n        dstore[\"metrics\"][\"val\"].append(dev_f1)\n        dstore[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        dstore[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        dstore[\"epochs\"].append(epoch)\n\n        # best preds\n        if dev_f1 > best_f1:\n            best_f1 = dev_f1\n            best_preds, best_gts = dv_preds, dv_gts\n\n    # ---- final test evaluation ----\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    print(f\"[wd={wd:.0e}] Best Dev F1={best_f1:.4f} | Test F1={test_f1:.4f}\")\n\n    # ---- save best preds ----\n    dstore[\"predictions\"] = best_preds\n    dstore[\"ground_truth\"] = best_gts\n    dstore[\"test_f1\"] = test_f1\n\n# ----------------- save ----------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"All experiment data saved.\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ----------------- misc setup ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ----------------- load SPR_BENCH ------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # fallback synthetic data\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict(\n        {\"train\": synth_split(512), \"dev\": synth_split(128), \"test\": synth_split(128)}\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ----------------- vocab & encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ----------------- dataloader helpers --------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------- model ---------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ----------------- hyper-parameter sweep -----------\nweight_decays = [0.0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\nepochs = 5\n\nexperiment_data = {\"weight_decay\": {\"SPR_BENCH\": {}}}\n\nfor wd in weight_decays:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"test_f1\": None,\n    }\n\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    best_f1, best_preds, best_gts = 0.0, [], []\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- eval ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n        print(\n            f\"[wd={wd:.0e}] Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_F1={dev_f1:.4f}\"\n        )\n\n        # ---- log ----\n        dstore = experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key]\n        dstore[\"metrics\"][\"train\"].append(train_f1)\n        dstore[\"metrics\"][\"val\"].append(dev_f1)\n        dstore[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        dstore[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        dstore[\"epochs\"].append(epoch)\n\n        # best preds\n        if dev_f1 > best_f1:\n            best_f1 = dev_f1\n            best_preds, best_gts = dv_preds, dv_gts\n\n    # ---- final test evaluation ----\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    print(f\"[wd={wd:.0e}] Best Dev F1={best_f1:.4f} | Test F1={test_f1:.4f}\")\n\n    # ---- save best preds ----\n    dstore[\"predictions\"] = best_preds\n    dstore[\"ground_truth\"] = best_gts\n    dstore[\"test_f1\"] = test_f1\n\n# ----------------- save ----------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"All experiment data saved.\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ----------------- misc setup ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ----------------- load SPR_BENCH ------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_path.exists():\n    dsets = load_spr_bench(data_path)\nelse:  # fallback synthetic data\n    from datasets import Dataset, DatasetDict\n\n    def synth_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            seq = \"\".join(\n                random.choices(list(string.ascii_lowercase) + [\"#\", \"@\", \"&\"], k=L)\n            )\n            labels.append(int(seq.count(\"#\") % 2 == 0))\n            seqs.append(seq)\n        return Dataset.from_dict(\n            {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n        )\n\n    dsets = DatasetDict(\n        {\"train\": synth_split(512), \"dev\": synth_split(128), \"test\": synth_split(128)}\n    )\nprint({k: len(v) for k, v in dsets.items()})\n\n# ----------------- vocab & encoding ----------------\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab.get(ch, vocab[UNK]) for ch in seq]\n\n\nfor split in dsets:\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"])}, remove_columns=[\"sequence\"]\n    )\n\n\n# ----------------- dataloader helpers --------------\ndef collate(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in ids)\n    padded = torch.full((len(ids), max_len), vocab[PAD], dtype=torch.long)\n    for i, seq in enumerate(ids):\n        padded[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n    return {\"input_ids\": padded, \"labels\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dsets[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------- model ---------------------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 2)\n\n    def forward(self, x):\n        em = self.emb(x)\n        out, _ = self.lstm(em)\n        pooled, _ = torch.max(out, dim=1)\n        return self.fc(pooled)\n\n\n# ----------------- hyper-parameter sweep -----------\nweight_decays = [0.0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\nepochs = 5\n\nexperiment_data = {\"weight_decay\": {\"SPR_BENCH\": {}}}\n\nfor wd in weight_decays:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"test_f1\": None,\n    }\n\n    model = SPRClassifier(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    best_f1, best_preds, best_gts = 0.0, [], []\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_losses, tr_preds, tr_gts = [], [], []\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_gts.extend(batch[\"labels\"].cpu().numpy())\n        train_f1 = f1_score(tr_gts, tr_preds, average=\"macro\")\n\n        # ---- eval ----\n        model.eval()\n        dv_losses, dv_preds, dv_gts = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                dv_losses.append(loss.item())\n                dv_preds.extend(logits.argmax(1).cpu().numpy())\n                dv_gts.extend(batch[\"labels\"].cpu().numpy())\n        dev_f1 = f1_score(dv_gts, dv_preds, average=\"macro\")\n        print(\n            f\"[wd={wd:.0e}] Epoch {epoch}: train_loss={np.mean(tr_losses):.4f}, \"\n            f\"val_loss={np.mean(dv_losses):.4f}, val_F1={dev_f1:.4f}\"\n        )\n\n        # ---- log ----\n        dstore = experiment_data[\"weight_decay\"][\"SPR_BENCH\"][key]\n        dstore[\"metrics\"][\"train\"].append(train_f1)\n        dstore[\"metrics\"][\"val\"].append(dev_f1)\n        dstore[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        dstore[\"losses\"][\"val\"].append(np.mean(dv_losses))\n        dstore[\"epochs\"].append(epoch)\n\n        # best preds\n        if dev_f1 > best_f1:\n            best_f1 = dev_f1\n            best_preds, best_gts = dv_preds, dv_gts\n\n    # ---- final test evaluation ----\n    model.eval()\n    tst_preds, tst_gts = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend(logits.argmax(1).cpu().numpy())\n            tst_gts.extend(batch[\"labels\"].cpu().numpy())\n    test_f1 = f1_score(tst_gts, tst_preds, average=\"macro\")\n    print(f\"[wd={wd:.0e}] Best Dev F1={best_f1:.4f} | Test F1={test_f1:.4f}\")\n\n    # ---- save best preds ----\n    dstore[\"predictions\"] = best_preds\n    dstore[\"ground_truth\"] = best_gts\n    dstore[\"test_f1\"] = test_f1\n\n# ----------------- save ----------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"All experiment data saved.\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 108933.06\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 89994.94\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 146418.49\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', '\\rMap:\n0%|          | 0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 22467.40 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 23516.47 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00,\n24857.93 examples/s]', '\\n', 'Epoch 1: train_loss=0.6754, val_loss=0.6559,\nval_macroF1=0.6780', '\\n', 'Epoch 2: train_loss=0.6353, val_loss=0.6525,\nval_macroF1=0.6779', '\\n', 'Epoch 3: train_loss=0.6258, val_loss=0.6381,\nval_macroF1=0.6760', '\\n', 'Epoch 4: train_loss=0.6164, val_loss=0.6362,\nval_macroF1=0.6779', '\\n', 'Epoch 5: train_loss=0.6121, val_loss=0.6347,\nval_macroF1=0.6860', '\\n', 'Best Dev Macro-F1 = 0.6860 | Test Macro-F1 =\n0.6948', '\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n143439.14 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 106546.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 183269.42\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', '\\rMap:\n0%|          | 0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 22640.03 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 23418.00 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00,\n24935.52 examples/s]', '\\n', '[epochs=5] Epoch 1: train_loss=0.6754\ndev_loss=0.6559 dev_F1=0.6780', '\\n', '[epochs=5] Epoch 2: train_loss=0.6353\ndev_loss=0.6525 dev_F1=0.6779', '\\n', '[epochs=5] Epoch 3: train_loss=0.6258\ndev_loss=0.6381 dev_F1=0.6760', '\\n', '[epochs=5] Epoch 4: train_loss=0.6164\ndev_loss=0.6362 dev_F1=0.6779', '\\n', 'Early stopping triggered', '\\n',\n'Finished setting epochs_5: best_dev_F1=0.6780, test_F1=0.6908', '\\n',\n'[epochs=10] Epoch 1: train_loss=0.6913 dev_loss=0.6838 dev_F1=0.3316', '\\n',\n'[epochs=10] Epoch 2: train_loss=0.6704 dev_loss=0.6533 dev_F1=0.6820', '\\n',\n'[epochs=10] Epoch 3: train_loss=0.6249 dev_loss=0.6388 dev_F1=0.6670', '\\n',\n'[epochs=10] Epoch 4: train_loss=0.6139 dev_loss=0.6290 dev_F1=0.6816', '\\n',\n'[epochs=10] Epoch 5: train_loss=0.6175 dev_loss=0.6351 dev_F1=0.6860', '\\n',\n'[epochs=10] Epoch 6: train_loss=0.6125 dev_loss=0.6446 dev_F1=0.6540', '\\n',\n'[epochs=10] Epoch 7: train_loss=0.6100 dev_loss=0.6323 dev_F1=0.6900', '\\n',\n'[epochs=10] Epoch 8: train_loss=0.6028 dev_loss=0.6396 dev_F1=0.6645', '\\n',\n'[epochs=10] Epoch 9: train_loss=0.6036 dev_loss=0.6294 dev_F1=0.6880', '\\n',\n'[epochs=10] Epoch 10: train_loss=0.5956 dev_loss=0.6321 dev_F1=0.6817', '\\n',\n'Early stopping triggered', '\\n', 'Finished setting epochs_10:\nbest_dev_F1=0.6900, test_F1=0.6915', '\\n', '[epochs=15] Epoch 1:\ntrain_loss=0.6755 dev_loss=0.6547 dev_F1=0.6665', '\\n', '[epochs=15] Epoch 2:\ntrain_loss=0.6291 dev_loss=0.6382 dev_F1=0.6859', '\\n', '[epochs=15] Epoch 3:\ntrain_loss=0.6221 dev_loss=0.6312 dev_F1=0.6899', '\\n', '[epochs=15] Epoch 4:\ntrain_loss=0.6171 dev_loss=0.6403 dev_F1=0.6920', '\\n', '[epochs=15] Epoch 5:\ntrain_loss=0.6121 dev_loss=0.6335 dev_F1=0.6900', '\\n', '[epochs=15] Epoch 6:\ntrain_loss=0.6081 dev_loss=0.6373 dev_F1=0.6774', '\\n', '[epochs=15] Epoch 7:\ntrain_loss=0.6061 dev_loss=0.6360 dev_F1=0.6733', '\\n', 'Early stopping\ntriggered', '\\n', 'Finished setting epochs_15: best_dev_F1=0.6920,\ntest_F1=0.6932', '\\n', '[epochs=20] Epoch 1: train_loss=0.6878 dev_loss=0.6765\ndev_F1=0.5495', '\\n', '[epochs=20] Epoch 2: train_loss=0.6537 dev_loss=0.6412\ndev_F1=0.6820', '\\n', '[epochs=20] Epoch 3: train_loss=0.6277 dev_loss=0.6434\ndev_F1=0.6740', '\\n', '[epochs=20] Epoch 4: train_loss=0.6189 dev_loss=0.6367\ndev_F1=0.6779', '\\n', '[epochs=20] Epoch 5: train_loss=0.6116 dev_loss=0.6350\ndev_F1=0.6859', '\\n', '[epochs=20] Epoch 6: train_loss=0.6054 dev_loss=0.6350\ndev_F1=0.6879', '\\n', '[epochs=20] Epoch 7: train_loss=0.6021 dev_loss=0.6448\ndev_F1=0.6690', '\\n', '[epochs=20] Epoch 8: train_loss=0.6002 dev_loss=0.6397\ndev_F1=0.6755', '\\n', '[epochs=20] Epoch 9: train_loss=0.5986 dev_loss=0.6393\ndev_F1=0.6898', '\\n', '[epochs=20] Epoch 10: train_loss=0.5957 dev_loss=0.6439\ndev_F1=0.6795', '\\n', '[epochs=20] Epoch 11: train_loss=0.5948 dev_loss=0.6608\ndev_F1=0.6434', '\\n', '[epochs=20] Epoch 12: train_loss=0.5921 dev_loss=0.6434\ndev_F1=0.6838', '\\n', 'Early stopping triggered', '\\n', 'Finished setting\nepochs_20: best_dev_F1=0.6898, test_F1=0.6815', '\\n', 'All results saved to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-48-\n06_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 59014.44\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 51408.34\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 141872.01\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', '\\rMap:\n0%|          | 0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 20999.24 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 23173.72 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00,\n25185.12 examples/s]', '\\n', '\\n=== Training with learning rate 0.0005 ===',\n'\\n', 'Epoch 1: lr=0.0005 train_loss=0.6843 val_loss=0.6759 val_macroF1=0.5722',\n'\\n', 'Epoch 2: lr=0.0005 train_loss=0.6610 val_loss=0.6535 val_macroF1=0.6760',\n'\\n', 'Epoch 3: lr=0.0005 train_loss=0.6332 val_loss=0.6452 val_macroF1=0.6820',\n'\\n', 'Epoch 4: lr=0.0005 train_loss=0.6268 val_loss=0.6422 val_macroF1=0.6820',\n'\\n', 'Epoch 5: lr=0.0005 train_loss=0.6188 val_loss=0.6406 val_macroF1=0.6820',\n'\\n', 'LR 0.0005: Best Dev F1 = 0.6820 | Test F1 = 0.6870', '\\n', '\\n===\nTraining with learning rate 0.001 ===', '\\n', 'Epoch 1: lr=0.001\ntrain_loss=0.6859 val_loss=0.6704 val_macroF1=0.6358', '\\n', 'Epoch 2: lr=0.001\ntrain_loss=0.6509 val_loss=0.6559 val_macroF1=0.6182', '\\n', 'Epoch 3: lr=0.001\ntrain_loss=0.6252 val_loss=0.6290 val_macroF1=0.6695', '\\n', 'Epoch 4: lr=0.001\ntrain_loss=0.6217 val_loss=0.6366 val_macroF1=0.6800', '\\n', 'Epoch 5: lr=0.001\ntrain_loss=0.6196 val_loss=0.6504 val_macroF1=0.6561', '\\n', 'LR 0.001: Best Dev\nF1 = 0.6800 | Test F1 = 0.6624', '\\n', '\\n=== Training with learning rate 0.002\n===', '\\n', 'Epoch 1: lr=0.002 train_loss=0.6949 val_loss=0.6732\nval_macroF1=0.6653', '\\n', 'Epoch 2: lr=0.002 train_loss=0.6473 val_loss=0.6345\nval_macroF1=0.6899', '\\n', 'Epoch 3: lr=0.002 train_loss=0.6205 val_loss=0.6308\nval_macroF1=0.6797', '\\n', 'Epoch 4: lr=0.002 train_loss=0.6069 val_loss=0.6348\nval_macroF1=0.6940', '\\n', 'Epoch 5: lr=0.002 train_loss=0.6025 val_loss=0.6326\nval_macroF1=0.6819', '\\n', 'LR 0.002: Best Dev F1 = 0.6940 | Test F1 = 0.6927',\n'\\n', '\\n=== Training with learning rate 0.005 ===', '\\n', 'Epoch 1: lr=0.005\ntrain_loss=0.7175 val_loss=0.6691 val_macroF1=0.6779', '\\n', 'Epoch 2: lr=0.005\ntrain_loss=0.6401 val_loss=0.6366 val_macroF1=0.6698', '\\n', 'Epoch 3: lr=0.005\ntrain_loss=0.6154 val_loss=0.6364 val_macroF1=0.6840', '\\n', 'Epoch 4: lr=0.005\ntrain_loss=0.6104 val_loss=0.6372 val_macroF1=0.6840', '\\n', 'Epoch 5: lr=0.005\ntrain_loss=0.6085 val_loss=0.6302 val_macroF1=0.6799', '\\n', 'LR 0.005: Best Dev\nF1 = 0.6840 | Test F1 = 0.6897', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-48-\n06_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 129900.86\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 108937.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 130011.59\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', '\\rMap:\n0%|          | 0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 23442.28 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 23545.52 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00,\n24537.85 examples/s]', '\\n', '[bs=32] Epoch 1: train_loss=0.6662,\nval_loss=0.6511, val_macroF1=0.6373', '\\n', '[bs=32] Epoch 2: train_loss=0.6259,\nval_loss=0.6362, val_macroF1=0.6900', '\\n', '[bs=32] Epoch 3: train_loss=0.6146,\nval_loss=0.6437, val_macroF1=0.6880', '\\n', '[bs=32] Epoch 4: train_loss=0.6103,\nval_loss=0.6372, val_macroF1=0.6839', '\\n', '[bs=32] Epoch 5: train_loss=0.6081,\nval_loss=0.6338, val_macroF1=0.6838', '\\n', '[bs=32] Best Dev Macro-F1=0.6900 |\nTest Macro-F1=0.6916', '\\n', '[bs=64] Epoch 1: train_loss=0.6854,\nval_loss=0.6577, val_macroF1=0.6509', '\\n', '[bs=64] Epoch 2: train_loss=0.6370,\nval_loss=0.6322, val_macroF1=0.6794', '\\n', '[bs=64] Epoch 3: train_loss=0.6197,\nval_loss=0.6463, val_macroF1=0.6655', '\\n', '[bs=64] Epoch 4: train_loss=0.6171,\nval_loss=0.6285, val_macroF1=0.6813', '\\n', '[bs=64] Epoch 5: train_loss=0.6107,\nval_loss=0.6287, val_macroF1=0.6838', '\\n', '[bs=64] Best Dev Macro-F1=0.6838 |\nTest Macro-F1=0.6915', '\\n', '[bs=128] Epoch 1: train_loss=0.6866,\nval_loss=0.6713, val_macroF1=0.6350', '\\n', '[bs=128] Epoch 2:\ntrain_loss=0.6464, val_loss=0.6413, val_macroF1=0.6698', '\\n', '[bs=128] Epoch\n3: train_loss=0.6262, val_loss=0.6344, val_macroF1=0.6879', '\\n', '[bs=128]\nEpoch 4: train_loss=0.6135, val_loss=0.6509, val_macroF1=0.6714', '\\n',\n'[bs=128] Epoch 5: train_loss=0.6097, val_loss=0.6382, val_macroF1=0.6797',\n'\\n', '[bs=128] Best Dev Macro-F1=0.6879 | Test Macro-F1=0.6845', '\\n',\n'Execution time: 6 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 135302.31\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 97464.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 183373.58\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', '\\rMap:\n0%|          | 0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 24126.90 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 23339.03 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00,\n25173.63 examples/s]', '\\n', '\\n=== Training with hidden size 64 ===', '\\n',\n'Epoch 1: train_loss=0.6878, val_loss=0.6778, val_F1=0.6210', '\\n', 'Epoch 2:\ntrain_loss=0.6640, val_loss=0.6554, val_F1=0.6568', '\\n', 'Epoch 3:\ntrain_loss=0.6358, val_loss=0.6429, val_F1=0.6542', '\\n', 'Epoch 4:\ntrain_loss=0.6213, val_loss=0.6392, val_F1=0.6632', '\\n', 'Epoch 5:\ntrain_loss=0.6151, val_loss=0.6321, val_F1=0.6716', '\\n', 'Finished hidden size\n64: Best Dev F1 = 0.6716 | Test F1 = 0.6905', '\\n', '\\n=== Training with hidden\nsize 128 ===', '\\n', 'Epoch 1: train_loss=0.6845, val_loss=0.6707,\nval_F1=0.6488', '\\n', 'Epoch 2: train_loss=0.6451, val_loss=0.6366,\nval_F1=0.6714', '\\n', 'Epoch 3: train_loss=0.6205, val_loss=0.6322,\nval_F1=0.6879', '\\n', 'Epoch 4: train_loss=0.6163, val_loss=0.6388,\nval_F1=0.6960', '\\n', 'Epoch 5: train_loss=0.6063, val_loss=0.6323,\nval_F1=0.6879', '\\n', 'Finished hidden size 128: Best Dev F1 = 0.6960 | Test F1\n= 0.6958', '\\n', '\\n=== Training with hidden size 256 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6722, val_loss=0.6469, val_F1=0.6423', '\\n', 'Epoch 2:\ntrain_loss=0.6297, val_loss=0.6357, val_F1=0.6711', '\\n', 'Epoch 3:\ntrain_loss=0.6119, val_loss=0.6304, val_F1=0.6919', '\\n', 'Epoch 4:\ntrain_loss=0.6177, val_loss=0.6306, val_F1=0.6858', '\\n', 'Epoch 5:\ntrain_loss=0.6114, val_loss=0.6376, val_F1=0.6920', '\\n', 'Finished hidden size\n256: Best Dev F1 = 0.6920 | Test F1 = 0.6939', '\\n', '\\n=== Training with hidden\nsize 512 ===', '\\n', 'Epoch 1: train_loss=0.6835, val_loss=0.6443,\nval_F1=0.6588', '\\n', 'Epoch 2: train_loss=0.6614, val_loss=0.6682,\nval_F1=0.5481', '\\n', 'Epoch 3: train_loss=0.6425, val_loss=0.6538,\nval_F1=0.6364', '\\n', 'Epoch 4: train_loss=0.6201, val_loss=0.6411,\nval_F1=0.6599', '\\n', 'Epoch 5: train_loss=0.6177, val_loss=0.6394,\nval_F1=0.6727', '\\n', 'Finished hidden size 512: Best Dev F1 = 0.6727 | Test F1\n= 0.6898', '\\n', 'Execution time: 11 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', '\\n=== training with dropout_p=0.0 ===', '\\n', '  Epoch 1:\ntrain_loss=0.6898, val_loss=0.6711, val_macroF1=0.5903', '\\n', '  Epoch 2:\ntrain_loss=0.6560, val_loss=0.6566, val_macroF1=0.6002', '\\n', '  Epoch 3:\ntrain_loss=0.6426, val_loss=0.6446, val_macroF1=0.6540', '\\n', '  Epoch 4:\ntrain_loss=0.6179, val_loss=0.6351, val_macroF1=0.6880', '\\n', '  Epoch 5:\ntrain_loss=0.6115, val_loss=0.6381, val_macroF1=0.6840', '\\n', '\\n=== training\nwith dropout_p=0.2 ===', '\\n', '  Epoch 1: train_loss=0.6913, val_loss=0.6763,\nval_macroF1=0.6880', '\\n', '  Epoch 2: train_loss=0.6622, val_loss=0.6478,\nval_macroF1=0.6509', '\\n', '  Epoch 3: train_loss=0.6210, val_loss=0.6496,\nval_macroF1=0.6717', '\\n', '  Epoch 4: train_loss=0.6179, val_loss=0.6275,\nval_macroF1=0.6898', '\\n', '  Epoch 5: train_loss=0.6122, val_loss=0.6286,\nval_macroF1=0.6838', '\\n', '\\n=== training with dropout_p=0.4 ===', '\\n', '\nEpoch 1: train_loss=0.6793, val_loss=0.6645, val_macroF1=0.6718', '\\n', '  Epoch\n2: train_loss=0.6434, val_loss=0.6381, val_macroF1=0.6815', '\\n', '  Epoch 3:\ntrain_loss=0.6199, val_loss=0.6354, val_macroF1=0.6755', '\\n', '  Epoch 4:\ntrain_loss=0.6234, val_loss=0.6598, val_macroF1=0.6495', '\\n', '  Epoch 5:\ntrain_loss=0.6281, val_loss=0.6420, val_macroF1=0.6820', '\\n', '\\n=== training\nwith dropout_p=0.6 ===', '\\n', '  Epoch 1: train_loss=0.6938, val_loss=0.6857,\nval_macroF1=0.3316', '\\n', '  Epoch 2: train_loss=0.6720, val_loss=0.6512,\nval_macroF1=0.6800', '\\n', '  Epoch 3: train_loss=0.6337, val_loss=0.6465,\nval_macroF1=0.6517', '\\n', '  Epoch 4: train_loss=0.6224, val_loss=0.6300,\nval_macroF1=0.6860', '\\n', '  Epoch 5: train_loss=0.6190, val_loss=0.6396,\nval_macroF1=0.6692', '\\n', '\\nBest dev macro-F1 = 0.6898 achieved with\ndropout_p=0.2', '\\n', 'Test macro-F1 with best dropout_p=0.2: 0.6966', '\\n',\n'Execution time: 10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'\\n===== Training with embed_dim=32 =====', '\\n', 'Epoch 1: train_loss=0.6878\nval_loss=0.6786  val_macroF1=0.6278', '\\n', 'Epoch 2: train_loss=0.6622\nval_loss=0.6517  val_macroF1=0.6460', '\\n', 'Epoch 3: train_loss=0.6300\nval_loss=0.6437  val_macroF1=0.6840', '\\n', 'Epoch 4: train_loss=0.6173\nval_loss=0.6291  val_macroF1=0.6858', '\\n', 'Epoch 5: train_loss=0.6158\nval_loss=0.6415  val_macroF1=0.6718', '\\n', '\\n===== Training with embed_dim=64\n=====', '\\n', 'Epoch 1: train_loss=0.6870  val_loss=0.6757  val_macroF1=0.5930',\n'\\n', 'Epoch 2: train_loss=0.6546  val_loss=0.6357  val_macroF1=0.6919', '\\n',\n'Epoch 3: train_loss=0.6228  val_loss=0.6333  val_macroF1=0.6793', '\\n', 'Epoch\n4: train_loss=0.6312  val_loss=0.6358  val_macroF1=0.6729', '\\n', 'Epoch 5:\ntrain_loss=0.6308  val_loss=0.6363  val_macroF1=0.6940', '\\n', '\\n===== Training\nwith embed_dim=128 =====', '\\n', 'Epoch 1: train_loss=0.6912  val_loss=0.6700\nval_macroF1=0.5943', '\\n', 'Epoch 2: train_loss=0.6465  val_loss=0.6571\nval_macroF1=0.6102', '\\n', 'Epoch 3: train_loss=0.6249  val_loss=0.6346\nval_macroF1=0.6900', '\\n', 'Epoch 4: train_loss=0.6123  val_loss=0.6287\nval_macroF1=0.6817', '\\n', 'Epoch 5: train_loss=0.6080  val_loss=0.6330\nval_macroF1=0.6732', '\\n', '\\n===== Training with embed_dim=256 =====', '\\n',\n'Epoch 1: train_loss=0.6793  val_loss=0.6557  val_macroF1=0.6358', '\\n', 'Epoch\n2: train_loss=0.6321  val_loss=0.6336  val_macroF1=0.6654', '\\n', 'Epoch 3:\ntrain_loss=0.6145  val_loss=0.6377  val_macroF1=0.6880', '\\n', 'Epoch 4:\ntrain_loss=0.6163  val_loss=0.6381  val_macroF1=0.6628', '\\n', 'Epoch 5:\ntrain_loss=0.6145  val_loss=0.6556  val_macroF1=0.6504', '\\n', '\\n*** Best\nembed_dim=64 with Dev Macro-F1=0.6940 ***', '\\n', 'Test Macro-F1 with best\nembed_dim=64: 0.6919', '\\n', 'Execution time: 6 seconds seconds (time limit is\n30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'[layers=1] Epoch 1: train_loss=0.6754, val_loss=0.6559, val_macroF1=0.6780',\n'\\n', '[layers=1] Epoch 2: train_loss=0.6353, val_loss=0.6525,\nval_macroF1=0.6779', '\\n', '[layers=1] Epoch 3: train_loss=0.6258,\nval_loss=0.6381, val_macroF1=0.6760', '\\n', '[layers=1] Epoch 4:\ntrain_loss=0.6164, val_loss=0.6362, val_macroF1=0.6779', '\\n', '[layers=1] Epoch\n5: train_loss=0.6121, val_loss=0.6347, val_macroF1=0.6860', '\\n', '[layers=1]\nBest Dev F1=0.6860 | Test F1=0.6948', '\\n', '[layers=2] Epoch 1:\ntrain_loss=0.6786, val_loss=0.6579, val_macroF1=0.6367', '\\n', '[layers=2] Epoch\n2: train_loss=0.6367, val_loss=0.6672, val_macroF1=0.6297', '\\n', '[layers=2]\nEpoch 3: train_loss=0.6243, val_loss=0.6578, val_macroF1=0.6697', '\\n',\n'[layers=2] Epoch 4: train_loss=0.6203, val_loss=0.6348, val_macroF1=0.6817',\n'\\n', '[layers=2] Epoch 5: train_loss=0.6093, val_loss=0.6309,\nval_macroF1=0.6880', '\\n', '[layers=2] Best Dev F1=0.6880 | Test F1=0.6967',\n'\\n', '[layers=3] Epoch 1: train_loss=0.6722, val_loss=0.6841,\nval_macroF1=0.6705', '\\n', '[layers=3] Epoch 2: train_loss=0.6398,\nval_loss=0.6383, val_macroF1=0.6659', '\\n', '[layers=3] Epoch 3:\ntrain_loss=0.6213, val_loss=0.6338, val_macroF1=0.6734', '\\n', '[layers=3] Epoch\n4: train_loss=0.6149, val_loss=0.6461, val_macroF1=0.6647', '\\n', '[layers=3]\nEpoch 5: train_loss=0.6131, val_loss=0.6369, val_macroF1=0.6617', '\\n',\n'[layers=3] Best Dev F1=0.6734 | Test F1=0.6738', '\\n', '\\nBest setting:\nnum_layers=2 with Dev F1=0.6880', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'[wd=0e+00] Epoch 1: train_loss=0.6754, val_loss=0.6559, val_F1=0.6780', '\\n',\n'[wd=0e+00] Epoch 2: train_loss=0.6353, val_loss=0.6525, val_F1=0.6779', '\\n',\n'[wd=0e+00] Epoch 3: train_loss=0.6258, val_loss=0.6381, val_F1=0.6760', '\\n',\n'[wd=0e+00] Epoch 4: train_loss=0.6164, val_loss=0.6362, val_F1=0.6779', '\\n',\n'[wd=0e+00] Epoch 5: train_loss=0.6121, val_loss=0.6347, val_F1=0.6860', '\\n',\n'[wd=0e+00] Best Dev F1=0.6860 | Test F1=0.6948', '\\n', '[wd=1e-05] Epoch 1:\ntrain_loss=0.6859, val_loss=0.6704, val_F1=0.6336', '\\n', '[wd=1e-05] Epoch 2:\ntrain_loss=0.6510, val_loss=0.6560, val_F1=0.6164', '\\n', '[wd=1e-05] Epoch 3:\ntrain_loss=0.6251, val_loss=0.6287, val_F1=0.6695', '\\n', '[wd=1e-05] Epoch 4:\ntrain_loss=0.6213, val_loss=0.6372, val_F1=0.6780', '\\n', '[wd=1e-05] Epoch 5:\ntrain_loss=0.6202, val_loss=0.6508, val_F1=0.6540', '\\n', '[wd=1e-05] Best Dev\nF1=0.6780 | Test F1=0.6603', '\\n', '[wd=5e-05] Epoch 1: train_loss=0.6899,\nval_loss=0.6814, val_F1=0.3437', '\\n', '[wd=5e-05] Epoch 2: train_loss=0.6608,\nval_loss=0.6453, val_F1=0.6563', '\\n', '[wd=5e-05] Epoch 3: train_loss=0.6228,\nval_loss=0.6332, val_F1=0.6878', '\\n', '[wd=5e-05] Epoch 4: train_loss=0.6115,\nval_loss=0.6343, val_F1=0.6920', '\\n', '[wd=5e-05] Epoch 5: train_loss=0.6079,\nval_loss=0.6318, val_F1=0.6839', '\\n', '[wd=5e-05] Best Dev F1=0.6920 | Test\nF1=0.6958', '\\n', '[wd=1e-04] Epoch 1: train_loss=0.6839, val_loss=0.6766,\nval_F1=0.3972', '\\n', '[wd=1e-04] Epoch 2: train_loss=0.6497, val_loss=0.6450,\nval_F1=0.6780', '\\n', '[wd=1e-04] Epoch 3: train_loss=0.6236, val_loss=0.6376,\nval_F1=0.6839', '\\n', '[wd=1e-04] Epoch 4: train_loss=0.6150, val_loss=0.6416,\nval_F1=0.6800', '\\n', '[wd=1e-04] Epoch 5: train_loss=0.6086, val_loss=0.6348,\nval_F1=0.6819', '\\n', '[wd=1e-04] Best Dev F1=0.6839 | Test F1=0.6918', '\\n',\n'[wd=5e-04] Epoch 1: train_loss=0.6853, val_loss=0.6785, val_F1=0.3316', '\\n',\n'[wd=5e-04] Epoch 2: train_loss=0.6534, val_loss=0.6430, val_F1=0.6818', '\\n',\n'[wd=5e-04] Epoch 3: train_loss=0.6229, val_loss=0.6384, val_F1=0.6900', '\\n',\n'[wd=5e-04] Epoch 4: train_loss=0.6120, val_loss=0.6349, val_F1=0.6960', '\\n',\n'[wd=5e-04] Epoch 5: train_loss=0.6087, val_loss=0.6368, val_F1=0.6900', '\\n',\n'[wd=5e-04] Best Dev F1=0.6960 | Test F1=0.6948', '\\n', '[wd=1e-03] Epoch 1:\ntrain_loss=0.6866, val_loss=0.6786, val_F1=0.4195', '\\n', '[wd=1e-03] Epoch 2:\ntrain_loss=0.6605, val_loss=0.6488, val_F1=0.6899', '\\n', '[wd=1e-03] Epoch 3:\ntrain_loss=0.6273, val_loss=0.6321, val_F1=0.6797', '\\n', '[wd=1e-03] Epoch 4:\ntrain_loss=0.6154, val_loss=0.6391, val_F1=0.6920', '\\n', '[wd=1e-03] Epoch 5:\ntrain_loss=0.6146, val_loss=0.6515, val_F1=0.6774', '\\n', '[wd=1e-03] Best Dev\nF1=0.6920 | Test F1=0.6806', '\\n', 'All experiment data saved.', '\\n',\n'Execution time: 13 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'[wd=0e+00] Epoch 1: train_loss=0.6838, val_loss=0.6740, val_F1=0.5265', '\\n',\n'[wd=0e+00] Epoch 2: train_loss=0.6496, val_loss=0.6676, val_F1=0.5508', '\\n',\n'[wd=0e+00] Epoch 3: train_loss=0.6283, val_loss=0.6693, val_F1=0.6419', '\\n',\n'[wd=0e+00] Epoch 4: train_loss=0.6293, val_loss=0.6692, val_F1=0.6395', '\\n',\n'[wd=0e+00] Epoch 5: train_loss=0.6216, val_loss=0.6581, val_F1=0.6676', '\\n',\n'[wd=0e+00] Best Dev F1=0.6676 | Test F1=0.6818', '\\n', '[wd=1e-05] Epoch 1:\ntrain_loss=0.6864, val_loss=0.6770, val_F1=0.3316', '\\n', '[wd=1e-05] Epoch 2:\ntrain_loss=0.6426, val_loss=0.6345, val_F1=0.6730', '\\n', '[wd=1e-05] Epoch 3:\ntrain_loss=0.6357, val_loss=0.6713, val_F1=0.5899', '\\n', '[wd=1e-05] Epoch 4:\ntrain_loss=0.6196, val_loss=0.6291, val_F1=0.6858', '\\n', '[wd=1e-05] Epoch 5:\ntrain_loss=0.6189, val_loss=0.6351, val_F1=0.6813', '\\n', '[wd=1e-05] Best Dev\nF1=0.6858 | Test F1=0.6992', '\\n', '[wd=5e-05] Epoch 1: train_loss=0.6867,\nval_loss=0.6714, val_F1=0.6350', '\\n', '[wd=5e-05] Epoch 2: train_loss=0.6468,\nval_loss=0.6416, val_F1=0.6718', '\\n', '[wd=5e-05] Epoch 3: train_loss=0.6264,\nval_loss=0.6346, val_F1=0.6899', '\\n', '[wd=5e-05] Epoch 4: train_loss=0.6135,\nval_loss=0.6516, val_F1=0.6672', '\\n', '[wd=5e-05] Epoch 5: train_loss=0.6100,\nval_loss=0.6379, val_F1=0.6797', '\\n', '[wd=5e-05] Best Dev F1=0.6899 | Test\nF1=0.6855', '\\n', '[wd=1e-04] Epoch 1: train_loss=0.6879, val_loss=0.6757,\nval_F1=0.6147', '\\n', '[wd=1e-04] Epoch 2: train_loss=0.6569, val_loss=0.6509,\nval_F1=0.6628', '\\n', '[wd=1e-04] Epoch 3: train_loss=0.6305, val_loss=0.6426,\nval_F1=0.6626', '\\n', '[wd=1e-04] Epoch 4: train_loss=0.6153, val_loss=0.6347,\nval_F1=0.6712', '\\n', '[wd=1e-04] Epoch 5: train_loss=0.6164, val_loss=0.6392,\nval_F1=0.6900', '\\n', '[wd=1e-04] Best Dev F1=0.6900 | Test F1=0.6900', '\\n',\n'[wd=5e-04] Epoch 1: train_loss=0.6869, val_loss=0.6732, val_F1=0.6366', '\\n',\n'[wd=5e-04] Epoch 2: train_loss=0.6495, val_loss=0.6499, val_F1=0.6759', '\\n',\n'[wd=5e-04] Epoch 3: train_loss=0.6262, val_loss=0.6395, val_F1=0.6628', '\\n',\n'[wd=5e-04] Epoch 4: train_loss=0.6248, val_loss=0.6351, val_F1=0.6819', '\\n',\n'[wd=5e-04] Epoch 5: train_loss=0.6153, val_loss=0.6373, val_F1=0.6754', '\\n',\n'[wd=5e-04] Best Dev F1=0.6819 | Test F1=0.6853', '\\n', '[wd=1e-03] Epoch 1:\ntrain_loss=0.6942, val_loss=0.6810, val_F1=0.6698', '\\n', '[wd=1e-03] Epoch 2:\ntrain_loss=0.6685, val_loss=0.6612, val_F1=0.6425', '\\n', '[wd=1e-03] Epoch 3:\ntrain_loss=0.6480, val_loss=0.6374, val_F1=0.6876', '\\n', '[wd=1e-03] Epoch 4:\ntrain_loss=0.6260, val_loss=0.6402, val_F1=0.6860', '\\n', '[wd=1e-03] Epoch 5:\ntrain_loss=0.6180, val_loss=0.6323, val_F1=0.6939', '\\n', '[wd=1e-03] Best Dev\nF1=0.6939 | Test F1=0.7008', '\\n', 'All experiment data saved.', '\\n',\n'Execution time: 14 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'[wd=0e+00] Epoch 1: train_loss=0.6957, val_loss=0.6816, val_F1=0.3359', '\\n',\n'[wd=0e+00] Epoch 2: train_loss=0.6625, val_loss=0.6458, val_F1=0.6674', '\\n',\n'[wd=0e+00] Epoch 3: train_loss=0.6349, val_loss=0.6572, val_F1=0.6672', '\\n',\n'[wd=0e+00] Epoch 4: train_loss=0.6191, val_loss=0.6383, val_F1=0.6800', '\\n',\n'[wd=0e+00] Epoch 5: train_loss=0.6101, val_loss=0.6344, val_F1=0.6800', '\\n',\n'[wd=0e+00] Best Dev F1=0.6800 | Test F1=0.6919', '\\n', '[wd=1e-05] Epoch 1:\ntrain_loss=0.6819, val_loss=0.6666, val_F1=0.6078', '\\n', '[wd=1e-05] Epoch 2:\ntrain_loss=0.6386, val_loss=0.6349, val_F1=0.6734', '\\n', '[wd=1e-05] Epoch 3:\ntrain_loss=0.6201, val_loss=0.6410, val_F1=0.6900', '\\n', '[wd=1e-05] Epoch 4:\ntrain_loss=0.6190, val_loss=0.6323, val_F1=0.6838', '\\n', '[wd=1e-05] Epoch 5:\ntrain_loss=0.6103, val_loss=0.6341, val_F1=0.6900', '\\n', '[wd=1e-05] Best Dev\nF1=0.6900 | Test F1=0.6960', '\\n', '[wd=5e-05] Epoch 1: train_loss=0.6821,\nval_loss=0.6603, val_F1=0.6900', '\\n', '[wd=5e-05] Epoch 2: train_loss=0.6438,\nval_loss=0.6523, val_F1=0.6814', '\\n', '[wd=5e-05] Epoch 3: train_loss=0.6221,\nval_loss=0.6320, val_F1=0.6798', '\\n', '[wd=5e-05] Epoch 4: train_loss=0.6125,\nval_loss=0.6277, val_F1=0.6879', '\\n', '[wd=5e-05] Epoch 5: train_loss=0.6126,\nval_loss=0.6385, val_F1=0.6649', '\\n', '[wd=5e-05] Best Dev F1=0.6900 | Test\nF1=0.6827', '\\n', '[wd=1e-04] Epoch 1: train_loss=0.6876, val_loss=0.6713,\nval_F1=0.6655', '\\n', '[wd=1e-04] Epoch 2: train_loss=0.6494, val_loss=0.6419,\nval_F1=0.6571', '\\n', '[wd=1e-04] Epoch 3: train_loss=0.6299, val_loss=0.6476,\nval_F1=0.6737', '\\n', '[wd=1e-04] Epoch 4: train_loss=0.6165, val_loss=0.6401,\nval_F1=0.6720', '\\n', '[wd=1e-04] Epoch 5: train_loss=0.6099, val_loss=0.6425,\nval_F1=0.6719', '\\n', '[wd=1e-04] Best Dev F1=0.6737 | Test F1=0.6828', '\\n',\n'[wd=5e-04] Epoch 1: train_loss=0.6948, val_loss=0.6780, val_F1=0.4059', '\\n',\n'[wd=5e-04] Epoch 2: train_loss=0.6593, val_loss=0.6455, val_F1=0.6610', '\\n',\n'[wd=5e-04] Epoch 3: train_loss=0.6300, val_loss=0.6631, val_F1=0.6750', '\\n',\n'[wd=5e-04] Epoch 4: train_loss=0.6219, val_loss=0.6331, val_F1=0.6838', '\\n',\n'[wd=5e-04] Epoch 5: train_loss=0.6112, val_loss=0.6349, val_F1=0.6799', '\\n',\n'[wd=5e-04] Best Dev F1=0.6838 | Test F1=0.6867', '\\n', '[wd=1e-03] Epoch 1:\ntrain_loss=0.6915, val_loss=0.6795, val_F1=0.3403', '\\n', '[wd=1e-03] Epoch 2:\ntrain_loss=0.6647, val_loss=0.6619, val_F1=0.5962', '\\n', '[wd=1e-03] Epoch 3:\ntrain_loss=0.6364, val_loss=0.6408, val_F1=0.6740', '\\n', '[wd=1e-03] Epoch 4:\ntrain_loss=0.6171, val_loss=0.6373, val_F1=0.6694', '\\n', '[wd=1e-03] Epoch 5:\ntrain_loss=0.6124, val_loss=0.6364, val_F1=0.6798', '\\n', '[wd=1e-03] Best Dev\nF1=0.6798 | Test F1=0.6886', '\\n', 'All experiment data saved.', '\\n',\n'Execution time: 8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'[wd=0e+00] Epoch 1: train_loss=0.6940, val_loss=0.6784, val_F1=0.6840', '\\n',\n'[wd=0e+00] Epoch 2: train_loss=0.6641, val_loss=0.6455, val_F1=0.6879', '\\n',\n'[wd=0e+00] Epoch 3: train_loss=0.6224, val_loss=0.6318, val_F1=0.6777', '\\n',\n'[wd=0e+00] Epoch 4: train_loss=0.6197, val_loss=0.6452, val_F1=0.6779', '\\n',\n'[wd=0e+00] Epoch 5: train_loss=0.6165, val_loss=0.6322, val_F1=0.6960', '\\n',\n'[wd=0e+00] Best Dev F1=0.6960 | Test F1=0.6978', '\\n', '[wd=1e-05] Epoch 1:\ntrain_loss=0.6856, val_loss=0.6712, val_F1=0.6703', '\\n', '[wd=1e-05] Epoch 2:\ntrain_loss=0.6465, val_loss=0.6354, val_F1=0.6734', '\\n', '[wd=1e-05] Epoch 3:\ntrain_loss=0.6250, val_loss=0.6420, val_F1=0.6900', '\\n', '[wd=1e-05] Epoch 4:\ntrain_loss=0.6124, val_loss=0.6446, val_F1=0.6920', '\\n', '[wd=1e-05] Epoch 5:\ntrain_loss=0.6147, val_loss=0.6361, val_F1=0.6694', '\\n', '[wd=1e-05] Best Dev\nF1=0.6920 | Test F1=0.6884', '\\n', '[wd=5e-05] Epoch 1: train_loss=0.6848,\nval_loss=0.6776, val_F1=0.3523', '\\n', '[wd=5e-05] Epoch 2: train_loss=0.6510,\nval_loss=0.6605, val_F1=0.5960', '\\n', '[wd=5e-05] Epoch 3: train_loss=0.6243,\nval_loss=0.6445, val_F1=0.6557', '\\n', '[wd=5e-05] Epoch 4: train_loss=0.6156,\nval_loss=0.6350, val_F1=0.6796', '\\n', '[wd=5e-05] Epoch 5: train_loss=0.6101,\nval_loss=0.6376, val_F1=0.6689', '\\n', '[wd=5e-05] Best Dev F1=0.6796 | Test\nF1=0.6871', '\\n', '[wd=1e-04] Epoch 1: train_loss=0.6791, val_loss=0.6631,\nval_F1=0.6860', '\\n', '[wd=1e-04] Epoch 2: train_loss=0.6417, val_loss=0.6648,\nval_F1=0.6666', '\\n', '[wd=1e-04] Epoch 3: train_loss=0.6172, val_loss=0.6384,\nval_F1=0.6647', '\\n', '[wd=1e-04] Epoch 4: train_loss=0.6158, val_loss=0.6344,\nval_F1=0.6920', '\\n', '[wd=1e-04] Epoch 5: train_loss=0.6099, val_loss=0.6316,\nval_F1=0.6879', '\\n', '[wd=1e-04] Best Dev F1=0.6920 | Test F1=0.6916', '\\n',\n'[wd=5e-04] Epoch 1: train_loss=0.6882, val_loss=0.6810, val_F1=0.3437', '\\n',\n'[wd=5e-04] Epoch 2: train_loss=0.6556, val_loss=0.6488, val_F1=0.6616', '\\n',\n'[wd=5e-04] Epoch 3: train_loss=0.6235, val_loss=0.6350, val_F1=0.6898', '\\n',\n'[wd=5e-04] Epoch 4: train_loss=0.6133, val_loss=0.6310, val_F1=0.6918', '\\n',\n'[wd=5e-04] Epoch 5: train_loss=0.6126, val_loss=0.6449, val_F1=0.6840', '\\n',\n'[wd=5e-04] Best Dev F1=0.6918 | Test F1=0.6960', '\\n', '[wd=1e-03] Epoch 1:\ntrain_loss=0.6825, val_loss=0.6763, val_F1=0.3403', '\\n', '[wd=1e-03] Epoch 2:\ntrain_loss=0.6491, val_loss=0.6408, val_F1=0.6755', '\\n', '[wd=1e-03] Epoch 3:\ntrain_loss=0.6205, val_loss=0.6430, val_F1=0.6840', '\\n', '[wd=1e-03] Epoch 4:\ntrain_loss=0.6260, val_loss=0.6341, val_F1=0.6837', '\\n', '[wd=1e-03] Epoch 5:\ntrain_loss=0.6167, val_loss=0.6378, val_F1=0.6712', '\\n', '[wd=1e-03] Best Dev\nF1=0.6840 | Test F1=0.6881', '\\n', 'All experiment data saved.', '\\n',\n'Execution time: 8 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The script executed successfully without any errors. It loaded the datasets,\ntrained the model, and evaluated it on the test set. The training and validation\nlosses decreased over epochs, and the model showed an improvement in F1 scores,\nindicating proper learning. The final results were also saved correctly.\nOverall, the implementation is functional and achieves its goals at this stage.", "The training script executed successfully without any errors or bugs. It\nperformed a hyperparameter sweep for different epoch configurations (5, 10, 15,\nand 20 epochs) and utilized early stopping to prevent overfitting. The best\nperformance was achieved with 15 epochs, resulting in a best development F1\nscore of 0.6920 and a test F1 score of 0.6932. The results were saved\nsuccessfully, and the execution time was well within the time limit. No issues\nwere identified.", "", "The training script executed successfully without any bugs. The model was\ntrained on synthetic data with three batch sizes (32, 64, 128) and evaluated\nusing Macro-F1 scores. The results showed consistent improvements in the\nvalidation F1 score across epochs, and the best test Macro-F1 scores were\nreasonable given the synthetic dataset. The execution time was efficient,\ncompleting in 6 seconds. No bugs were observed in the implementation or\nexecution.", "", "The execution output indicates that the training process ran successfully\nwithout any errors or bugs. The script experimented with different dropout\nprobabilities to optimize the model's performance. The best macro-F1 score on\nthe development set was achieved with a dropout probability of 0.2, and this\nconfiguration also performed well on the test set with a macro-F1 score of\n0.6966. The results suggest that the model is functioning as expected and that\nthe training and evaluation processes are working correctly.", "The training script executed successfully without any bugs. The training loop\niterated over different embedding dimensions and selected the best-performing\nconfiguration based on the validation Macro-F1 score. The final test Macro-F1\nscore achieved was 0.6919, which aligns with the expected behavior. No issues\nwere identified with the implementation or execution.", "", "The execution of the training script completed successfully without any errors\nor bugs. The script performed a hyperparameter sweep over different weight decay\nvalues and logged the corresponding performance metrics, including training\nloss, validation loss, validation F1 score, and test F1 score. The best test F1\nscore achieved was 0.6958 for a weight decay of 5e-05. All experiment data was\nsaved successfully. No issues were observed.", "The execution of the training script was successful, with no bugs or errors\nobserved. The script performed a hyperparameter sweep on the weight decay\nparameter, trained the SPRClassifier model, and evaluated it on the validation\nand test datasets. The results were logged, and the best test F1 score achieved\nwas 0.7008 with a weight decay of 1e-03. All experiment data was saved\nsuccessfully.", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score, which evaluates the balance between precision and recall for each class.", "data": [{"dataset_name": "training", "final_value": 0.6974, "best_value": 0.6974}, {"dataset_name": "validation", "final_value": 0.686, "best_value": 0.686}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error in predictions. Lower values indicate better performance.", "data": [{"dataset_name": "training", "final_value": 0.6121, "best_value": 0.6121}, {"dataset_name": "validation", "final_value": 0.6347, "best_value": 0.6347}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.699}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score calculated on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6815, "best_value": 0.6932}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5921, "best_value": 0.5921}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6434, "best_value": 0.6321}]}]}, {"metric_names": [{"metric_name": "Train F1 Score", "lower_is_better": false, "description": "F1 Score on the training dataset.", "data": [{"dataset_name": "lr_0.0005", "final_value": 0.6908, "best_value": 0.6908}, {"dataset_name": "lr_0.001", "final_value": 0.6885, "best_value": 0.6885}, {"dataset_name": "lr_0.002", "final_value": 0.7005, "best_value": 0.7005}, {"dataset_name": "lr_0.005", "final_value": 0.6914, "best_value": 0.6914}]}, {"metric_name": "Validation F1 Score", "lower_is_better": false, "description": "F1 Score on the validation dataset.", "data": [{"dataset_name": "lr_0.0005", "final_value": 0.682, "best_value": 0.682}, {"dataset_name": "lr_0.001", "final_value": 0.6561, "best_value": 0.68}, {"dataset_name": "lr_0.002", "final_value": 0.6819, "best_value": 0.694}, {"dataset_name": "lr_0.005", "final_value": 0.6799, "best_value": 0.684}]}, {"metric_name": "Test F1 Score", "lower_is_better": false, "description": "F1 Score on the test dataset.", "data": [{"dataset_name": "lr_0.0005", "final_value": 0.687, "best_value": 0.687}, {"dataset_name": "lr_0.001", "final_value": 0.6624, "best_value": 0.6624}, {"dataset_name": "lr_0.002", "final_value": 0.6927, "best_value": 0.6927}, {"dataset_name": "lr_0.005", "final_value": 0.6897, "best_value": 0.6897}]}, {"metric_name": "Train Loss", "lower_is_better": true, "description": "Loss on the training dataset.", "data": [{"dataset_name": "lr_0.0005", "final_value": 0.6188, "best_value": 0.6188}, {"dataset_name": "lr_0.001", "final_value": 0.6196, "best_value": 0.6196}, {"dataset_name": "lr_0.002", "final_value": 0.6025, "best_value": 0.6025}, {"dataset_name": "lr_0.005", "final_value": 0.6085, "best_value": 0.6085}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Loss on the validation dataset.", "data": [{"dataset_name": "lr_0.0005", "final_value": 0.6406, "best_value": 0.6406}, {"dataset_name": "lr_0.001", "final_value": 0.6504, "best_value": 0.6504}, {"dataset_name": "lr_0.002", "final_value": 0.6326, "best_value": 0.6326}, {"dataset_name": "lr_0.005", "final_value": 0.6302, "best_value": 0.6302}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score during training, which is a harmonic mean of precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score on the validation set, indicating model performance during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6879, "best_value": 0.69}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, which measures the error between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6081, "best_value": 0.6081}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation set, measuring error during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6287, "best_value": 0.6287}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score on the test set, indicating model performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6916, "best_value": 0.6916}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score for the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6914, "best_value": 0.6914}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss for the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6151, "best_value": 0.6151}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6321, "best_value": 0.6321}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score for the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6716, "best_value": 0.6716}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score for the test dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6905, "best_value": 0.6905}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6803, "best_value": 0.694}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.686, "best_value": 0.6898}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6224, "best_value": 0.6179}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.63, "best_value": 0.6275}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training phase", "data": [{"dataset_name": "embed_dim=32", "final_value": 0.6915, "best_value": 0.6915}, {"dataset_name": "embed_dim=64", "final_value": 0.681, "best_value": 0.681}, {"dataset_name": "embed_dim=128", "final_value": 0.6934, "best_value": 0.6934}, {"dataset_name": "embed_dim=256", "final_value": 0.6943, "best_value": 0.6943}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation phase", "data": [{"dataset_name": "embed_dim=32", "final_value": 0.6858, "best_value": 0.6858}, {"dataset_name": "embed_dim=64", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "embed_dim=128", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "embed_dim=256", "final_value": 0.688, "best_value": 0.688}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "embed_dim=32", "final_value": 0.6158, "best_value": 0.6158}, {"dataset_name": "embed_dim=64", "final_value": 0.6228, "best_value": 0.6228}, {"dataset_name": "embed_dim=128", "final_value": 0.608, "best_value": 0.608}, {"dataset_name": "embed_dim=256", "final_value": 0.6145, "best_value": 0.6145}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "embed_dim=32", "final_value": 0.6291, "best_value": 0.6291}, {"dataset_name": "embed_dim=64", "final_value": 0.6333, "best_value": 0.6333}, {"dataset_name": "embed_dim=128", "final_value": 0.6287, "best_value": 0.6287}, {"dataset_name": "embed_dim=256", "final_value": 0.6336, "best_value": 0.6336}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6093, "best_value": 0.6093}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6309, "best_value": 0.6309}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6995, "best_value": 0.6995}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.688, "best_value": 0.688}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score computed on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6967, "best_value": 0.6967}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6918, "best_value": 0.6985}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Best F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6079, "best_value": 0.6079}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6318, "best_value": 0.6318}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6958, "best_value": 0.6958}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "The F1 score achieved on the training dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6894, "best_value": 0.6894}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The best F1 score achieved on the validation dataset during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value on the training dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.618, "best_value": 0.618}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6323, "best_value": 0.6323}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score achieved on the test dataset during the final evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7008, "best_value": 0.7008}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score of the training dataset at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6949, "best_value": 0.6949}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Best F1 score achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6737, "best_value": 0.69}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value of the training dataset at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6099, "best_value": 0.6099}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value of the validation dataset at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6341, "best_value": 0.6341}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score of the test dataset at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score for the training dataset, measured at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6924, "best_value": 0.6924}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Best F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6918, "best_value": 0.696}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss for the training dataset, measured at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6099, "best_value": 0.6099}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset, measured at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6316, "best_value": 0.6316}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score for the test dataset, measured at the final evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6978, "best_value": 0.6978}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_best_dev_f1_bar.png", "../../logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_F1_curves.png", "../../logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_dev_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_dev_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_25406472a7f842df80796dc476f3d732_proc_3330952/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_25406472a7f842df80796dc476f3d732_proc_3330952/SPR_BENCH_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_f1_curves_epochs.png", "../../logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_best_valF1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_loss_curves_best_p0.2.png"], ["../../logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_best_f1_bar.png", "../../logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_confmat_dim64.png"], ["../../logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_F1_curves.png", "../../logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_Loss_curves.png", "../../logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_Dev_Test_F1_bar.png", "../../logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L1.png", "../../logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L2.png", "../../logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L3.png"], ["../../logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_devF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_testF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_confusion_best_wd_0.0005.png"], ["../../logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_devF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_testF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_confusion_best_wd_0.001.png"], ["../../logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_devF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_testF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_confusion_best_wd_1e-05.png"], ["../../logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_devF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_testF1_vs_wd.png", "../../logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_confusion_best_wd_0.0.png"], ["../../logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_loss_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_f1_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_devF1_vs_wd_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_testF1_vs_wd_agg.png"]], "plot_paths": [["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_best_dev_f1_bar.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_F1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_loss_curves.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_dev_test_f1_bar.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_dev_test_f1_bar.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25406472a7f842df80796dc476f3d732_proc_3330952/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25406472a7f842df80796dc476f3d732_proc_3330952/SPR_BENCH_loss_curves.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_f1_curves_epochs.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_best_valF1_vs_dropout.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_loss_curves_best_p0.2.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_best_f1_bar.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_confmat_dim64.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_F1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_Loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_Dev_Test_F1_bar.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L1.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L2.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L3.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_devF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_testF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_confusion_best_wd_0.0005.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_devF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_testF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_confusion_best_wd_0.001.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_devF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_testF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_confusion_best_wd_1e-05.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_devF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_testF1_vs_wd.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_confusion_best_wd_0.0.png"], ["experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_loss_curves_agg.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_f1_curves_agg.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_devF1_vs_wd_agg.png", "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a1c98ef166de44bdb6210ab38d988c8d/SPR_BENCH_testF1_vs_wd_agg.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation loss curves over 5 epochs. Both curves decrease steadily, indicating that the model is learning effectively. The training loss reduces more sharply than the validation loss, but they remain close, suggesting that the model is not overfitting. The final loss values are close to convergence, which is a positive sign for the model's stability.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot illustrates the training and validation Macro-F1 scores over 5 epochs. The training Macro-F1 score increases sharply in the first epoch and then shows gradual improvement, reaching a stable value. The validation Macro-F1 score follows a similar trend, with a slightly lower but consistent increase. The proximity of the two curves indicates that the model generalizes well to unseen data.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_f1_curve.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's performance on the best development set. The model correctly predicts 170 true negatives and 173 true positives, while there are 78 false positives and 79 false negatives. This indicates a relatively balanced performance, though there is room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6e78e64bcaa34a7789da26644258e623_proc_3327789/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The bar chart shows the best F1 scores achieved on the validation set for different epoch settings (5, 10, 15, 20). All configurations achieve similar performance, with F1 scores close to 0.7. This suggests that increasing the number of epochs beyond 5 does not significantly improve the model's best validation performance, indicating potential early convergence or overfitting.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_best_dev_f1_bar.png"}, {"analysis": "The line plot illustrates the F1 score progression over epochs for training and validation sets across different epoch configurations. Training F1 scores generally increase and stabilize quickly, while validation F1 scores fluctuate slightly after initial stabilization. This indicates that the model learns well in the early epochs, but further training does not yield significant improvements in validation performance, again pointing to early convergence or overfitting.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_F1_curves.png"}, {"analysis": "The line plot depicts the cross-entropy loss progression over epochs for training and validation sets across different epoch configurations. Training loss decreases steadily, showing effective learning. However, validation loss stabilizes or fluctuates after initial improvement, with some configurations (e.g., 20 epochs) showing increased fluctuations. This behavior suggests overfitting, as the model continues to fit the training data without corresponding validation performance improvements.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_073f4ee2159146c2aa146f3d1f7b9eb2_proc_3330949/SPR_BENCH_loss_curves.png"}], [{"analysis": "The loss curves indicate that for all learning rates, the training loss decreases steadily across epochs, showing that the models are learning effectively. However, the validation loss behavior varies. For a learning rate of 0.0005, the validation loss initially decreases but starts increasing after epoch 3, indicating potential overfitting. Similarly, for learning rates of 0.001 and 0.002, the validation loss stabilizes, but for 0.005, it increases more significantly, suggesting overfitting at higher learning rates.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that the models with learning rates of 0.002 and 0.005 achieve the best training and validation F1 scores, indicating their ability to balance precision and recall effectively. However, for learning rates of 0.0005 and 0.001, the validation F1 score plateaus early, showing slower convergence. The discrepancy between training and validation F1 for higher learning rates suggests overfitting, particularly for lr_0.005.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_f1_curves.png"}, {"analysis": "The bar chart comparing the best development and test F1 scores for different learning rates shows that learning rates of 0.002 and 0.005 achieve the highest F1 scores on both the development and test sets. However, the marginal difference between development and test F1 scores for lr_0.002 indicates better generalization compared to lr_0.005, which shows a slightly larger gap, further confirming overfitting at the highest learning rate.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_436a67a0e4024c10bc8aff329698cdf6_proc_3330950/SPR_BENCH_dev_test_f1_bar.png"}], [{"analysis": "The training Macro-F1 scores increase steadily across epochs for all batch sizes, with smaller batch sizes (e.g., bs=32) achieving slightly higher performance initially. However, by the final epoch, the differences between batch sizes diminish. On the validation side, the Macro-F1 scores exhibit more variability, particularly for larger batch sizes (e.g., bs=128). Smaller batch sizes demonstrate more stable improvements, suggesting better generalization.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_f1_curves.png"}, {"analysis": "The training loss decreases smoothly across epochs for all batch sizes, indicating that the model is learning effectively. Smaller batch sizes (e.g., bs=32) show a steeper decline in loss initially, which aligns with their higher training Macro-F1. The validation loss, however, fluctuates more significantly, especially for larger batch sizes, which may indicate overfitting or sensitivity to batch size during validation.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_loss_curves.png"}, {"analysis": "The comparison of the best Macro-F1 scores on the development and test sets for different batch sizes reveals consistent performance across both sets. Smaller batch sizes (e.g., bs=32) achieve slightly higher Macro-F1 scores compared to larger batch sizes, suggesting that smaller batch sizes may lead to better generalization on unseen data.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25f8524d8cbd48328e58f1332ac19bf8_proc_3330951/SPR_BENCH_dev_test_f1_bar.png"}], [{"analysis": "The F1 score curves show the performance of the model across different hidden sizes (hs_64, hs_128, hs_256, hs_512) over five epochs. The train and validation curves are plotted separately. The hs_256 configuration demonstrates stable and consistent improvement in both train and validation scores, achieving the highest validation F1 score. The hs_512 configuration exhibits overfitting, as the validation F1 score decreases after an initial rise, while the training score continues to improve. The hs_64 and hs_128 configurations show moderate improvements, with hs_128 slightly outperforming hs_64 in validation scores. The results suggest that hs_256 is the optimal hidden size for this task, balancing training and validation performance effectively.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25406472a7f842df80796dc476f3d732_proc_3330952/SPR_BENCH_f1_curves.png"}, {"analysis": "The loss curves provide insights into how the cross-entropy loss evolves for different hidden sizes over the epochs. The hs_256 configuration shows a steady decrease in both training and validation loss, indicating effective learning and generalization. The hs_512 configuration, however, shows divergence between training and validation loss, signaling overfitting. The hs_64 and hs_128 configurations show consistent decreases in loss, with hs_128 slightly outperforming hs_64 in validation loss reduction. These observations further support the conclusion that hs_256 is the most balanced configuration, achieving both low training and validation loss.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_25406472a7f842df80796dc476f3d732_proc_3330952/SPR_BENCH_loss_curves.png"}], [{"analysis": "The first plot shows the Macro-F1 scores for training and validation sets across epochs for different dropout probabilities. As the epochs progress, both training and validation Macro-F1 scores improve, with a general trend of convergence. Dropout probabilities do not appear to cause significant divergence between training and validation performance, suggesting that overfitting is well-controlled. The performance for dropout p=0.2 is particularly stable and achieves high scores, making it a promising candidate for further tuning.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_f1_curves_epochs.png"}, {"analysis": "The second plot compares the best validation Macro-F1 scores achieved for different dropout probabilities. The scores are very close, with dropout probabilities of 0.0, 0.2, and 0.6 achieving the highest scores (approximately 0.69). This indicates that the model is robust to different dropout probabilities, with no significant degradation in performance. However, p=0.2 might be slightly more consistent given its stability in the first plot.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_best_valF1_vs_dropout.png"}, {"analysis": "The third plot illustrates the cross-entropy loss for training and validation sets across epochs with the best dropout probability (p=0.2). Both losses decrease steadily, indicating effective learning. The gap between training and validation losses remains small, showing good generalization and no significant overfitting. The validation loss plateaus around epoch 3, suggesting that further training may not yield significant improvements.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_fa3340f136c44a7995aa7676402919a5_proc_3330950/SPR_BENCH_loss_curves_best_p0.2.png"}], [{"analysis": "This plot shows the Macro-F1 score on the validation set as a function of training epochs for different embedding dimensions (32, 64, 128, 256). The embedding dimension of 64 achieves the highest Macro-F1 score at around epoch 3, indicating better generalization and performance compared to other dimensions. Embedding dimensions 32 and 256 show a consistent increase in Macro-F1 but plateau at slightly lower values. Dimension 128 exhibits a sharp improvement initially but drops after epoch 3, suggesting potential overfitting or instability. This indicates that embedding dimension and training duration significantly impact performance.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_f1_curves.png"}, {"analysis": "This plot compares the train and validation cross-entropy loss across epochs for different embedding dimensions. The trend shows a steady decrease in both train and validation loss, indicating effective learning. However, for embedding dimension 64, the validation loss begins to plateau after epoch 3, whereas the training loss continues to decrease, suggesting slight overfitting. Embedding dimension 256 maintains the lowest training loss but shows a divergence in validation loss, hinting at overfitting. Embedding dimension 64 achieves the best balance between train and validation loss, reinforcing its selection as an optimal configuration.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_loss_curves.png"}, {"analysis": "This bar chart summarizes the best validation Macro-F1 score achieved for each embedding dimension. Embedding dimension 64 achieves the highest score, followed closely by dimensions 128 and 32. Dimension 256 performs slightly worse, indicating that higher embedding dimensions do not necessarily improve performance in this task. This reinforces the conclusion that dimension 64 provides the most robust performance for this experiment.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_best_f1_bar.png"}, {"analysis": "This confusion matrix evaluates the performance of the model with an embedding dimension of 64, which was identified as the best configuration. The model achieves a balanced prediction between the two classes, with slightly more errors in predicting class 1 as class 0 (79 instances). The high count of correct predictions (174 and 173 for classes 0 and 1, respectively) indicates strong performance. However, further optimization could reduce the misclassification rates to improve overall accuracy.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_76652ec21b8c4bd9ad7bc633401f780a_proc_3330951/spr_bench_confmat_dim64.png"}], [{"analysis": "The plot shows the Macro F1 scores for training and validation sets across epochs for models with different LSTM layer configurations (L1, L2, L3). The training performance improves steadily with epochs for all configurations, with L2 and L3 exhibiting faster convergence compared to L1. Validation performance for L1 stabilizes early and remains consistent, while L2 and L3 show slight fluctuations, with L3 achieving the highest validation F1 score by the last epoch. This indicates that deeper LSTMs might generalize better for this task, though L3 shows some instability in validation performance.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_F1_curves.png"}, {"analysis": "The plot illustrates the cross-entropy loss for training and validation sets over epochs for various LSTM layer configurations. Training loss decreases consistently for all configurations, with L3 showing the steepest decline. Validation loss for L1 stabilizes early, while L2 and L3 exhibit less stability, with L3 showing the highest variability. This suggests that deeper LSTMs may overfit slightly, evidenced by the less consistent validation loss.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_Loss_curves.png"}, {"analysis": "This bar chart compares the best Macro F1 scores on the development and test sets for different LSTM layer configurations. All configurations achieve nearly identical performance on both sets, with L1 and L2 slightly outperforming L3. This indicates that while deeper LSTMs (L3) perform well during training, their generalization ability might be slightly worse compared to shallower configurations.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_Dev_Test_F1_bar.png"}, {"analysis": "The confusion matrix for the 1-layer LSTM shows balanced performance across the two classes, with slightly more errors in predicting the second class. This configuration demonstrates decent generalization and balanced class predictions, though there is room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L1.png"}, {"analysis": "The confusion matrix for the 2-layer LSTM shows a slight improvement in class prediction accuracy compared to the 1-layer configuration, with fewer misclassifications for both classes. This suggests that adding a second LSTM layer enhances the model's ability to capture the symbolic rules governing the sequences.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L2.png"}, {"analysis": "The confusion matrix for the 3-layer LSTM shows a decline in performance compared to the 2-layer configuration, with more misclassifications, particularly in the second class. This suggests that the deeper model may be overfitting to the training data, leading to reduced generalization.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b972f224f5234b8b9fb5d2806a32719e_proc_3330949/SPR_BENCH_confusion_matrix_L3.png"}], [{"analysis": "The plot illustrates the training and validation loss trends across different weight decay values. Lower weight decay values (e.g., wd=0.0 and wd=1e-05) exhibit a faster reduction in training loss, but their validation loss stabilizes at higher values, indicating potential overfitting. Moderate weight decay values (e.g., wd=5e-05 and wd=0.0001) achieve a balanced reduction in both training and validation losses, suggesting better generalization. Higher weight decay values (e.g., wd=0.001) result in slower convergence and higher loss, likely due to underfitting.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares the training and validation Macro-F1 scores for different weight decay values. Models with moderate weight decay values (e.g., wd=5e-05 and wd=0.0001) achieve the highest validation F1 scores, indicating optimal regularization. Extremely low weight decay (e.g., wd=0.0) results in a large gap between training and validation F1 scores, suggesting overfitting. High weight decay (e.g., wd=0.001) results in lower F1 scores for both training and validation, indicative of underfitting.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_f1_curves.png"}, {"analysis": "The plot shows the final Macro-F1 score on the development set as a function of weight decay. The best performance is achieved at a weight decay of 0.0005, with a slight drop in performance for lower and higher weight decay values. This suggests that 0.0005 provides the optimal balance between underfitting and overfitting for this dataset.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_devF1_vs_wd.png"}, {"analysis": "This plot shows the final Macro-F1 score on the test set for different weight decay values. The trend is consistent with the development set, with the best performance achieved at a weight decay of 0.0005. This consistency indicates that the model generalizes well to unseen data at this weight decay value.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_testF1_vs_wd.png"}, {"analysis": "The confusion matrix for the best-performing weight decay value (wd=0.0005) reveals that the model performs well on both classes, with relatively balanced true positive rates. However, there is some misclassification, as indicated by the off-diagonal values. This suggests that while the model is effective, there is still room for improvement in capturing the nuances of the data.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4ae98398efcc4757abe9e72139c5bba9_proc_3330952/SPR_BENCH_confusion_best_wd_0.0005.png"}], [{"analysis": "The training and validation loss curves show a consistent decrease over the epochs for all weight decay (wd) values, indicating that the model is learning effectively. However, higher weight decay values (e.g., wd=0.001) result in slower convergence, as evidenced by the higher initial losses and delayed reduction. The validation loss fluctuates more compared to the training loss, especially for smaller weight decay values, suggesting potential overfitting in these cases. Overall, a moderate weight decay (e.g., wd=5e-05 or wd=0.0001) appears to balance the training and validation loss reduction effectively.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_loss_curves.png"}, {"analysis": "The macro-F1 scores for both training and validation improve over the epochs, reflecting enhanced classification performance. Lower weight decay values (e.g., wd=1e-05) lead to faster initial improvements in F1 scores but exhibit higher variability in validation F1, indicating overfitting. In contrast, higher weight decay values (e.g., wd=0.001) show more stable validation F1 scores but slower improvement. The results suggest that moderate weight decay values (e.g., wd=5e-05 or wd=0.0001) provide a good trade-off between training speed and validation stability.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_f1_curves.png"}, {"analysis": "The plot reveals that the final dev macro-F1 score is highest for the largest weight decay value (wd=0.001), indicating that stronger regularization positively impacts the generalization performance. Lower weight decay values (e.g., wd=1e-05) result in slightly lower F1 scores, possibly due to overfitting. The results highlight the importance of tuning weight decay to achieve optimal performance.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_devF1_vs_wd.png"}, {"analysis": "The test macro-F1 score plot aligns with the dev F1 results, showing that the highest weight decay (wd=0.001) yields the best performance. Lower weight decay values lead to slightly reduced test F1 scores, suggesting that the model generalizes better with stronger regularization. The trend confirms the significance of weight decay as a hyperparameter for improving test performance.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_testF1_vs_wd.png"}, {"analysis": "The confusion matrix for the best weight decay (wd=0.001) shows that the model performs well in classifying both classes, with 170 and 177 correct predictions for each class, respectively. However, there are notable misclassifications (78 and 75), indicating room for improvement. The results suggest that while the model is effective, further optimization of hyperparameters or additional regularization techniques might be necessary to reduce misclassification rates.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/SPR_BENCH_confusion_best_wd_0.001.png"}], [{"analysis": "This plot shows the training and validation loss trends for different weight decay (wd) values over five epochs. The training loss consistently decreases for all weight decay values, indicating that the model is learning effectively. However, the validation loss trends reveal overfitting for higher weight decay values (e.g., wd=0.001), where the validation loss starts to increase after an initial decrease. Lower weight decay values (e.g., wd=1e-05) show a better balance between training and validation loss, suggesting they may be more optimal for this task.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation macro-F1 scores for various weight decay values. The F1 scores improve steadily during the initial epochs for all configurations. However, higher weight decay values (e.g., wd=0.001) lead to slower validation F1 improvement and lower peak scores, indicating underfitting. Lower weight decay values (e.g., wd=1e-05) achieve higher and more stable validation F1 scores, suggesting better generalization.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot depicts the final development macro-F1 score as a function of weight decay. The highest F1 score is achieved at wd=1e-05, while increasing weight decay results in a sharp drop in performance, followed by gradual recovery. This trend highlights the sensitivity of the model to weight decay, with lower values providing better regularization for the SPR_BENCH task.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_devF1_vs_wd.png"}, {"analysis": "This plot shows the final test macro-F1 score across different weight decay values. Similar to the development F1 trends, the test F1 score is highest at wd=1e-05, with performance declining as weight decay increases. This consistency between development and test results reinforces the conclusion that lower weight decay values are more effective for this task.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_testF1_vs_wd.png"}, {"analysis": "The confusion matrix for the best-performing weight decay (wd=1e-05) shows the distribution of true and predicted labels. While the diagonal entries indicate correct predictions, the off-diagonal values highlight misclassified instances. The model performs better on one class (lower misclassifications) compared to the other, suggesting potential class imbalance or differing levels of difficulty in classification. Further analysis could explore methods to address these issues, such as class-weighted loss or data augmentation.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/SPR_BENCH_confusion_best_wd_1e-05.png"}], [{"analysis": "The plot illustrates the training and validation loss across epochs for different weight decay (wd) values. As expected, the loss decreases over epochs for all configurations. Lower weight decay values (e.g., 0.0, 1e-05) lead to slightly lower final training loss, indicating better convergence. However, higher weight decay values (e.g., 0.001) show higher validation loss in later epochs, suggesting potential over-regularization or underfitting. This indicates that weight decay significantly impacts model generalization and must be carefully tuned.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares the macro F1 scores for training and validation across epochs for varying weight decay values. The trends show that models with lower weight decay values (e.g., 0.0001, 0.0005) achieve higher F1 scores on both training and validation sets, indicating better learning and generalization. However, higher weight decay (e.g., 0.001) leads to a plateau or even a decline in validation F1, suggesting over-regularization. The results highlight the need to balance regularization to achieve optimal performance on unseen data.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot shows the final development macro F1 score as a function of weight decay. The highest F1 score is achieved at a weight decay value of 0.0001, which suggests an optimal balance between regularization and model flexibility. Lower weight decay values (e.g., 1e-05) perform worse, likely due to overfitting, while higher values (e.g., 0.001) degrade performance due to excessive regularization.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_devF1_vs_wd.png"}, {"analysis": "This plot depicts the final test macro F1 score as a function of weight decay. Similar to the development set, the best performance is observed at a weight decay value of 0.0001. This consistency across development and test sets confirms the robustness of the model at this regularization level. The sharp decline in performance at higher weight decay (e.g., 0.001) reinforces the need for careful tuning to avoid underfitting.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_testF1_vs_wd.png"}, {"analysis": "The confusion matrix provides insights into the model's classification performance for a weight decay value of 0.0. The model correctly predicts a majority of the samples in both classes but shows some misclassifications. Notably, the number of false positives and false negatives is relatively balanced, indicating no significant bias toward either class. This suggests that the model is reasonably well-calibrated at this weight decay setting but may still benefit from further optimization to minimize errors.", "plot_path": "experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/SPR_BENCH_confusion_best_wd_0.0.png"}], []], "vlm_feedback_summary": ["The provided plots indicate that the model is learning effectively and\ngeneralizing well to unseen data. The loss curves show steady convergence, and\nthe Macro-F1 scores demonstrate consistent improvement. The confusion matrix\nreveals balanced performance with some scope for reducing misclassifications.", "The plots indicate that increasing the number of epochs does not significantly\nimprove validation performance, with F1 scores stabilizing early. Training loss\ndecreases consistently, but validation loss shows signs of overfitting,\nparticularly for higher epoch settings. Adjustments such as early stopping or\nregularization may be needed to improve generalization.", "The loss curves suggest effective learning but potential overfitting at higher\nlearning rates. The Macro-F1 curves highlight strong performance for lr_0.002\nand lr_0.005, but overfitting is evident for lr_0.005. The bar chart confirms\nthat lr_0.002 balances performance and generalization better than other learning\nrates.", "The plots provide valuable insights into the impact of batch size on training\nand validation performance. Smaller batch sizes demonstrate better\ngeneralization and more stable validation performance, while larger batch sizes\nshow higher variability and potential overfitting. The results suggest that\noptimizing batch size is crucial for achieving robust performance in Symbolic\nPolyRule Reasoning tasks.", "The plots reveal that the hs_256 configuration achieves the best balance between\ntraining and validation performance, with the highest validation F1 score and\nsteady loss reduction. The hs_512 configuration overfits, as indicated by the\ndivergence between training and validation metrics. The hs_64 and hs_128\nconfigurations perform moderately, with hs_128 slightly better than hs_64.\nOverall, hs_256 stands out as the optimal choice for this experiment.", "The plots indicate that the model performs robustly across different dropout\nprobabilities, with dropout p=0.2 showing slightly better stability and\nperformance. The Macro-F1 and loss curves suggest effective learning and good\ngeneralization, with minimal overfitting observed. Further tuning could focus on\noptimizing other hyperparameters while keeping dropout at p=0.2.", "The analysis of the plots highlights that embedding dimension 64 provides the\nbest performance in terms of validation Macro-F1 score and balanced\ntrain/validation loss. The confusion matrix confirms strong performance but\nsuggests room for improvement in reducing misclassification rates. These results\nsupport the hypothesis that careful tuning of hyperparameters, such as embedding\ndimension, significantly impacts model performance.", "The plots provide valuable insights into the performance of different LSTM\nconfigurations on the SPR_BENCH benchmark. Deeper LSTMs (L2 and L3) show faster\nconvergence and better training performance, but their validation performance is\nless stable, with L3 showing signs of overfitting. The confusion matrices\nhighlight that while L2 achieves the best balance between accuracy and\ngeneralization, deeper configurations like L3 may suffer from reduced\ngeneralization. Overall, L2 appears to be the most robust configuration for this\ntask.", "The plots indicate that the model achieves the best performance with a weight\ndecay of 0.0005, balancing underfitting and overfitting. Moderate weight decay\nvalues lead to better generalization, as evidenced by lower validation loss and\nhigher Macro-F1 scores on both development and test sets. The confusion matrix\nconfirms that the model is effective but could benefit from further refinement\nto reduce misclassifications.", "The plots provide valuable insights into the impact of weight decay on training,\nvalidation, and test performance. Moderate to high weight decay values improve\ngeneralization and stability, as evidenced by lower validation loss variability\nand higher macro-F1 scores. The confusion matrix highlights the model's\neffectiveness but also points to areas for further enhancement to reduce\nmisclassifications.", "The plots provide a comprehensive view of model performance across different\nweight decay values. Lower weight decay values (e.g., wd=1e-05) consistently\noutperform higher values, as evidenced by lower validation loss, higher macro-F1\nscores, and better confusion matrix results. These findings suggest that careful\ntuning of weight decay is critical for optimizing performance on the SPR_BENCH\ntask.", "The plots provide valuable insights into the impact of weight decay on model\nperformance. Lower weight decay values generally result in better training and\nvalidation performance, while excessive regularization leads to underfitting.\nThe optimal weight decay (0.0001) achieves the best macro F1 scores on both\ndevelopment and test sets, demonstrating robust generalization. The confusion\nmatrix highlights balanced performance but indicates room for improvement in\nreducing misclassification errors.", "[]"], "exec_time": [3.684948205947876, 15.077224254608154, 6.584712743759155, 6.551280498504639, 11.66691780090332, 10.56554913520813, 6.147841691970825, 5.707090377807617, 13.664381504058838, 14.679347038269043, 8.323052883148193, 8.77620244026184, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n\n# ensure working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"SPR_BENCH\", {})\nepochs = np.array(data.get(\"epochs\", []))\ntrain_ls = np.array(data.get(\"losses\", {}).get(\"train\", []))\nval_ls = np.array(data.get(\"losses\", {}).get(\"val\", []))\ntrain_f1 = np.array(data.get(\"metrics\", {}).get(\"train\", []))\nval_f1 = np.array(data.get(\"metrics\", {}).get(\"val\", []))\npreds = np.array(data.get(\"predictions\", []))\ngts = np.array(data.get(\"ground_truth\", []))\n\n# print best dev F1 if available\nif len(val_f1):\n    print(f\"Best Dev Macro-F1 (stored) = {val_f1.max():.4f}\")\n\n# -------- 1) Loss curve ------------\ntry:\n    if len(epochs) and len(train_ls) and len(val_ls):\n        plt.figure()\n        plt.plot(epochs, train_ls, label=\"Train Loss\")\n        plt.plot(epochs, val_ls, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# -------- 2) F1 curve ---------------\ntry:\n    if len(epochs) and len(train_f1) and len(val_f1):\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Val Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curve.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# -------- 3) Confusion matrix -------\ntry:\n    if len(preds) and len(gts):\n        cm = confusion_matrix(gts, preds, labels=[0, 1])\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n        plt.figure()\n        disp.plot(cmap=\"Blues\", values_format=\"d\")\n        plt.title(\"SPR_BENCH: Confusion Matrix (Best Dev)\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working dir path\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment results --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# quick exit if data missing\nif not experiment_data:\n    quit()\n\ncfgs = experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"configurations\"]\ncfg_names = sorted(cfgs.keys(), key=lambda x: int(x.split(\"_\")[-1]))\n\n# print summary table\nprint(\"\\nConfig\\tBest Dev F1\\tTest F1\")\nfor n in cfg_names:\n    r = cfgs[n]\n    print(f\"{n}\\t{r['best_dev_f1']:.4f}\\t{r['test_f1']:.4f}\")\n\nbest_cfg = max(cfg_names, key=lambda c: cfgs[c][\"best_dev_f1\"])\nprint(f\"\\nBest configuration on dev set: {best_cfg}\")\n\n# -------- 1) bar chart of best dev F1 --------\ntry:\n    plt.figure()\n    best_f1s = [cfgs[n][\"best_dev_f1\"] for n in cfg_names]\n    plt.bar(cfg_names, best_f1s, color=\"skyblue\")\n    plt.ylabel(\"Best Dev F1\")\n    plt.title(\"SPR_BENCH \u2013 Best Dev-set F1 per Epoch Setting\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_best_dev_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n\n# -------- helper for multi-line curves --------\ndef multilines(metric_key, ylabel, fname_suffix):\n    plt.figure()\n    for n in cfg_names:\n        epochs = cfgs[n][\"epochs\"]\n        tr = cfgs[n][metric_key][\"train\"]\n        val = cfgs[n][metric_key][\"val\"]\n        plt.plot(epochs, tr, \"--\", label=f\"{n}-train\")\n        plt.plot(epochs, val, \"-\", label=f\"{n}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.title(f\"SPR_BENCH \u2013 {ylabel} Curves for Different Epoch Settings\")\n    plt.legend(fontsize=8)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_{fname_suffix}_curves.png\"))\n    plt.close()\n\n\n# -------- 2) F1 curves --------\ntry:\n    multilines(\"metrics\", \"F1 Score\", \"F1\")\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# -------- 3) Loss curves --------\ntry:\n    multilines(\"losses\", \"Cross-Entropy Loss\", \"loss\")\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------- setup & data loading ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    lr_runs = experiment_data.get(\"learning_rate\", {})\n    lrs = sorted(lr_runs.keys(), key=lambda x: float(x.split(\"_\")[1]))\n    dataset_name = \"SPR_BENCH\"  # fallback name\n    epochs = lr_runs[lrs[0]][\"epochs\"] if lrs else []\n\n    # -------- Plot 1: Loss curves ----------\n    try:\n        plt.figure()\n        for lr_key in lrs:\n            d = lr_runs[lr_key][\"losses\"]\n            plt.plot(epochs, d[\"train\"], \"--\", label=f\"{lr_key} train\")\n            plt.plot(epochs, d[\"val\"], \"-\", label=f\"{lr_key} val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dataset_name} Loss Curves\\nLeft: dashed=train, solid=val\")\n        plt.legend(fontsize=\"small\")\n        fname = os.path.join(working_dir, f\"{dataset_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------- Plot 2: F1 curves ------------\n    try:\n        plt.figure()\n        for lr_key in lrs:\n            m = lr_runs[lr_key][\"metrics\"]\n            plt.plot(epochs, m[\"train\"], \"--\", label=f\"{lr_key} train\")\n            plt.plot(epochs, m[\"val\"], \"-\", label=f\"{lr_key} val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dataset_name} Macro-F1 Curves\\nLeft: dashed=train, solid=val\")\n        plt.legend(fontsize=\"small\")\n        fname = os.path.join(working_dir, f\"{dataset_name}_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # -------- Plot 3: Dev/Test bar chart ---\n    try:\n        best_dev = [lr_runs[k][\"best_dev_f1\"] for k in lrs]\n        test_f1 = [lr_runs[k][\"test_f1\"] for k in lrs]\n        x = np.arange(len(lrs))\n        width = 0.35\n\n        plt.figure()\n        plt.bar(x - width / 2, best_dev, width, label=\"Best Dev F1\")\n        plt.bar(x + width / 2, test_f1, width, label=\"Test F1\")\n        plt.xticks(x, lrs, rotation=45)\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dataset_name} Dev vs Test F1 per Learning Rate\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dataset_name}_dev_test_f1_bar.png\")\n        plt.tight_layout()\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar chart: {e}\")\n        plt.close()\n\n    # -------- Print numeric summary -------\n    print(\"Learning Rate | Best Dev F1 | Test F1\")\n    for lr_key in lrs:\n        print(\n            f\"{lr_key:<13} {lr_runs[lr_key]['best_dev_f1']:.4f}      {lr_runs[lr_key]['test_f1']:.4f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract once for convenience\nbench = experiment_data.get(\"batch_size\", {}).get(\"SPR_BENCH\", {})\n\n# --- Plot 1: Macro-F1 curves -------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    for i, (bs, log) in enumerate(bench.items()):\n        epochs = log[\"epochs\"]\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, log[\"metrics\"][\"train\"], label=f\"bs={bs}\")\n        plt.title(\"Train Macro-F1 (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"F1\")\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, log[\"metrics\"][\"val\"], label=f\"bs={bs}\")\n        plt.title(\"Val Macro-F1 (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"F1\")\n    for ax in plt.gcf().axes:\n        ax.legend()\n    plt.suptitle(\"Left: Training, Right: Validation Macro-F1\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# --- Plot 2: Loss curves -----------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    for bs, log in bench.items():\n        epochs = log[\"epochs\"]\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, log[\"losses\"][\"train\"], label=f\"bs={bs}\")\n        plt.title(\"Train Loss (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy\")\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, log[\"losses\"][\"val\"], label=f\"bs={bs}\")\n        plt.title(\"Val Loss (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy\")\n    for ax in plt.gcf().axes:\n        ax.legend()\n    plt.suptitle(\"Left: Training, Right: Validation Loss\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss curves: {e}\")\n    plt.close()\n\n# --- Plot 3: Dev vs Test best Macro-F1 ---------------------------------------\ntry:\n    bs_list, dev_best, test_f1 = [], [], []\n    for bs, log in bench.items():\n        bs_list.append(int(bs))\n        dev_best.append(max(log[\"metrics\"][\"val\"]))\n        test_f1.append(log[\"test_f1\"])\n    x = np.arange(len(bs_list))\n    width = 0.35\n    plt.figure(figsize=(6, 4))\n    plt.bar(x - width / 2, dev_best, width, label=\"Best Dev F1\")\n    plt.bar(x + width / 2, test_f1, width, label=\"Test F1\")\n    plt.xticks(x, bs_list)\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Dev vs Test Macro-F1 by Batch Size\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_dev_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"hidden_size\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No SPR_BENCH data found, aborting plots.\")\n    exit()\n\n# ---------------- helper ----------------\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\nhs_keys = sorted(runs.keys(), key=lambda x: int(x.split(\"_\")[1]))\n\n# ------------- F1 curves ----------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(hs_keys):\n        epochs = runs[k][\"epochs\"]\n        train_f1 = runs[k][\"metrics\"][\"train\"]\n        val_f1 = runs[k][\"metrics\"][\"val\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, train_f1, linestyle=\"--\", color=c, label=f\"{k} train\")\n        plt.plot(epochs, val_f1, linestyle=\"-\", color=c, label=f\"{k} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH F1 Curves\\nLeft: Train (dashed), Right: Validation (solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ------------- Loss curves --------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(hs_keys):\n        epochs = runs[k][\"epochs\"]\n        train_loss = runs[k][\"losses\"][\"train\"]\n        val_loss = runs[k][\"losses\"][\"val\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, train_loss, linestyle=\"--\", color=c, label=f\"{k} train\")\n        plt.plot(epochs, val_loss, linestyle=\"-\", color=c, label=f\"{k} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train (dashed), Right: Validation (solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------ load experiment data -------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"dropout_prob\", {}).get(\"SPR_BENCH\", {})\n\n# quick numeric summary --------------------------------------------------------\nbest_f1_per_p = {}\nfor p_str, rec in spr_data.items():\n    val_f1 = rec[\"metrics\"][\"val\"]\n    if val_f1:\n        best_f1_per_p[float(p_str)] = max(val_f1)\nprint(\"Best validation macro-F1 per dropout value:\")\nfor k, v in sorted(best_f1_per_p.items()):\n    print(f\"  p={k:>3.1f} --> {v:.4f}\")\n\n# ---------------- PLOT 1: per-epoch F1 curves ---------------------------------\ntry:\n    plt.figure()\n    for p_str, rec in spr_data.items():\n        epochs = rec[\"epochs\"]\n        train_f1 = rec[\"metrics\"][\"train\"]\n        val_f1 = rec[\"metrics\"][\"val\"]\n        # Train curve (dashed)\n        plt.plot(epochs, train_f1, \"--\", label=f\"train p={p_str}\")\n        # Validation curve (solid)\n        plt.plot(epochs, val_f1, \"-\", label=f\"val p={p_str}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Train vs. Val Macro-F1 across Epochs (all dropouts)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves_epochs.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves plot: {e}\")\n    plt.close()\n\n# ---------------- PLOT 2: bar chart of best F1 --------------------------------\ntry:\n    plt.figure()\n    ps, bests = zip(*sorted(best_f1_per_p.items()))\n    plt.bar([str(p) for p in ps], bests, color=\"skyblue\")\n    for i, v in enumerate(bests):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.xlabel(\"Dropout probability\")\n    plt.ylabel(\"Best Val Macro-F1\")\n    plt.title(\"SPR_BENCH: Best Validation F1 vs. Dropout Probability\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_best_valF1_vs_dropout.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating best-F1 bar chart: {e}\")\n    plt.close()\n\n# -------------- PLOT 3: loss curves for best p --------------------------------\ntry:\n    if best_f1_per_p:\n        best_p = max(best_f1_per_p, key=best_f1_per_p.get)\n        rec = spr_data[str(best_p)]\n        epochs = rec[\"epochs\"]\n        tr_loss = rec[\"losses\"][\"train\"]\n        val_loss = rec[\"losses\"][\"val\"]\n\n        plt.figure()\n        plt.plot(epochs, tr_loss, \"--o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, \"-o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"SPR_BENCH: Loss Curves (Best dropout p={best_p})\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_loss_curves_best_p{best_p}.png\")\n        plt.savefig(fname, dpi=150)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    embed_dict = experiment_data.get(\"embed_dim\", {})\n    dims = sorted(embed_dict.keys())\n    # --------- collect per-dim arrays ----------\n    f1_curves, loss_curves = {}, {}\n    best_f1s, best_dim = [], None\n    overall_best = -1\n    for d in dims:\n        ed = embed_dict[d]\n        f1_curves[d] = ed[\"metrics\"][\"val\"]\n        loss_curves[d] = ed[\"losses\"]\n        final_f1 = ed[\"metrics\"][\"val\"][-1] if ed[\"metrics\"][\"val\"] else np.nan\n        best_f1s.append(final_f1)\n        if final_f1 > overall_best:\n            overall_best = final_f1\n            best_dim = d\n\n    # ---------- 1: Macro-F1 curves ----------\n    try:\n        plt.figure()\n        for d in dims:\n            plt.plot(embed_dict[d][\"epochs\"], f1_curves[d], label=f\"dim={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Validation Macro-F1 vs Epoch (per embedding dim)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # ---------- 2: Loss curves ----------\n    try:\n        plt.figure()\n        for d in dims:\n            plt.plot(\n                embed_dict[d][\"epochs\"],\n                loss_curves[d][\"train\"],\n                \"--\",\n                label=f\"train dim={d}\",\n            )\n            plt.plot(\n                embed_dict[d][\"epochs\"], loss_curves[d][\"val\"], label=f\"val dim={d}\"\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Train/Val Loss vs Epoch (all embedding dims)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- 3: Best F1 per dim ----------\n    try:\n        plt.figure()\n        plt.bar([str(d) for d in dims], best_f1s, color=\"skyblue\")\n        plt.xlabel(\"Embedding Dimension\")\n        plt.ylabel(\"Best Validation Macro-F1\")\n        plt.title(\"SPR_BENCH: Best Dev Macro-F1 per Embedding Dim\")\n        plt.savefig(os.path.join(working_dir, \"spr_bench_best_f1_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n    # ---------- 4: Confusion Matrix for best dim ----------\n    try:\n        from sklearn.metrics import confusion_matrix\n\n        preds = embed_dict[best_dim][\"predictions\"]\n        gts = embed_dict[best_dim][\"ground_truth\"]\n        cm = confusion_matrix(gts, preds, labels=[0, 1])\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"SPR_BENCH: Confusion Matrix (Best dim={best_dim})\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"red\")\n        plt.savefig(os.path.join(working_dir, f\"spr_bench_confmat_dim{best_dim}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# --- dirs ---\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --- load data ---\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper to collect logs\nlayers_dict = experiment_data.get(\"num_lstm_layers\", {}).get(\"SPR_BENCH\", {})\nlayer_keys = sorted(layers_dict.keys(), key=int)  # ['1','2','3']\n\n# ---- 1) F1 curves ----\ntry:\n    plt.figure()\n    for k in layer_keys:\n        epochs = layers_dict[k][\"epochs\"]\n        plt.plot(epochs, layers_dict[k][\"metrics\"][\"train\"], label=f\"train_L{k}\")\n        plt.plot(\n            epochs, layers_dict[k][\"metrics\"][\"val\"], label=f\"val_L{k}\", linestyle=\"--\"\n        )\n    plt.title(\"SPR_BENCH: Macro F1 vs Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_F1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# ---- 2) Loss curves ----\ntry:\n    plt.figure()\n    for k in layer_keys:\n        epochs = layers_dict[k][\"epochs\"]\n        plt.plot(epochs, layers_dict[k][\"losses\"][\"train\"], label=f\"train_L{k}\")\n        plt.plot(\n            epochs, layers_dict[k][\"losses\"][\"val\"], label=f\"val_L{k}\", linestyle=\"--\"\n        )\n    plt.title(\"SPR_BENCH: Loss vs Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_Loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss curve plot: {e}\")\n    plt.close()\n\n# ---- 3) Bar chart of best Dev/Test F1 ----\ntry:\n    best_dev = [layers_dict[k][\"best_dev_f1\"] for k in layer_keys]\n    test_f1 = [layers_dict[k][\"test_f1\"] for k in layer_keys]\n    x = np.arange(len(layer_keys))\n    width = 0.35\n    plt.figure()\n    plt.bar(x - width / 2, best_dev, width, label=\"Best Dev F1\")\n    plt.bar(x + width / 2, test_f1, width, label=\"Test F1\")\n    plt.xticks(x, [f\"L{k}\" for k in layer_keys])\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH: Dev vs Test F1 by #LSTM layers\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_Dev_Test_F1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# ---- 4-6) Confusion matrices (one per layer) ----\nfor k in layer_keys:\n    try:\n        preds = layers_dict[k][\"predictions\"]\n        gts = layers_dict[k][\"ground_truth\"]\n        if preds is None or gts is None or len(preds) == 0:\n            continue\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.title(f\"SPR_BENCH Confusion Matrix - LSTM Layers={k}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.colorbar()\n        fname = f\"SPR_BENCH_confusion_matrix_L{k}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for L{k}: {e}\")\n        plt.close()\n\n# ---- print summary table ----\nprint(\"Layer | Best Dev F1 | Test F1\")\nfor k in layer_keys:\n    print(\n        f\"  {k}   |  {layers_dict[k]['best_dev_f1']:.4f}   | {layers_dict[k]['test_f1']:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wds = sorted(spr_data.keys(), key=lambda x: float(x))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data, wds = {}, []\n\n\n# ---------------- helper --------------------\ndef get_curve(wd_key, field, split):\n    \"\"\"Return y-values curve for given weight-decay key.\"\"\"\n    return spr_data[wd_key][field][split]\n\n\n# -------- 1. Loss curves --------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"losses\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs, get_curve(wd, \"losses\", \"val\"), linestyle=\"--\", label=f\"val wd={wd}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# -------- 2. F1 curves ----------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"metrics\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs,\n            get_curve(wd, \"metrics\", \"val\"),\n            linestyle=\"--\",\n            label=f\"val wd={wd}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation F1\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# -------- 3. Dev F1 vs WD -------------------\ntry:\n    dev_f1 = [get_curve(wd, \"metrics\", \"val\")[-1] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], dev_f1, marker=\"o\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Dev Macro-F1\")\n    plt.title(\"SPR_BENCH: Dev F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_devF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Dev-F1 plot: {e}\")\n    plt.close()\n\n# -------- 4. Test F1 vs WD ------------------\ntry:\n    test_f1 = [spr_data[wd][\"test_f1\"] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], test_f1, marker=\"s\", color=\"green\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH: Test F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_testF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\n    print(\"Test F1 per weight decay:\", dict(zip(wds, test_f1)))\nexcept Exception as e:\n    print(f\"Error creating Test-F1 plot: {e}\")\n    plt.close()\n\n# -------- 5. Confusion matrix ---------------\ntry:\n    # pick best wd on dev\n    best_idx = int(np.argmax(dev_f1))\n    best_wd = wds[best_idx]\n    preds = np.array(spr_data[best_wd][\"predictions\"])\n    gts = np.array(spr_data[best_wd][\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for g, p in zip(gts, preds):\n        cm[g, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.title(\n        f\"SPR_BENCH Confusion Matrix (Best WD={best_wd})\\nLeft: Ground Truth, Right: Predicted\"\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.colorbar()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_best_wd_{best_wd}.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wds = sorted(spr_data.keys(), key=lambda x: float(x))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data, wds = {}, []\n\n\n# ---------------- helper --------------------\ndef get_curve(wd_key, field, split):\n    \"\"\"Return y-values curve for given weight-decay key.\"\"\"\n    return spr_data[wd_key][field][split]\n\n\n# -------- 1. Loss curves --------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"losses\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs, get_curve(wd, \"losses\", \"val\"), linestyle=\"--\", label=f\"val wd={wd}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# -------- 2. F1 curves ----------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"metrics\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs,\n            get_curve(wd, \"metrics\", \"val\"),\n            linestyle=\"--\",\n            label=f\"val wd={wd}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation F1\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# -------- 3. Dev F1 vs WD -------------------\ntry:\n    dev_f1 = [get_curve(wd, \"metrics\", \"val\")[-1] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], dev_f1, marker=\"o\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Dev Macro-F1\")\n    plt.title(\"SPR_BENCH: Dev F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_devF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Dev-F1 plot: {e}\")\n    plt.close()\n\n# -------- 4. Test F1 vs WD ------------------\ntry:\n    test_f1 = [spr_data[wd][\"test_f1\"] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], test_f1, marker=\"s\", color=\"green\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH: Test F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_testF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\n    print(\"Test F1 per weight decay:\", dict(zip(wds, test_f1)))\nexcept Exception as e:\n    print(f\"Error creating Test-F1 plot: {e}\")\n    plt.close()\n\n# -------- 5. Confusion matrix ---------------\ntry:\n    # pick best wd on dev\n    best_idx = int(np.argmax(dev_f1))\n    best_wd = wds[best_idx]\n    preds = np.array(spr_data[best_wd][\"predictions\"])\n    gts = np.array(spr_data[best_wd][\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for g, p in zip(gts, preds):\n        cm[g, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.title(\n        f\"SPR_BENCH Confusion Matrix (Best WD={best_wd})\\nLeft: Ground Truth, Right: Predicted\"\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.colorbar()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_best_wd_{best_wd}.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wds = sorted(spr_data.keys(), key=lambda x: float(x))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data, wds = {}, []\n\n\n# ---------------- helper --------------------\ndef get_curve(wd_key, field, split):\n    \"\"\"Return y-values curve for given weight-decay key.\"\"\"\n    return spr_data[wd_key][field][split]\n\n\n# -------- 1. Loss curves --------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"losses\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs, get_curve(wd, \"losses\", \"val\"), linestyle=\"--\", label=f\"val wd={wd}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# -------- 2. F1 curves ----------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"metrics\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs,\n            get_curve(wd, \"metrics\", \"val\"),\n            linestyle=\"--\",\n            label=f\"val wd={wd}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation F1\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# -------- 3. Dev F1 vs WD -------------------\ntry:\n    dev_f1 = [get_curve(wd, \"metrics\", \"val\")[-1] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], dev_f1, marker=\"o\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Dev Macro-F1\")\n    plt.title(\"SPR_BENCH: Dev F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_devF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Dev-F1 plot: {e}\")\n    plt.close()\n\n# -------- 4. Test F1 vs WD ------------------\ntry:\n    test_f1 = [spr_data[wd][\"test_f1\"] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], test_f1, marker=\"s\", color=\"green\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH: Test F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_testF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\n    print(\"Test F1 per weight decay:\", dict(zip(wds, test_f1)))\nexcept Exception as e:\n    print(f\"Error creating Test-F1 plot: {e}\")\n    plt.close()\n\n# -------- 5. Confusion matrix ---------------\ntry:\n    # pick best wd on dev\n    best_idx = int(np.argmax(dev_f1))\n    best_wd = wds[best_idx]\n    preds = np.array(spr_data[best_wd][\"predictions\"])\n    gts = np.array(spr_data[best_wd][\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for g, p in zip(gts, preds):\n        cm[g, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.title(\n        f\"SPR_BENCH Confusion Matrix (Best WD={best_wd})\\nLeft: Ground Truth, Right: Predicted\"\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.colorbar()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_best_wd_{best_wd}.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wds = sorted(spr_data.keys(), key=lambda x: float(x))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data, wds = {}, []\n\n\n# ---------------- helper --------------------\ndef get_curve(wd_key, field, split):\n    \"\"\"Return y-values curve for given weight-decay key.\"\"\"\n    return spr_data[wd_key][field][split]\n\n\n# -------- 1. Loss curves --------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"losses\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs, get_curve(wd, \"losses\", \"val\"), linestyle=\"--\", label=f\"val wd={wd}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# -------- 2. F1 curves ----------------------\ntry:\n    plt.figure()\n    for wd in wds:\n        epochs = spr_data[wd][\"epochs\"]\n        plt.plot(epochs, get_curve(wd, \"metrics\", \"train\"), label=f\"train wd={wd}\")\n        plt.plot(\n            epochs,\n            get_curve(wd, \"metrics\", \"val\"),\n            linestyle=\"--\",\n            label=f\"val wd={wd}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation F1\")\n    plt.legend(fontsize=\"small\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# -------- 3. Dev F1 vs WD -------------------\ntry:\n    dev_f1 = [get_curve(wd, \"metrics\", \"val\")[-1] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], dev_f1, marker=\"o\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Dev Macro-F1\")\n    plt.title(\"SPR_BENCH: Dev F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_devF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Dev-F1 plot: {e}\")\n    plt.close()\n\n# -------- 4. Test F1 vs WD ------------------\ntry:\n    test_f1 = [spr_data[wd][\"test_f1\"] for wd in wds]\n    plt.figure()\n    plt.plot([float(w) for w in wds], test_f1, marker=\"s\", color=\"green\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH: Test F1 vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_testF1_vs_wd.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\n    print(\"Test F1 per weight decay:\", dict(zip(wds, test_f1)))\nexcept Exception as e:\n    print(f\"Error creating Test-F1 plot: {e}\")\n    plt.close()\n\n# -------- 5. Confusion matrix ---------------\ntry:\n    # pick best wd on dev\n    best_idx = int(np.argmax(dev_f1))\n    best_wd = wds[best_idx]\n    preds = np.array(spr_data[best_wd][\"predictions\"])\n    gts = np.array(spr_data[best_wd][\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for g, p in zip(gts, preds):\n        cm[g, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.title(\n        f\"SPR_BENCH Confusion Matrix (Best WD={best_wd})\\nLeft: Ground Truth, Right: Predicted\"\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.colorbar()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_best_wd_{best_wd}.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- set up ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load all experiment files ----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4446f682f90649ae8a2d17bbd7e675c1_proc_3330951/experiment_data.npy\",\n    \"experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28ad3b8fb6414163b032cec557308c09_proc_3330950/experiment_data.npy\",\n    \"experiments/2025-08-17_18-48-06_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f2742b7adc764989ad02a6a9becebe80_proc_3330949/experiment_data.npy\",\n]\n\nall_runs = []\ntry:\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        run_data = np.load(full_path, allow_pickle=True).item()\n        all_runs.append(run_data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_runs = []\n\n\n# Helper to collect data safely\ndef safe_get(dic, keys, default=None):\n    cur = dic\n    for k in keys:\n        if k not in cur:\n            return default\n        cur = cur[k]\n    return cur\n\n\n# ---------- aggregate SPR_BENCH ----------\nagg = {}  # structure: agg[wd]['epochs'] , 'losses_train' list-of-arrays, etc.\nfor run in all_runs:\n    spr = safe_get(run, [\"weight_decay\", \"SPR_BENCH\"])\n    if spr is None:\n        continue\n    for wd, info in spr.items():\n        wd_dict = agg.setdefault(\n            wd,\n            {\n                \"epochs\": None,\n                \"loss_train\": [],\n                \"loss_val\": [],\n                \"f1_train\": [],\n                \"f1_val\": [],\n                \"test_f1\": [],\n            },\n        )\n        if wd_dict[\"epochs\"] is None:\n            wd_dict[\"epochs\"] = np.array(info[\"epochs\"])\n        # collect curves\n        wd_dict[\"loss_train\"].append(np.array(info[\"losses\"][\"train\"]))\n        wd_dict[\"loss_val\"].append(np.array(info[\"losses\"][\"val\"]))\n        wd_dict[\"f1_train\"].append(np.array(info[\"metrics\"][\"train\"]))\n        wd_dict[\"f1_val\"].append(np.array(info[\"metrics\"][\"val\"]))\n        wd_dict[\"test_f1\"].append(info[\"test_f1\"])\n\n# sort weight decays numerically\nwds_sorted = sorted(agg.keys(), key=lambda x: float(x))\n\n\n# ---------- plotting helpers ----------\ndef mean_stderr(arr_list):\n    stacked = np.stack(arr_list, axis=0)\n    mean = stacked.mean(axis=0)\n    stderr = stacked.std(axis=0) / np.sqrt(stacked.shape[0])\n    return mean, stderr\n\n\n# ========== 1. Aggregated Loss Curves ==========\ntry:\n    plt.figure()\n    for wd in wds_sorted:\n        d = agg[wd]\n        mean_tr, err_tr = mean_stderr(d[\"loss_train\"])\n        mean_val, err_val = mean_stderr(d[\"loss_val\"])\n        epochs = d[\"epochs\"]\n        plt.plot(epochs, mean_tr, label=f\"train wd={wd}\")\n        plt.fill_between(epochs, mean_tr - err_tr, mean_tr + err_tr, alpha=0.2)\n        plt.plot(epochs, mean_val, linestyle=\"--\", label=f\"val wd={wd}\")\n        plt.fill_between(epochs, mean_val - err_val, mean_val + err_val, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss (mean \u00b1 stderr)\")\n    plt.legend(fontsize=\"small\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves_agg.png\"), dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curves: {e}\")\n    plt.close()\n\n# ========== 2. Aggregated F1 Curves ==========\ntry:\n    plt.figure()\n    for wd in wds_sorted:\n        d = agg[wd]\n        mean_tr, err_tr = mean_stderr(d[\"f1_train\"])\n        mean_val, err_val = mean_stderr(d[\"f1_val\"])\n        epochs = d[\"epochs\"]\n        plt.plot(epochs, mean_tr, label=f\"train wd={wd}\")\n        plt.fill_between(epochs, mean_tr - err_tr, mean_tr + err_tr, alpha=0.2)\n        plt.plot(epochs, mean_val, linestyle=\"--\", label=f\"val wd={wd}\")\n        plt.fill_between(epochs, mean_val - err_val, mean_val + err_val, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation F1 (mean \u00b1 stderr)\")\n    plt.legend(fontsize=\"small\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves_agg.png\"), dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated F1 curves: {e}\")\n    plt.close()\n\n# ========== 3. Final Dev F1 vs WD ==========\ntry:\n    plt.figure()\n    dev_mean, dev_err, xs = [], [], []\n    for wd in wds_sorted:\n        val_curves = agg[wd][\"f1_val\"]\n        final_vals = [c[-1] for c in val_curves]\n        dev_mean.append(np.mean(final_vals))\n        dev_err.append(np.std(final_vals) / np.sqrt(len(final_vals)))\n        xs.append(float(wd))\n    plt.errorbar(xs, dev_mean, yerr=dev_err, fmt=\"o-\", capsize=3)\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Dev Macro-F1\")\n    plt.title(\"SPR_BENCH: Dev F1 vs Weight Decay (mean \u00b1 stderr)\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_devF1_vs_wd_agg.png\"), dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated Dev-F1 plot: {e}\")\n    plt.close()\n\n# ========== 4. Final Test F1 vs WD ==========\ntry:\n    plt.figure()\n    test_mean, test_err, xs = [], [], []\n    for wd in wds_sorted:\n        vals = agg[wd][\"test_f1\"]\n        test_mean.append(np.mean(vals))\n        test_err.append(np.std(vals) / np.sqrt(len(vals)))\n        xs.append(float(wd))\n    plt.errorbar(xs, test_mean, yerr=test_err, fmt=\"s-\", color=\"green\", capsize=3)\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH: Test F1 vs Weight Decay (mean \u00b1 stderr)\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_testF1_vs_wd_agg.png\"), dpi=150)\n    plt.close()\n    print(\n        \"Aggregated Test F1 (mean \u00b1 stderr):\",\n        {wd: (m, e) for wd, m, e in zip(wds_sorted, test_mean, test_err)},\n    )\nexcept Exception as e:\n    print(f\"Error creating aggregated Test-F1 plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We will load the serialized experiment_data, pull out the stored epochs, training/validation losses, and macro-F1 scores, and make three standard plots: (1) train/val loss vs. epoch, (2) train/val macro-F1 vs. epoch, and (3) a confusion matrix for the best-validation predictions. Each figure is wrapped in its own try-except so failures in one plot don\u2019t block the rest. The figures are titled descriptively and saved to working_dir with informative filenames that include the dataset name and plot type. After plotting we recompute and print the best dev macro-F1 directly from the stored arrays to give an immediate metric check. All figures are closed explicitly to avoid memory leaks.", "We will load the saved experiment_data.npy, focus on the SPR_BENCH dataset and its four \u201cepochs_X\u201d configurations, and pull out the stored metrics, losses, and best-F1 values. First, we print a concise table that shows each configuration\u2019s best dev F1 and test F1 so users immediately see which run performed best. Then we generate up to three key visualizations (staying within the \u22645-figure rule): (1) a bar chart that compares best dev-set F1 across the four epoch settings, (2) a multi-line plot showing the full training and validation F1 curves for every configuration, and (3) a similar multi-line plot for training and validation loss. Each figure is created in its own try-except block, given a clear title, saved with a descriptive filename in working_dir, and the figure is always closed afterward. We stick strictly to data found in experiment_data.npy\u2014no synthetic numbers are introduced. After plotting, we also print the name of the configuration that achieved the highest dev F1 so it\u2019s visible in notebook or log output. This satisfies the plotting guidelines while giving an at-a-glance summary and deeper per-epoch insight. Finally, every import and directory setup follows the required template.", "We load the saved experiment_data.npy, determine which learning-rate runs are present, and extract per-epoch training/validation losses and macro-F1 scores plus the final best_dev_f1 and test_f1. Three concise visualizations are produced: (1) a multi-line plot of training/validation losses versus epoch for all learning rates, (2) a matching plot for macro-F1 scores, and (3) a bar chart comparing best dev F1 and test F1 across learning rates. Each figure is wrapped in its own try-except block, has a clear title and subtitle indicating the dataset (SPR_BENCH or synthetic) and what is being shown, and is saved into working_dir with descriptive filenames. We cap ourselves at these three plots to remain under the five-figure limit. After plotting, we print a small table of best_dev_f1 and test_f1 per learning rate so users can see the numeric results directly. All figures are closed to avoid memory leaks, and any file-loading or plotting errors are caught and reported without stopping the script. The code is concise, uses only matplotlib and numpy, and respects all stated guidelines.", "We will load experiment_data.npy from the working directory, extract the logged information for the single dataset (\u201cSPR_BENCH\u201d) and for each of the three batch-size runs (32, 64, 128). With those arrays we generate at most three concise but informative figures: (1) training vs. validation macro-F1 across epochs, shown on a two-panel plot so users can compare convergence; (2) analogous training vs. validation loss across epochs; (3) a bar chart summarising the best dev macro-F1 and the final test macro-F1 obtained for every batch size. Each plotting block is wrapped in try-except, figures are titled clearly and saved to the pre-defined working_dir with descriptive names. We respect the \u201c\u2264 5 plots\u201d rule, always close figures, and do not fabricate any numbers\u2014everything is taken directly from experiment_data. The code below is concise, starts with the required imports/working_dir creation, and prints nothing except optional error messages while also saving the plots.", "The solution loads the stored experiment_data dictionary, extracts the results for the SPR_BENCH dataset, and then builds two concise plots: (1) F1-score training/validation curves and (2) loss curves, each containing one line per hidden-size setting. These combined plots keep the total number of figures \u2264 5 while still conveying the main trends. Every plotting operation is wrapped in its own try\u2013except to avoid breaking the script if a particular key is missing, and each figure is properly closed afterwards. The figures are saved in working_dir with descriptive names that include the dataset and curve type. Titles clarify what is shown and specify that both train and validation metrics are displayed. Only real values stored in experiment_data.npy are used\u2014nothing is fabricated. The code is intentionally compact and self-contained so it can be run as-is inside the current environment.", "We first load experiment_data.npy from the working directory and unpack the nested dictionary. From this we gather (1) per-epoch train/validation macro-F1 for every tested dropout probability, (2) per-epoch train/validation loss, and (3) the best validation F1 reached by each dropout setting. We then create up to three concise figures: (a) a single panel where every dropout curve is overlaid showing train vs. validation macro-F1 across epochs; (b) a bar chart of the best validation F1 obtained for each dropout probability, allowing rapid hyper-parameter comparison; (c) for the dropout that achieved the overall best validation F1, a line plot of train and validation loss across epochs. Each figure is wrapped in its own try-except block, saved with descriptive names inside working_dir, titled clearly (including dataset and content description), and the figure is closed afterwards. Finally, we print the table of best validation F1 scores so the user can see numerical values alongside the plots. All operations strictly rely on the data already stored in experiment_data.npy\u2014no values are fabricated or simulated.", "We will load the saved experiment_data.npy, iterate over the embed_dim sub-dictionary, and collect per-epoch train/val losses and macro-F1 scores. A first figure will overlay the macro-F1 learning curves for every embedding dimension so we can compare convergence behaviour; a second figure will similarly overlay the train/val loss curves. Next, we will compute the best validation macro-F1 for each dimension (already stored as the last value in metrics['val']) and create a bar chart to highlight the overall winner. Finally, for the single best-performing dimension we will build a 2\u00d72 confusion-matrix heat-map (dev set) to visualise class-wise performance. All figures are built inside independent try-except blocks, saved into working_dir with descriptive filenames, closed afterwards, and each titled clearly with the dataset name and plot type. We cap ourselves at these four plots (<5) to satisfy the plotting-interval constraint. If any expected key is missing the corresponding plot will be skipped gracefully.", "We first load the stored experiment_data.npy, which contains the per-epoch logs for every LSTM\u2010layer setting on the SPR_BENCH dataset. From each setting we pull the epoch indices, training/validation macro-F1, and training/validation loss so we can draw learning curves. A combined line plot is produced for F1 and another for loss, each overlaying the three layer depths with clearly labelled legends; this allows direct comparison while limiting the number of figures. We then summarise overall performance with a bar chart that puts best-dev and test F1 scores side-by-side for every layer. Finally, for qualitative insight we build a confusion-matrix heat map for each layer\u2019s best dev predictions (three matrices total, still within the five-figure cap). Every figure is wrapped in its own try-except block, given a descriptive title, saved to working_dir using meaningful filenames, and closed afterwards. No data are invented; everything plotted comes straight from experiment_data.npy. The code below implements this logic and prints the best-dev and test F1 table for verification.", "We first load the stored experiment_data.npy, extract the sub-dictionary for the SPR_BENCH weight-decay sweep, and sort the six weight-decay keys for consistent plotting.  From every entry we retrieve epoch-level training / validation losses and macro-F1 scores, plus the single scalar dev and test F1 obtained after training.  Five separate plots are produced, each wrapped in its own try-except so that a failure in one does not stop the rest: (1) training and validation loss curves across epochs for all weight decays, (2) training and validation F1 curves, (3) final dev F1 versus weight decay, (4) test F1 versus weight decay, and (5) a confusion-matrix heat-map on the dev split for the best performing weight decay.  Each figure is given a clear title that names the dataset and describes the content, saved with descriptive filenames inside working_dir, and then closed to free resources.  We print out the test F1 scores so the script remains self-contained with a quick textual summary.  Only data already present in experiment_data.npy are used; no new values are fabricated.  At most five figures are created to comply with the interval requirement, and basic matplotlib is used throughout without external styling.  The resulting plots give an at-a-glance view of how different weight-decay settings affect optimization dynamics and final performance on SPR_BENCH.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "batch_size", "hidden_size", "dropout_prob", "embed_dim", "num_lstm_layers", "weight_decay", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate the working directory, load the stored numpy dictionary,\nand iterate over every dataset it contains.   For each dataset it will gather\nthe training / validation macro-F1 lists and loss lists that were recorded\nduring training.   It will then compute the best (maximum) F1 scores and the\nbest (minimum) losses and print them with explicit, descriptive labels so there\nis no ambiguity about what each value represents.   All code is placed at global\nscope so the file runs immediately when executed.", "The script will load the saved NumPy dictionary, drill down to the \u201cSPR_BENCH\u201d\nexperiments, and for every epochs_* configuration it will retrieve the stored\nlists of training/validation F1 scores and losses together with the recorded\nbest validation and test F1 scores.   For each configuration, it prints the\ndataset name first, then clearly-labelled single values: the last training F1\nscore, the best validation F1 score, the test F1 score, the last training loss,\nand the last validation loss.   Everything is executed at the top level so the\nfile runs immediately when invoked.", "The code will load the saved NumPy file from the `working` directory, navigate\nthrough the nested dictionary that groups results by learning-rate experiments,\nand then pull out the final (last-epoch) train/validation losses and F1 scores,\nalong with the explicitly stored \u201cbest validation F1 score\u201d and \u201ctest F1 score.\u201d\nFor each learning-rate run, it will first print the run name (treated here as\nthe \u201cdataset\u201d identifier) and then print each metric\u2019s name followed by its\nvalue, ensuring clear, specific labels exactly as required.   All logic is\nplaced at top-level so the script executes immediately upon running, without any\n`if __name__ == \"__main__\":` guard.", "We will load the NumPy file from the working directory, iterate through the\nstored hyper-parameter settings, and for every dataset print (1) the final\ntraining macro-F1, (2) the best validation macro-F1 obtained during training,\n(3) the final training loss, (4) the final validation loss, and (5) the test\nmacro-F1. Each metric is clearly labeled, and the script runs immediately on\nexecution.", "We will load the saved NumPy dictionary, drill down to the \u201chidden_size \u2192\n<dataset> \u2192 <run>\u201d level, and for every run collect the last-epoch training\nnumbers together with the stored \u201cbest_dev_f1\u201d and \u201ctest_f1\u201d.   For clarity we\nprint the dataset name once, then for each hidden-size run print: final train F1\nscore, final train loss, final validation loss, best validation F1 score, and\ntest F1 score, each preceded by an explicit metric label.   No plots are\ngenerated, all code is at global scope, and execution starts immediately when\nthe script is run.", "The script will load experiment_data.npy from the working directory, unpack the\nnested dictionaries, and iterate over every recorded dropout-probability run for\nthe SPR_BENCH dataset.   For each run it finds the epoch that achieved the\nhighest validation macro-F1 score, then collects the corresponding training\nmacro-F1 score, validation macro-F1 score, training loss, and validation loss.\nThese best values are printed with clear, fully-qualified metric names so the\noutput is self-describing and easy to read.", "We will load the stored numpy dictionary, loop through every hyper-parameter\nsetting (each \u201cdataset\u201d here is an embedding-dimension experiment), and for\nevery setting print its name followed by the best (maximum) macro-F1 and the\nlowest loss on both the training and validation splits. Only the single best\nvalue per metric is shown, and the script runs from top level without any\nspecial entry point.", "The script loads the saved NumPy dictionary, locates the working directory, and\nreads the experiment data into memory.   For each hyper-parameter family (here:\nnumber of LSTM layers) and for every dataset inside it, it selects the\nconfiguration that achieved the highest validation F1 score.   It then prints\nthe dataset name once, followed by clearly labelled metrics taken from that\nbest-performing configuration: the final training loss, final validation loss,\nfinal training F1 score, best validation F1 score, and test F1 score.   All code\nexecutes immediately at the global scope and produces plain-text output without\nany plots.", "The script will first recreate the working directory path exactly as in the\ntraining run, then load the pickled dictionary stored in experiment_data.npy.\nFor each dataset key contained in the file (e.g., \u201cSPR_BENCH\u201d), it will iterate\nover all weight-decay settings, compute the relevant summary statistics (last\ntrain F1 score, best validation F1 score, final train loss, final validation\nloss, and final test F1 score), and print them with clear metric names.\nDataset names will be printed once before the block of results belonging to that\ndataset, and every metric will be prefixed with an explicit, descriptive label.\nThe code is written at global scope so it executes immediately when the script\nis run, with no reliance on an `if __name__ == \"__main__\":` guard.", "The script will first recreate the working directory path exactly as in the\ntraining run, then load the pickled dictionary stored in experiment_data.npy.\nFor each dataset key contained in the file (e.g., \u201cSPR_BENCH\u201d), it will iterate\nover all weight-decay settings, compute the relevant summary statistics (last\ntrain F1 score, best validation F1 score, final train loss, final validation\nloss, and final test F1 score), and print them with clear metric names.\nDataset names will be printed once before the block of results belonging to that\ndataset, and every metric will be prefixed with an explicit, descriptive label.\nThe code is written at global scope so it executes immediately when the script\nis run, with no reliance on an `if __name__ == \"__main__\":` guard.", "The script will first recreate the working directory path exactly as in the\ntraining run, then load the pickled dictionary stored in experiment_data.npy.\nFor each dataset key contained in the file (e.g., \u201cSPR_BENCH\u201d), it will iterate\nover all weight-decay settings, compute the relevant summary statistics (last\ntrain F1 score, best validation F1 score, final train loss, final validation\nloss, and final test F1 score), and print them with clear metric names.\nDataset names will be printed once before the block of results belonging to that\ndataset, and every metric will be prefixed with an explicit, descriptive label.\nThe code is written at global scope so it executes immediately when the script\nis run, with no reliance on an `if __name__ == \"__main__\":` guard.", "The script will first recreate the working directory path exactly as in the\ntraining run, then load the pickled dictionary stored in experiment_data.npy.\nFor each dataset key contained in the file (e.g., \u201cSPR_BENCH\u201d), it will iterate\nover all weight-decay settings, compute the relevant summary statistics (last\ntrain F1 score, best validation F1 score, final train loss, final validation\nloss, and final test F1 score), and print them with clear metric names.\nDataset names will be printed once before the block of results belonging to that\ndataset, and every metric will be prefixed with an explicit, descriptive label.\nThe code is written at global scope so it executes immediately when the script\nis run, with no reliance on an `if __name__ == \"__main__\":` guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate and load the saved experiment information\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# helper for safe extraction and best-value picking\n# -------------------------------------------------\ndef best_value(values, higher_is_better=True):\n    if not values:\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# -------------------------------------------------\n# iterate through every dataset and report metrics\n# -------------------------------------------------\nfor dataset_name, content in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # Macro-F1 scores\n    train_f1_best = best_value(\n        content[\"metrics\"].get(\"train\", []), higher_is_better=True\n    )\n    val_f1_best = best_value(content[\"metrics\"].get(\"val\", []), higher_is_better=True)\n\n    if train_f1_best is not None:\n        print(f\"best training macro F1 score: {train_f1_best:.4f}\")\n    if val_f1_best is not None:\n        print(f\"best validation macro F1 score: {val_f1_best:.4f}\")\n\n    # Cross-entropy losses\n    train_loss_best = best_value(\n        content[\"losses\"].get(\"train\", []), higher_is_better=False\n    )\n    val_loss_best = best_value(content[\"losses\"].get(\"val\", []), higher_is_better=False)\n\n    if train_loss_best is not None:\n        print(f\"best training loss: {train_loss_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"best validation loss: {val_loss_best:.4f}\")\n\n    # blank line between datasets\n    print()\n", "import os\nimport numpy as np\n\n# ---------- locate and load the results ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ---------- helper to fetch the desired scalar from a list ----------\ndef last(lst):\n    \"\"\"Return the last element of a list; works even if list is empty.\"\"\"\n    return lst[-1] if lst else None\n\n\n# ---------- iterate over structure and print metrics ----------\nhp_group = experiment_data.get(\"num_epochs\", {})\nfor dataset_name, dataset_block in hp_group.items():  # e.g., \"SPR_BENCH\"\n    configs = dataset_block.get(\"configurations\", {})\n    for cfg_name, cfg_results in configs.items():  # e.g., \"epochs_5\"\n        # Extract stored arrays / scalars\n        train_f1_list = cfg_results[\"metrics\"][\"train\"]\n        val_f1_list = cfg_results[\"metrics\"][\"val\"]\n        train_loss_list = cfg_results[\"losses\"][\"train\"]\n        val_loss_list = cfg_results[\"losses\"][\"val\"]\n\n        best_val_f1 = cfg_results[\"best_dev_f1\"]\n        test_f1 = cfg_results[\"test_f1\"]\n\n        # Print block header\n        print(f\"\\nDataset: {dataset_name} | Configuration: {cfg_name}\")\n\n        # Print clearly-named metrics\n        print(f\"training F1 score: {last(train_f1_list):.4f}\")\n        print(f\"validation F1 score: {best_val_f1:.4f}\")\n        print(f\"test F1 score: {test_f1:.4f}\")\n        print(f\"training loss: {last(train_loss_list):.4f}\")\n        print(f\"validation loss: {last(val_loss_list):.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Locate the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# 1. Load the experiment data\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# 2. Iterate over each learning-rate experiment (treated as separate \u201cdatasets\u201d)\nfor dataset_name, data_dict in experiment_data.get(\"learning_rate\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Extract sequences of metrics and losses\n    train_f1_history = data_dict[\"metrics\"][\"train\"]\n    val_f1_history = data_dict[\"metrics\"][\"val\"]\n    train_loss_hist = data_dict[\"losses\"][\"train\"]\n    val_loss_hist = data_dict[\"losses\"][\"val\"]\n\n    # 5. Grab the final or best values as requested\n    final_train_f1 = train_f1_history[-1]\n    final_validation_f1 = val_f1_history[-1]\n    best_validation_f1 = data_dict[\"best_dev_f1\"]\n    test_f1_score = data_dict[\"test_f1\"]\n    final_train_loss = train_loss_hist[-1]\n    final_validation_loss = val_loss_hist[-1]\n\n    # 3 & 4. Print each metric with explicit, precise labels\n    print(f\"Final Train F1 Score: {final_train_f1:.4f}\")\n    print(f\"Final Validation F1 Score: {final_validation_f1:.4f}\")\n    print(f\"Best Validation F1 Score: {best_validation_f1:.4f}\")\n    print(f\"Test F1 Score: {test_f1_score:.4f}\")\n    print(f\"Final Train Loss: {final_train_loss:.4f}\")\n    print(f\"Final Validation Loss: {final_validation_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------- load --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------- print -------------------\nfor sweep_name, datasets in experiment_data.items():  # e.g. 'batch_size'\n    for dataset_name, sweep_results in datasets.items():  # e.g. 'SPR_BENCH'\n        print(f\"Dataset: {dataset_name}\")\n        for hp_value, log in sweep_results.items():  # e.g. '32', '64', '128'\n            # extract metrics\n            final_train_f1 = log[\"metrics\"][\"train\"][-1]\n            best_val_f1 = max(log[\"metrics\"][\"val\"])\n            final_train_loss = log[\"losses\"][\"train\"][-1]\n            final_val_loss = log[\"losses\"][\"val\"][-1]\n            test_f1 = log[\"test_f1\"]\n\n            # print with clear labels\n            print(f\"  Batch size: {hp_value}\")\n            print(f\"    Final training macro F1 score: {final_train_f1:.4f}\")\n            print(f\"    Best validation macro F1 score: {best_val_f1:.4f}\")\n            print(f\"    Final training loss: {final_train_loss:.4f}\")\n            print(f\"    Final validation loss: {final_val_loss:.4f}\")\n            print(f\"    Test macro F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------\n# 0. Locate and load the experiment data file\n# ---------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------\n# 1. Iterate through datasets and runs, printing final / best stats\n# ---------------------------------------------------------------\nhidden_size_dict = experiment_data.get(\"hidden_size\", {})\n\nfor dataset_name, runs in hidden_size_dict.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for run_name, run_dict in runs.items():\n        train_f1_final = (\n            run_dict[\"metrics\"][\"train\"][-1]\n            if run_dict[\"metrics\"][\"train\"]\n            else float(\"nan\")\n        )\n        train_loss_final = (\n            run_dict[\"losses\"][\"train\"][-1]\n            if run_dict[\"losses\"][\"train\"]\n            else float(\"nan\")\n        )\n        val_loss_final = (\n            run_dict[\"losses\"][\"val\"][-1] if run_dict[\"losses\"][\"val\"] else float(\"nan\")\n        )\n\n        best_val_f1 = run_dict.get(\"best_dev_f1\", float(\"nan\"))\n        test_f1 = run_dict.get(\"test_f1\", float(\"nan\"))\n\n        print(f\"  Run: {run_name}\")\n        print(f\"    final train F1 score: {train_f1_final:.4f}\")\n        print(f\"    final train loss: {train_loss_final:.4f}\")\n        print(f\"    final validation loss: {val_loss_final:.4f}\")\n        print(f\"    best validation F1 score: {best_val_f1:.4f}\")\n        print(f\"    test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# --------- load data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# --------- iterate & report ----------\nfor dataset_name, runs in experiment_data.get(\"dropout_prob\", {}).items():\n    print(f\"{dataset_name} dataset:\")\n    for dropout_p, run_data in runs.items():\n        # Retrieve metric lists\n        val_f1_list = run_data[\"metrics\"][\"val\"]\n        best_idx = int(np.argmax(val_f1_list))  # epoch with best validation F1\n        # Extract corresponding metrics\n        train_f1 = run_data[\"metrics\"][\"train\"][best_idx]\n        val_f1 = val_f1_list[best_idx]\n        train_loss = run_data[\"losses\"][\"train\"][best_idx]\n        val_loss = run_data[\"losses\"][\"val\"][best_idx]\n\n        print(f\"  Dropout probability: {dropout_p}\")\n        print(f\"    train macro F1 score: {train_f1:.4f}\")\n        print(f\"    validation macro F1 score: {val_f1:.4f}\")\n        print(f\"    training loss: {train_loss:.4f}\")\n        print(f\"    validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate the working directory and load the saved experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Iterate over each experiment (keyed by embedding dimension)\n# ------------------------------------------------------------------\nfor dim in sorted(experiment_data[\"embed_dim\"].keys()):\n    edata = experiment_data[\"embed_dim\"][dim]\n\n    # ---------- extract arrays ----------\n    train_f1_history = np.array(edata[\"metrics\"][\"train\"])\n    val_f1_history = np.array(edata[\"metrics\"][\"val\"])\n    train_loss_hist = np.array(edata[\"losses\"][\"train\"])\n    val_loss_hist = np.array(edata[\"losses\"][\"val\"])\n\n    # ---------- compute best values ----------\n    best_train_f1 = train_f1_history.max()\n    best_val_f1 = val_f1_history.max()\n    lowest_train_loss = train_loss_hist.min()\n    lowest_val_loss = val_loss_hist.min()\n\n    # ---------- pretty printing ----------\n    print(f\"Dataset: embed_dim={dim}\")\n    print(f\"  training macro F1 score: {best_train_f1:.4f}\")\n    print(f\"  validation macro F1 score: {best_val_f1:.4f}\")\n    print(f\"  training loss: {lowest_train_loss:.4f}\")\n    print(f\"  validation loss: {lowest_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor hp_name, datasets in experiment_data.items():  # e.g. 'num_lstm_layers'\n    for dataset_name, configs in datasets.items():  # e.g. 'SPR_BENCH'\n        # pick the configuration (e.g. '1', '2', '3' layers) with best dev F1\n        best_config_key = max(configs, key=lambda k: configs[k][\"best_dev_f1\"])\n        logs = configs[best_config_key]\n\n        # extract metrics\n        final_train_loss = (\n            logs[\"losses\"][\"train\"][-1] if logs[\"losses\"][\"train\"] else None\n        )\n        final_val_loss = logs[\"losses\"][\"val\"][-1] if logs[\"losses\"][\"val\"] else None\n        final_train_f1 = (\n            logs[\"metrics\"][\"train\"][-1] if logs[\"metrics\"][\"train\"] else None\n        )\n        best_val_f1 = logs[\"best_dev_f1\"]\n        test_f1 = logs[\"test_f1\"]\n\n        # ---------- print results ----------\n        print(f\"Dataset: {dataset_name}\")\n        print(f\"best hyperparameter setting ({hp_name}): {best_config_key}\")\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"final validation loss: {final_val_loss:.4f}\")\n        if final_train_f1 is not None:\n            print(f\"final training F1 score: {final_train_f1:.4f}\")\n        print(f\"best validation F1 score: {best_val_f1:.4f}\")\n        print(f\"test F1 score: {test_f1:.4f}\")\n        print()  # blank line between datasets (if any)\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate the saved experiment file\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# ------------------------------------------------------------------\n# Load the experiment data\n# ------------------------------------------------------------------\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to format floats nicely\n# ------------------------------------------------------------------\ndef fmt(x, ndigits=4):\n    return f\"{x:.{ndigits}f}\"\n\n\n# ------------------------------------------------------------------\n# Traverse the data structure and print the requested metrics\n# ------------------------------------------------------------------\nfor dataset_name, wd_dict in experiment_data[\"weight_decay\"].items():\n    # Print dataset header\n    print(f\"\\nDATASET: {dataset_name}\")\n    # Iterate over every weight-decay hyper-parameter setting\n    for wd_value, stats in wd_dict.items():\n        # Extract arrays\n        train_f1_list = stats[\"metrics\"][\"train\"]\n        val_f1_list = stats[\"metrics\"][\"val\"]\n        train_loss_list = stats[\"losses\"][\"train\"]\n        val_loss_list = stats[\"losses\"][\"val\"]\n        test_f1 = stats[\"test_f1\"]\n\n        # Compute summary numbers\n        final_train_f1 = train_f1_list[-1] if train_f1_list else None\n        best_validation_f1 = max(val_f1_list) if val_f1_list else None\n        final_train_loss = train_loss_list[-1] if train_loss_list else None\n        final_validation_loss = val_loss_list[-1] if val_loss_list else None\n\n        # Print block for this weight decay setting\n        print(f\"\\n  Weight decay: {wd_value}\")\n        if final_train_f1 is not None:\n            print(f\"    train F1 score (final epoch): {fmt(final_train_f1)}\")\n        if best_validation_f1 is not None:\n            print(f\"    validation F1 score (best):   {fmt(best_validation_f1)}\")\n        if final_train_loss is not None:\n            print(f\"    train loss (final epoch):     {fmt(final_train_loss)}\")\n        if final_validation_loss is not None:\n            print(f\"    validation loss (final epoch): {fmt(final_validation_loss)}\")\n        if test_f1 is not None:\n            print(f\"    test F1 score (final):        {fmt(test_f1)}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate the saved experiment file\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# ------------------------------------------------------------------\n# Load the experiment data\n# ------------------------------------------------------------------\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to format floats nicely\n# ------------------------------------------------------------------\ndef fmt(x, ndigits=4):\n    return f\"{x:.{ndigits}f}\"\n\n\n# ------------------------------------------------------------------\n# Traverse the data structure and print the requested metrics\n# ------------------------------------------------------------------\nfor dataset_name, wd_dict in experiment_data[\"weight_decay\"].items():\n    # Print dataset header\n    print(f\"\\nDATASET: {dataset_name}\")\n    # Iterate over every weight-decay hyper-parameter setting\n    for wd_value, stats in wd_dict.items():\n        # Extract arrays\n        train_f1_list = stats[\"metrics\"][\"train\"]\n        val_f1_list = stats[\"metrics\"][\"val\"]\n        train_loss_list = stats[\"losses\"][\"train\"]\n        val_loss_list = stats[\"losses\"][\"val\"]\n        test_f1 = stats[\"test_f1\"]\n\n        # Compute summary numbers\n        final_train_f1 = train_f1_list[-1] if train_f1_list else None\n        best_validation_f1 = max(val_f1_list) if val_f1_list else None\n        final_train_loss = train_loss_list[-1] if train_loss_list else None\n        final_validation_loss = val_loss_list[-1] if val_loss_list else None\n\n        # Print block for this weight decay setting\n        print(f\"\\n  Weight decay: {wd_value}\")\n        if final_train_f1 is not None:\n            print(f\"    train F1 score (final epoch): {fmt(final_train_f1)}\")\n        if best_validation_f1 is not None:\n            print(f\"    validation F1 score (best):   {fmt(best_validation_f1)}\")\n        if final_train_loss is not None:\n            print(f\"    train loss (final epoch):     {fmt(final_train_loss)}\")\n        if final_validation_loss is not None:\n            print(f\"    validation loss (final epoch): {fmt(final_validation_loss)}\")\n        if test_f1 is not None:\n            print(f\"    test F1 score (final):        {fmt(test_f1)}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate the saved experiment file\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# ------------------------------------------------------------------\n# Load the experiment data\n# ------------------------------------------------------------------\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to format floats nicely\n# ------------------------------------------------------------------\ndef fmt(x, ndigits=4):\n    return f\"{x:.{ndigits}f}\"\n\n\n# ------------------------------------------------------------------\n# Traverse the data structure and print the requested metrics\n# ------------------------------------------------------------------\nfor dataset_name, wd_dict in experiment_data[\"weight_decay\"].items():\n    # Print dataset header\n    print(f\"\\nDATASET: {dataset_name}\")\n    # Iterate over every weight-decay hyper-parameter setting\n    for wd_value, stats in wd_dict.items():\n        # Extract arrays\n        train_f1_list = stats[\"metrics\"][\"train\"]\n        val_f1_list = stats[\"metrics\"][\"val\"]\n        train_loss_list = stats[\"losses\"][\"train\"]\n        val_loss_list = stats[\"losses\"][\"val\"]\n        test_f1 = stats[\"test_f1\"]\n\n        # Compute summary numbers\n        final_train_f1 = train_f1_list[-1] if train_f1_list else None\n        best_validation_f1 = max(val_f1_list) if val_f1_list else None\n        final_train_loss = train_loss_list[-1] if train_loss_list else None\n        final_validation_loss = val_loss_list[-1] if val_loss_list else None\n\n        # Print block for this weight decay setting\n        print(f\"\\n  Weight decay: {wd_value}\")\n        if final_train_f1 is not None:\n            print(f\"    train F1 score (final epoch): {fmt(final_train_f1)}\")\n        if best_validation_f1 is not None:\n            print(f\"    validation F1 score (best):   {fmt(best_validation_f1)}\")\n        if final_train_loss is not None:\n            print(f\"    train loss (final epoch):     {fmt(final_train_loss)}\")\n        if final_validation_loss is not None:\n            print(f\"    validation loss (final epoch): {fmt(final_validation_loss)}\")\n        if test_f1 is not None:\n            print(f\"    test F1 score (final):        {fmt(test_f1)}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate the saved experiment file\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# ------------------------------------------------------------------\n# Load the experiment data\n# ------------------------------------------------------------------\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to format floats nicely\n# ------------------------------------------------------------------\ndef fmt(x, ndigits=4):\n    return f\"{x:.{ndigits}f}\"\n\n\n# ------------------------------------------------------------------\n# Traverse the data structure and print the requested metrics\n# ------------------------------------------------------------------\nfor dataset_name, wd_dict in experiment_data[\"weight_decay\"].items():\n    # Print dataset header\n    print(f\"\\nDATASET: {dataset_name}\")\n    # Iterate over every weight-decay hyper-parameter setting\n    for wd_value, stats in wd_dict.items():\n        # Extract arrays\n        train_f1_list = stats[\"metrics\"][\"train\"]\n        val_f1_list = stats[\"metrics\"][\"val\"]\n        train_loss_list = stats[\"losses\"][\"train\"]\n        val_loss_list = stats[\"losses\"][\"val\"]\n        test_f1 = stats[\"test_f1\"]\n\n        # Compute summary numbers\n        final_train_f1 = train_f1_list[-1] if train_f1_list else None\n        best_validation_f1 = max(val_f1_list) if val_f1_list else None\n        final_train_loss = train_loss_list[-1] if train_loss_list else None\n        final_validation_loss = val_loss_list[-1] if val_loss_list else None\n\n        # Print block for this weight decay setting\n        print(f\"\\n  Weight decay: {wd_value}\")\n        if final_train_f1 is not None:\n            print(f\"    train F1 score (final epoch): {fmt(final_train_f1)}\")\n        if best_validation_f1 is not None:\n            print(f\"    validation F1 score (best):   {fmt(best_validation_f1)}\")\n        if final_train_loss is not None:\n            print(f\"    train loss (final epoch):     {fmt(final_train_loss)}\")\n        if final_validation_loss is not None:\n            print(f\"    validation loss (final epoch): {fmt(final_validation_loss)}\")\n        if test_f1 is not None:\n            print(f\"    test F1 score (final):        {fmt(test_f1)}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'best training macro F1 score: 0.6974', '\\n', 'best\nvalidation macro F1 score: 0.6860', '\\n', 'best training loss: 0.6121', '\\n',\n'best validation loss: 0.6347', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH | Configuration: epochs_5', '\\n', 'training F1 score:\n0.6844', '\\n', 'validation F1 score: 0.6780', '\\n', 'test F1 score: 0.6908',\n'\\n', 'training loss: 0.6164', '\\n', 'validation loss: 0.6362', '\\n',\n'\\nDataset: SPR_BENCH | Configuration: epochs_10', '\\n', 'training F1 score:\n0.6990', '\\n', 'validation F1 score: 0.6900', '\\n', 'test F1 score: 0.6915',\n'\\n', 'training loss: 0.5956', '\\n', 'validation loss: 0.6321', '\\n',\n'\\nDataset: SPR_BENCH | Configuration: epochs_15', '\\n', 'training F1 score:\n0.6939', '\\n', 'validation F1 score: 0.6920', '\\n', 'test F1 score: 0.6932',\n'\\n', 'training loss: 0.6061', '\\n', 'validation loss: 0.6360', '\\n',\n'\\nDataset: SPR_BENCH | Configuration: epochs_20', '\\n', 'training F1 score:\n0.6960', '\\n', 'validation F1 score: 0.6898', '\\n', 'test F1 score: 0.6815',\n'\\n', 'training loss: 0.5921', '\\n', 'validation loss: 0.6434', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: lr_0.0005', '\\n', 'Final Train F1 Score: 0.6908', '\\n', 'Final\nValidation F1 Score: 0.6820', '\\n', 'Best Validation F1 Score: 0.6820', '\\n',\n'Test F1 Score: 0.6870', '\\n', 'Final Train Loss: 0.6188', '\\n', 'Final\nValidation Loss: 0.6406', '\\n', '\\nDataset: lr_0.001', '\\n', 'Final Train F1\nScore: 0.6885', '\\n', 'Final Validation F1 Score: 0.6561', '\\n', 'Best\nValidation F1 Score: 0.6800', '\\n', 'Test F1 Score: 0.6624', '\\n', 'Final Train\nLoss: 0.6196', '\\n', 'Final Validation Loss: 0.6504', '\\n', '\\nDataset:\nlr_0.002', '\\n', 'Final Train F1 Score: 0.7005', '\\n', 'Final Validation F1\nScore: 0.6819', '\\n', 'Best Validation F1 Score: 0.6940', '\\n', 'Test F1 Score:\n0.6927', '\\n', 'Final Train Loss: 0.6025', '\\n', 'Final Validation Loss:\n0.6326', '\\n', '\\nDataset: lr_0.005', '\\n', 'Final Train F1 Score: 0.6914',\n'\\n', 'Final Validation F1 Score: 0.6799', '\\n', 'Best Validation F1 Score:\n0.6840', '\\n', 'Test F1 Score: 0.6897', '\\n', 'Final Train Loss: 0.6085', '\\n',\n'Final Validation Loss: 0.6302', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Batch size: 32', '\\n', '    Final training macro\nF1 score: 0.6955', '\\n', '    Best validation macro F1 score: 0.6900', '\\n', '\nFinal training loss: 0.6081', '\\n', '    Final validation loss: 0.6338', '\\n', '\nTest macro F1 score: 0.6916', '\\n', '  Batch size: 64', '\\n', '    Final\ntraining macro F1 score: 0.6980', '\\n', '    Best validation macro F1 score:\n0.6838', '\\n', '    Final training loss: 0.6107', '\\n', '    Final validation\nloss: 0.6287', '\\n', '    Test macro F1 score: 0.6915', '\\n', '  Batch size:\n128', '\\n', '    Final training macro F1 score: 0.6935', '\\n', '    Best\nvalidation macro F1 score: 0.6879', '\\n', '    Final training loss: 0.6097',\n'\\n', '    Final validation loss: 0.6382', '\\n', '    Test macro F1 score:\n0.6845', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Run: hs_64', '\\n', '    final train F1 score:\n0.6914', '\\n', '    final train loss: 0.6151', '\\n', '    final validation loss:\n0.6321', '\\n', '    best validation F1 score: 0.6716', '\\n', '    test F1 score:\n0.6905', '\\n', '  Run: hs_128', '\\n', '    final train F1 score: 0.6948', '\\n',\n'    final train loss: 0.6063', '\\n', '    final validation loss: 0.6323', '\\n',\n'    best validation F1 score: 0.6960', '\\n', '    test F1 score: 0.6958', '\\n',\n'  Run: hs_256', '\\n', '    final train F1 score: 0.6917', '\\n', '    final\ntrain loss: 0.6114', '\\n', '    final validation loss: 0.6376', '\\n', '    best\nvalidation F1 score: 0.6920', '\\n', '    test F1 score: 0.6939', '\\n', '  Run:\nhs_512', '\\n', '    final train F1 score: 0.6934', '\\n', '    final train loss:\n0.6177', '\\n', '    final validation loss: 0.6394', '\\n', '    best validation\nF1 score: 0.6727', '\\n', '    test F1 score: 0.6898', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH dataset:', '\\n', '  Dropout probability: 0.0', '\\n', '    train\nmacro F1 score: 0.6914', '\\n', '    validation macro F1 score: 0.6880', '\\n', '\ntraining loss: 0.6179', '\\n', '    validation loss: 0.6351', '\\n', '  Dropout\nprobability: 0.2', '\\n', '    train macro F1 score: 0.6940', '\\n', '\nvalidation macro F1 score: 0.6898', '\\n', '    training loss: 0.6179', '\\n', '\nvalidation loss: 0.6275', '\\n', '  Dropout probability: 0.4', '\\n', '    train\nmacro F1 score: 0.6775', '\\n', '    validation macro F1 score: 0.6820', '\\n', '\ntraining loss: 0.6281', '\\n', '    validation loss: 0.6420', '\\n', '  Dropout\nprobability: 0.6', '\\n', '    train macro F1 score: 0.6803', '\\n', '\nvalidation macro F1 score: 0.6860', '\\n', '    training loss: 0.6224', '\\n', '\nvalidation loss: 0.6300', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['Dataset: embed_dim=32', '\\n', '  training macro F1 score: 0.6915', '\\n', '\nvalidation macro F1 score: 0.6858', '\\n', '  training loss: 0.6158', '\\n', '\nvalidation loss: 0.6291', '\\n', 'Dataset: embed_dim=64', '\\n', '  training macro\nF1 score: 0.6810', '\\n', '  validation macro F1 score: 0.6940', '\\n', '\ntraining loss: 0.6228', '\\n', '  validation loss: 0.6333', '\\n', 'Dataset:\nembed_dim=128', '\\n', '  training macro F1 score: 0.6934', '\\n', '  validation\nmacro F1 score: 0.6900', '\\n', '  training loss: 0.6080', '\\n', '  validation\nloss: 0.6287', '\\n', 'Dataset: embed_dim=256', '\\n', '  training macro F1 score:\n0.6943', '\\n', '  validation macro F1 score: 0.6880', '\\n', '  training loss:\n0.6145', '\\n', '  validation loss: 0.6336', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best hyperparameter setting (num_lstm_layers): 2',\n'\\n', 'final training loss: 0.6093', '\\n', 'final validation loss: 0.6309',\n'\\n', 'final training F1 score: 0.6995', '\\n', 'best validation F1 score:\n0.6880', '\\n', 'test F1 score: 0.6967', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDATASET: SPR_BENCH', '\\n', '\\n  Weight decay: 0.0', '\\n', '    train F1\nscore (final epoch): 0.6974', '\\n', '    validation F1 score (best):   0.6860',\n'\\n', '    train loss (final epoch):     0.6121', '\\n', '    validation loss\n(final epoch): 0.6347', '\\n', '    test F1 score (final):        0.6948', '\\n',\n'\\n  Weight decay: 1e-05', '\\n', '    train F1 score (final epoch): 0.6875',\n'\\n', '    validation F1 score (best):   0.6780', '\\n', '    train loss (final\nepoch):     0.6202', '\\n', '    validation loss (final epoch): 0.6508', '\\n', '\ntest F1 score (final):        0.6603', '\\n', '\\n  Weight decay: 5e-05', '\\n', '\ntrain F1 score (final epoch): 0.6985', '\\n', '    validation F1 score (best):\n0.6920', '\\n', '    train loss (final epoch):     0.6079', '\\n', '    validation\nloss (final epoch): 0.6318', '\\n', '    test F1 score (final):        0.6958',\n'\\n', '\\n  Weight decay: 0.0001', '\\n', '    train F1 score (final epoch):\n0.6925', '\\n', '    validation F1 score (best):   0.6839', '\\n', '    train loss\n(final epoch):     0.6086', '\\n', '    validation loss (final epoch): 0.6348',\n'\\n', '    test F1 score (final):        0.6918', '\\n', '\\n  Weight decay:\n0.0005', '\\n', '    train F1 score (final epoch): 0.6939', '\\n', '    validation\nF1 score (best):   0.6960', '\\n', '    train loss (final epoch):     0.6087',\n'\\n', '    validation loss (final epoch): 0.6368', '\\n', '    test F1 score\n(final):        0.6948', '\\n', '\\n  Weight decay: 0.001', '\\n', '    train F1\nscore (final epoch): 0.6918', '\\n', '    validation F1 score (best):   0.6920',\n'\\n', '    train loss (final epoch):     0.6146', '\\n', '    validation loss\n(final epoch): 0.6515', '\\n', '    test F1 score (final):        0.6806', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDATASET: SPR_BENCH', '\\n', '\\n  Weight decay: 0.0', '\\n', '    train F1\nscore (final epoch): 0.6822', '\\n', '    validation F1 score (best):   0.6676',\n'\\n', '    train loss (final epoch):     0.6216', '\\n', '    validation loss\n(final epoch): 0.6581', '\\n', '    test F1 score (final):        0.6818', '\\n',\n'\\n  Weight decay: 1e-05', '\\n', '    train F1 score (final epoch): 0.6868',\n'\\n', '    validation F1 score (best):   0.6858', '\\n', '    train loss (final\nepoch):     0.6189', '\\n', '    validation loss (final epoch): 0.6351', '\\n', '\ntest F1 score (final):        0.6992', '\\n', '\\n  Weight decay: 5e-05', '\\n', '\ntrain F1 score (final epoch): 0.6950', '\\n', '    validation F1 score (best):\n0.6899', '\\n', '    train loss (final epoch):     0.6100', '\\n', '    validation\nloss (final epoch): 0.6379', '\\n', '    test F1 score (final):        0.6855',\n'\\n', '\\n  Weight decay: 0.0001', '\\n', '    train F1 score (final epoch):\n0.6887', '\\n', '    validation F1 score (best):   0.6900', '\\n', '    train loss\n(final epoch):     0.6164', '\\n', '    validation loss (final epoch): 0.6392',\n'\\n', '    test F1 score (final):        0.6900', '\\n', '\\n  Weight decay:\n0.0005', '\\n', '    train F1 score (final epoch): 0.6929', '\\n', '    validation\nF1 score (best):   0.6819', '\\n', '    train loss (final epoch):     0.6153',\n'\\n', '    validation loss (final epoch): 0.6373', '\\n', '    test F1 score\n(final):        0.6853', '\\n', '\\n  Weight decay: 0.001', '\\n', '    train F1\nscore (final epoch): 0.6894', '\\n', '    validation F1 score (best):   0.6939',\n'\\n', '    train loss (final epoch):     0.6180', '\\n', '    validation loss\n(final epoch): 0.6323', '\\n', '    test F1 score (final):        0.7008', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDATASET: SPR_BENCH', '\\n', '\\n  Weight decay: 0.0', '\\n', '    train F1\nscore (final epoch): 0.6914', '\\n', '    validation F1 score (best):   0.6800',\n'\\n', '    train loss (final epoch):     0.6101', '\\n', '    validation loss\n(final epoch): 0.6344', '\\n', '    test F1 score (final):        0.6919', '\\n',\n'\\n  Weight decay: 1e-05', '\\n', '    train F1 score (final epoch): 0.6913',\n'\\n', '    validation F1 score (best):   0.6900', '\\n', '    train loss (final\nepoch):     0.6103', '\\n', '    validation loss (final epoch): 0.6341', '\\n', '\ntest F1 score (final):        0.6960', '\\n', '\\n  Weight decay: 5e-05', '\\n', '\ntrain F1 score (final epoch): 0.6949', '\\n', '    validation F1 score (best):\n0.6900', '\\n', '    train loss (final epoch):     0.6126', '\\n', '    validation\nloss (final epoch): 0.6385', '\\n', '    test F1 score (final):        0.6827',\n'\\n', '\\n  Weight decay: 0.0001', '\\n', '    train F1 score (final epoch):\n0.6894', '\\n', '    validation F1 score (best):   0.6737', '\\n', '    train loss\n(final epoch):     0.6099', '\\n', '    validation loss (final epoch): 0.6425',\n'\\n', '    test F1 score (final):        0.6828', '\\n', '\\n  Weight decay:\n0.0005', '\\n', '    train F1 score (final epoch): 0.6914', '\\n', '    validation\nF1 score (best):   0.6838', '\\n', '    train loss (final epoch):     0.6112',\n'\\n', '    validation loss (final epoch): 0.6349', '\\n', '    test F1 score\n(final):        0.6867', '\\n', '\\n  Weight decay: 0.001', '\\n', '    train F1\nscore (final epoch): 0.6879', '\\n', '    validation F1 score (best):   0.6798',\n'\\n', '    train loss (final epoch):     0.6124', '\\n', '    validation loss\n(final epoch): 0.6364', '\\n', '    test F1 score (final):        0.6886', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDATASET: SPR_BENCH', '\\n', '\\n  Weight decay: 0.0', '\\n', '    train F1\nscore (final epoch): 0.6859', '\\n', '    validation F1 score (best):   0.6960',\n'\\n', '    train loss (final epoch):     0.6165', '\\n', '    validation loss\n(final epoch): 0.6322', '\\n', '    test F1 score (final):        0.6978', '\\n',\n'\\n  Weight decay: 1e-05', '\\n', '    train F1 score (final epoch): 0.6919',\n'\\n', '    validation F1 score (best):   0.6920', '\\n', '    train loss (final\nepoch):     0.6147', '\\n', '    validation loss (final epoch): 0.6361', '\\n', '\ntest F1 score (final):        0.6884', '\\n', '\\n  Weight decay: 5e-05', '\\n', '\ntrain F1 score (final epoch): 0.6944', '\\n', '    validation F1 score (best):\n0.6796', '\\n', '    train loss (final epoch):     0.6101', '\\n', '    validation\nloss (final epoch): 0.6376', '\\n', '    test F1 score (final):        0.6871',\n'\\n', '\\n  Weight decay: 0.0001', '\\n', '    train F1 score (final epoch):\n0.6909', '\\n', '    validation F1 score (best):   0.6920', '\\n', '    train loss\n(final epoch):     0.6099', '\\n', '    validation loss (final epoch): 0.6316',\n'\\n', '    test F1 score (final):        0.6916', '\\n', '\\n  Weight decay:\n0.0005', '\\n', '    train F1 score (final epoch): 0.6924', '\\n', '    validation\nF1 score (best):   0.6918', '\\n', '    train loss (final epoch):     0.6126',\n'\\n', '    validation loss (final epoch): 0.6449', '\\n', '    test F1 score\n(final):        0.6960', '\\n', '\\n  Weight decay: 0.001', '\\n', '    train F1\nscore (final epoch): 0.6904', '\\n', '    validation F1 score (best):   0.6840',\n'\\n', '    train loss (final epoch):     0.6167', '\\n', '    validation loss\n(final epoch): 0.6378', '\\n', '    test F1 score (final):        0.6881', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}