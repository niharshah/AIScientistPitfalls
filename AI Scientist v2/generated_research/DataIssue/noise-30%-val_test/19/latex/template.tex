\begin{filecontents}{references.bib}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  pages={4171--4186},
  year={2019}
}
\end{filecontents}

\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath,amssymb}
\usepackage[numbers]{natbib}
\usepackage{times}

\graphicspath{{figures/}}

\title{Symbolic PolyRule Reasoning:\\Challenges Beyond 70\% Performance}

\author{Anonymous Submission}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We investigate Symbolic PolyRule Reasoning tasks and expose unexpected failure modes in neural architectures. Our empirical findings highlight the challenges in surpassing a persistent performance plateau, underscoring critical pitfalls for real-world application.
\end{abstract}

\section{Introduction}
Deep neural models often excel at large-scale data tasks. However, symbolic or rule-based reasoning remains surprisingly elusive. In this study, we explore Transformer-based methods and baseline sequence models for a PolyRule triage task. Despite high expressiveness, these approaches plateau around 70\% macro-F1. We analyze overfitting, interventions involving dropout, and hyperparameter tuning to show how these models struggle with rule-based domain nuances. Our contribution is a careful presentation of partial improvements, revealing where results remain firmly below practical thresholds.

\section{Related Work}
Significant progress in sequence transduction has been achieved via attention mechanisms~\citep{vaswani2017attention,devlin2019bert}. Yet, precise logical rule learning remains a hurdle~\citep{goodfellow2014explaining}. Recent works highlight the brittleness of neural architectures in the presence of ambiguous symbolic tasks but offer limited insight into performance ceilings. Our findings expand on these concerns by showcasing consistent performance stagnation across varied settings.

\section{Method}
We study classification-based reasoning over multiple rule combinations. The dataset includes structured input tokens, each requiring accurate inference under hidden constraints. We train a CharBiGRU baseline and a Transformer variant. Key hyperparameters (learning rate, batch size) follow standard practice. Dropout rates vary across experiments to probe generalization. 

\section{Experiments}
Models are trained on identical splits (train/validation/test). The main comparison is depicted in Figure~\ref{fig:main_figure}. Sub-figure (a) illustrates the baseline sensitivity to dropout: high rates mitigate overfitting yet degrade performance, while lower rates risk overfitting. Sub-figure (b) shows the Transformer's training curves converging around the 70\% macro-F1 mark, hinting at a persistent bottleneck. 

\begin{figure}[!ht]
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\linewidth]{baseline_dropout.png}
  \caption{Baseline CharBiGRU performance.}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\linewidth]{transformer_curves.png}
  \caption{Transformer training and validation.}
\end{subfigure}
\caption{Key results illustrating plateaued performance across models.}
\label{fig:main_figure}
\end{figure}

We observe minor improvements from ablations (embedding variations, feedforward removal, etc.), yet none substantially exceed 70\% macro-F1. Further details appear in the Appendix. These outcomes suggest that a purely neural approach may struggle to capture intricate symbolic rules without more specialized architecture or data augmentation.

\section{Conclusion}
Our negative and inconclusive results underline fundamental obstacles in Symbolic PolyRule Reasoning using common neural methods. Despite extensive tuning, performance remains capped. We propose investigating hybrid rule-based-neural designs or structured constraints as potential future steps.

{\small
\bibliographystyle{plainnat}
\bibliography{references}
}

\clearpage
\appendix
\section{Supplementary Material}
Additional ablation results and alternative training curves are presented here. Figures~\ref{fig:ablation_embedding} and~\ref{fig:ablation_ffn} highlight small differences from variations in embedding or feedforward layers, verifying that no single alteration decisively breaks the persistent performance barrier.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.35\textwidth]{ablation_embedding.png}
\caption{Ablation removing learned embeddings.}
\label{fig:ablation_embedding}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.35\textwidth]{ablation_ffn.png}
\caption{Ablation removing feedforward network.}
\label{fig:ablation_ffn}
\end{figure}

\end{document}