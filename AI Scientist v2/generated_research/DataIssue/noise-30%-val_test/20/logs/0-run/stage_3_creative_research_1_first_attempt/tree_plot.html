<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 2], [0, 4], [0, 3], [2, 7], [2, 8], [2, 5], [3, 6], [5, 12], [5, 11], [5, 10], [5, 9]], "layout": [[0.6363636363636364, 0.0], [0.2727272727272727, 0.33333333333333337], [0.45454545454545453, 0.33333333333333337], [0.8181818181818182, 0.33333333333333337], [1.0, 0.33333333333333337], [0.2727272727272727, 0.6666666666666667], [0.8181818181818182, 0.6666666666666667], [0.45454545454545453, 0.6666666666666667], [0.6363636363636364, 0.6666666666666667], [0.0, 1.0], [0.18181818181818182, 1.0], [0.36363636363636365, 1.0], [0.5454545454545454, 1.0]], "plan": ["Hyperparam tuning name: batch_size. We will sweep over four candidate batch\nsizes (32, 64, 128, 256).   For each value we rebuild the dataloaders, train the\nsame GRU for five epochs, record per-epoch train/validation loss and macro-F1,\nand keep the final-epoch predictions/labels.   All results are stored in a\nhierarchical experiment_data dictionary under the top-level key \"batch_size\",\nthen saved to experiment_data.npy for later plotting/analysis.", "Our first attempt replaces the previous GRU with a light Transformer encoder,\nadds the requested Complexity-Weighted Accuracy (CWA) metric, saves every\nmetric/loss in the required `experiment_data` structure, and keeps the most\nstable batch-size = 32 setting found earlier.  We automatically fall back to a\nsynthetic dataset if SPR_BENCH is absent and treat the optional \u201ccomplexity\u201d\ncolumn (number of atomic predicates) as weight; if missing we simply use weight\n= 1, so the script always runs.  The code trains for ten epochs with early-\nstopping patience = 3, prints loss/accuracy/CWA each epoch, and finally stores\n`experiment_data.npy` in the ./working directory.", "We switch from the previous GRU to a light Transformer encoder that can attend\nover the whole symbol string and capture long-range poly-factor relations. Each\nsequence is prepended with a learned <cls> token whose hidden state is used for\nclassification. We keep the best batch size (32) found earlier, train for 8\nepochs with Adam and cosine LR-decay, and compute the requested Complexity-\nWeighted Accuracy (CWA) in addition to macro-F1 and the usual losses. If the\nbenchmark provides an explicit \u201ccomplexity\u201d column we use it; otherwise we\ndefault to weight = 1 so the metric collapses to vanilla accuracy. Results,\nlosses and predictions are stored in the hierarchical experiment_data dict and\nsaved to ./working/experiment_data.npy for later analysis.", "We switch to a lightweight Transformer encoder that can model long-range symbol\ninteractions better than our previous GRU.  Each character is embedded, enriched\nwith a learnable positional embedding, passed through two self-attention layers,\nmean-pooled, and classified.  We keep a compact hidden size (128) so the full\ntraining set (20 k sequences) can run in just a few minutes on one GPU/CPU.  We\nadd early-stopping patience of three epochs, AdamW with warm-up, and gradient\nclipping for stability.  Besides macro-F1 we implement the required Complexity-\nWeighted Accuracy (CWA) by weighting each prediction with its sequence length (a\nproxy for rule complexity in the absence of explicit predicate counts).  During\ntraining we log train/val loss and CWA every epoch and store everything in the\nmandated experiment_data structure, which is finally saved to\n./working/experiment_data.npy.  A synthetic dataset is auto-generated if\nSPR_BENCH is missing so the script is fully self-contained.  The whole pipeline\nobeys the device-handling rules and should clearly outperform the earlier GRU\nbaseline.", "Our first refinement replaces the GRU with a small Transformer encoder, which is\nbetter suited to capture the long-range, multi-factor dependencies that govern\nSPR sequences.  We keep the best-observed batch size (32), add learned\npositional embeddings, label-smoothing cross-entropy, cosine learning-rate decay\nand early stopping on the dev loss.  Complexity-Weighted Accuracy (CWA) is\ncomputed every epoch; if a column called \u201ccomplexity\u201d (or \u201crule_complexity\u201d)\nexists it is used as a per-example weight, otherwise a weight of 1 is assumed.\nAll training / validation losses, macro-F1 and CWA are logged to an\nexperiment_data dict and saved at the end.  The whole script is self-contained,\nGPU-aware, and should finish in <30 min on a single GPU.", "We enhance the previous baseline with three ideas: (1) a deeper Transformer (4\nlayers, larger hidden size) plus learnable relative-positional bias to better\ncapture long-range poly-factor rules; (2) label-smoothing Cross-Entropy to\nimprove robustness on ambiguous rules; (3) curriculum training by initially\ndown-weighting examples with high estimated complexity (unique-token count) and\nthen annealing weights toward 1.0 \u2013 this steadies early learning yet emphasises\ndifficult rules later, boosting final Complexity-Weighted Accuracy. We keep the\ndata pipeline, ensure full device handling, track CWA and Macro-F1 every epoch,\nand stop early when validation loss hasn\u2019t improved for 3 epochs. The script\nstores all metrics/predictions under ./working and prints the final scores.", "The validation crash occurred because `compute_metrics` expected a 2-D logits\ntensor (so it could do `argmax(1)`), but at the end of the loop we mistakenly\npassed it a 1-D vector of already-predicted class indices. The fix is to (i)\nmake `compute_metrics` robust enough to accept either logits (2-D) or plain\npredictions (1-D) and (ii) compute the final epoch-level CWA/F1 directly from\nthe aggregated predictions without calling `compute_metrics` again. The rest of\nthe pipeline (device placement, early stopping, saving artefacts, etc.) stays\nunchanged.", "We boost reasoning ability by adding two self-supervised auxiliary tasks derived\ndirectly from the strings (length-parity and bucketized unique-symbol-count).  A\nsingle Transformer encoder feeds three heads; the auxiliary heads guide the\nrepresentation toward counting and set-cardinality features that mimic common\nSPR atomic predicates.  Loss is the weighted sum of main CE and the two\nauxiliary CEs, with weights 1.0, 0.3, 0.3.  We keep past successes (batch=32,\ncosine LR, early stopping) and log Complexity-Weighted Accuracy.  The code falls\nback to a small synthetic dataset if SPR_BENCH is absent so it runs anywhere.", "We will boost performance and CWA by (1) increasing model capacity (128-dim\nembeddings, 4-layer transformer, RoPE-like sinusoid positional encoding), (2)\ntraining with complexity-weighted cross-entropy so harder (more predicates /\nlonger) sequences drive the gradient, and (3) using early-stopping with the\nbest-CWA checkpoint. The same char vocabulary is kept, and if the dataset lacks\nan explicit \u201ccomplexity\u201d column we proxy it with sequence length. We track loss,\nMacro-F1 and CWA each epoch, save all metrics/predictions, and automatically\nrestore the best model before final test evaluation.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\n# -------------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\n# hyperparameter sweep over batch sizes\nbatch_sizes = [32, 64, 128, 256]\nepochs = 5\nexperiment_data = {\"batch_size\": {}}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size = {bs} ===\")\n    # dataloaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate)\n    dev_loader = DataLoader(dev_ds, batch_size=512, shuffle=False, collate_fn=collate)\n    # model, loss, optim\n    model = CharGRU(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # logs\n    logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tot_loss, tot_items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n        train_loss = tot_loss / tot_items\n        # validation\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds.tolist())\n                all_labels.extend(labels.tolist())\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        # record\n        logs[\"losses\"][\"train\"].append(train_loss)\n        logs[\"losses\"][\"val\"].append(val_loss)\n        logs[\"metrics\"][\"train\"].append(None)  # not computing train F1 to save time\n        logs[\"metrics\"][\"val\"].append(macro_f1)\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | Macro-F1={macro_f1:.4f}\"\n        )\n    # store final epoch preds/labels\n    logs[\"predictions\"] = all_preds\n    logs[\"ground_truth\"] = all_labels\n    # save under experiment_data\n    if \"SPR_BENCH\" not in experiment_data[\"batch_size\"]:\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"] = {}\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][f\"bs_{bs}\"] = logs\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------------------------------------------------------\n# Mandatory working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------------------------------------------------\n# ------------- helper: load SPR_BENCH or build synthetic -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    # ---------- quick synthetic generator with a 'complexity' column ----------\n    def synth_split(n, start_id=0):\n        rows = []\n        for i in range(start_id, start_id + n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:12], k=seq_len))\n            complexity = random.randint(1, 5)  # pretend #atomic predicates\n            # a 2-predicate xor rule as a toy\n            even_a = seq.count(\"A\") % 2 == 0\n            has_c = \"C\" in seq\n            label = int(even_a ^ has_c)\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": label, \"complexity\": complexity}\n            )\n        return rows\n\n    print(\"SPR_BENCH not found \u2013 creating synthetic data.\")\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": synth_split(4000)}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": synth_split(800, 4000)}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": synth_split(800, 4800)}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# --------------------------------------------------------------------\n# ------------------------- vocabulary --------------------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# --------------------------------------------------------------------\n# ---------------------- Torch Dataset wrapper ------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n        self.has_complex = \"complexity\" in hf_dataset.column_names\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        w = ex[\"complexity\"] if \"complexity\" in ex else 1.0\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weight\": torch.tensor(float(w), dtype=torch.float),\n        }\n\n\ndef collate(batch):\n    return {\n        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n        \"labels\": torch.stack([b[\"label\"] for b in batch]),\n        \"weights\": torch.stack([b[\"weight\"] for b in batch]),\n    }\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\nbatch_size = 32\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------------------------------------------------------------\n# ---------------------------- Model ----------------------------------\nclass TinyTransformer(nn.Module):\n    def __init__(self, vocab, emb_dim=128, nhead=4, nlayers=2, nclass=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=nhead, dim_feedforward=emb_dim * 2\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.fc = nn.Linear(emb_dim, nclass)\n\n    def forward(self, x):\n        mask = x == 0  # pad mask\n        emb = self.emb(x).transpose(0, 1)  # (S,B,E) for transformer\n        enc = self.encoder(emb, src_key_padding_mask=mask).transpose(\n            0, 1\n        )  # back to (B,S,E)\n        pooled = enc.mean(dim=1)\n        return self.fc(pooled)\n\n\nmodel = TinyTransformer(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# --------------------------------------------------------------------\n# ------------------ Metric: Complexity-Weighted Accuracy -------------\ndef complexity_weighted_accuracy(preds, labels, weights):\n    correct = (preds == labels).astype(float)\n    return (correct * weights).sum() / weights.sum()\n\n\n# --------------------------------------------------------------------\n# --------------- experiment data container ---------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"accuracy\": [], \"cwa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\n# --------------------------------------------------------------------\n# -------------------------- Training ---------------------------------\nepochs = 10\nbest_cwa, patience, wait = 0.0, 3, 0\n\nfor epoch in range(1, epochs + 1):\n    # ----------------- train -----------------\n    model.train()\n    running_loss, n_items = 0.0, 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n        n_items += batch[\"labels\"].size(0)\n    train_loss = running_loss / n_items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ----------------- validation -----------------\n    model.eval()\n    val_loss, n_val = 0.0, 0\n    all_preds, all_labels, all_w = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch_cpu = {k: v for k, v in batch.items()}\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            n_val += batch[\"labels\"].size(0)\n            preds = logits.argmax(1).cpu().numpy()\n            labels = batch_cpu[\"labels\"].numpy()\n            wts = batch_cpu[\"weights\"].numpy()\n            all_preds.extend(preds.tolist())\n            all_labels.extend(labels.tolist())\n            all_w.extend(wts.tolist())\n    val_loss /= n_val\n    acc = accuracy_score(all_labels, all_preds)\n    cwa = complexity_weighted_accuracy(\n        np.array(all_preds), np.array(all_labels), np.array(all_w)\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"accuracy\"].append(acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"cwa\"].append(cwa)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, Acc={acc:.4f}, CWA={cwa:.4f}\")\n\n    # early stopping on best CWA\n    if cwa > best_cwa:\n        best_cwa, wait = cwa, 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# --------------------------------------------------------------------\n# -------- evaluate on test set with best saved parameters -----------\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best_model.pt\"), map_location=device)\n)\nmodel.eval()\ntest_preds, test_labels, test_w = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch_cpu = {k: v for k, v in batch.items()}\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        preds = logits.argmax(1).cpu().numpy()\n        test_preds.extend(preds.tolist())\n        test_labels.extend(batch_cpu[\"labels\"].numpy().tolist())\n        test_w.extend(batch_cpu[\"weights\"].numpy().tolist())\ntest_acc = accuracy_score(test_labels, test_preds)\ntest_cwa = complexity_weighted_accuracy(\n    np.array(test_preds), np.array(test_labels), np.array(test_w)\n)\nprint(f\"\\nTest Accuracy = {test_acc:.4f}, Test CWA = {test_cwa:.4f}\")\n\n# store predictions/gt/weights\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = test_w\n\n# --------------------------------------------------------------------\n# ---------------- save experiment data -------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# workspace / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# load SPR_BENCH or synthesize dummy data\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\n\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating small synthetic dataset.\")\n\n    def synth_split(n, start_id=0):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:12], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append(\n                {\n                    \"id\": start_id + i,\n                    \"sequence\": seq,\n                    \"label\": label,\n                    \"complexity\": seq_len % 4 + 1,\n                }\n            )\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(4000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(800, 4000)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(800, 4800)}, split=\"train\"\n    )\n\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary incl. <pad>, <unk>, <cls>\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(ch, vocab[\"<unk>\"]) for ch in seq][\n        : max_len - 1\n    ]\n    if len(ids) < max_len:\n        ids += [vocab[\"<pad>\"]] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)  # +1 for <cls>\n\n\n# -------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        seq_ids = torch.tensor(encode(ex[\"sequence\"], MAX_LEN), dtype=torch.long)\n        label = torch.tensor(int(ex[\"label\"]), dtype=torch.long)\n        weight = torch.tensor(float(ex.get(\"complexity\", 1.0)), dtype=torch.float)\n        return {\"input_ids\": seq_ids, \"labels\": label, \"weights\": weight}\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = map(\n    SPRTorchDataset, (spr[\"train\"], spr[\"dev\"], spr[\"test\"])\n)\n\n\n# -------------------------------------------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, emb_dim=64, nhead=4, num_layers=2, dim_ff=128, num_classes=2\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.pos = nn.Parameter(torch.randn(MAX_LEN, emb_dim))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=nhead,\n            dim_feedforward=dim_ff,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        mask = x == vocab[\"<pad>\"]\n        h = self.emb(x) + self.pos[: x.size(1)]\n        h = self.encoder(h, src_key_padding_mask=mask)\n        cls_h = h[:, 0]  # take <cls>\n        return self.fc(cls_h)\n\n\n# -------------------------------------------------\ndef complexity_weighted_accuracy(preds, labels, weights):\n    correct = (preds == labels).astype(float)\n    return (correct * weights).sum() / weights.sum()\n\n\n# -------------------------------------------------\nbatch_size = 32\nepochs = 8\nmodel = CharTransformer(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nfor epoch in range(1, epochs + 1):\n    # ---- training ----\n    model.train()\n    tot_loss, tot_items = 0.0, 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        tot_items += batch[\"labels\"].size(0)\n    train_loss = tot_loss / tot_items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    val_loss, val_items = 0.0, 0\n    all_preds, all_labels, all_w = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            val_items += batch[\"labels\"].size(0)\n            preds = logits.argmax(1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            weights = batch[\"weights\"].cpu().numpy()\n            all_preds.extend(preds.tolist())\n            all_labels.extend(labels.tolist())\n            all_w.extend(weights.tolist())\n    val_loss /= val_items\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    cwa = complexity_weighted_accuracy(\n        np.array(all_preds), np.array(all_labels), np.array(all_w)\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": macro_f1, \"cwa\": cwa}\n    )\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | Macro-F1={macro_f1:.4f} | CWA={cwa:.4f}\"\n    )\n    scheduler.step()\n\n# store last predictions\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = all_w\n\n# -------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------- housekeeping & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------- experiment_data skeleton --------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_cwa\": [], \"val_cwa\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ----------------- data loading utilities ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found \u2013 falling back to synthetic toy data.\")\n\n    def synth(n):\n        rows = []\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 20))\n            )\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_hf(rows):\n        return load_dataset(\"json\", data_files={\"train\": rows}, split=\"train\")\n\n    spr = DatasetDict()\n    spr[\"train\"] = to_hf(synth(4000))\n    spr[\"dev\"] = to_hf(synth(800))\n    spr[\"test\"] = to_hf(synth(800))\n\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------------- vocab & encoding ----------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nmax_len = min(120, max(len(ex[\"sequence\"]) for ex in spr[\"train\"]))\nprint(f\"vocab_size={vocab_size}, max_len={max_len}\")\n\n\ndef encode(seq):\n    ids = [vocab.get(c, 1) for c in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.data = hf\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        ids = torch.tensor(encode(ex[\"sequence\"]), dtype=torch.long)\n        lbl = torch.tensor(int(ex[\"label\"]), dtype=torch.long)\n        length = torch.tensor(min(len(ex[\"sequence\"]), max_len), dtype=torch.long)\n        return {\"input_ids\": ids, \"labels\": lbl, \"lengths\": length}\n\n\ndef collate(batch):\n    out = {}\n    for k in batch[0]:\n        out[k] = torch.stack([b[k] for b in batch])\n    return out\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\n\n\n# ----------------- Transformer model --------------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        emb_dim=128,\n        n_heads=4,\n        hidden_dim=256,\n        n_layers=2,\n        num_classes=2,\n        p_drop=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.pos = nn.Embedding(max_len, emb_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim,\n            dropout=p_drop,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        b, l = x.shape\n        pos_ids = torch.arange(l, device=x.device).unsqueeze(0).expand(b, l)\n        h = self.emb(x) + self.pos(pos_ids)\n        h = self.transformer(h)\n        h = (h.masked_fill((x == 0).unsqueeze(-1), 0.0)).sum(1) / (x != 0).sum(\n            1, keepdim=True\n        ).clamp(\n            min=1\n        )  # mean over non-pad\n        return self.fc(h)\n\n\n# ----------------- training hyperparams -----------------\nbatch_size = 32\nepochs = 15\npatience = 3\nlr = 1e-4\nwarmup_steps = 200\nclip = 1.0\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nmodel = CharTransformer(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer, lambda step: min((step + 1) / warmup_steps, 1.0)\n)\n\nbest_val_cwa = -1\nstale = 0\n\n\n# ----------------- helper for metrics -------------------\ndef compute_metrics(logits, labels, lengths):\n    preds = logits.argmax(1)\n    correct = (preds == labels).cpu().numpy()\n    weights = lengths.cpu().numpy().astype(np.float32)\n    cwa = (correct * weights).sum() / weights.sum()\n    f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n    return cwa, f1, preds.cpu().tolist()\n\n\n# ----------------- training loop ------------------------\nfor epoch in range(1, epochs + 1):\n    # --- train\n    model.train()\n    tot_loss, tot_tokens = 0, 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        scheduler.step()\n        bsz = batch[\"labels\"].size(0)\n        tot_loss += loss.item() * bsz\n        tot_tokens += bsz\n    train_loss = tot_loss / tot_tokens\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # --- validation\n    model.eval()\n    val_loss, tot = 0, 0\n    all_preds, all_labels, all_lengths = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            tot += batch[\"labels\"].size(0)\n            cwa, _, preds = compute_metrics(logits, batch[\"labels\"], batch[\"lengths\"])\n            all_preds.extend(preds)\n            all_labels.extend(batch[\"labels\"].cpu().tolist())\n            all_lengths.extend(batch[\"lengths\"].cpu().tolist())\n    val_loss /= tot\n    val_cwa, val_f1, _ = compute_metrics(\n        torch.tensor(all_preds), torch.tensor(all_labels), torch.tensor(all_lengths)\n    )\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | CWA={val_cwa:.4f} | Macro-F1={val_f1:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_cwa\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cwa\"].append(val_cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    # --- early stopping\n    if val_cwa > best_val_cwa:\n        best_val_cwa = val_cwa\n        stale = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        stale += 1\n        if stale >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# ----------------- test evaluation ----------------------\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best_model.pt\"), map_location=device)\n)\nmodel.eval()\nall_preds, all_labels, all_lengths = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        preds = logits.argmax(1)\n        all_preds.extend(preds.cpu().tolist())\n        all_labels.extend(batch[\"labels\"].cpu().tolist())\n        all_lengths.extend(batch[\"lengths\"].cpu().tolist())\ntest_cwa = (\n    (np.array(all_preds) == np.array(all_labels)) * np.array(all_lengths)\n).sum() / np.array(all_lengths).sum()\ntest_f1 = f1_score(all_labels, all_preds, average=\"macro\")\nprint(f\"TEST   : CWA={test_cwa:.4f} | Macro-F1={test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n\n# ----------------- save artefacts -----------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# optional benchmark loader (from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    out = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        out[split] = _load(f\"{split}.csv\")\n    return out\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    # synthetic fallback (simple parity rule)\n    def synth(n, start_id=0):\n        rows = []\n        for i in range(start_id, start_id + n):\n            L = random.randint(5, 18)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:12], k=L))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth(4000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth(800, 5000)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth(800, 6000)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------------\n# vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\nprint(f\"Vocab size={vocab_size}, max_len={max_len}\")\n\n\ndef encode(seq):\n    ids = [vocab.get(c, 1) for c in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\n# ------------------------------------------------------------------\n# torch dataset\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.data = hf_ds\n        self.weight_key = (\n            \"complexity\"\n            if \"complexity\" in hf_ds.column_names\n            else (\n                \"rule_complexity\" if \"rule_complexity\" in hf_ds.column_names else None\n            )\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        item = {\n            \"input_ids\": torch.tensor(encode(ex[\"sequence\"]), dtype=torch.long),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weight\": (\n                torch.tensor(float(ex[self.weight_key]))\n                if self.weight_key\n                else torch.tensor(1.0)\n            ),\n        }\n        return item\n\n\ndef collate(batch):\n    out = {}\n    for k in batch[0]:\n        out[k] = torch.stack([b[k] for b in batch])\n    out[\"labels\"] = out.pop(\"label\")\n    return out\n\n\ntrain_ds, dev_ds, test_ds = (SPRTorchDataset(spr[s]) for s in [\"train\", \"dev\", \"test\"])\n\n\n# ------------------------------------------------------------------\n# Transformer classifier\nclass TransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab,\n        d_model=128,\n        nhead=4,\n        nlayers=2,\n        dim_ff=256,\n        n_classes=2,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.fc = nn.Linear(d_model, n_classes)\n\n    def forward(self, x):\n        h = self.embed(x) + self.pos[:, : x.size(1), :]\n        h = self.enc(h, src_key_padding_mask=(x == 0))\n        h = h.masked_fill((x == 0).unsqueeze(-1), 0)  # zero-out pads\n        h = h.sum(1) / (x != 0).sum(1, keepdim=True).clamp(min=1)  # mean pooling\n        return self.fc(h)\n\n\nmodel = TransformerClassifier(vocab_size).to(device)\n\n# ------------------------------------------------------------------\n# training prep\nbatch_size = 32\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\n\n# ------------------------------------------------------------------\ndef complexity_weighted_acc(preds, labels, weights):\n    correct = (preds == labels).astype(float)\n    return (correct * weights).sum() / weights.sum()\n\n\n# experiment data dict\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"val_macroF1\": [], \"val_CWA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nbest_val_loss, patience, es_counter = float(\"inf\"), 3, 0\nepochs = 15\nfor epoch in range(1, epochs + 1):\n    # ----------------- train -----------------\n    model.train()\n    tot_loss = tot_items = 0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        tot_items += batch[\"labels\"].size(0)\n    train_loss = tot_loss / tot_items\n    # ----------------- validate -----------------\n    model.eval()\n    v_loss = v_items = 0\n    all_preds, all_labels, all_w = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            v_loss += loss.item() * batch[\"labels\"].size(0)\n            v_items += batch[\"labels\"].size(0)\n            preds = logits.argmax(1).cpu().numpy()\n            lbls = batch[\"labels\"].cpu().numpy()\n            wts = batch[\"weight\"].cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(lbls)\n            all_w.extend(wts)\n    val_loss = v_loss / v_items\n    macroF1 = f1_score(all_labels, all_preds, average=\"macro\")\n    CWA = complexity_weighted_acc(\n        np.array(all_preds), np.array(all_labels), np.array(all_w)\n    )\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_macroF1={macroF1:.4f} | CWA={CWA:.4f}\"\n    )\n    # record\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(macroF1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_CWA\"].append(CWA)\n    scheduler.step()\n    # early stopping\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        es_counter = 0\n    else:\n        es_counter += 1\n        if es_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# store last dev preds for inspection\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Saved logs to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):  # simple parity rule on \u201cA\u201d\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\n# estimated complexity = number of unique tokens (proxy if not provided)\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):  # simple T5-style bias\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = torch.arange(klen)[None] - torch.arange(qlen)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)  # [qlen,klen,heads]\n\n    def forward(self, qlen, klen):\n        return self.forward__(qlen, klen).permute(2, 0, 1)  # [heads,qlen,klen]\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encs = []\n        for _ in range(layers):\n            encs.append(\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n            )\n        self.enc = nn.ModuleList(encs)\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS token\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# curriculum weight schedule\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))  # linearly to 1 by half epochs\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        # down-weight complex examples early\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(\n        f\"Epoch {epoch+1}: validation_loss = {vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\"\n    )\n    # early stopping\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, math, random, string, pathlib, time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- housekeeping ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data dict ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_cwa\": [], \"val_cwa\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- data loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found \u2013 using synthetic toy data.\")\n\n    def synth(n):\n        rows = []\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 20))\n            )\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_hf(rows):\n        return load_dataset(\"json\", data_files={\"train\": rows}, split=\"train\")\n\n    spr = DatasetDict()\n    spr[\"train\"] = to_hf(synth(2000))\n    spr[\"dev\"] = to_hf(synth(500))\n    spr[\"test\"] = to_hf(synth(1000))\n\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- vocab ----------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nmax_len = min(120, max(len(ex[\"sequence\"]) for ex in spr[\"train\"]))\nprint(f\"vocab_size={vocab_size}, max_len={max_len}\")\n\n\ndef encode(seq):\n    ids = [vocab.get(c, 1) for c in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\n# ---------- dataset ----------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        ids = torch.tensor(encode(ex[\"sequence\"]), dtype=torch.long)\n        lbl = torch.tensor(int(ex[\"label\"]), dtype=torch.long)\n        length = torch.tensor(min(len(ex[\"sequence\"]), max_len), dtype=torch.long)\n        return {\"input_ids\": ids, \"labels\": lbl, \"lengths\": length}\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRDataset(spr[\"train\"]),\n    SPRDataset(spr[\"dev\"]),\n    SPRDataset(spr[\"test\"]),\n)\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab,\n        emb_dim=128,\n        n_heads=4,\n        hidden_dim=256,\n        n_layers=2,\n        num_classes=2,\n        p_drop=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=0)\n        self.pos = nn.Embedding(max_len, emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim,\n            dropout=p_drop,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        b, l = x.shape\n        pos_ids = torch.arange(l, device=x.device).unsqueeze(0).expand(b, l)\n        h = self.emb(x) + self.pos(pos_ids)\n        h = self.transformer(h)\n        h = (h.masked_fill((x == 0).unsqueeze(-1), 0.0)).sum(1) / (x != 0).sum(\n            1, keepdim=True\n        ).clamp(min=1)\n        return self.fc(h)\n\n\n# ---------- hyperparams ----------\nbatch_size = 32\nepochs = 15\npatience = 3\nlr = 1e-4\nwarmup_steps = 200\ngrad_clip = 1.0\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nmodel = CharTransformer(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.LambdaLR(\n    optimizer, lambda step: min((step + 1) / warmup_steps, 1.0)\n)\n\nbest_val_cwa, stale = -1, 0\n\n\n# ---------- metric helper ----------\ndef compute_metrics(logits_or_preds, labels, lengths):\n    \"\"\"\n    Accepts either raw logits (2-D) or already argmax'ed predictions (1-D).\n    Returns CWA, macro-F1, preds list.\n    \"\"\"\n    if logits_or_preds.dim() == 2:  # logits\n        preds = logits_or_preds.argmax(1)\n    else:  # already predictions\n        preds = logits_or_preds\n    correct = (preds == labels).cpu().numpy()\n    w = lengths.cpu().numpy().astype(np.float32)\n    cwa = (correct * w).sum() / w.sum()\n    f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average=\"macro\")\n    return cwa, f1, preds.cpu().tolist()\n\n\n# ---------- training ----------\nfor epoch in range(1, epochs + 1):\n    # ----- train -----\n    model.train()\n    tot_loss, tot_samples = 0.0, 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        scheduler.step()\n        bs = batch[\"labels\"].size(0)\n        tot_loss += loss.item() * bs\n        tot_samples += bs\n    train_loss = tot_loss / tot_samples\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ----- validation -----\n    model.eval()\n    val_loss, val_samples = 0.0, 0\n    all_preds, all_labels, all_lengths = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            val_loss += loss.item() * bs\n            val_samples += bs\n            _, _, preds = compute_metrics(logits, batch[\"labels\"], batch[\"lengths\"])\n            all_preds.extend(preds)\n            all_labels.extend(batch[\"labels\"].cpu().tolist())\n            all_lengths.extend(batch[\"lengths\"].cpu().tolist())\n    val_loss /= val_samples\n    val_cwa = (\n        (np.array(all_preds) == np.array(all_labels)) * np.array(all_lengths)\n    ).sum() / np.array(all_lengths).sum()\n    val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | CWA={val_cwa:.4f} | Macro-F1={val_f1:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_cwa\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cwa\"].append(val_cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    # ----- early stopping -----\n    if val_cwa > best_val_cwa:\n        best_val_cwa = val_cwa\n        stale = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        stale += 1\n        if stale >= patience:\n            print(\"Early stopping.\")\n            break\n\n# ---------- test ----------\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best_model.pt\"), map_location=device)\n)\nmodel.eval()\nall_preds, all_labels, all_lengths = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        preds = logits.argmax(1)\n        all_preds.extend(preds.cpu().tolist())\n        all_labels.extend(batch[\"labels\"].cpu().tolist())\n        all_lengths.extend(batch[\"lengths\"].cpu().tolist())\ntest_cwa = (\n    (np.array(all_preds) == np.array(all_labels)) * np.array(all_lengths)\n).sum() / np.array(all_lengths).sum()\ntest_f1 = f1_score(all_labels, all_preds, average=\"macro\")\nprint(f\"TEST   : CWA={test_cwa:.4f} | Macro-F1={test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n\n# ---------- save artefacts ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch, math\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        d[sp] = _load(f\"{sp}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    # tiny synthetic fallback\n    def synth(n, sid=0):\n        rows = []\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": sid + i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth(3000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth(600, 3000)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth(600, 3600)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        vocab.setdefault(ch, len(vocab))\nvsz = len(vocab)\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef encode(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: max_len - 1]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\ndef aux_labels(seq):\n    # aux1: parity of length (0=even,1=odd)\n    aux1 = len(seq) % 2\n    # aux2: bucketized unique symbol count: 0:(1-4),1:(5-8),2:(9+)\n    u = len(set(seq))\n    aux2 = 0 if u <= 4 else 1 if u <= 8 else 2\n    return aux1, aux2\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.d = hf_ds\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        ids = torch.tensor(encode(ex[\"sequence\"]), dtype=torch.long)\n        y = torch.tensor(int(ex[\"label\"]), dtype=torch.long)\n        a1, a2 = aux_labels(ex[\"sequence\"])\n        return {\n            \"input_ids\": ids,\n            \"labels\": y,\n            \"aux1\": torch.tensor(a1),\n            \"aux2\": torch.tensor(a2),\n            \"weights\": torch.tensor(float(ex.get(\"complexity\", 1.0))),\n        }\n\n\ndef collate(batch):\n    out = {}\n    for k in batch[0]:\n        out[k] = torch.stack([b[k] for b in batch])\n    return out\n\n\ntrain_ds, dev_ds, test_ds = map(\n    SPRTorchDataset, (spr[\"train\"], spr[\"dev\"], spr[\"test\"])\n)\n\n\n# -------------------------------------------------\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        emb_dim = 64\n        self.emb = nn.Embedding(vsz, emb_dim, padding_idx=0)\n        self.pos = nn.Parameter(torch.randn(max_len, emb_dim))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=4, dim_feedforward=128, dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=3)\n        self.main_head = nn.Linear(emb_dim, 2)\n        self.aux1_head = nn.Linear(emb_dim, 2)\n        self.aux2_head = nn.Linear(emb_dim, 3)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[: x.size(1)]\n        h = self.encoder(h, src_key_padding_mask=mask)\n        cls = h[:, 0]\n        return self.main_head(cls), self.aux1_head(cls), self.aux2_head(cls)\n\n\ndef cw_accuracy(pred, lab, wt):\n    correct = (pred == lab).astype(float)\n    return (correct * wt).sum() / wt.sum()\n\n\n# -------------------------------------------------\nbatch = 32\nepochs = 10\nearly_patience = 3\nmodel = Model().to(device)\nopt = torch.optim.AdamW(model.parameters(), lr=3e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\nce = nn.CrossEntropyLoss()\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nbest_cwa = 0\nno_improve = 0\nfor ep in range(1, epochs + 1):\n    model.train()\n    tl = 0\n    n = 0\n    for bt in train_loader:\n        bt = {k: v.to(device) for k, v in bt.items()}\n        opt.zero_grad()\n        out_main, out_a1, out_a2 = model(bt[\"input_ids\"])\n        loss = (\n            ce(out_main, bt[\"labels\"])\n            + 0.3 * ce(out_a1, bt[\"aux1\"])\n            + 0.3 * ce(out_a2, bt[\"aux2\"])\n        )\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tl += loss.item() * bt[\"labels\"].size(0)\n        n += bt[\"labels\"].size(0)\n    train_loss = tl / n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # validation\n    model.eval()\n    vl = 0\n    vn = 0\n    preds = []\n    labs = []\n    wts = []\n    with torch.no_grad():\n        for bt in dev_loader:\n            bt = {k: v.to(device) for k, v in bt.items()}\n            o_main, o_a1, o_a2 = model(bt[\"input_ids\"])\n            loss = (\n                ce(o_main, bt[\"labels\"])\n                + 0.3 * ce(o_a1, bt[\"aux1\"])\n                + 0.3 * ce(o_a2, bt[\"aux2\"])\n            )\n            vl += loss.item() * bt[\"labels\"].size(0)\n            vn += bt[\"labels\"].size(0)\n            p = o_main.argmax(1).cpu().numpy()\n            preds.extend(p.tolist())\n            labs.extend(bt[\"labels\"].cpu().numpy().tolist())\n            wts.extend(bt[\"weights\"].cpu().numpy().tolist())\n    val_loss = vl / vn\n    macro = f1_score(labs, preds, average=\"macro\")\n    cwa = cw_accuracy(np.array(preds), np.array(labs), np.array(wts))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": macro, \"cwa\": cwa}\n    )\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | Macro-F1={macro:.4f} | CWA={cwa:.4f}\"\n    )\n    if cwa > best_cwa + 1e-4:\n        best_cwa = cwa\n        no_improve = 0\n    else:\n        no_improve += 1\n    if no_improve >= early_patience:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labs\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = wts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---- working dir / device ---------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---- load SPR_BENCH (fall back to toy synth) --------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dd[split] = _load(f\"{split}.csv\")\n    return dd\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic demo if real data unavailable\n    print(\"SPR_BENCH not found \u2013 building synthetic toy data\")\n\n    def synth(n, start_id=0):\n        rows = []\n        for i in range(n):\n            L = random.randint(5, 17)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:12], k=L))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append(\n                {\"id\": start_id + i, \"sequence\": seq, \"label\": label, \"complexity\": L}\n            )\n        return rows\n\n    spr = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [4000, 800, 800]):\n        spr[split] = load_dataset(\"json\", data_files={\"train\": synth(n)}, split=\"train\")\n\nprint({k: len(v) for k, v in spr.items()})\n\n# ---- vocabulary & encoding ---------------------------------------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: max_len - 1]\n    ids = ids + ([0] * (max_len - len(ids)))\n    return ids\n\n\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\n# ---- dataset to torch --------------------------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.ds = hf_ds\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        ex = self.ds[idx]\n        seq_ids = torch.tensor(encode(ex[\"sequence\"], MAX_LEN), dtype=torch.long)\n        label = torch.tensor(int(ex[\"label\"]), dtype=torch.long)\n        if \"complexity\" in ex:\n            w = float(ex[\"complexity\"])\n        else:\n            w = float(len(ex[\"sequence\"]))\n        return {\"input_ids\": seq_ids, \"labels\": label, \"weights\": torch.tensor(w)}\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = map(SPRDataset, (spr[\"train\"], spr[\"dev\"], spr[\"test\"]))\n\n\n# ---- sinusoidal positional enc ----------------------------------------------\ndef sinusoid_position_encoding(L, D, device):\n    pe = torch.zeros(L, D, device=device)\n    pos = torch.arange(0, L, device=device).float()[:, None]\n    i = torch.arange(0, D, device=device).float()[None, :]\n    angle = pos / torch.pow(10000, (2 * (i // 2)) / D)\n    pe[:, 0::2] = torch.sin(angle[:, 0::2])\n    pe[:, 1::2] = torch.cos(angle[:, 1::2])\n    return pe\n\n\n# ---- model -------------------------------------------------------------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, emb_dim=128, nhead=8, layers=4, ff_dim=256, classes=2\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.register_buffer(\n            \"pos_pe\", sinusoid_position_encoding(MAX_LEN, emb_dim, \"cpu\")\n        )\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=nhead,\n            dim_feedforward=ff_dim,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, layers)\n        self.norm = nn.LayerNorm(emb_dim)\n        self.out = nn.Linear(emb_dim, classes)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos_pe[: x.size(1)]\n        h = self.encoder(h, src_key_padding_mask=mask)\n        cls = self.norm(h[:, 0])\n        return self.out(cls)\n\n\n# ---- complexity weighted accuracy -------------------------------------------\ndef cwa(preds, labels, weights):\n    correct = (preds == labels).astype(float)\n    return (correct * weights).sum() / weights.sum()\n\n\n# ---- dataloaders -------------------------------------------------------------\nbatch_size = 32\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n# ---- training setup ----------------------------------------------------------\nmodel = CharTransformer(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")  # we'll weight manually\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\nepochs = 15\nbest_cwa = -1\nbest_state = None\npatience = 4\nwait = 0\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\n# ---- training loop -----------------------------------------------------------\nfor epoch in range(1, epochs + 1):\n    model.train()\n    epoch_loss = 0\n    n_items = 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        per_ex_loss = criterion(logits, batch[\"labels\"])\n        weights = batch[\"weights\"]\n        loss = (per_ex_loss * weights).mean()  # weight the loss\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n        n_items += batch[\"labels\"].size(0)\n    train_loss = epoch_loss / n_items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----------------------------------------------------------\n    model.eval()\n    v_loss = 0\n    v_items = 0\n    preds_all = []\n    labels_all = []\n    w_all = []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            per_ex = criterion(logits, batch[\"labels\"])\n            loss = (per_ex * batch[\"weights\"]).mean()\n            v_loss += loss.item() * batch[\"labels\"].size(0)\n            v_items += batch[\"labels\"].size(0)\n            preds = logits.argmax(1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            w = batch[\"weights\"].cpu().numpy()\n            preds_all.extend(preds)\n            labels_all.extend(labels)\n            w_all.extend(w)\n    v_loss /= v_items\n    macro_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n    val_cwa = cwa(np.array(preds_all), np.array(labels_all), np.array(w_all))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": macro_f1, \"cwa\": val_cwa}\n    )\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | Macro-F1={macro_f1:.4f} | CWA={val_cwa:.4f}\"\n    )\n\n    # early stopping on CWA\n    if val_cwa > best_cwa + 1e-4:\n        best_cwa = val_cwa\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n    if wait >= patience:\n        print(\"Early stopping triggered.\")\n        break\n\n# ---- restore best model ------------------------------------------------------\nif best_state is not None:\n    model.load_state_dict(best_state)\n\n# ---- final test evaluation ---------------------------------------------------\nmodel.eval()\npreds_test = []\nlabels_test = []\nw_test = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        preds = logits.argmax(1).cpu().numpy()\n        labels = batch[\"labels\"].cpu().numpy()\n        w = batch[\"weights\"].cpu().numpy()\n        preds_test.extend(preds)\n        labels_test.extend(labels)\n        w_test.extend(w)\nmacro_f1_test = f1_score(labels_test, preds_test, average=\"macro\")\ncwa_test = cwa(np.array(preds_test), np.array(labels_test), np.array(w_test))\nprint(f\"TEST: Macro-F1={macro_f1_test:.4f} | CWA={cwa_test:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_test\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_test\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = w_test\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):  # simple parity rule on \u201cA\u201d\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\n# estimated complexity = number of unique tokens (proxy if not provided)\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):  # simple T5-style bias\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = torch.arange(klen)[None] - torch.arange(qlen)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)  # [qlen,klen,heads]\n\n    def forward(self, qlen, klen):\n        return self.forward__(qlen, klen).permute(2, 0, 1)  # [heads,qlen,klen]\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encs = []\n        for _ in range(layers):\n            encs.append(\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n            )\n        self.enc = nn.ModuleList(encs)\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS token\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# curriculum weight schedule\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))  # linearly to 1 by half epochs\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        # down-weight complex examples early\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(\n        f\"Epoch {epoch+1}: validation_loss = {vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\"\n    )\n    # early stopping\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):  # simple parity rule on \u201cA\u201d\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\n# estimated complexity = number of unique tokens (proxy if not provided)\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):  # simple T5-style bias\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = torch.arange(klen)[None] - torch.arange(qlen)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)  # [qlen,klen,heads]\n\n    def forward(self, qlen, klen):\n        return self.forward__(qlen, klen).permute(2, 0, 1)  # [heads,qlen,klen]\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encs = []\n        for _ in range(layers):\n            encs.append(\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n            )\n        self.enc = nn.ModuleList(encs)\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS token\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# curriculum weight schedule\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))  # linearly to 1 by half epochs\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        # down-weight complex examples early\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(\n        f\"Epoch {epoch+1}: validation_loss = {vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\"\n    )\n    # early stopping\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):  # simple parity rule on \u201cA\u201d\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\n# estimated complexity = number of unique tokens (proxy if not provided)\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):  # simple T5-style bias\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = torch.arange(klen)[None] - torch.arange(qlen)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)  # [qlen,klen,heads]\n\n    def forward(self, qlen, klen):\n        return self.forward__(qlen, klen).permute(2, 0, 1)  # [heads,qlen,klen]\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encs = []\n        for _ in range(layers):\n            encs.append(\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n            )\n        self.enc = nn.ModuleList(encs)\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS token\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# curriculum weight schedule\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))  # linearly to 1 by half epochs\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        # down-weight complex examples early\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(\n        f\"Epoch {epoch+1}: validation_loss = {vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\"\n    )\n    # early stopping\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 96517.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 90982.73\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 100054.96\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '11', '\\n', '\\n=== Training with batch_size = 32 ===', '\\n', 'Epoch\n1/5 | train_loss=0.6826 | val_loss=0.6755 | Macro-F1=0.5956', '\\n', 'Epoch 2/5 |\ntrain_loss=0.6699 | val_loss=0.6886 | Macro-F1=0.6047', '\\n', 'Epoch 3/5 |\ntrain_loss=0.6648 | val_loss=0.6528 | Macro-F1=0.6454', '\\n', 'Epoch 4/5 |\ntrain_loss=0.6418 | val_loss=0.6389 | Macro-F1=0.6820', '\\n', 'Epoch 5/5 |\ntrain_loss=0.6295 | val_loss=0.6436 | Macro-F1=0.6840', '\\n', '\\n=== Training\nwith batch_size = 64 ===', '\\n', 'Epoch 1/5 | train_loss=0.6881 |\nval_loss=0.7011 | Macro-F1=0.5601', '\\n', 'Epoch 2/5 | train_loss=0.6736 |\nval_loss=0.6640 | Macro-F1=0.6480', '\\n', 'Epoch 3/5 | train_loss=0.6827 |\nval_loss=0.7561 | Macro-F1=0.3316', '\\n', 'Epoch 4/5 | train_loss=0.6928 |\nval_loss=0.6888 | Macro-F1=0.4890', '\\n', 'Epoch 5/5 | train_loss=0.6773 |\nval_loss=0.6889 | Macro-F1=0.5567', '\\n', '\\n=== Training with batch_size = 128\n===', '\\n', 'Epoch 1/5 | train_loss=0.6917 | val_loss=0.6933 | Macro-F1=0.5319',\n'\\n', 'Epoch 2/5 | train_loss=0.6772 | val_loss=0.6981 | Macro-F1=0.5792', '\\n',\n'Epoch 3/5 | train_loss=0.6684 | val_loss=0.6770 | Macro-F1=0.6299', '\\n',\n'Epoch 4/5 | train_loss=0.6465 | val_loss=0.6398 | Macro-F1=0.6839', '\\n',\n'Epoch 5/5 | train_loss=0.6535 | val_loss=0.6497 | Macro-F1=0.6596', '\\n',\n'\\n=== Training with batch_size = 256 ===', '\\n', 'Epoch 1/5 | train_loss=0.6921\n| val_loss=0.6898 | Macro-F1=0.5580', '\\n', 'Epoch 2/5 | train_loss=0.6791 |\nval_loss=0.6903 | Macro-F1=0.5668', '\\n', 'Epoch 3/5 | train_loss=0.6696 |\nval_loss=0.6873 | Macro-F1=0.5737', '\\n', 'Epoch 4/5 | train_loss=0.6599 |\nval_loss=0.6668 | Macro-F1=0.6400', '\\n', 'Epoch 5/5 | train_loss=0.6384 |\nval_loss=0.6420 | Macro-F1=0.6790', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 132561.24\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 112147.17\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 216301.58\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '11', '\\n', 'Epoch 1: val_loss=0.6536, Acc=0.6960, CWA=0.6960',\n'\\n', 'Epoch 2: val_loss=0.6360, Acc=0.6680, CWA=0.6680', '\\n', 'Epoch 3:\nval_loss=0.6402, Acc=0.6620, CWA=0.6620', '\\n', 'Epoch 4: val_loss=0.6487,\nAcc=0.6860, CWA=0.6860', '\\n', 'Early stopping triggered.', '\\n', '\\nTest\nAccuracy = 0.6930, Test CWA = 0.6930', '\\n', 'Experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 131482.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 121552.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 191154.13\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '12', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6596\n| Macro-F1=0.6270 | CWA=0.6300', '\\n', 'Epoch 2: validation_loss = 0.6450 |\nMacro-F1=0.6751 | CWA=0.6760', '\\n', 'Epoch 3: validation_loss = 0.6374 |\nMacro-F1=0.6814 | CWA=0.6820', '\\n', 'Epoch 4: validation_loss = 0.6469 |\nMacro-F1=0.6739 | CWA=0.6740', '\\n', 'Epoch 5: validation_loss = 0.6366 |\nMacro-F1=0.6858 | CWA=0.6860', '\\n', 'Epoch 6: validation_loss = 0.6361 |\nMacro-F1=0.6899 | CWA=0.6900', '\\n', 'Epoch 7: validation_loss = 0.6364 |\nMacro-F1=0.6879 | CWA=0.6880', '\\n', 'Epoch 8: validation_loss = 0.6363 |\nMacro-F1=0.6879 | CWA=0.6880', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 97658.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 93627.04\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 125446.51\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'vocab_size=11, max_len=95', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 219, in <module>\\n    val_cwa, val_f1, _ = compute_metrics(\\n\n^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 175, in compute_metrics\\n    preds =\nlogits.argmax(1)\\n            ^^^^^^^^^^^^^^^^\\nIndexError: Dimension out of\nrange (expected to be in range of [-1, 0], but got 1)\\n', 'Execution time: 3\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 158917.29\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 122062.28\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 162261.75\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize=11, max_len=95', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6826 |\nval_loss=0.6576 | val_macroF1=0.6960 | CWA=0.6960', '\\n', 'Epoch 2:\ntrain_loss=0.6386 | val_loss=0.6412 | val_macroF1=0.6692 | CWA=0.6700', '\\n',\n'Epoch 3: train_loss=0.6381 | val_loss=0.6449 | val_macroF1=0.6607 |\nCWA=0.6620', '\\n', 'Epoch 4: train_loss=0.6327 | val_loss=0.6535 |\nval_macroF1=0.6759 | CWA=0.6760', '\\n', 'Epoch 5: train_loss=0.6330 |\nval_loss=0.6645 | val_macroF1=0.6855 | CWA=0.6860', '\\n', 'Early stopping\ntriggered.', '\\n', 'Saved logs to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-14/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: validation_loss = 0.6511 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Epoch\n2: validation_loss = 0.6362 | Macro-F1=0.672 | CWA=0.672', '\\n', 'Epoch 3:\nvalidation_loss = 0.6652 | Macro-F1=0.637 | CWA=0.642', '\\n', 'Epoch 4:\nvalidation_loss = 0.6425 | Macro-F1=0.663 | CWA=0.664', '\\n', 'Epoch 5:\nvalidation_loss = 0.6335 | Macro-F1=0.684 | CWA=0.684', '\\n', 'Epoch 6:\nvalidation_loss = 0.6323 | Macro-F1=0.684 | CWA=0.684', '\\n', 'Epoch 7:\nvalidation_loss = 0.6342 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 8:\nvalidation_loss = 0.6399 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Epoch 9:\nvalidation_loss = 0.6272 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Epoch 10:\nvalidation_loss = 0.6279 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Epoch 11:\nvalidation_loss = 0.6310 | Macro-F1=0.694 | CWA=0.694', '\\n', 'Epoch 12:\nvalidation_loss = 0.6266 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Saved metrics\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Execution time: 20 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'vocab_size=11, max_len=95', '\\n', 'Epoch 1: train_loss=0.6943 | val_loss=0.6913\n| CWA=0.5200 | Macro-F1=0.5008', '\\n', 'Epoch 2: train_loss=0.6844 |\nval_loss=0.6799 | CWA=0.5400 | Macro-F1=0.4624', '\\n', 'Epoch 3:\ntrain_loss=0.6605 | val_loss=0.6466 | CWA=0.6520 | Macro-F1=0.6485', '\\n',\n'Epoch 4: train_loss=0.6281 | val_loss=0.6325 | CWA=0.6720 | Macro-F1=0.6711',\n'\\n', 'Epoch 5: train_loss=0.6231 | val_loss=0.6299 | CWA=0.6920 |\nMacro-F1=0.6918', '\\n', 'Epoch 6: train_loss=0.6182 | val_loss=0.6306 |\nCWA=0.6880 | Macro-F1=0.6878', '\\n', 'Epoch 7: train_loss=0.6155 |\nval_loss=0.6292 | CWA=0.6860 | Macro-F1=0.6857', '\\n', 'Epoch 8:\ntrain_loss=0.6135 | val_loss=0.6441 | CWA=0.6660 | Macro-F1=0.6633', '\\n',\n'Early stopping.', '\\n', 'TEST   : CWA=0.6890 | Macro-F1=0.6884', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6727\n| Macro-F1=0.6064 | CWA=0.6160', '\\n', 'Epoch 2: validation_loss = 0.6517 |\nMacro-F1=0.6438 | CWA=0.6480', '\\n', 'Epoch 3: validation_loss = 0.6394 |\nMacro-F1=0.6879 | CWA=0.6880', '\\n', 'Epoch 4: validation_loss = 0.7062 |\nMacro-F1=0.5199 | CWA=0.5660', '\\n', 'Epoch 5: validation_loss = 0.6399 |\nMacro-F1=0.6818 | CWA=0.6820', '\\n', 'Epoch 6: validation_loss = 0.6408 |\nMacro-F1=0.6840 | CWA=0.6840', '\\n', 'Early stopping.', '\\n', 'Saved experiment\ndata to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size', ' ', '12', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 63.4685\n| Macro-F1=0.6120 | CWA=0.6240', '\\n', 'Epoch 2: validation_loss = 68.4557 |\nMacro-F1=0.3316 | CWA=0.4960', '\\n', 'Epoch 3: validation_loss = 60.9734 |\nMacro-F1=0.6880 | CWA=0.6880', '\\n', 'Epoch 4: validation_loss = 60.6377 |\nMacro-F1=0.6879 | CWA=0.6880', '\\n', 'Epoch 5: validation_loss = 62.4486 |\nMacro-F1=0.6501 | CWA=0.6540', '\\n', 'Epoch 6: validation_loss = 60.6893 |\nMacro-F1=0.6820 | CWA=0.6820', '\\n', 'Epoch 7: validation_loss = 59.6666 |\nMacro-F1=0.6840 | CWA=0.6840', '\\n', 'Early stopping triggered.', '\\n', 'TEST:\nMacro-F1=0.6919 | CWA=0.6920', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 12 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: validation_loss = 0.6446 | Macro-F1=0.664 | CWA=0.666', '\\n', 'Epoch\n2: validation_loss = 0.6874 | Macro-F1=0.483 | CWA=0.540', '\\n', 'Epoch 3:\nvalidation_loss = 0.6570 | Macro-F1=0.662 | CWA=0.664', '\\n', 'Epoch 4:\nvalidation_loss = 0.6387 | Macro-F1=0.665 | CWA=0.666', '\\n', 'Epoch 5:\nvalidation_loss = 0.6410 | Macro-F1=0.674 | CWA=0.674', '\\n', 'Epoch 6:\nvalidation_loss = 0.6313 | Macro-F1=0.684 | CWA=0.684', '\\n', 'Epoch 7:\nvalidation_loss = 0.6496 | Macro-F1=0.678 | CWA=0.678', '\\n', 'Epoch 8:\nvalidation_loss = 0.6384 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 9:\nvalidation_loss = 0.6346 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Early stopping.',\n'\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-11/working/experiment_data.npy', '\\n', 'Execution time:\n16 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: validation_loss = 0.6398 | Macro-F1=0.669 | CWA=0.670', '\\n', 'Epoch\n2: validation_loss = 0.6747 | Macro-F1=0.647 | CWA=0.654', '\\n', 'Epoch 3:\nvalidation_loss = 0.6668 | Macro-F1=0.662 | CWA=0.664', '\\n', 'Epoch 4:\nvalidation_loss = 0.6365 | Macro-F1=0.667 | CWA=0.668', '\\n', 'Epoch 5:\nvalidation_loss = 0.6604 | Macro-F1=0.676 | CWA=0.676', '\\n', 'Epoch 6:\nvalidation_loss = 0.6351 | Macro-F1=0.692 | CWA=0.692', '\\n', 'Epoch 7:\nvalidation_loss = 0.6353 | Macro-F1=0.682 | CWA=0.682', '\\n', 'Epoch 8:\nvalidation_loss = 0.6296 | Macro-F1=0.692 | CWA=0.692', '\\n', 'Epoch 9:\nvalidation_loss = 0.6316 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 10:\nvalidation_loss = 0.6294 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 11:\nvalidation_loss = 0.6303 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 12:\nvalidation_loss = 0.6291 | Macro-F1=0.692 | CWA=0.692', '\\n', 'Saved metrics\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Execution time: 14 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: validation_loss = 0.6706 | Macro-F1=0.625 | CWA=0.636', '\\n', 'Epoch\n2: validation_loss = 0.6372 | Macro-F1=0.672 | CWA=0.672', '\\n', 'Epoch 3:\nvalidation_loss = 0.6400 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Epoch 4:\nvalidation_loss = 0.6372 | Macro-F1=0.667 | CWA=0.668', '\\n', 'Epoch 5:\nvalidation_loss = 0.6321 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 6:\nvalidation_loss = 0.6342 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 7:\nvalidation_loss = 0.6360 | Macro-F1=0.674 | CWA=0.674', '\\n', 'Epoch 8:\nvalidation_loss = 0.6386 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Early stopping.',\n'\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-14/working/experiment_data.npy', '\\n', 'Execution time:\n10 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "", "The execution failed due to an IndexError in the compute_metrics function.\nSpecifically, the error occurred when attempting to calculate 'preds =\nlogits.argmax(1)'. This suggests that the logits tensor does not have the\nexpected dimensions, likely because the batch size or the number of classes is\nnot as anticipated. To fix this, ensure that the logits tensor has the correct\nshape (batch_size, num_classes) before calling argmax. Debugging the forward\npass of the model to confirm the shape of the logits tensor would help identify\nthe root cause.", "The execution of the training script was successful. The model trained as\nexpected, and early stopping was triggered based on validation loss. The logs\nwere saved for further inspection. There are no bugs or issues in the output.", "", "The execution output indicates that the model training and evaluation process\ncompleted successfully without any errors or bugs. The implementation utilized a\nsynthetic dataset due to the absence of the SPR_BENCH dataset. The training\nprocess showed improvement in metrics such as CWA and Macro-F1 over epochs, and\nearly stopping was triggered after the performance plateaued. The final test\nresults achieved a CWA of 0.6890 and a Macro-F1 of 0.6884, which, while decent,\ndo not surpass the SOTA benchmark of 70%. The experiment data was saved\nsuccessfully for further analysis.", "", "", "The training script executed successfully, and the model was trained on the\nSPR_BENCH dataset. The validation loss decreased steadily, and the Macro-F1\nscore and CWA metric showed improvement over epochs. Early stopping was\ntriggered after three consecutive epochs without significant improvement in\nvalidation loss. The final saved metrics indicate reasonable performance, but\nthe Macro-F1 and CWA scores (0.696) are still below the stated SOTA benchmark of\n70%. Further experimentation and model tuning might be necessary to achieve or\nexceed the SOTA performance.", "", "", ""], "exc_type": [null, null, null, "IndexError", null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, {"args": ["Dimension out of range (expected to be in range of [-1, 0], but got 1)"]}, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 219, "<module>", "val_cwa, val_f1, _ = compute_metrics("], ["runfile.py", 175, "compute_metrics", "preds = logits.argmax(1)"]], null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error in predictions during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6295, "best_value": 0.6295}, {"dataset_name": "SPR_BENCH", "final_value": 0.6773, "best_value": 0.6773}, {"dataset_name": "SPR_BENCH", "final_value": 0.6535, "best_value": 0.6535}, {"dataset_name": "SPR_BENCH", "final_value": 0.6384, "best_value": 0.6384}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error in predictions on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6436, "best_value": 0.6389}, {"dataset_name": "SPR_BENCH", "final_value": 0.6889, "best_value": 0.664}, {"dataset_name": "SPR_BENCH", "final_value": 0.6497, "best_value": 0.6398}, {"dataset_name": "SPR_BENCH", "final_value": 0.642, "best_value": 0.642}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Evaluates the balance between precision and recall on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}, {"dataset_name": "SPR_BENCH", "final_value": 0.5567, "best_value": 0.648}, {"dataset_name": "SPR_BENCH", "final_value": 0.6596, "best_value": 0.6839}, {"dataset_name": "SPR_BENCH", "final_value": 0.679, "best_value": 0.679}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the error rate of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.628755, "best_value": 0.628755}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, used to measure the model's performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.635956, "best_value": 0.635956}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "Accuracy on the validation set, adjusted for complexity.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.693, "best_value": 0.693}]}, {"metric_name": "test complexity-weighted accuracy", "lower_is_better": false, "description": "Accuracy on the test set, adjusted for complexity.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.693, "best_value": 0.693}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.610493, "best_value": 0.610493}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.636137, "best_value": 0.636137}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Measures the macro-average F1 score for validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6899, "best_value": 0.6899}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy weighted by complexity during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.69, "best_value": 0.69}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final loss value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.633, "best_value": 0.633}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final loss value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6645, "best_value": 0.6645}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The final macro F1 score on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6855, "best_value": 0.6855}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "The final complexity-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.686, "best_value": 0.686}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.61965, "best_value": 0.61965}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.626568, "best_value": 0.626568}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695981, "best_value": 0.695981}]}, {"metric_name": "validation curriculum-weighted accuracy", "lower_is_better": false, "description": "The curriculum-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6135, "best_value": 0.6135}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6292, "best_value": 0.6292}]}, {"metric_name": "validation character-weighted accuracy", "lower_is_better": false, "description": "The character-weighted accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6918, "best_value": 0.6918}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score during test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6884, "best_value": 0.6884}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6167, "best_value": 0.6167}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6394, "best_value": 0.6394}]}, {"metric_name": "validation macro F1", "lower_is_better": false, "description": "Macro-averaged F1 score during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6879, "best_value": 0.6879}]}, {"metric_name": "validation class-weighted accuracy", "lower_is_better": false, "description": "Class-weighted accuracy during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.688, "best_value": 0.688}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 59.0337, "best_value": 59.0337}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 59.6666, "best_value": 59.6666}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.688, "best_value": 0.688}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The CWA score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.688, "best_value": 0.688}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6919, "best_value": 0.6919}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "The CWA score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "This measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.624065, "best_value": 0.624065}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "This measures the error on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.631272, "best_value": 0.631272}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "This measures the F1 score on the validation dataset, considering all classes equally. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695981, "best_value": 0.695981}]}, {"metric_name": "validation curriculum-weighted accuracy", "lower_is_better": false, "description": "This measures the accuracy on the validation dataset, weighted by curriculum. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.620969, "best_value": 0.620969}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.629051, "best_value": 0.629051}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.691956, "best_value": 0.691956}]}, {"metric_name": "validation curriculum-weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss value after completion of training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.627987, "best_value": 0.627987}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Best validation loss observed during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.632125, "best_value": 0.632125}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Best macro F1 score observed during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.689939, "best_value": 0.689939}]}, {"metric_name": "validation curriculum-weighted accuracy", "lower_is_better": false, "description": "Best curriculum-weighted accuracy observed during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.69, "best_value": 0.69}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_confusion_matrix_bs_32.png"], ["../../logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_accuracy_cwa_curves.png", "../../logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_complexity_scatter.png"], ["../../logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_cwa_curve.png", "../../logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_CWA_curve.png", "../../logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_macro_f1.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_cwa.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_weight_hist.png"], ["../../logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_final_scores.png", "../../logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_cwa_curve.png", "../../logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_macro_f1.png", "../../logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_cwa.png", "../../logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_weight_hist.png"], ["../../logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_macro_f1.png", "../../logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_cwa.png", "../../logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_weight_hist.png"], ["../../logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_macro_f1.png", "../../logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_cwa.png", "../../logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_weight_hist.png"], ["../../logs/0-run/experiment_results/seed_aggregation_622b4938b35d470080aad10a6fa79aa2/SPR_BENCH_loss_curves_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_622b4938b35d470080aad10a6fa79aa2/SPR_BENCH_macro_f1_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_622b4938b35d470080aad10a6fa79aa2/SPR_BENCH_cwa_aggregated.png"]], "plot_paths": [["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_confusion_matrix_bs_32.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_accuracy_cwa_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_complexity_scatter.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_cwa_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_final_metrics_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_CWA_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_macro_f1.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_cwa.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_weight_hist.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_metric_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_final_scores.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_cwa_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_metric_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_final_metrics_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_macro_f1.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_cwa.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_weight_hist.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_macro_f1.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_cwa.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_weight_hist.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_macro_f1.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_cwa.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_weight_hist.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_622b4938b35d470080aad10a6fa79aa2/SPR_BENCH_loss_curves_aggregated.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_622b4938b35d470080aad10a6fa79aa2/SPR_BENCH_macro_f1_aggregated.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_622b4938b35d470080aad10a6fa79aa2/SPR_BENCH_cwa_aggregated.png"]], "plot_analyses": [[{"analysis": "This plot compares training and validation loss across different batch sizes (32, 64, 128, 256) as a function of epochs. Smaller batch sizes (e.g., 32) show a steady decrease in both training and validation loss, indicating stable convergence. However, larger batch sizes (e.g., 64) exhibit erratic behavior in validation loss, with a sharp spike at epoch 3, suggesting overfitting or instability in gradient updates. Batch sizes 128 and 256 maintain a more consistent decline in both losses, but their validation loss remains higher compared to batch size 32, implying suboptimal generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot focuses on the macro-F1 score on the validation set for different batch sizes over epochs. Batch size 32 consistently delivers a high macro-F1 score, peaking at around 0.68, indicating robust performance. Batch size 64 shows significant instability, with a drop in macro-F1 score at epoch 3, aligning with the loss spike observed earlier. Batch sizes 128 and 256 show gradual improvement, eventually converging to competitive macro-F1 scores, but they do not outperform batch size 32.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "This bar chart summarizes the final macro-F1 scores for different batch sizes. Batch sizes 32 and 256 achieve the highest final macro-F1 scores (0.68), while batch size 64 performs the worst (0.56). Batch size 128 achieves a decent score of 0.66, but overall, smaller and larger batch sizes (32 and 256) seem to be more effective for this task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_final_f1_bar.png"}, {"analysis": "This confusion matrix evaluates the classification performance for batch size 32. The model correctly predicts 172 true positives and 170 true negatives, but it also misclassifies 76 false positives and 82 false negatives. This indicates a balanced performance with room for improvement in reducing misclassifications, particularly false negatives.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_confusion_matrix_bs_32.png"}], [{"analysis": "The plot indicates that the training loss consistently decreases over epochs, which suggests that the model is learning effectively on the training data. However, the validation loss initially decreases but starts to increase after the second epoch. This pattern is a classic sign of overfitting, where the model starts to memorize the training data rather than generalizing well to unseen data. It would be beneficial to introduce regularization techniques, such as dropout or weight decay, to mitigate overfitting.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation accuracy and CWA (Class-Weighted Accuracy) both decrease initially and then recover towards the end of training. This U-shaped curve suggests that the model might be struggling to generalize to the validation set during the initial epochs. The recovery in later epochs indicates some learning stabilization, but the overall trend does not show a significant improvement in accuracy. This might suggest that the model architecture or hyperparameters need further tuning to achieve better performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_accuracy_cwa_curves.png"}, {"analysis": "The confusion matrix reveals that the model correctly classifies a significant number of sequences in both classes but also shows notable misclassifications. Specifically, 150 sequences from class 0 are misclassified as class 1, and 157 sequences from class 1 are misclassified as class 0. This indicates that the model may not be effectively capturing the distinguishing features of the classes. Further analysis of the misclassified samples could provide insights into specific patterns or rules that the model is failing to learn.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The plot shows a binary distribution of correctness versus sequence complexity, with one point for 'correct' and one for 'incorrect'. This suggests that the current model does not show a nuanced understanding of sequence complexity as it relates to correctness. The lack of a clear trend implies that the model's performance is not strongly correlated with sequence complexity. Further experiments could involve stratifying the dataset by complexity levels to assess performance more granularly and potentially improve the model's ability to handle complex sequences.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ba3da4a4e8224228a496328275074a3e_proc_3445457/SPR_BENCH_complexity_scatter.png"}], [{"analysis": "The training loss decreases steadily across epochs, indicating that the model is learning effectively from the training data. The validation loss initially decreases but plateaus after a few epochs, suggesting that the model's performance on unseen data stabilizes. The absence of significant divergence between training and validation loss indicates that the model is not overfitting.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 score increases steadily across epochs and begins to plateau after epoch 5. This indicates that the model's classification performance is improving and stabilizing, with balanced performance across classes.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The validation Complexity-Weighted Accuracy (CWA) follows a similar trend to the Macro-F1 score, increasing steadily across epochs and plateauing after epoch 5. This suggests that the model is handling sequences of varying rule complexities effectively.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_cwa_curve.png"}, {"analysis": "The final Macro-F1 score and CWA are both 0.688, which are identical. This indicates that the model's performance is consistent across both metrics, reflecting balanced classification and effective handling of complex rules.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix shows that the model performs better in certain classes than others. True positives for both classes are reasonably high, but there are notable false positives and false negatives, which indicate areas for improvement in classification accuracy.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbfc2909e733420188c11d923855523e_proc_3445458/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning from the training data. However, the validation loss initially decreases but starts to increase after epoch 2, suggesting potential overfitting. This means the model is likely memorizing the training data instead of generalizing to unseen data. Adjustments such as regularization or early stopping might help mitigate overfitting.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 score starts high but drops significantly by epoch 3, followed by a recovery trend. This pattern aligns with the overfitting observed in the loss plot. The recovery after epoch 3 might indicate that the model is slowly adjusting to the validation set but still struggles with generalization. This metric's fluctuation suggests the need for better hyperparameter tuning or model architecture adjustments.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The validation complexity-weighted accuracy (CWA) shows a similar trend to the Macro-F1 score, with an initial decrease followed by a recovery. This indicates that the model's performance on more complex rules is inconsistent. The drop and subsequent rise suggest that the model might be overfitting to simpler patterns in the data and struggling with more complex sequences. Further analysis and targeted training on complex rules might improve this metric.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_CWA_curve.png"}, {"analysis": "The confusion matrix shows a significant number of misclassifications for both classes. While there is a reasonable number of correct predictions, the model struggles with differentiating between the two classes. This imbalance in predictions could be addressed by rebalancing the dataset, improving the model's sensitivity to underrepresented patterns, or using a more sophisticated architecture.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_99d1266cee414a808997740ded406caa_proc_3445460/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot illustrates the cross-entropy loss for both the training and validation datasets over 12 epochs. The training loss decreases sharply at the beginning, indicating effective learning, but then stabilizes and aligns with the validation loss around epoch 6. This alignment suggests that the model is not overfitting. However, the relatively high and stable validation loss (~0.6) indicates that the model struggles to generalize well to unseen data, possibly due to the complexity of the symbolic rules in SPR_BENCH.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the macro-F1 score on the validation set over 12 epochs. The macro-F1 score initially drops significantly, indicating early instability in the model's predictions. After epoch 4, the score improves steadily, surpassing 0.69 by the final epochs. This reflects that the model eventually learns to balance precision and recall across classes effectively, although the initial instability may suggest the need for better initialization or early training strategies.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_macro_f1.png"}, {"analysis": "This plot displays the complexity-weighted accuracy (CWA) on the validation set. Similar to the macro-F1 trend, the CWA drops sharply early on, reaching a minimum around epoch 4, but then recovers and stabilizes above 0.69. This suggests that the model increasingly adapts to the complexity of the rules, but the early drop highlights potential difficulties in learning from complex examples during initial training.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_cwa.png"}, {"analysis": "The confusion matrix for the final epoch reveals a balance between true positives and true negatives, with 172 and 176 correctly classified examples, respectively. However, there are 76 false positives and 76 false negatives, indicating that while the model performs reasonably well, there is room for improvement in distinguishing between classes. This could involve refining the model architecture or incorporating additional features to better capture the nuances of the symbolic rules.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_confusion_matrix.png"}, {"analysis": "This plot presents the distribution of example weights, which appears uniform. This uniformity suggests that the model is treating all examples equally without biasing towards specific samples. While this is generally desirable, it may also indicate that the model is not leveraging the potential importance of more challenging examples, which could be an area for future investigation.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_weight_hist.png"}], [{"analysis": "The training loss decreases steadily across epochs, indicating that the model is learning from the training data. The validation loss decreases initially but starts to increase slightly after epoch 6, suggesting potential overfitting. This behavior indicates that while the model generalizes well initially, further training might degrade its performance on unseen data. Early stopping might be beneficial to prevent overfitting.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_loss_curve.png"}, {"analysis": "Both the Validation CWA and Macro-F1 scores increase significantly until epoch 4, after which they plateau and slightly decline. This trend aligns with the validation loss behavior, confirming that the model's performance on validation data peaks early and then deteriorates slightly. The alignment of CWA and Macro-F1 trends suggests that the model is consistently performing across different metrics.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_metric_curves.png"}, {"analysis": "The final validation scores for CWA and Macro-F1 are 0.666 and 0.663, respectively. These values are close to each other, demonstrating that the model performs uniformly across the metrics. However, these scores fall short of the state-of-the-art benchmark of 70%, indicating room for improvement in the model's handling of complex symbolic rules.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_final_scores.png"}, {"analysis": "The confusion matrix shows that while the model correctly classifies a significant number of samples in both classes, there is noticeable misclassification. Specifically, 163 samples from the negative class and 148 samples from the positive class are misclassified. This indicates that the model struggles with certain decision boundaries, which could be addressed by refining the model architecture or incorporating additional features to improve discrimination.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6454d9b35a5e43879f38d41884305911_proc_3445458/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases consistently over epochs, indicating that the model is learning from the training data. However, the validation loss does not follow a smooth decreasing trend and exhibits fluctuations, particularly a spike at epoch 4. This suggests potential overfitting or instability in the model's generalization to unseen data. The gap between training and validation loss remains relatively small, indicating that the model is not severely overfitting but still requires stability improvements.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 score shows an increasing trend initially, peaking at epoch 3, but then plummets drastically at epoch 4 before recovering in subsequent epochs. This sharp drop and recovery suggest that the model's performance on handling class imbalances or specific rule complexities is unstable. The final Macro-F1 score stabilizes at a level close to its earlier peak, indicating partial recovery but also highlighting the need for further tuning to ensure consistent performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The validation Cost-Weighted Accuracy (CWA) follows a trend similar to the Macro-F1 score, with an increase up to epoch 3, a sharp drop at epoch 4, and recovery afterward. This metric reflects the model's ability to balance classification accuracy across varying cost factors, and the observed instability could indicate challenges in consistently handling the diverse complexities of the SPR task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_cwa_curve.png"}, {"analysis": "The confusion matrix for the final epoch reveals that the model achieves a balanced performance across the two classes, with true positives and true negatives being comparable. However, there is still a noticeable number of false positives and false negatives (79 each), suggesting room for improvement in distinguishing between the two classes. This aligns with the earlier observations of fluctuating validation metrics and indicates that the model struggles with certain SPR rule complexities.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94a680db21a64fea9d7eda437dff2d40_proc_3445457/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training and validation loss curves show a general downward trend, indicating that the model is learning over epochs. However, the validation loss initially spikes in the second epoch before stabilizing and decreasing. This suggests some overfitting or instability early in training, which is later mitigated. The convergence of training and validation losses towards the end suggests good generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation metrics (Macro-F1 and CWA) exhibit similar behavior, with a sharp drop in the second epoch followed by a recovery and stabilization around 0.68. This indicates that the model adapts well after initial fluctuations and maintains consistent performance across metrics.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_metric_curves.png"}, {"analysis": "The final epoch metrics show that both Macro-F1 and CWA scores are equal at 0.68, which matches the stabilization observed in the validation metrics plot. This demonstrates that the model achieves balanced performance and handles the classification task effectively.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well in distinguishing between the two classes. However, there is still a notable number of misclassifications (149 false positives and 159 false negatives). This suggests that while the model achieves good overall accuracy, there is room for improvement in reducing these specific errors.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ca257d6e71d14e1b81d0de1b8ecc8834_proc_3445460/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss consistently decreases over epochs, indicating that the model is learning from the training data. However, the validation loss decreases initially but then starts to increase after epoch 5, suggesting potential overfitting. The model may be memorizing the training data instead of generalizing well to unseen data. Regularization techniques or early stopping could be applied to mitigate this issue.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 score shows an initial dip at epoch 2 but then improves steadily, peaking at around epoch 8. This indicates that the model's ability to balance precision and recall across classes improves with training, despite the overfitting observed in the loss plot. The steady improvement suggests that the model is learning to handle the complexities of the SPR task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_macro_f1.png"}, {"analysis": "The validation complexity-weighted accuracy (CWA) follows a similar trend to the Macro-F1 score, with an initial dip followed by consistent improvement. This metric accounts for the varying complexities of the symbolic rules, and its steady increase suggests that the model is becoming better at handling more complex cases as training progresses.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_cwa.png"}, {"analysis": "The confusion matrix at the final epoch shows a relatively balanced performance between the two classes, with 172 and 176 correct predictions for each class. However, the number of misclassifications (76 for each class) indicates room for improvement. Techniques such as class-specific loss weighting or additional data augmentation might help reduce these errors.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The distribution of weights appears uniform, which may indicate that the model's training process is not biased towards specific examples or classes. This is a positive sign as it suggests that the model is treating all examples equally during training. However, further investigation into the impact of sample weights on performance could provide additional insights.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/SPR_BENCH_weight_hist.png"}], [{"analysis": "The training vs validation loss plot shows a rapid increase in training loss during the initial epochs, which then stabilizes and converges with the validation loss. This suggests that the model initially struggles to adapt to the complex symbolic rules but eventually learns to generalize better. The convergence of training and validation losses indicates that overfitting is not a significant issue, and the model maintains generalization capability.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 score plot demonstrates a steady improvement over epochs, with fluctuations likely due to the complexity of the task and the model's adjustments to different rule types. The upward trend in Macro-F1 indicates that the model is progressively learning to classify sequences governed by complex rules more effectively, though the fluctuations suggest that further tuning or architectural improvements might stabilize performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_macro_f1.png"}, {"analysis": "The validation Complexity-Weighted Accuracy (CWA) plot shows a similar trend to the Macro-F1 score, with a general upward trajectory and some fluctuations. This metric's improvement over epochs confirms that the model is gradually becoming adept at handling sequences with varying levels of rule complexity. The fluctuations could be due to specific challenges in adapting to particularly complex rule sets, which might require further investigation or targeted training strategies.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_cwa.png"}, {"analysis": "The confusion matrix for the final epoch reveals that the model performs reasonably well, with a higher number of correct predictions for both classes. However, there are still noticeable misclassifications, especially for one of the classes. This indicates room for improvement, potentially through better handling of class imbalance or incorporating additional features to aid in distinguishing between the classes.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The distribution of weights plot shows a uniform distribution, suggesting that the model applies consistent weighting across examples. This could indicate that the model does not inherently favor certain sequences or rules, which is desirable for ensuring fairness and robustness in classification. However, it might also imply a lack of adaptive weighting, which could be explored as a potential enhancement.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/SPR_BENCH_weight_hist.png"}], [{"analysis": "The loss curves indicate that the model is overfitting. The training loss decreases steadily, while the validation loss remains nearly constant and does not improve beyond 0.6. This suggests that the model is not generalizing well to unseen data and may require regularization techniques or adjustments to the model architecture to address this issue.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 score fluctuates across epochs but shows a general upward trend, peaking around 0.69. This suggests that the model captures some aspects of the task but lacks stability, possibly due to overfitting or insufficient training data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_macro_f1.png"}, {"analysis": "The validation Complexity-Weighted Accuracy (CWA) follows a similar trend to the Macro-F1 score, with fluctuations and a peak around 0.69. This indicates that the model performs reasonably well on sequences of varying complexity but may benefit from further optimization to achieve consistency.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_cwa.png"}, {"analysis": "The confusion matrix shows that the model performs moderately well in distinguishing between the two classes, with a slight imbalance in misclassifications. This indicates room for improvement in handling edge cases or ambiguous examples, which could be addressed through better feature engineering or model refinement.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The distribution of weights plot appears uniform, suggesting that the model assigns equal importance to all examples. While this is generally desirable, it may be worth exploring whether reweighting certain examples could improve performance on challenging cases.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/SPR_BENCH_weight_hist.png"}], []], "vlm_feedback_summary": ["The results indicate that batch size 32 is the most stable and effective for\nachieving low loss and high macro-F1 scores. Larger batch sizes (e.g., 256) also\nperform well but show slightly less stability in validation metrics. Batch size\n64 exhibits instability and poor performance, making it less suitable for this\ntask. The confusion matrix for batch size 32 reveals balanced performance but\nhighlights the need to reduce false negatives for further improvement.", "The plots collectively reveal that the model is learning but exhibits signs of\noverfitting and struggles with generalization. The validation accuracy and\nclass-weighted accuracy trends indicate potential issues with the model's\nability to adapt to validation data. The confusion matrix highlights specific\nmisclassification patterns, and the correctness versus complexity analysis\nsuggests a need for further exploration of the model's handling of complex\nsequences. Regularization techniques, better hyperparameter tuning, and\nstratified analysis could improve performance.", "The experimental results demonstrate that the model is learning effectively and\nstabilizing its performance on validation data. Both Macro-F1 and Complexity-\nWeighted Accuracy metrics show consistent improvement and reach a stable score\nof 0.688, indicating balanced and effective classification. The confusion matrix\nhighlights room for improvement in reducing false positives and false negatives.", "[]", "The provided plots indicate overfitting in the model, as seen in the loss and\nperformance metrics. The Macro-F1 and CWA scores reveal challenges in\ngeneralization and handling complex rules. The confusion matrix highlights\nissues with class imbalance and misclassification. Regularization, better\nhyperparameter tuning, and targeted training on complex rules are recommended to\nimprove performance.", "The plots reveal key insights into the model's training and evaluation process.\nWhile the model shows improvement in validation metrics over time, early\ninstability and relatively high loss indicate challenges in generalizing to\ncomplex rules. The confusion matrix highlights a balanced but imperfect\nclassification, and the uniform weight distribution suggests no bias but also no\nemphasis on challenging examples. Future work could focus on refining early\ntraining strategies, enhancing model architecture, and investigating the impact\nof weighting challenging samples.", "The plots reveal that the model learns effectively during initial epochs but\nstarts to overfit as training progresses. Validation metrics suggest consistent\nperformance but fall short of the state-of-the-art benchmark. The confusion\nmatrix highlights areas where the model could improve its classification\naccuracy, particularly in handling challenging samples.", "The plots reveal that while the model demonstrates learning and achieves\nreasonable performance, it suffers from instability and fluctuations in\nvalidation metrics. The sharp drop in performance at epoch 4 across multiple\nmetrics is particularly concerning and suggests that the model may be sensitive\nto certain complexities or imbalances in the data. The confusion matrix further\nhighlights areas for improvement in classification accuracy. Further tuning and\npotential architectural changes are recommended to address these issues and\nachieve more consistent performance.", "The plots indicate that the model demonstrates effective learning and\ngeneralization, with validation metrics stabilizing at a competitive level (0.68\nMacro-F1 and CWA). Early instability in training is overcome, and the confusion\nmatrix highlights areas for improvement in reducing misclassifications.", "The results indicate that the model is learning and improving its performance on\nthe SPR task, as evidenced by the decreasing training loss, increasing\nvalidation Macro-F1 score, and complexity-weighted accuracy. However, the\ndivergence between training and validation loss suggests overfitting, which\ncould be addressed with regularization or early stopping. The confusion matrix\nhighlights balanced but imperfect classification performance, and the uniform\nweight distribution suggests unbiased training. Further optimization and\nexperimentation are recommended to achieve better generalization and reduce\nmisclassifications.", "The plots provide meaningful insights into the model's training and evaluation\nperformance. The results show that the model learns progressively, with\nimprovements in validation metrics over epochs. However, fluctuations in metrics\nand misclassifications indicate that there is still room for optimization and\nrefinement to achieve better performance on the SPR_BENCH dataset.", "The results indicate that the model shows moderate performance but suffers from\noverfitting and instability across metrics. The training process needs\nrefinement, potentially through regularization, data augmentation, or\nreweighting strategies to handle challenging cases effectively. The validation\nmetrics suggest the model captures the task's complexity but lacks consistency.", "[]"], "exec_time": [6.715019464492798, 7.701811790466309, 6.6606221199035645, 3.6129376888275146, 7.991558074951172, 20.778529167175293, 6.672387599945068, 7.04530668258667, 12.571267366409302, 16.53144073486328, 14.09651494026184, 10.194839000701904, null], "exec_time_feedback": ["", "", "", "", "Implementation works but runs too quickly (0.11 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[]"], ["[\"SPR_BENCH\"]"], [], ["[]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[]"], ["[\"experiment_data\"]"], ["['experiment_data']"], ["['SPR_BENCH']"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nbatch_sizes = sorted(int(k.split(\"_\")[-1]) for k in exp.keys())\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            logs[\"losses\"][\"train\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, logs[\"losses\"][\"val\"], color=c, linestyle=\"--\", label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"metrics\"][\"val\"]) + 1)\n        plt.plot(epochs, logs[\"metrics\"][\"val\"], color=c, label=f\"val F1 bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Final-epoch F1 bar plot\ntry:\n    final_f1 = [exp[f\"bs_{bs}\"][\"metrics\"][\"val\"][-1] for bs in batch_sizes]\n    plt.figure()\n    plt.bar([str(bs) for bs in batch_sizes], final_f1, color=colors[: len(batch_sizes)])\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Final Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Macro-F1 vs Batch Size\")\n    for x, y in zip(batch_sizes, final_f1):\n        plt.text(str(x), y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best batch size\ntry:\n    best_idx = int(np.argmax(final_f1))\n    best_bs = batch_sizes[best_idx]\n    logs = exp[f\"bs_{best_bs}\"]\n    preds = np.array(logs[\"predictions\"])\n    gts = np.array(logs[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (bs={best_bs})\")\n    plt.colorbar()\n    plt.savefig(\n        os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_bs_{best_bs}.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data & compute final metrics -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n\n# Helper to compute metrics\ndef complexity_weighted_acc(preds, gts, wts):\n    correct = (preds == gts).astype(float)\n    return (correct * wts).sum() / wts.sum()\n\n\ntry:\n    preds = np.array(exp.get(\"predictions\", []))\n    gts = np.array(exp.get(\"ground_truth\", []))\n    wts = np.array(exp.get(\"weights\", []))\n    test_acc = (preds == gts).mean() if len(preds) else np.nan\n    test_cwa = complexity_weighted_acc(preds, gts, wts) if len(preds) else np.nan\n    print(f\"Test Accuracy: {test_acc:.4f}, Test CWA: {test_cwa:.4f}\")\nexcept Exception as e:\n    print(f\"Error computing metrics: {e}\")\n\n# --------------------------- plotting -------------------------------\n# 1) Loss curves\ntry:\n    plt.figure()\n    epochs = np.arange(1, len(exp[\"losses\"][\"train\"]) + 1)\n    plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Accuracy & CWA curves\ntry:\n    plt.figure()\n    epochs = np.arange(1, len(exp[\"metrics\"][\"accuracy\"]) + 1)\n    plt.plot(epochs, exp[\"metrics\"][\"accuracy\"], label=\"Val Accuracy\")\n    plt.plot(epochs, exp[\"metrics\"][\"cwa\"], label=\"Val CWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Validation Accuracy & CWA\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_cwa_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy/CWA curves: {e}\")\n    plt.close()\n\n# 3) Confusion matrix\ntry:\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"SPR_BENCH Confusion Matrix (Test)\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# 4) Complexity vs correctness scatter\ntry:\n    correctness = (preds == gts).astype(int)\n    plt.figure()\n    plt.scatter(wts, correctness, alpha=0.4, s=10)\n    plt.yticks([0, 1], [\"Incorrect\", \"Correct\"])\n    plt.xlabel(\"Sequence Complexity (weight)\")\n    plt.ylabel(\"Prediction Outcome\")\n    plt.title(\"SPR_BENCH: Correctness vs Complexity (Test)\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_complexity_scatter.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating complexity scatter: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n\n# Helper to maybe subsample epochs to max 5 points\ndef epoch_idx(n, max_points=5):\n    if n <= max_points:\n        return np.arange(n)\n    step = max(1, int(np.ceil(n / max_points)))\n    return np.arange(0, n, step)\n\n\n# ----------------------------------------\n# 1) Loss curves\ntry:\n    epochs = np.arange(1, len(exp[\"losses\"][\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train\", color=\"tab:blue\")\n    plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Validation\", color=\"tab:orange\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ----------------------------------------\n# 2) Macro-F1 over epochs\ntry:\n    f1_vals = [m[\"macro_f1\"] for m in exp[\"metrics\"][\"val\"]]\n    ep_sel = epoch_idx(len(f1_vals))\n    plt.figure()\n    plt.plot(np.arange(1, len(f1_vals) + 1)[ep_sel], np.array(f1_vals)[ep_sel])\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 curve: {e}\")\n    plt.close()\n\n# ----------------------------------------\n# 3) Complexity-Weighted Accuracy over epochs\ntry:\n    cwa_vals = [m[\"cwa\"] for m in exp[\"metrics\"][\"val\"]]\n    ep_sel = epoch_idx(len(cwa_vals))\n    plt.figure()\n    plt.plot(np.arange(1, len(cwa_vals) + 1)[ep_sel], np.array(cwa_vals)[ep_sel])\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CWA\")\n    plt.title(\"SPR_BENCH: Validation Complexity-Weighted Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_cwa_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA curve: {e}\")\n    plt.close()\n\n# ----------------------------------------\n# 4) Final-epoch Macro-F1 vs CWA bar plot\ntry:\n    final_f1 = f1_vals[-1]\n    final_cwa = cwa_vals[-1]\n    plt.figure()\n    plt.bar([\"Macro-F1\", \"CWA\"], [final_f1, final_cwa], color=[\"tab:green\", \"tab:red\"])\n    for x, y in zip([\"Macro-F1\", \"CWA\"], [final_f1, final_cwa]):\n        plt.text(x, y + 0.005, f\"{y:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylim(0, 1)\n    plt.title(\"SPR_BENCH: Final-Epoch Metrics\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_metrics_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final metrics bar plot: {e}\")\n    plt.close()\n\n# ----------------------------------------\n# 5) Confusion matrix\ntry:\n    preds = np.array(exp[\"predictions\"])\n    gts = np.array(exp[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\n        \"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ----------------------------------------\n# Print final metrics\ntry:\n    print(f\"Final Macro-F1: {final_f1:.4f} | Final CWA: {final_cwa:.4f}\")\nexcept Exception as e:\n    print(f\"Error printing final metrics: {e}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# ------------------------------------------------------------------\n# Helper: get epochs\nepochs = np.arange(1, len(exp.get(\"losses\", {}).get(\"train\", [])) + 1)\n\n# 1) Loss curves ----------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train\", color=\"tab:blue\")\n    plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Validation\", color=\"tab:orange\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curve -------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, exp[\"metrics\"][\"val_macroF1\"], marker=\"o\", color=\"tab:green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1 over Epochs\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 curve: {e}\")\n    plt.close()\n\n# 3) Complexity-Weighted Accuracy curve -----------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, exp[\"metrics\"][\"val_CWA\"], marker=\"s\", color=\"tab:red\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Complexity-Weighted Accuracy\")\n    plt.title(\"SPR_BENCH: Validation CWA over Epochs\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_CWA_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA curve: {e}\")\n    plt.close()\n\n# 4) Confusion matrix (final epoch) ---------------------------------\ntry:\n    preds = np.array(exp[\"predictions\"])\n    gts = np.array(exp[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\n        \"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Generated Predictions\"\n    )\n    plt.colorbar()\n    fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print final evaluation metrics\nif exp:\n    final_F1 = exp[\"metrics\"][\"val_macroF1\"][-1]\n    final_CWA = exp[\"metrics\"][\"val_CWA\"][-1]\n    print(f\"Final Validation Macro-F1: {final_F1:.4f}\")\n    print(f\"Final Validation CWA     : {final_CWA:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate datasets ----------\nfor dname, logs in experiment_data.items():\n    # basic sanity\n    train_loss = np.array(logs[\"losses\"].get(\"train\", []), dtype=float)\n    val_loss = np.array(logs[\"losses\"].get(\"val\", []), dtype=float)\n    val_metrics = logs[\"metrics\"].get(\"val\", [])\n    macro_f1 = (\n        np.array([m[\"macro_f1\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    cwa = (\n        np.array([m[\"cwa\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    preds = np.array(logs.get(\"predictions\", []))\n    gts = np.array(logs.get(\"ground_truth\", []))\n    wts = np.array(logs.get(\"weights\", []))\n\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # 1) loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"train\", color=\"tab:blue\")\n        plt.plot(epochs, val_loss, label=\"val\", color=\"tab:orange\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dname}: {e}\")\n        plt.close()\n\n    # 2) macro-F1\n    try:\n        if macro_f1.size:\n            plt.figure()\n            plt.plot(epochs, macro_f1, color=\"tab:green\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Validation Macro-F1\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_macro_f1.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 plot for {dname}: {e}\")\n        plt.close()\n\n    # 3) CWA\n    try:\n        if cwa.size:\n            plt.figure()\n            plt.plot(epochs, cwa, color=\"tab:red\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-Weighted Acc.\")\n            plt.title(f\"{dname}: Validation CWA\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_cwa.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot for {dname}: {e}\")\n        plt.close()\n\n    # 4) confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i in range(num_cls):\n                for j in range(num_cls):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (final epoch)\")\n            plt.colorbar()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # 5) weight histogram\n    try:\n        if wts.size:\n            plt.figure()\n            plt.hist(wts, bins=min(30, len(np.unique(wts))), color=\"tab:purple\")\n            plt.xlabel(\"Example Weight\")\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dname}: Distribution of Weights\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_weight_hist.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating weight histogram for {dname}: {e}\")\n        plt.close()\n\n    # ---- quick metric summary ----\n    if macro_f1.size:\n        print(\n            f\"{dname}: best Macro-F1={macro_f1.max():.3f} | final Macro-F1={macro_f1[-1]:.3f}\"\n        )\n    if cwa.size:\n        print(f\"{dname}: best CWA={cwa.max():.3f} | final CWA={cwa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_exp = experiment_data.get(\"SPR_BENCH\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_exp = {}\n\nepochs = np.array(spr_exp.get(\"epochs\", []))\ntrain_loss = np.array(spr_exp.get(\"losses\", {}).get(\"train\", []))\nval_loss = np.array(spr_exp.get(\"losses\", {}).get(\"val\", []))\nval_cwa = np.array(spr_exp.get(\"metrics\", {}).get(\"val_cwa\", []))\nval_f1 = np.array(spr_exp.get(\"metrics\", {}).get(\"val_f1\", []))\npreds = np.array(spr_exp.get(\"predictions\", []))\ngts = np.array(spr_exp.get(\"ground_truth\", []))\n\n# 1) Loss curves -------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\", color=\"tab:blue\")\n    plt.plot(\n        epochs, val_loss, label=\"Validation Loss\", color=\"tab:orange\", linestyle=\"--\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) Metric curves -----------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, val_cwa, label=\"Val CWA\", color=\"tab:green\")\n    plt.plot(epochs, val_f1, label=\"Val Macro-F1\", color=\"tab:red\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Validation CWA & Macro-F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_metric_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curves: {e}\")\n    plt.close()\n\n# 3) Final-epoch bar chart ---------------------------------------------------\ntry:\n    plt.figure()\n    bars = [\"CWA\", \"Macro-F1\"]\n    final_scores = [\n        val_cwa[-1] if val_cwa.size else 0,\n        val_f1[-1] if val_f1.size else 0,\n    ]\n    plt.bar(bars, final_scores, color=[\"tab:green\", \"tab:red\"])\n    for x, y in zip(bars, final_scores):\n        plt.text(x, y + 0.01, f\"{y:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylim(0, 1.05)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Final Validation Scores\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_scores.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final score bar chart: {e}\")\n    plt.close()\n\n# 4) Confusion matrix --------------------------------------------------------\ntry:\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# -------- optional quick print ----------\nif val_cwa.size and val_f1.size:\n    print(f\"Final Validation CWA: {val_cwa[-1]:.4f}\")\n    print(f\"Final Validation Macro-F1: {val_f1[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# helper: safely fetch lists\nloss_tr = exp.get(\"losses\", {}).get(\"train\", [])\nloss_val = exp.get(\"losses\", {}).get(\"val\", [])\nmetrics_val = exp.get(\"metrics\", {}).get(\"val\", [])\nmacro_f1 = [m[\"macro_f1\"] for m in metrics_val] if metrics_val else []\ncwa_vals = [m[\"cwa\"] for m in metrics_val] if metrics_val else []\nepochs = np.arange(1, len(loss_tr) + 1)\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_tr, label=\"Train Loss\", color=\"tab:blue\")\n    plt.plot(epochs, loss_val, label=\"Val Loss\", color=\"tab:orange\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curve\ntry:\n    plt.figure()\n    plt.plot(epochs, macro_f1, label=\"Val Macro-F1\", color=\"tab:green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1 over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 curve: {e}\")\n    plt.close()\n\n# 3) CWA curve\ntry:\n    plt.figure()\n    plt.plot(epochs, cwa_vals, label=\"Val CWA\", color=\"tab:red\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cost-Weighted Accuracy\")\n    plt.title(\"SPR_BENCH: Validation CWA over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_cwa_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA curve: {e}\")\n    plt.close()\n\n# 4) Confusion matrix at final epoch\ntry:\n    preds = np.array(exp.get(\"predictions\", []))\n    gts = np.array(exp.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        labels = sorted(np.unique(np.concatenate([preds, gts])))\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        for i in range(len(labels)):\n            for j in range(len(labels)):\n                plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH Confusion Matrix (Final Epoch)\")\n        plt.colorbar()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# Print final metrics\nif macro_f1 and cwa_vals:\n    print(f\"Final Macro-F1: {macro_f1[-1]:.4f}\")\n    print(f\"Final CWA     : {cwa_vals[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------------------------------------------------------------- #\n# paths & data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# --------------------------------------------------------------------------- #\n# helper to extract metric arrays safely\ndef get_metric_array(metric_list, key):\n    if not metric_list:\n        return np.array([])\n    return np.array([m.get(key, np.nan) for m in metric_list])\n\n\n# --------------------------------------------------------------------------- #\n# iterate over datasets\nfor dset_name, logs in experiment_data.items():\n    # ---------- 1) loss curves --------------------------------------------- #\n    try:\n        train_losses = np.array(logs[\"losses\"].get(\"train\", []))\n        val_losses = np.array(logs[\"losses\"].get(\"val\", []))\n        if train_losses.size and val_losses.size:\n            epochs = np.arange(1, len(train_losses) + 1)\n            plt.figure()\n            plt.plot(epochs, train_losses, label=\"Train\", color=\"tab:blue\")\n            plt.plot(epochs, val_losses, label=\"Val\", color=\"tab:orange\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dset_name}: Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dset_name}: {e}\")\n        plt.close()\n\n    # ---------- 2) metric curves (macro-F1 & CWA) --------------------------- #\n    try:\n        val_metrics = logs[\"metrics\"].get(\"val\", [])\n        macro_f1 = get_metric_array(val_metrics, \"macro_f1\")\n        cwa = get_metric_array(val_metrics, \"cwa\")\n        if macro_f1.size and cwa.size:\n            epochs = np.arange(1, len(macro_f1) + 1)\n            plt.figure()\n            plt.plot(epochs, macro_f1, label=\"Macro-F1\", color=\"tab:green\")\n            plt.plot(epochs, cwa, label=\"CWA\", color=\"tab:red\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{dset_name}: Validation Metrics\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_metric_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating metric curves for {dset_name}: {e}\")\n        plt.close()\n\n    # ---------- 3) final-epoch bar plot ------------------------------------- #\n    try:\n        if macro_f1.size and cwa.size:\n            final_vals = [macro_f1[-1], cwa[-1]]\n            labels = [\"Macro-F1\", \"CWA\"]\n            colors = [\"tab:green\", \"tab:red\"]\n            plt.figure()\n            plt.bar(labels, final_vals, color=colors)\n            for x, y in zip(labels, final_vals):\n                plt.text(x, y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{dset_name}: Final-Epoch Metrics\")\n            fname = os.path.join(working_dir, f\"{dset_name}_final_metrics_bar.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot for {dset_name}: {e}\")\n        plt.close()\n\n    # ---------- 4) confusion matrix ---------------------------------------- #\n    try:\n        preds = np.array(logs.get(\"predictions\", []))\n        gts = np.array(logs.get(\"ground_truth\", []))\n        if preds.size and gts.size and preds.shape == gts.shape:\n            cm = np.zeros((2, 2), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i in range(2):\n                for j in range(2):\n                    plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{dset_name}: Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.colorbar()\n            fname = os.path.join(working_dir, f\"{dset_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate datasets ----------\nfor dname, logs in experiment_data.items():\n    # basic sanity\n    train_loss = np.array(logs[\"losses\"].get(\"train\", []), dtype=float)\n    val_loss = np.array(logs[\"losses\"].get(\"val\", []), dtype=float)\n    val_metrics = logs[\"metrics\"].get(\"val\", [])\n    macro_f1 = (\n        np.array([m[\"macro_f1\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    cwa = (\n        np.array([m[\"cwa\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    preds = np.array(logs.get(\"predictions\", []))\n    gts = np.array(logs.get(\"ground_truth\", []))\n    wts = np.array(logs.get(\"weights\", []))\n\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # 1) loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"train\", color=\"tab:blue\")\n        plt.plot(epochs, val_loss, label=\"val\", color=\"tab:orange\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dname}: {e}\")\n        plt.close()\n\n    # 2) macro-F1\n    try:\n        if macro_f1.size:\n            plt.figure()\n            plt.plot(epochs, macro_f1, color=\"tab:green\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Validation Macro-F1\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_macro_f1.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 plot for {dname}: {e}\")\n        plt.close()\n\n    # 3) CWA\n    try:\n        if cwa.size:\n            plt.figure()\n            plt.plot(epochs, cwa, color=\"tab:red\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-Weighted Acc.\")\n            plt.title(f\"{dname}: Validation CWA\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_cwa.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot for {dname}: {e}\")\n        plt.close()\n\n    # 4) confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i in range(num_cls):\n                for j in range(num_cls):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (final epoch)\")\n            plt.colorbar()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # 5) weight histogram\n    try:\n        if wts.size:\n            plt.figure()\n            plt.hist(wts, bins=min(30, len(np.unique(wts))), color=\"tab:purple\")\n            plt.xlabel(\"Example Weight\")\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dname}: Distribution of Weights\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_weight_hist.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating weight histogram for {dname}: {e}\")\n        plt.close()\n\n    # ---- quick metric summary ----\n    if macro_f1.size:\n        print(\n            f\"{dname}: best Macro-F1={macro_f1.max():.3f} | final Macro-F1={macro_f1[-1]:.3f}\"\n        )\n    if cwa.size:\n        print(f\"{dname}: best CWA={cwa.max():.3f} | final CWA={cwa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate datasets ----------\nfor dname, logs in experiment_data.items():\n    # basic sanity\n    train_loss = np.array(logs[\"losses\"].get(\"train\", []), dtype=float)\n    val_loss = np.array(logs[\"losses\"].get(\"val\", []), dtype=float)\n    val_metrics = logs[\"metrics\"].get(\"val\", [])\n    macro_f1 = (\n        np.array([m[\"macro_f1\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    cwa = (\n        np.array([m[\"cwa\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    preds = np.array(logs.get(\"predictions\", []))\n    gts = np.array(logs.get(\"ground_truth\", []))\n    wts = np.array(logs.get(\"weights\", []))\n\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # 1) loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"train\", color=\"tab:blue\")\n        plt.plot(epochs, val_loss, label=\"val\", color=\"tab:orange\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dname}: {e}\")\n        plt.close()\n\n    # 2) macro-F1\n    try:\n        if macro_f1.size:\n            plt.figure()\n            plt.plot(epochs, macro_f1, color=\"tab:green\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Validation Macro-F1\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_macro_f1.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 plot for {dname}: {e}\")\n        plt.close()\n\n    # 3) CWA\n    try:\n        if cwa.size:\n            plt.figure()\n            plt.plot(epochs, cwa, color=\"tab:red\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-Weighted Acc.\")\n            plt.title(f\"{dname}: Validation CWA\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_cwa.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot for {dname}: {e}\")\n        plt.close()\n\n    # 4) confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i in range(num_cls):\n                for j in range(num_cls):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (final epoch)\")\n            plt.colorbar()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # 5) weight histogram\n    try:\n        if wts.size:\n            plt.figure()\n            plt.hist(wts, bins=min(30, len(np.unique(wts))), color=\"tab:purple\")\n            plt.xlabel(\"Example Weight\")\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dname}: Distribution of Weights\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_weight_hist.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating weight histogram for {dname}: {e}\")\n        plt.close()\n\n    # ---- quick metric summary ----\n    if macro_f1.size:\n        print(\n            f\"{dname}: best Macro-F1={macro_f1.max():.3f} | final Macro-F1={macro_f1[-1]:.3f}\"\n        )\n    if cwa.size:\n        print(f\"{dname}: best CWA={cwa.max():.3f} | final CWA={cwa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate datasets ----------\nfor dname, logs in experiment_data.items():\n    # basic sanity\n    train_loss = np.array(logs[\"losses\"].get(\"train\", []), dtype=float)\n    val_loss = np.array(logs[\"losses\"].get(\"val\", []), dtype=float)\n    val_metrics = logs[\"metrics\"].get(\"val\", [])\n    macro_f1 = (\n        np.array([m[\"macro_f1\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    cwa = (\n        np.array([m[\"cwa\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    preds = np.array(logs.get(\"predictions\", []))\n    gts = np.array(logs.get(\"ground_truth\", []))\n    wts = np.array(logs.get(\"weights\", []))\n\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # 1) loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"train\", color=\"tab:blue\")\n        plt.plot(epochs, val_loss, label=\"val\", color=\"tab:orange\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dname}: {e}\")\n        plt.close()\n\n    # 2) macro-F1\n    try:\n        if macro_f1.size:\n            plt.figure()\n            plt.plot(epochs, macro_f1, color=\"tab:green\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Validation Macro-F1\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_macro_f1.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 plot for {dname}: {e}\")\n        plt.close()\n\n    # 3) CWA\n    try:\n        if cwa.size:\n            plt.figure()\n            plt.plot(epochs, cwa, color=\"tab:red\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-Weighted Acc.\")\n            plt.title(f\"{dname}: Validation CWA\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_cwa.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot for {dname}: {e}\")\n        plt.close()\n\n    # 4) confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i in range(num_cls):\n                for j in range(num_cls):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (final epoch)\")\n            plt.colorbar()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # 5) weight histogram\n    try:\n        if wts.size:\n            plt.figure()\n            plt.hist(wts, bins=min(30, len(np.unique(wts))), color=\"tab:purple\")\n            plt.xlabel(\"Example Weight\")\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dname}: Distribution of Weights\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_weight_hist.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating weight histogram for {dname}: {e}\")\n        plt.close()\n\n    # ---- quick metric summary ----\n    if macro_f1.size:\n        print(\n            f\"{dname}: best Macro-F1={macro_f1.max():.3f} | final Macro-F1={macro_f1[-1]:.3f}\"\n        )\n    if cwa.size:\n        print(f\"{dname}: best CWA={cwa.max():.3f} | final CWA={cwa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- basic setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load multiple experiment_data files ----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_55e0cbe980f946eba96e761779d63bfd_proc_3445457/experiment_data.npy\",\n    \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_159c8cc43ce144c3914215ae77f987e5_proc_3445459/experiment_data.npy\",\n    \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e8d7b33f2e73432b92dd2f19a72ab4ac_proc_3445460/experiment_data.npy\",\n]\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n\n# ---------- aggregate across runs ----------\ndef _stack_and_crop(list_of_arrays):\n    \"\"\"Stack 1-D arrays to shape (runs, epochs), cropped to min length.\"\"\"\n    if not list_of_arrays:\n        return np.empty((0, 0))\n    min_len = min(len(a) for a in list_of_arrays)\n    stacked = np.vstack([a[:min_len] for a in list_of_arrays])\n    return stacked\n\n\nfor dname in {d for exp in all_experiment_data for d in exp.keys()}:\n    # gather per-run curves\n    train_losses, val_losses, macro_f1s, cwas = [], [], [], []\n    for exp in all_experiment_data:\n        logs = exp.get(dname, None)\n        if logs is None:\n            continue\n        train_losses.append(np.asarray(logs[\"losses\"].get(\"train\", []), dtype=float))\n        val_losses.append(np.asarray(logs[\"losses\"].get(\"val\", []), dtype=float))\n        v_metrics = logs[\"metrics\"].get(\"val\", [])\n        macro_f1s.append(\n            np.asarray([m[\"macro_f1\"] for m in v_metrics], dtype=float)\n            if v_metrics\n            else np.array([])\n        )\n        cwas.append(\n            np.asarray([m[\"cwa\"] for m in v_metrics], dtype=float)\n            if v_metrics\n            else np.array([])\n        )\n\n    # convert to (runs, epochs) matrices cropped to common length\n    train_mat = _stack_and_crop([a for a in train_losses if a.size])\n    val_mat = _stack_and_crop([a for a in val_losses if a.size])\n    f1_mat = _stack_and_crop([a for a in macro_f1s if a.size])\n    cwa_mat = _stack_and_crop([a for a in cwas if a.size])\n\n    epochs = np.arange(1, train_mat.shape[1] + 1) if train_mat.size else None\n    n_runs = train_mat.shape[0] if train_mat.size else 0\n\n    # ---------- aggregated loss curves ----------\n    try:\n        if train_mat.size and val_mat.size:\n            plt.figure()\n            # train\n            train_mean = train_mat.mean(axis=0)\n            train_se = train_mat.std(axis=0, ddof=1) / np.sqrt(n_runs)\n            plt.plot(epochs, train_mean, label=\"train mean\", color=\"tab:blue\")\n            plt.fill_between(\n                epochs,\n                train_mean - train_se,\n                train_mean + train_se,\n                alpha=0.3,\n                color=\"tab:blue\",\n                label=\"train \u00b1 SE\",\n            )\n            # val\n            val_mean = val_mat.mean(axis=0)\n            val_se = val_mat.std(axis=0, ddof=1) / np.sqrt(n_runs)\n            plt.plot(\n                epochs, val_mean, label=\"val mean\", color=\"tab:orange\", linestyle=\"--\"\n            )\n            plt.fill_between(\n                epochs,\n                val_mean - val_se,\n                val_mean + val_se,\n                alpha=0.3,\n                color=\"tab:orange\",\n                label=\"val \u00b1 SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Aggregated Training/Validation Loss (n={n_runs} runs)\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dname}_loss_curves_aggregated.png\")\n            )\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss curves for {dname}: {e}\")\n        plt.close()\n\n    # ---------- aggregated macro-F1 ----------\n    try:\n        if f1_mat.size:\n            epochs_f1 = np.arange(1, f1_mat.shape[1] + 1)\n            mean_f1 = f1_mat.mean(axis=0)\n            se_f1 = f1_mat.std(axis=0, ddof=1) / np.sqrt(f1_mat.shape[0])\n            plt.figure()\n            plt.plot(epochs_f1, mean_f1, color=\"tab:green\", label=\"macro-F1 mean\")\n            plt.fill_between(\n                epochs_f1,\n                mean_f1 - se_f1,\n                mean_f1 + se_f1,\n                alpha=0.3,\n                color=\"tab:green\",\n                label=\"macro-F1 \u00b1 SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(\n                f\"{dname}: Aggregated Validation Macro-F1 (n={f1_mat.shape[0]} runs)\"\n            )\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_macro_f1_aggregated.png\"))\n            plt.close()\n\n            # quick console summary\n            best_epoch = np.argmax(mean_f1)\n            print(\n                f\"{dname}: best mean Macro-F1={mean_f1[best_epoch]:.3f} at epoch {best_epoch+1}\"\n            )\n    except Exception as e:\n        print(f\"Error creating aggregated macro-F1 for {dname}: {e}\")\n        plt.close()\n\n    # ---------- aggregated CWA ----------\n    try:\n        if cwa_mat.size:\n            epochs_cwa = np.arange(1, cwa_mat.shape[1] + 1)\n            mean_cwa = cwa_mat.mean(axis=0)\n            se_cwa = cwa_mat.std(axis=0, ddof=1) / np.sqrt(cwa_mat.shape[0])\n            plt.figure()\n            plt.plot(epochs_cwa, mean_cwa, color=\"tab:red\", label=\"CWA mean\")\n            plt.fill_between(\n                epochs_cwa,\n                mean_cwa - se_cwa,\n                mean_cwa + se_cwa,\n                alpha=0.3,\n                color=\"tab:red\",\n                label=\"CWA \u00b1 SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-Weighted Accuracy\")\n            plt.title(f\"{dname}: Aggregated Validation CWA (n={cwa_mat.shape[0]} runs)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_cwa_aggregated.png\"))\n            plt.close()\n\n            # quick console summary\n            best_epoch = np.argmax(mean_cwa)\n            print(\n                f\"{dname}: best mean CWA={mean_cwa[best_epoch]:.3f} at epoch {best_epoch+1}\"\n            )\n    except Exception as e:\n        print(f\"Error creating aggregated CWA for {dname}: {e}\")\n        plt.close()\n"], "plot_plan": ["We will load the saved numpy dictionary, locate the \u201cSPR_BENCH\u201d results, and iterate over the four batch-size sub-experiments. For every experiment we extract the per-epoch training/validation losses and the per-epoch validation macro-F1 scores. Plot 1 overlays the training and validation loss curves for all batch sizes, giving each setting a distinct colour/style. Plot 2 overlays the validation macro-F1 curves. Plot 3 is a bar chart of the final-epoch macro-F1 versus batch size. We then pick the batch size that achieved the best final macro-F1, compute its 2\u00d72 confusion matrix from the stored final-epoch predictions and ground-truth labels, and visualise it as a heat-map (Plot 4). Each figure is wrapped in its own try-except block, saved to the working directory with descriptive names, and explicitly closed. No fabricated data are introduced and we create at most four figures, all derived from experiment_data.npy.", "We first load the saved experiment_data.npy, extract the single dataset (\u201cSPR_BENCH\u201d), and compute final-epoch metrics (accuracy and complexity-weighted accuracy) from the stored predictions/labels/weights for reporting.  \nFour figures are generated, each inside its own try-except block and saved into the working directory:  \n(1) training vs. validation cross-entropy loss across epochs;  \n(2) validation accuracy and CWA across epochs;  \n(3) the 2\u00d72 confusion matrix on the test split;  \n(4) a scatterplot of instance complexity (weight) versus correctness, giving intuition about where the model struggles.  \nEach figure is titled clearly and saved with descriptive filenames that include the dataset name and plot type.  \nOnly data present in experiment_data.npy are used; no synthetic values are created.  \nFigures are closed after saving to free resources, and potential errors are caught and printed without stopping execution.  \nFinally, the script prints the computed test accuracy and CWA so the evaluation metrics are visible in the job logs.", "We load experiment_data.npy, fetch the single \u201cSPR_BENCH\u201d record and unpack losses, per-epoch validation metrics, predictions and ground-truth labels. Five basic plots are produced: (1) train vs validation cross-entropy loss curves; (2) validation macro-F1 across epochs; (3) validation complexity-weighted-accuracy (CWA) across epochs; (4) side-by-side bars of final-epoch macro-F1 and CWA; (5) a 2\u00d72 confusion matrix for the final model. Each figure is wrapped in its own try-except, saved to working_dir with informative names, then closed. When epochs exceed 20 we down-sample to at most 5 evenly spaced checkpoints, but the current run has only eight epochs so all are shown. After plotting we also print the final-epoch macro-F1 and CWA to stdout for quick reference. The code follows the required import order, creates working_dir if missing, never invents data, and keeps to plain matplotlib defaults.", null, "We will load the saved experiment_data dictionary, grab the SPR_BENCH entry, and extract per-epoch logs for training/validation loss and the two validation metrics (macro-F1 and complexity-weighted accuracy).  \nFour plots will be generated: (1) train vs. validation loss curves, (2) validation macro-F1 curve, (3) validation CWA curve, and (4) a confusion matrix for the final-epoch predictions vs. ground-truth labels.  \nEach plot is wrapped in its own try-except block, given an informative title/subtitle, saved to working_dir with descriptive filenames, and the figure is always closed afterward.  \nNo fake data are introduced\u2014everything is drawn directly from experiment_data.npy.  \nWe compute the final values for macro-F1 and CWA and print them to provide quick numeric feedback.  \nOnly basic matplotlib is used; no custom styles are applied.  \nThe code is concise, starts with the required imports, makes sure working_dir exists, and adheres to all naming and safety guidelines.", "Load experiment_data.npy, loop over each dataset key, and safely extract losses, validation metrics, predictions, ground-truth labels, and optional weights. For every dataset, create up to five standard figures: (1) training vs. validation loss curves; (2) validation macro-F1 over epochs; (3) validation CWA over epochs; (4) a final-epoch confusion matrix; and (5) a histogram of per-example weights (if weight information exists). Each plotting section is wrapped in its own try-except block to guarantee figure closure even on error. Use clear titles that embed the dataset name and concise subtitles when appropriate, and save each figure to working_dir with descriptive filenames like \u201cSPR_BENCH_loss_curves.png\u201d. After plotting, compute and print the best and last macro-F1/CWA values so the script also outputs a quick numerical summary. All plotting relies exclusively on data present in experiment_data.npy; no synthetic values are introduced. The code begins with the required imports, ensures the working directory exists, and remains compact and easily extensible to multiple datasets.", "We first load experiment_data.npy from the working directory and safely fall back to an empty dict if missing. From the SPR_BENCH sub-dict we extract losses (train/val), metrics (val_cwa, val_f1), predictions, ground_truth and epoch indices. Four plots are produced inside independent try-except blocks: (1) training vs validation loss curve; (2) validation CWA and Macro-F1 curves; (3) bar chart comparing the last-epoch CWA and F1; (4) 2\u00d72 confusion matrix built from predictions vs ground truth. Each figure is given a clear title and saved to working_dir using descriptive filenames such as \u201cSPR_BENCH_loss_curve.png\u201d. Figures are always closed, and errors are caught and printed without crashing subsequent plots. Finally the script prints final CWA and F1 for quick inspection.", "We first load experiment_data.npy, focusing on the \u201cSPR_BENCH\u201d entry. For each available epoch we gather training/validation losses plus validation macro-F1 and cost-weighted accuracy (CWA). Four plots are produced\u2014loss curves, macro-F1 curve, CWA curve, and a final-epoch confusion matrix\u2014each wrapped in its own try-except so failures don\u2019t halt the script. All figures are titled clearly and saved to the working directory with descriptive names that include the dataset and plot type. We respect the \u201cat most five figures\u201d rule by generating exactly four. After plotting, we print the final macro-F1 and CWA so the user sees the key metrics in the console. Every figure is explicitly closed to avoid memory leaks, and we only visualize data that actually exists in the numpy file. The code remains concise and uses only standard matplotlib and numpy.", "First, load experiment_data.npy and iterate over every dataset present; if only one is available, the loop still works seamlessly. For each dataset gather the arrays for training and validation losses plus validation-time macro-F1 and CWA that were stored as dictionaries. Inside separate try-except blocks create at most four figures per dataset: (1) a loss curve with both training and validation losses, (2) a metric curve displaying macro-F1 and CWA together, (3) a bar plot comparing the final-epoch macro-F1 and CWA, and (4) a 2\u00d72 confusion matrix built from test predictions vs. ground truth. Each figure is given a clear title and subtitle, then saved to working_dir with descriptive filenames that include the dataset name. All figures are closed after saving to avoid memory leaks. The code relies solely on the content of experiment_data.npy and uses basic matplotlib; any missing keys fall back gracefully to empty arrays so the script never crashes. No more than the required plots are generated, keeping the total below the five-figure limit. Finally, the script ends without returning any extraneous output, satisfying the given developer instructions.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["batch_size", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved NumPy dictionary, traverse the nested structure\nto reach each batch-size configuration for the single dataset (SPR_BENCH), and\nthen print the final and best values for training loss, validation loss, and\nvalidation macro-F1. All printing is done with explicit metric names so the\noutput is clear and unambiguous.", "The script will load experiment_data.npy from the working directory, iterate\nover each stored dataset, and print the most informative single number for every\nrecorded metric. For accuracy-type metrics it reports the highest (best) value;\nfor loss-type metrics it reports the lowest (best) value. It also recomputes and\nprints test-set scores from the saved predictions so the output covers training,\nvalidation, and test phases without creating any plots.", "The script will directly load the saved NumPy file from the working directory,\nunpack the dictionary, and iterate through every dataset it contains (here,\n\u201cSPR_BENCH\u201d). For each dataset it will (1) obtain the final training loss, (2)\nfind the best validation loss, (3) find the best validation macro-F1 score, and\n(4) find the best validation complexity-weighted accuracy. It prints these\nfigures with explicit, descriptive labels so it is clear which split and metric\neach value represents. No plots are generated and the code runs immediately on\nexecution.", "", "The script will locate the stored experiment_data.npy in the \u201cworking\u201d\ndirectory, load it, and iterate over each dataset entry (e.g., \u201cSPR_BENCH\u201d). For\nevery dataset it extracts the last (i.e., final) value recorded for each tracked\nitem: training loss, validation loss, validation macro F1 score, and validation\ncomplexity-weighted accuracy. It then prints the dataset name followed by these\nclearly-labeled metrics. The code runs immediately on execution\u2014no special entry\npoint is required.", "The script will locate the \u201cworking\u201d directory, load the saved NumPy dictionary,\nand iterate over each dataset contained in it.   For every dataset, the code\nretrieves (1) the final training loss, (2) the best (minimum) validation loss,\nand (3) the best values of every validation-set metric recorded (macro-F1 and\ncurriculum-weighted accuracy).   Each datum is printed with a clear, explicit\nlabel so there is no ambiguity about what is being shown.   The whole routine is\nplaced at the global level so it runs immediately when executed.", "The script below loads the saved experiment data, identifies the relevant\nmetrics for each dataset, computes the best (or final) values as appropriate,\nand prints them with explicit, descriptive names. For validation metrics we\nreport the best (highest CWA and macro-F1, lowest loss), while for training loss\nwe report the final epoch value. If test predictions are present, the script\nalso computes and prints the final test macro-F1 score. All code runs\nimmediately at the top level without any special entry point.", "The script will locate the working directory, read the stored numpy file into a\nPython dictionary, and then iterate over every dataset contained in it. For each\ndataset it will compute the \u201cbest\u201d (i.e., minimum for losses and maximum for\nscores) values of training loss, validation loss, validation macro-F1, and\nvalidation class-weighted accuracy. The code finally prints the dataset name\nfollowed by each metric name and its best value, providing a concise summary\nwithout generating any plots.", "We will load the serialized dictionary from the working directory, compute\nsummary statistics (final training loss, best validation loss, best validation\nmacro-F1, best validation CWA, test macro-F1 and test CWA), and print them with\nclear, descriptive labels for every dataset found in the file.  To recompute the\ntest metrics we use scikit-learn\u2019s f1_score and the original CWA definition.\nEverything runs immediately at import time\u2014no `if __name__ == \"__main__\":` guard\nis used.", "The script will locate the \u201cworking\u201d directory, load the saved NumPy dictionary,\nand iterate over each dataset contained in it.   For every dataset, the code\nretrieves (1) the final training loss, (2) the best (minimum) validation loss,\nand (3) the best values of every validation-set metric recorded (macro-F1 and\ncurriculum-weighted accuracy).   Each datum is printed with a clear, explicit\nlabel so there is no ambiguity about what is being shown.   The whole routine is\nplaced at the global level so it runs immediately when executed.", "The script will locate the \u201cworking\u201d directory, load the saved NumPy dictionary,\nand iterate over each dataset contained in it.   For every dataset, the code\nretrieves (1) the final training loss, (2) the best (minimum) validation loss,\nand (3) the best values of every validation-set metric recorded (macro-F1 and\ncurriculum-weighted accuracy).   Each datum is printed with a clear, explicit\nlabel so there is no ambiguity about what is being shown.   The whole routine is\nplaced at the global level so it runs immediately when executed.", "The script will locate the \u201cworking\u201d directory, load the saved NumPy dictionary,\nand iterate over each dataset contained in it.   For every dataset, the code\nretrieves (1) the final training loss, (2) the best (minimum) validation loss,\nand (3) the best values of every validation-set metric recorded (macro-F1 and\ncurriculum-weighted accuracy).   Each datum is printed with a clear, explicit\nlabel so there is no ambiguity about what is being shown.   The whole routine is\nplaced at the global level so it runs immediately when executed.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# 1. Iterate through datasets and configurations\nfor dataset_name, cfgs in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for cfg_name, logs in cfgs.items():\n        train_losses = logs[\"losses\"][\"train\"]\n        val_losses = logs[\"losses\"][\"val\"]\n        val_f1s = logs[\"metrics\"][\"val\"]\n\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        final_val_f1 = val_f1s[-1]\n\n        best_val_loss = min(val_losses)\n        best_val_f1 = max(val_f1s)\n\n        print(f\"  Configuration: {cfg_name}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    best validation loss: {best_val_loss:.4f}\")\n        print(f\"    final validation macro F1 score: {final_val_f1:.4f}\")\n        print(f\"    best validation macro F1 score: {best_val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef complexity_weighted_accuracy(preds, labels, weights):\n    correct = (preds == labels).astype(float)\n    return (correct * weights).sum() / weights.sum()\n\n\n# ------------------------------------------------------------------\nfor dataset_name, ds in experiment_data.items():\n    print(f\"\\n=== {dataset_name} ===\")\n\n    # ------------------- losses -------------------\n    losses = ds.get(\"losses\", {})\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        print(f\"best validation loss: {min(val_losses):.6f}\")\n\n    # ------------------- validation metrics -------------------\n    metrics = ds.get(\"metrics\", {})\n    val_accuracies = metrics.get(\"accuracy\", [])\n    val_cwa = metrics.get(\"cwa\", [])\n\n    if val_accuracies:\n        print(f\"best validation accuracy: {max(val_accuracies):.6f}\")\n    if val_cwa:\n        print(f\"best validation complexity-weighted accuracy: {max(val_cwa):.6f}\")\n\n    # ------------------- test metrics -------------------\n    preds = np.array(ds.get(\"predictions\", []))\n    labels = np.array(ds.get(\"ground_truth\", []))\n    weights = np.array(ds.get(\"weights\", []))\n\n    if len(preds) == len(labels) and len(preds) > 0:\n        test_acc = (preds == labels).mean()\n        print(f\"test accuracy: {test_acc:.6f}\")\n\n        if len(weights) == len(preds):\n            test_cwa = complexity_weighted_accuracy(preds, labels, weights)\n            print(f\"test complexity-weighted accuracy: {test_cwa:.6f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# Helper to compute best (or final) metric values\ndef summarize_dataset(name, data_dict):\n    # Losses\n    train_losses = data_dict[\"losses\"][\"train\"]\n    val_losses = data_dict[\"losses\"][\"val\"]\n\n    final_train_loss = train_losses[-1] if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n\n    # Validation metrics\n    val_metrics = data_dict[\"metrics\"][\"val\"]\n    if val_metrics:\n        best_macro_f1 = max(m[\"macro_f1\"] for m in val_metrics)\n        best_cwa = max(m[\"cwa\"] for m in val_metrics)\n    else:\n        best_macro_f1 = best_cwa = None\n\n    # Printing\n    print(f\"\\nDataset: {name}\")\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n    if best_macro_f1 is not None:\n        print(f\"Best validation macro F1 score: {best_macro_f1:.6f}\")\n    if best_cwa is not None:\n        print(f\"Best validation complexity-weighted accuracy: {best_cwa:.6f}\")\n\n\n# -------------------------------------------------\n# Iterate over all datasets in the experiment file\nfor dataset_name, dataset_data in experiment_data.items():\n    summarize_dataset(dataset_name, dataset_data)\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# iterate through datasets and display final metrics\nfor dataset_name, data_dict in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset header\n\n    # retrieve final values (last entry in each list)\n    train_loss_final = (\n        data_dict[\"losses\"][\"train\"][-1] if data_dict[\"losses\"][\"train\"] else None\n    )\n    val_loss_final = (\n        data_dict[\"losses\"][\"val\"][-1] if data_dict[\"losses\"][\"val\"] else None\n    )\n    val_macroF1_final = (\n        data_dict[\"metrics\"][\"val_macroF1\"][-1]\n        if data_dict[\"metrics\"][\"val_macroF1\"]\n        else None\n    )\n    val_CWA_final = (\n        data_dict[\"metrics\"][\"val_CWA\"][-1] if data_dict[\"metrics\"][\"val_CWA\"] else None\n    )\n\n    # print metrics with explicit, descriptive names\n    if train_loss_final is not None:\n        print(f\"Final training loss: {train_loss_final:.4f}\")\n    if val_loss_final is not None:\n        print(f\"Final validation loss: {val_loss_final:.4f}\")\n    if val_macroF1_final is not None:\n        print(f\"Final validation macro F1 score: {val_macroF1_final:.4f}\")\n    if val_CWA_final is not None:\n        print(f\"Final validation complexity-weighted accuracy: {val_CWA_final:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate the .npy file and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# helper to print best / final statistics\n# -------------------------------------------------\nfor dataset_name, ds_data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    # ----- metrics -----\n    val_metrics = ds_data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # Find the epoch with the best macro-F1\n        best_macro_f1_idx = int(\n            np.argmax([m.get(\"macro_f1\", -np.inf) for m in val_metrics])\n        )\n        best_metrics = val_metrics[best_macro_f1_idx]\n\n        macro_f1 = best_metrics.get(\"macro_f1\")\n        cwa = best_metrics.get(\"cwa\")\n\n        if macro_f1 is not None:\n            print(f\"best validation macro F1 score: {macro_f1:.6f}\")\n        if cwa is not None:\n            print(f\"best validation curriculum-weighted accuracy: {cwa:.6f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Cannot find experiment data at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- process and print ----------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- losses ---\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # --- validation metrics ---\n    metrics = data.get(\"metrics\", {})\n    val_cwa_list = metrics.get(\"val_cwa\", [])\n    val_f1_list = metrics.get(\"val_f1\", [])\n\n    if val_cwa_list:\n        best_val_cwa = max(val_cwa_list)\n        print(f\"best validation character-weighted accuracy: {best_val_cwa:.4f}\")\n\n    if val_f1_list:\n        best_val_f1 = max(val_f1_list)\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # --- test metrics ---\n    preds = data.get(\"predictions\", [])\n    truths = data.get(\"ground_truth\", [])\n    if preds and truths:\n        test_f1 = f1_score(truths, preds, average=\"macro\")\n        print(f\"test macro F1 score: {test_f1:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate and load the data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# 1. Helper to print a metric only if it exists\ndef _print_metric(name: str, value):\n    if value is None:\n        return\n    if isinstance(value, float):\n        print(f\"{name}: {value:.4f}\")\n    else:  # fallback for non-floats\n        print(f\"{name}: {value}\")\n\n\n# -------------------------------------------------\n# 2. Iterate over datasets and report metrics\nfor ds_name, ds_blob in experiment_data.items():\n    print(ds_name)  # Dataset header\n\n    # Losses\n    train_losses = ds_blob.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_blob.get(\"losses\", {}).get(\"val\", [])\n\n    best_train_loss = min(train_losses) if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n\n    # Validation metrics\n    val_metrics = ds_blob.get(\"metrics\", {}).get(\"val\", [])\n    macro_f1_list = [m.get(\"macro_f1\") for m in val_metrics if \"macro_f1\" in m]\n    cwa_list = [m.get(\"cwa\") for m in val_metrics if \"cwa\" in m]\n\n    best_macro_f1 = max(macro_f1_list) if macro_f1_list else None\n    best_cwa = max(cwa_list) if cwa_list else None\n\n    # Print results\n    _print_metric(\"best training loss\", best_train_loss)\n    _print_metric(\"best validation loss\", best_val_loss)\n    _print_metric(\"best validation macro F1\", best_macro_f1)\n    _print_metric(\"best validation class-weighted accuracy\", best_cwa)\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n\n# ---------- helpers -----------------------------------------------------------\ndef cwa(preds: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> float:\n    correct = (preds == labels).astype(float)\n    return float((correct * weights).sum() / weights.sum())\n\n\n# ---------- load experiment data ---------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- iterate over datasets & print metrics -----------------------------\nfor dset_name, info in experiment_data.items():\n    print(f\"{dset_name}:\")\n    # ----- losses -----\n    train_losses = info.get(\"losses\", {}).get(\"train\", [])\n    val_losses = info.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"  final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"  best validation loss: {min(val_losses):.4f}\")\n\n    # ----- validation metrics -----\n    val_metrics_list = info.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics_list:\n        best_macro_f1 = max(m[\"macro_f1\"] for m in val_metrics_list)\n        best_cwa = max(m[\"cwa\"] for m in val_metrics_list)\n        print(f\"  best validation macro F1 score: {best_macro_f1:.4f}\")\n        print(f\"  best validation CWA: {best_cwa:.4f}\")\n\n    # ----- test metrics -----\n    preds = np.asarray(info.get(\"predictions\", []))\n    gts = np.asarray(info.get(\"ground_truth\", []))\n    wts = np.asarray(info.get(\"weights\", []))\n    if preds.size and gts.size and wts.size:\n        test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n        test_cwa = cwa(preds, gts, wts)\n        print(f\"  test macro F1 score: {test_macro_f1:.4f}\")\n        print(f\"  test CWA: {test_cwa:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate the .npy file and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# helper to print best / final statistics\n# -------------------------------------------------\nfor dataset_name, ds_data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    # ----- metrics -----\n    val_metrics = ds_data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # Find the epoch with the best macro-F1\n        best_macro_f1_idx = int(\n            np.argmax([m.get(\"macro_f1\", -np.inf) for m in val_metrics])\n        )\n        best_metrics = val_metrics[best_macro_f1_idx]\n\n        macro_f1 = best_metrics.get(\"macro_f1\")\n        cwa = best_metrics.get(\"cwa\")\n\n        if macro_f1 is not None:\n            print(f\"best validation macro F1 score: {macro_f1:.6f}\")\n        if cwa is not None:\n            print(f\"best validation curriculum-weighted accuracy: {cwa:.6f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate the .npy file and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# helper to print best / final statistics\n# -------------------------------------------------\nfor dataset_name, ds_data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    # ----- metrics -----\n    val_metrics = ds_data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # Find the epoch with the best macro-F1\n        best_macro_f1_idx = int(\n            np.argmax([m.get(\"macro_f1\", -np.inf) for m in val_metrics])\n        )\n        best_metrics = val_metrics[best_macro_f1_idx]\n\n        macro_f1 = best_metrics.get(\"macro_f1\")\n        cwa = best_metrics.get(\"cwa\")\n\n        if macro_f1 is not None:\n            print(f\"best validation macro F1 score: {macro_f1:.6f}\")\n        if cwa is not None:\n            print(f\"best validation curriculum-weighted accuracy: {cwa:.6f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate the .npy file and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# helper to print best / final statistics\n# -------------------------------------------------\nfor dataset_name, ds_data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    # ----- metrics -----\n    val_metrics = ds_data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # Find the epoch with the best macro-F1\n        best_macro_f1_idx = int(\n            np.argmax([m.get(\"macro_f1\", -np.inf) for m in val_metrics])\n        )\n        best_metrics = val_metrics[best_macro_f1_idx]\n\n        macro_f1 = best_metrics.get(\"macro_f1\")\n        cwa = best_metrics.get(\"cwa\")\n\n        if macro_f1 is not None:\n            print(f\"best validation macro F1 score: {macro_f1:.6f}\")\n        if cwa is not None:\n            print(f\"best validation curriculum-weighted accuracy: {cwa:.6f}\")\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', '  Configuration: bs_32', '\\n', '    final\ntraining loss: 0.6295', '\\n', '    final validation loss: 0.6436', '\\n', '\nbest validation loss: 0.6389', '\\n', '    final validation macro F1 score:\n0.6840', '\\n', '    best validation macro F1 score: 0.6840', '\\n', '\nConfiguration: bs_64', '\\n', '    final training loss: 0.6773', '\\n', '    final\nvalidation loss: 0.6889', '\\n', '    best validation loss: 0.6640', '\\n', '\nfinal validation macro F1 score: 0.5567', '\\n', '    best validation macro F1\nscore: 0.6480', '\\n', '  Configuration: bs_128', '\\n', '    final training loss:\n0.6535', '\\n', '    final validation loss: 0.6497', '\\n', '    best validation\nloss: 0.6398', '\\n', '    final validation macro F1 score: 0.6596', '\\n', '\nbest validation macro F1 score: 0.6839', '\\n', '  Configuration: bs_256', '\\n',\n'    final training loss: 0.6384', '\\n', '    final validation loss: 0.6420',\n'\\n', '    best validation loss: 0.6420', '\\n', '    final validation macro F1\nscore: 0.6790', '\\n', '    best validation macro F1 score: 0.6790', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== SPR_BENCH ===', '\\n', 'final training loss: 0.628755', '\\n', 'best\nvalidation loss: 0.635956', '\\n', 'best validation accuracy: 0.696000', '\\n',\n'best validation complexity-weighted accuracy: 0.696000', '\\n', 'test accuracy:\n0.693000', '\\n', 'test complexity-weighted accuracy: 0.693000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.610493', '\\n', 'Best\nvalidation loss: 0.636137', '\\n', 'Best validation macro F1 score: 0.689900',\n'\\n', 'Best validation complexity-weighted accuracy: 0.690000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "", "['\\nSPR_BENCH', '\\n', 'Final training loss: 0.6330', '\\n', 'Final validation\nloss: 0.6645', '\\n', 'Final validation macro F1 score: 0.6855', '\\n', 'Final\nvalidation complexity-weighted accuracy: 0.6860', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.619650', '\\n', 'best\nvalidation loss: 0.626568', '\\n', 'best validation macro F1 score: 0.695981',\n'\\n', 'best validation curriculum-weighted accuracy: 0.696000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.6135', '\\n', 'best validation loss:\n0.6292', '\\n', 'best validation character-weighted accuracy: 0.6920', '\\n',\n'best validation macro F1 score: 0.6918', '\\n', 'test macro F1 score: 0.6884',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best training loss: 0.6167', '\\n', 'best validation loss:\n0.6394', '\\n', 'best validation macro F1: 0.6879', '\\n', 'best validation class-\nweighted accuracy: 0.6880', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  final training loss: 59.0337', '\\n', '  best validation\nloss: 59.6666', '\\n', '  best validation macro F1 score: 0.6880', '\\n', '  best\nvalidation CWA: 0.6880', '\\n', '  test macro F1 score: 0.6919', '\\n', '  test\nCWA: 0.6920', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.624065', '\\n', 'best\nvalidation loss: 0.631272', '\\n', 'best validation macro F1 score: 0.695981',\n'\\n', 'best validation curriculum-weighted accuracy: 0.696000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.620969', '\\n', 'best\nvalidation loss: 0.629051', '\\n', 'best validation macro F1 score: 0.691956',\n'\\n', 'best validation curriculum-weighted accuracy: 0.692000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.627987', '\\n', 'best\nvalidation loss: 0.632125', '\\n', 'best validation macro F1 score: 0.689939',\n'\\n', 'best validation curriculum-weighted accuracy: 0.690000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
