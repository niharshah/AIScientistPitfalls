
% This paper discusses the foundational work on Gated Recurrent Units (GRU) by Chung et al. (2014) and their application in sequence modeling tasks. It is relevant to cite this work to provide background on the GRU model used as the baseline in this study. The citation will be included in the section discussing the baseline model architecture and its relevance to symbolic sequence classification.
@article{tsakalos2018sentimentcu,
 author = {Vasileios Tsakalos and R. Henriques},
 booktitle = {International Conference on Knowledge Discovery and Information Retrieval},
 pages = {147-152},
 title = {Sentiment Classification using N-ary Tree-Structured Gated Recurrent Unit Networks},
 year = {2018}
}

% The paper 'ACRES: A framework for (semi)automatic generation of rule‐based expert systems with uncertainty from datasets' provides foundational insights into symbolic reasoning and rule-based systems, including methods for rule extraction and interpretability. It is relevant to cite in the introduction or related work section to highlight existing frameworks for symbolic rule generation and the gap in addressing poly-factor symbolic rules, which our research aims to address.
@article{kovas2024acresaf,
 author = {Konstantinos Kovas and Ioannis Hatzilygeroudis},
 booktitle = {Expert Syst. J. Knowl. Eng.},
 journal = {Expert Systems},
 title = {ACRES: A framework for (semi)automatic generation of rule‐based expert systems with uncertainty from datasets},
 volume = {41},
 year = {2024}
}

% The paper 'Convergence rates for the Adam optimizer' acknowledges the original work by Kingma and Ba (2014) and provides insights into the optimization method's convergence properties. It is relevant to cite this work in the methodology section where the Adam optimizer is used for training machine learning models, to credit its origin and highlight its importance in deep learning applications.
@article{dereich2024convergencerf,
 author = {Steffen Dereich and Arnulf Jentzen},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Convergence rates for the Adam optimizer},
 volume = {abs/2407.21078},
 year = {2024}
}

% The paper 'NeSyMoF: A Neuro-Symbolic Model for Motion Forecasting' is relevant for highlighting recent advancements in neuro-symbolic reasoning and its application to interpretable logic-based models. It addresses challenges in combining deep learning with symbolic reasoning and discusses the development of first-order logic rules for interpretability. This citation will be used in the related work section to emphasize the research gap in handling poly-factor symbolic rules and to contextualize the need for robust methodologies like ours in symbolic pattern recognition tasks.
@article{doula2024nesymofan,
 author = {Achref Doula and Huijie Yin and Max Mühlhäuser and Alejandro Sánchez Guinea},
 booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
 journal = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 pages = {919-926},
 title = {NeSyMoF: A Neuro-Symbolic Model for Motion Forecasting},
 year = {2024}
}

% The paper 'CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition' introduces a curriculum learning strategy embedded into the loss function to address easy samples in the early training stage and hard samples in the later stage. This work is relevant to the methodology section of our research, as it provides insights into adaptive curriculum learning, which aligns with our focus on stabilizing learning dynamics and emphasizing complex examples in symbolic sequence classification tasks.
@article{huang2020curricularfaceac,
 author = {Y. Huang and Yuhan Wang and Ying Tai and Xiaoming Liu and Pengcheng Shen and Shaoxin Li and Jilin Li and Feiyue Huang},
 booktitle = {Computer Vision and Pattern Recognition},
 journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {5900-5909},
 title = {CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition},
 year = {2020}
}

% The KANDY Benchmark introduces a framework for symbolic reasoning and classification tasks with increasing complexity, focusing on symbol compositionality and interpretable solutions. This work is relevant for contextualizing the SPR_BENCH dataset used in this research and highlighting the challenges of symbolic sequence classification. It should be cited in the Dataset or Related Work section to provide insights into existing benchmarks and their limitations in addressing poly-factor symbolic rules.
@article{lorello2024thekb,
 author = {Luca Salvatore Lorello and Marco Lippi and S. Melacci},
 booktitle = {Machine-mediated learning},
 journal = {Mach. Learn.},
 pages = {161},
 title = {The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns},
 volume = {114},
 year = {2024}
}

% The paper 'A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers' explores the application of Transformers to symbolic reasoning and sequence classification tasks. It evaluates the generalization capabilities of Transformers in reasoning problems and highlights their strengths and limitations. This citation is relevant to support the experimental section of the paper where a Transformer encoder is used for symbolic sequence classification, and it can also contribute to the discussion on model generalization and reasoning capabilities.
@article{meadows2023asf,
 author = {Jordan Meadows and Marco Valentino and Damien Teney and André Freitas},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 pages = {1505-1523},
 title = {A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers},
 year = {2023}
}
