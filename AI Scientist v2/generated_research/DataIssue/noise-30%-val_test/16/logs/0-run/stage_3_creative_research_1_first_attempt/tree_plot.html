<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 3], [0, 4], [0, 2], [1, 6], [1, 7], [1, 8], [4, 5], [6, 11], [6, 9], [6, 10], [6, 12]], "layout": [[0.7272727272727273, 0.0], [0.45454545454545453, 0.33333333333333337], [0.6363636363636364, 0.33333333333333337], [0.8181818181818182, 0.33333333333333337], [1.0, 0.33333333333333337], [1.0, 0.6666666666666667], [0.2727272727272727, 0.6666666666666667], [0.45454545454545453, 0.6666666666666667], [0.6363636363636364, 0.6666666666666667], [0.0, 1.0], [0.18181818181818182, 1.0], [0.36363636363636365, 1.0], [0.5454545454545454, 1.0]], "plan": ["Hyperparam tuning name: LEARNING_RATE. We sweep over four learning-rates (3 e-4,\n5 e-4, 1 e-3, 2 e-3).   For every rate we (1) initialise a fresh Bi-LSTM, (2)\ntrain for the same fixed number of epochs, (3) record epoch-wise losses and\nMCCs, and (4) keep the model that attains the best validation MCC.   After the\nsweep we evaluate this best model on the test split, save all collected data to\nexperiment_data.npy, and plot curves for the winning learning-rate.   The code\nbelow is fully self-contained and runnable.", "To strengthen baseline results we switch from the small bi-LSTM to a lightweight\nTransformer encoder that can reason over the whole symbol sequence through self-\nattention, and we enrich the input with a learnable [CLS] token so global\nfeatures are explicitly captured.  Learned positional embeddings keep the order\ninformation that is crucial for PolyRule relations while still allowing parallel\ncomputation on GPU.   We build a character-level vocabulary, pad/clip sequences\nto a fixed length, and automatically fall back to a small synthetic dataset if\nthe real SPR_BENCH folder is not available so the script is always runnable.\nTraining uses Adam with a moderate learning-rate (1e-3), BCEWithLogits loss, and\nruns for a handful of epochs to stay within the time budget.  After every epoch\nwe compute and print validation loss, overall accuracy, Matthews Correlation\nCoefficient, and the requested Rule-Macro Accuracy (RMA) obtained by averaging\nper-rule accuracies (rule id heuristically extracted from the row id).   All\nmetrics, losses, predictions, and ground-truth labels are stored in\n`experiment_data` and saved to disk; plots can be produced later from that file.\nThe model, data, and every tensor are consistently moved to GPU when available,\nmeeting the mandatory hardware-handling rules.   Finally we evaluate on the test\nsplit and print the obtained scores so the run is fully reproducible and\nimmediately informative.", "We will switch from the earlier Bi-LSTM to a lightweight Transformer encoder,\nwhich should model long-range, multi-factor interactions better.   The code\nbelow   1) loads SPR_BENCH (or synthesises data if the path is missing),   2)\nbuilds a char-level vocabulary and pads / truncates every sequence to `max_len`,\n3) trains a 2-layer Transformer encoder whose CLS token is fed to a linear head,\n4) tracks loss, MCC, Accuracy and Rule-Macro-Accuracy (RMA \u2013 falling back on the\nsample id if an explicit rule id is absent), printing validation loss every\nepoch,   5) evaluates on the test split with the best-validation model,   6)\nstores all metrics / predictions in `experiment_data.npy` inside the required\n`./working` dir.   The whole script is self-contained and honours all GPU /\ndata-saving constraints.", "To boost performance and rule-level robustness we switch from the previous Bi-\nLSTM to a light Transformer encoder that can attend over the whole symbol\nsequence, add sinusoidal position encodings, and train with label-smoothed BCE\nto regularise against over-confidence.  Each sequence is padded to a fixed max\nlength, embedded, passed through 2 transformer layers (4 heads, 64 hidden) and\npooled via masked mean before classification.  We track three metrics every\nepoch: loss, Matthews Correlation Coefficient, and Rule-Macro Accuracy (RMA).\nBecause the official rule-id column is absent, we approximate \u201crule\u201d by the\nsequence length bucket (len % 5); this yields a non-trivial stratification and\ndemonstrates the RMA computation which can be swapped for real rule IDs later.\nA tiny learning-rate sweep (5e-4, 1e-3) is run and the best model (by validation\nMCC) is evaluated on the test set.  All curves, predictions and metrics are\nstored in `experiment_data.npy` under the mandatory `working` directory.  The\nscript is fully self-contained, GPU-aware, and terminates in a few minutes on a\nstandard GPU/CPU.", "I will switch from the small Bi-LSTM to a lightweight Transformer encoder that\ncan model long-range symbol relations better, add positional embeddings, and\ntrain it with cyclic cosine learning-rate scheduling and label-smoothing.   A\nsingle sweep over two hidden sizes (64/128) will be run; the winner on dev MCC\nis kept for test evaluation.   We also implement Rule-Macro-Accuracy (RMA); if\nthe CSV contains a column named \u201crule\u201d, we use it, otherwise we fall back to\nnormal accuracy with a warning.   All losses, MCCs, RMAs, and predictions are\nstored in the mandated experiment_data structure and saved under ./working.\nThe whole script respects the GPU/CPU handling checklist.", "The previous implementation computed Rule-Macro Accuracy on the training set\nwith only the last batch\u2019s rule-ids, which severely distorted the metric and\naffected any adaptive decisions that might rely on it.   The fix is simply to\naggregate rule identifiers for every batch (just as we already do for the\nvalidation loop) and then call rule_macro_accuracy with the full list. No other\nlogic changes are required; we keep the model, training loop, and logging\nstructure intact while ensuring all tensors and models reside on the correct\ndevice.   The code below implements this fix, keeps full metric tracking,\nsatisfies all GPU/IO guidelines, and stores results in ./working.", "We extend the baseline Transformer with explicit symbolic-count features that\ncapture global rule cues (e.g., counts, length) which a pure sequence model may\nlearn only slowly.   Each example now provides (i) token ids and (ii) a real-\nvalued vector containing normalised counts of every symbol plus the normalised\nsequence length.   The model embeds the sequence, passes it through a\nlightweight Transformer encoder, projects the count vector into the same space,\nconcatenates both representations and decides with a small MLP.   Label-\nsmoothing and an aggressive One-Cycle learning-rate schedule are added for\nbetter generalisation, and all metrics (Acc/MCC/RMA) are tracked and saved.\nBecause count features explicitly expose many atomic predicates used in\nPolyRules, this hybrid architecture typically boosts Rule-Macro Accuracy and\noverall accuracy beyond the plain Transformer while remaining lightweight.   The\nscript trains for a few epochs (<10 min on GPU), prints validation metrics each\nepoch and finishes with a full test evaluation, storing everything in\n./working/experiment_data.npy.   It gracefully falls back to a synthetic dataset\nif SPR_BENCH is unavailable so the experiment is always runnable.", "I will augment the previous Transformer baseline with an auxiliary \u201csymbol-\ncount\u201d channel: every sequence is represented both token-wise (for order\ninformation) and as a normalized histogram of symbol occurrences (for pure\ncounting predicates).  A lightweight MLP embeds this count vector and its output\nis concatenated with the CLS embedding coming from a deeper Transformer encoder.\nThis hybrid representation should help when the PolyRules depend on parity,\nmajority, or exact counts, while still keeping order sensitivity for positional\nrules.  I also add label-smoothing (\u03f5 = 0.1) to reduce over-confidence and a\ncosine learning-rate scheduler with warm-up to stabilise training.  The rest of\nthe pipeline (data handling, metric tracking, saving) stays identical to keep\ncomparability with prior runs.", "We first add a MASK token and let one compact Transformer serve two heads: an\nMLM head (predict masked symbols) and a CLS head (binary rule-validity).\nStep-1: do a few quick epochs of self-supervised pre-training on the\nconcatenation of train+dev sequences, masking 15 % tokens; this lets the encoder\ndiscover the symbol syntax without labels.   Step-2: fine-tune the same network\nfor classification with label-smoothing, cyclical learning-rate and early\nstopping on dev loss.   Tiny/fast hyper-parameters (4 MLM + 6 CLS epochs) keep\nruntime < 30 min while usually giving a noticeable jump in accuracy/RMA over the\nprevious plain-from-scratch run.   At the end we compute the best decision\nthreshold on the dev set and apply it to test, report loss/Acc/MCC/RMA, and\npersist all metrics to working/experiment_data.npy as required.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, json, math, time\nimport numpy as np\nimport torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef\n\n# ------------------------ experiment container ------------------------------\nexperiment_data = {\n    \"LEARNING_RATE\": {\"SPR_BENCH\": {}}  # each lr will get its own dict here\n}\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------------- reproducibility ------------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# ----------------------------- device ---------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# --------------------- dataset loading / synthesis --------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv_name):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_load_real_dataset() -> DatasetDict:\n    env_path = os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    root = pathlib.Path(env_path)\n    if root.exists():\n        print(\"Loading real SPR_BENCH from\", root)\n        return load_spr_bench(root)\n    print(\"Real dataset not found \u2192 generating synthetic data\")\n    from datasets import Dataset as HFDataset\n\n    syms = list(\"ABCDEFGH\")\n\n    def synth_split(n):\n        seqs, labs = [], []\n        for _ in range(n):\n            ln = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)  # parity on 'A'\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    ddict = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        ddict[split] = HFDataset.from_dict(synth_split(n))\n    return ddict\n\n\nspr_bench = maybe_load_real_dataset()\nprint(\"Splits:\", spr_bench.keys())\n\n# -------------------------- vocabulary --------------------------------------\nall_text = \"\".join(spr_bench[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 == PAD\nitos = {i: ch for ch, i in stoi.items()}\npad_idx = 0\nmax_len = min(40, max(len(s) for s in spr_bench[\"train\"][\"sequence\"]))\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs, self.labs = hf_ds[\"sequence\"], hf_ds[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[i]), dtype=torch.long),\n            \"y\": torch.tensor(self.labs[i], dtype=torch.float32),\n        }\n\n\ntrain_ds, val_ds, test_ds = (SPRTorch(spr_bench[s]) for s in [\"train\", \"dev\", \"test\"])\n\n\n# ----------------------------- model ----------------------------------------\nclass CharBiLSTM(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=32, hidden=64):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz + 1, emb_dim, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(self.emb(x))\n        return self.fc(out.mean(1)).squeeze(1)\n\n\n# ------------------------ training util -------------------------------------\ndef run_training(lr, epochs=5, batch=128):\n    tr_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=256)\n    model = CharBiLSTM(len(vocab)).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    crit = nn.BCEWithLogitsLoss()\n    rec = {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n    }\n\n    for ep in range(1, epochs + 1):\n        # ---- training ----\n        model.train()\n        tloss = 0\n        tp, tt = [], []\n        for b in tr_loader:\n            b = {k: v.to(device) for k, v in b.items()}\n            opt.zero_grad()\n            logits = model(b[\"x\"])\n            loss = crit(logits, b[\"y\"])\n            loss.backward()\n            opt.step()\n            tloss += loss.item() * b[\"x\"].size(0)\n            tp += list((torch.sigmoid(logits).detach().cpu().numpy()) > 0.5)\n            tt += list(b[\"y\"].cpu().numpy())\n        tr_loss = tloss / len(train_ds)\n        tr_mcc = matthews_corrcoef(tt, tp)\n\n        # ---- validation ----\n        model.eval()\n        vloss = 0\n        vp, vt = [], []\n        with torch.no_grad():\n            for b in va_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                logits = model(b[\"x\"])\n                loss = crit(logits, b[\"y\"])\n                vloss += loss.item() * b[\"x\"].size(0)\n                vp += list((torch.sigmoid(logits).cpu().numpy()) > 0.5)\n                vt += list(b[\"y\"].cpu().numpy())\n        va_loss = vloss / len(val_ds)\n        va_mcc = matthews_corrcoef(vt, vp)\n\n        rec[\"losses\"][\"train\"].append(tr_loss)\n        rec[\"losses\"][\"val\"].append(va_loss)\n        rec[\"metrics\"][\"train_MCC\"].append(tr_mcc)\n        rec[\"metrics\"][\"val_MCC\"].append(va_mcc)\n        rec[\"epochs\"].append(ep)\n        print(\n            f\"lr {lr:.0e} | epoch {ep}: train_loss={tr_loss:.4f}, val_loss={va_loss:.4f}, val_MCC={va_mcc:.4f}\"\n        )\n    rec[\"model_state\"] = model.state_dict()  # save weights for best later\n    return rec\n\n\n# -------------------------- hyperparam sweep --------------------------------\nsweep_lrs = [3e-4, 5e-4, 1e-3, 2e-3]\nbest_lr, best_val = None, -1\nfor lr in sweep_lrs:\n    record = run_training(lr)\n    experiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][f\"{lr:.0e}\"] = record\n    top_val = max(record[\"metrics\"][\"val_MCC\"])\n    if top_val > best_val:\n        best_val, best_lr = top_val, lr\nprint(f\"Best LR: {best_lr:.0e} with peak val_MCC={best_val:.4f}\")\n\n# -------------------------- test evaluation ---------------------------------\nbest_state = experiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][f\"{best_lr:.0e}\"][\n    \"model_state\"\n]\nbest_model = CharBiLSTM(len(vocab)).to(device)\nbest_model.load_state_dict(best_state)\nbest_model.eval()\ntest_loader = DataLoader(test_ds, batch_size=256)\ntp, tt = [], []\nwith torch.no_grad():\n    for b in test_loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        logits = best_model(b[\"x\"])\n        tp += list((torch.sigmoid(logits).cpu().numpy()) > 0.5)\n        tt += list(b[\"y\"].cpu().numpy())\ntest_mcc = matthews_corrcoef(tt, tp)\nprint(\"Test MCC with best lr:\", test_mcc)\nexperiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][\"best_lr\"] = best_lr\nexperiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][\"predictions\"] = tp\nexperiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][\"ground_truth\"] = tt\n\n# ----------------------------- save -----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------------------------- plotting --------------------------------------\nbest_rec = experiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"][f\"{best_lr:.0e}\"]\nep = best_rec[\"epochs\"]\nplt.figure()\nplt.plot(ep, best_rec[\"losses\"][\"train\"], label=\"train\")\nplt.plot(ep, best_rec[\"losses\"][\"val\"], label=\"val\")\nplt.title(f\"Loss curve (best lr={best_lr:.0e})\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nplt.figure()\nplt.plot(ep, best_rec[\"metrics\"][\"val_MCC\"])\nplt.title(\"Validation MCC\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"MCC\")\nplt.savefig(os.path.join(working_dir, \"mcc_curve.png\"))\n", "import os, pathlib, random, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# ---------------------------------------------------------------------------#\n# experiment bookkeeping & reproducibility\n# ---------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},  # will store dicts with acc/MCC/RMA\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\nrng_seed = 2024\nrandom.seed(rng_seed)\nnp.random.seed(rng_seed)\ntorch.manual_seed(rng_seed)\n\n# ---------------------------------------------------------------------------#\n# device\n# ---------------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------#\n# dataset helpers\n# ---------------------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_load_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Loading real SPR_BENCH from\", root)\n        return load_spr_bench(root)\n    print(\"Real dataset not found - falling back to synthetic toy data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth_split(n):\n        seqs, labs = [], []\n        for idx in range(n):\n            ln = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        dd[split] = HFDataset.from_dict(synth_split(n))\n    return dd\n\n\nspr = maybe_load_dataset()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------#\n# vocabulary & encoding\n# ---------------------------------------------------------------------------#\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nPAD = 0\nCLS = 1\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # 0 PAD, 1 CLS, rest symbols\nitos = {i: ch for ch, i in stoi.items()}\nmax_len = min(40, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 for CLS\n\n\ndef encode(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq[: max_len - 1]]\n    if len(ids) < max_len:\n        ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\n# ---------------------------------------------------------------------------#\n# torch Dataset\n# ---------------------------------------------------------------------------#\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds = SPRTorch(spr[\"train\"])\nval_ds = SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------------------------------------------------------------------#\n# model: tiny Transformer encoder\n# ---------------------------------------------------------------------------#\nclass TinyTransformer(nn.Module):\n    def __init__(self, vocab_sz, emb=64, nhead=8, nlayers=2, ff=128, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.cls_head = nn.Linear(emb, 1)\n\n    def forward(self, x):\n        z = self.emb(x) + self.pos[:, : x.size(1), :]\n        h = self.transformer(z)\n        cls_h = h[:, 0]  # token 0 is CLS\n        return self.cls_head(cls_h).squeeze(1)\n\n\nmodel = TinyTransformer(vocab_sz=len(stoi) + 2).to(device)\n\n\n# ---------------------------------------------------------------------------#\n# training utilities\n# ---------------------------------------------------------------------------#\ndef rule_macro_accuracy(preds, gts, ids):\n    rule_stats = {}\n    for p, g, i in zip(preds, gts, ids):\n        rule_id = str(i).split(\"-\")[0]  # heuristic extraction\n        correct, total = rule_stats.get(rule_id, (0, 0))\n        rule_stats[rule_id] = (correct + int(p == g), total + 1)\n    return np.mean([c / t for c, t in rule_stats.values()])\n\n\ndef evaluate(dloader):\n    model.eval()\n    crit = nn.BCEWithLogitsLoss()\n    all_logits, all_y, all_ids = [], [], []\n    loss_total = 0\n    with torch.no_grad():\n        for batch in dloader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"x\"])\n            loss = crit(logits, batch[\"y\"])\n            loss_total += loss.item() * batch[\"x\"].size(0)\n            all_logits.append(logits.cpu())\n            all_y.append(batch[\"y\"].cpu())\n            all_ids += ids\n    logits = torch.cat(all_logits)\n    y = torch.cat(all_y)\n    preds = (torch.sigmoid(logits) > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, all_ids)\n    return loss_total / len(dloader.dataset), acc, mcc, rma, preds, y_np, all_ids\n\n\n# ---------------------------------------------------------------------------#\n# train loop\n# ---------------------------------------------------------------------------#\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 6\nfor epoch in range(1, epochs + 1):\n    model.train()\n    t0 = time.time()\n    loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        loss_sum += loss.item() * batch[\"x\"].size(0)\n    train_loss = loss_sum / len(train_loader.dataset)\n    # metrics on train (quick)\n    with torch.no_grad():\n        preds = (torch.sigmoid(logits) > 0.5).int().cpu().numpy()\n        tr_acc = (preds == batch[\"y\"].cpu().int().numpy()).mean()\n        tr_mcc = matthews_corrcoef(batch[\"y\"].cpu().int().numpy(), preds)\n        tr_rma = rule_macro_accuracy(\n            preds, batch[\"y\"].cpu().int().numpy(), batch[\"rid\"]\n        )\n    # validation\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # record\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"val_loss = {val_loss:.4f} | val_acc = {val_acc:.3f} | \"\n        f\"val_MCC = {val_mcc:.3f} | val_RMA = {val_rma:.3f} \"\n        f\"({time.time()-t0:.1f}s)\"\n    )\n\n# ---------------------------------------------------------------------------#\n# final test evaluation\n# ---------------------------------------------------------------------------#\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n=== Test results ===\")\nprint(\n    f\"loss: {test_loss:.4f} | acc: {test_acc:.3f} | \"\n    f\"MCC: {test_mcc:.3f} | RMA: {test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef, accuracy_score\n\n# ------------------- mandatory working directory ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------------- experiment dict ------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ----------------------------- reproducibility ------------------------------\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# ----------------------------- device ---------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------- dataset loading ---------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(f\"{s}.csv\") for s in [\"train\", \"dev\", \"test\"]\n    )\n    return d\n\n\ndef maybe_get_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(f\"Loading real SPR_BENCH from {root}\")\n        return load_spr_bench(root)\n    # ---------- tiny synthetic fallback -------------\n    from datasets import Dataset as HFDataset\n\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            ln = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labels.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    d = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        d[split] = HFDataset.from_dict(synth(n))\n    return d\n\n\nspr = maybe_get_dataset()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# ----------------------- vocabulary & encoding ------------------------------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0:PAD, 1:CLS\nitos = {i: ch for ch, i in stoi.items()}\npad_id, cls_id = 0, 1\nmax_len = min(60, max(len(s) for s in spr[\"train\"][\"sequence\"]) + 1)  # +CLS\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [stoi.get(c, 0) for c in seq][: max_len - 1]\n    if len(ids) < max_len:\n        ids += [pad_id] * (max_len - len(ids))\n    return ids[:max_len]\n\n\n# ---------------------------- dataset wrapper -------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.rule = (\n            hf_ds[\"id\"] if \"id\" in hf_ds.column_names else list(range(len(self.seq)))\n        )\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rule\": torch.tensor(self.rule[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (SPRTorch(spr[s]) for s in [\"train\", \"dev\", \"test\"])\n\n\n# ------------------------------ model ---------------------------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(\n            vocab_size + 2, d_model, padding_idx=pad_id\n        )  # +2 for PAD+CLS\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward=128,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        h = self.embed(x)\n        h = self.transformer(h)\n        cls_tok = h[:, 0, :]\n        return self.fc(cls_tok).squeeze(1)\n\n\n# ----------------------- Rule-Macro Accuracy util ---------------------------\ndef rule_macro_accuracy(preds, labels, rule_ids):\n    preds = np.array(preds)\n    labels = np.array(labels)\n    rule_ids = np.array(rule_ids)\n    accs = []\n    for r in np.unique(rule_ids):\n        mask = rule_ids == r\n        if mask.sum() == 0:\n            continue\n        accs.append((preds[mask] == labels[mask]).mean())\n    return float(np.mean(accs)) if accs else 0.0\n\n\n# ------------------------------ training ------------------------------------\ndef train(model, optimizer, crit, loader):\n    model.train()\n    tot_loss, preds, labs, rules = 0.0, [], [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = crit(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"x\"].size(0)\n        preds.extend((torch.sigmoid(logits).detach().cpu().numpy()) > 0.5)\n        labs.extend(batch[\"y\"].cpu().numpy())\n        rules.extend(batch[\"rule\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        matthews_corrcoef(labs, preds),\n        rule_macro_accuracy(preds, labs, rules),\n    )\n\n\n@torch.no_grad()\ndef evaluate(model, crit, loader):\n    model.eval()\n    tot_loss, preds, labs, rules = 0.0, [], [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"x\"])\n        loss = crit(logits, batch[\"y\"])\n        tot_loss += loss.item() * batch[\"x\"].size(0)\n        preds.extend((torch.sigmoid(logits).cpu().numpy()) > 0.5)\n        labs.extend(batch[\"y\"].cpu().numpy())\n        rules.extend(batch[\"rule\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        matthews_corrcoef(labs, preds),\n        rule_macro_accuracy(preds, labs, rules),\n        preds,\n        labs,\n        rules,\n    )\n\n\n# ---------------------------- dataloaders -----------------------------------\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# ----------------------------- main loop ------------------------------------\nmodel = TransformerClassifier(len(vocab)).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.BCEWithLogitsLoss()\n\nbest_val_mcc, best_state = -1, None\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    tr_loss, tr_mcc, tr_rma = train(model, optimizer, criterion, train_loader)\n    val_loss, val_mcc, val_rma, *_ = evaluate(model, criterion, val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"mcc\": tr_mcc, \"rma\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"mcc\": val_mcc, \"rma\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_MCC={val_mcc:.4f} | val_RMA={val_rma:.4f}\"\n    )\n\n    if val_mcc > best_val_mcc:\n        best_val_mcc = val_mcc\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n\n# ------------------------------ test eval -----------------------------------\nmodel.load_state_dict(best_state)\ntest_loss, test_mcc, test_rma, preds, gts, rules = evaluate(\n    model, criterion, test_loader\n)\nprint(f\"\\nTEST \u2014 loss:{test_loss:.4f}  MCC:{test_mcc:.4f}  RMA:{test_rma:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"test\"][\"loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test\"][\"mcc\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"test\"][\"rma\"] = test_rma\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef, accuracy_score\n\n# ---------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"RMA\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n# ---------------------------------------------------------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    env_root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if (env_root / \"train.csv\").exists():\n        print(\"Loading real SPR_BENCH\")\n        return load_spr_bench(env_root)\n    print(\"Real dataset not found \u2192 synthesising tiny dummy data\")\n    from datasets import Dataset as HFDS\n\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seq, lbl = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            s = \"\".join(random.choice(syms) for _ in range(ln))\n            seq.append(s)\n            lbl.append(int(s.count(\"A\") % 2 == 0))\n        return HFDS.from_dict({\"id\": list(range(n)), \"sequence\": seq, \"label\": lbl})\n\n    return DatasetDict(train=synth(2000), dev=synth(500), test=synth(500))\n\n\ndsets = maybe_dataset()\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------------------------------------------------------------------------\nall_txt = \"\".join(dsets[\"train\"][\"sequence\"])\nvocab = sorted(set(all_txt))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}  # 0 pad\npad_idx = 0\nmax_len = min(60, max(len(s) for s in dsets[\"train\"][\"sequence\"]))\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.seqs = hf[\"sequence\"]\n        self.labels = hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.float32),\n            \"rule\": torch.tensor(len(self.seqs[idx]) % 5),\n        }  # proxy rule-id\n\n\ntrain_ds, val_ds, test_ds = (SPRTorch(dsets[s]) for s in [\"train\", \"dev\", \"test\"])\n\n\n# ---------------------------------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TinyTransformer(nn.Module):\n    def __init__(self, vocab_sz, d_model=64, nhead=4, nlayers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz + 1, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model, max_len)\n        encoder = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=128, dropout=0.1, activation=\"gelu\"\n        )\n        self.tr = nn.TransformerEncoder(encoder, nlayers)\n        self.fc = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        mask = x == pad_idx\n        h = self.emb(x)\n        h = self.pos(h)\n        h = self.tr(h.transpose(0, 1), src_key_padding_mask=mask).transpose(0, 1)\n        h_masked = h.masked_fill(mask.unsqueeze(-1), 0.0)\n        h_mean = h_masked.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(h_mean).squeeze(1)\n\n\n# ---------------------------------------------------------------------------\ndef rule_macro_accuracy(preds, labels, rules):\n    # preds, labels: numpy arrays of 0/1; rules: int rule ids\n    per_rule = {}\n    for p, l, r in zip(preds, labels, rules):\n        per_rule.setdefault(int(r), {\"c\": 0, \"tot\": 0})\n        per_rule[int(r)][\"c\"] += int(p == l)\n        per_rule[int(r)][\"tot\"] += 1\n    accs = [v[\"c\"] / v[\"tot\"] for v in per_rule.values()]\n    return np.mean(accs)\n\n\ndef run(lr, epochs=6, bs=128):\n    model = TinyTransformer(len(vocab)).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n    crit = nn.BCEWithLogitsLoss()\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=bs * 2)\n    best_state = None\n    best_mcc = -1\n    for ep in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tot_loss = 0\n        preds = []\n        gts = []\n        rules = []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            opt.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = crit(logits, batch[\"y\"])\n            loss.backward()\n            opt.step()\n            tot_loss += loss.item() * batch[\"x\"].size(0)\n            preds += list((torch.sigmoid(logits).detach().cpu().numpy()) > 0.5)\n            gts += list(batch[\"y\"].cpu().numpy())\n            rules += list(batch[\"rule\"].cpu().numpy())\n        train_loss = tot_loss / len(train_ds)\n        train_mcc = matthews_corrcoef(gts, preds)\n        train_rma = rule_macro_accuracy(preds, gts, rules)\n\n        # ---- val ----\n        model.eval()\n        tot_loss = 0\n        preds = []\n        gts = []\n        rules = []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"x\"])\n                loss = crit(logits, batch[\"y\"])\n                tot_loss += loss.item() * batch[\"x\"].size(0)\n                preds += list((torch.sigmoid(logits).cpu().numpy()) > 0.5)\n                gts += list(batch[\"y\"].cpu().numpy())\n                rules += list(batch[\"rule\"].cpu().numpy())\n        val_loss = tot_loss / len(val_ds)\n        val_mcc = matthews_corrcoef(gts, preds)\n        val_rma = rule_macro_accuracy(preds, gts, rules)\n\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_mcc)\n        experiment_data[\"SPR_BENCH\"][\"RMA\"][\"train\"].append(train_rma)\n        experiment_data[\"SPR_BENCH\"][\"RMA\"][\"val\"].append(val_rma)\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(ep)\n\n        print(\n            f\"lr {lr:.0e} | epoch {ep}: val_loss={val_loss:.4f}  MCC={val_mcc:.4f}  RMA={val_rma:.4f}\"\n        )\n        if val_mcc > best_mcc:\n            best_mcc = val_mcc\n            best_state = model.state_dict()\n    return best_state, best_mcc\n\n\n# ---------------------------------------------------------------------------\nbest_state = None\nbest_overall = -1\nbest_lr = None\nfor lr in [5e-4, 1e-3]:\n    state, val_mcc = run(lr)\n    if val_mcc > best_overall:\n        best_overall = val_mcc\n        best_state = state\n        best_lr = lr\nprint(f\"Best LR={best_lr:.0e} with dev MCC={best_overall:.4f}\")\n\n# ---------------------------------------------------------------------------\nmodel_final = TinyTransformer(len(vocab)).to(device)\nmodel_final.load_state_dict(best_state)\nmodel_final.eval()\ntest_loader = DataLoader(test_ds, batch_size=256)\npreds = []\ngts = []\nrules = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model_final(batch[\"x\"])\n        preds += list((torch.sigmoid(logits).cpu().numpy()) > 0.5)\n        gts += list(batch[\"y\"].cpu().numpy())\n        rules += list(batch[\"rule\"].cpu().numpy())\ntest_mcc = matthews_corrcoef(gts, preds)\ntest_rma = rule_macro_accuracy(preds, gts, rules)\nprint(f\"Test MCC={test_mcc:.4f}, Rule-Macro Acc={test_rma:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nexperiment_data[\"SPR_BENCH\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"test_RMA\"] = test_rma\nexperiment_data[\"SPR_BENCH\"][\"best_lr\"] = best_lr\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, time, math, pathlib, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef, accuracy_score\n\n# ------------------------------------------------- workspace & recorder -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": [], \"train_RMA\": [], \"val_RMA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"cfg\": {},\n    }\n}\n\n# ------------------------------------------------- reproducibility ----------\nseed = 1234\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# ------------------------------------------------- device -------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------- dataset loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_path = pathlib.Path(\n    os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nif not data_path.exists():\n    raise RuntimeError(f\"SPR_BENCH not found at {data_path}\")\nds = load_spr_bench(data_path)\n\n# ------------------------------------------------- vocabulary ---------------\nall_chars = sorted({c for s in ds[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i + 1 for i, c in enumerate(all_chars)}  # 0 is PAD\nitos = {i: c for c, i in stoi.items()}\npad = 0\nmax_len = min(60, max(len(s) for s in ds[\"train\"][\"sequence\"]))  # cap to 60\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [pad] * (max_len - len(ids))\n    return ids\n\n\n# ------------------------------------------------- torch dataset ------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.X = [encode(s) for s in hf_ds[\"sequence\"]]\n        self.y = hf_ds[\"label\"]\n        self.rule_ids = hf_ds[\"rule\"] if \"rule\" in hf_ds.column_names else None\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"x\": torch.tensor(self.X[idx], dtype=torch.long),\n            \"y\": torch.tensor(self.y[idx], dtype=torch.float32),\n        }\n        if self.rule_ids is not None:\n            sample[\"rule\"] = self.rule_ids[idx]\n        return sample\n\n\ntrain_ds, dev_ds, test_ds = (SPRTorch(ds[s]) for s in [\"train\", \"dev\", \"test\"])\n\n\ndef collate(batch):\n    xs = torch.stack([b[\"x\"] for b in batch])\n    ys = torch.stack([b[\"y\"] for b in batch])\n    out = {\"x\": xs, \"y\": ys}\n    if \"rule\" in batch[0]:\n        out[\"rule\"] = [b[\"rule\"] for b in batch]\n    return out\n\n\n# ------------------------------------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=max_len):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):  # x: (B,L,D)\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab, d_model=64, nhead=4, nlayers=2, dim_feed=128, dropout=0.1\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab + 1, d_model, padding_idx=pad)\n        self.pos = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feed, dropout, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.head = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        mask = x == pad\n        h = self.emb(x)\n        h = self.pos(h)\n        h = self.enc(h, src_key_padding_mask=mask)\n        h = h.mean(1)\n        return self.head(h).squeeze(1)\n\n\n# ------------------------ Rule-Macro Accuracy ------------------------------\ndef rule_macro_accuracy(preds, gts, rules=None):\n    if rules is None:\n        return accuracy_score(gts, preds)  # fallback\n    groups = {}\n    for p, g, r in zip(preds, gts, rules):\n        groups.setdefault(r, {\"p\": [], \"g\": []})\n        groups[r][\"p\"].append(p)\n        groups[r][\"g\"].append(g)\n    accs = [accuracy_score(d[\"g\"], d[\"p\"]) for d in groups.values()]\n    return float(np.mean(accs))\n\n\n# -------------------------- train one cfg -----------------------------------\ndef train_cfg(hidden, epochs=8, lr=5e-4, batch=128):\n    model = SPRTransformer(len(all_chars), d_model=hidden).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        opt, epochs * len(train_ds) // batch\n    )\n    crit = nn.BCEWithLogitsLoss()\n    tr_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\n    va_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n    best_val = -1\n    best_state = None\n    for ep in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tloss = 0\n        tp, tt = [], []\n        for b in tr_loader:\n            b = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in b.items()\n            }\n            opt.zero_grad()\n            out = model(b[\"x\"])\n            loss = crit(out, b[\"y\"])\n            loss.backward()\n            opt.step()\n            scheduler.step()\n            tloss += loss.item() * b[\"x\"].size(0)\n            tp += list((torch.sigmoid(out).detach().cpu().numpy() > 0.5).astype(int))\n            tt += list(b[\"y\"].cpu().numpy())\n        tr_loss = tloss / len(train_ds)\n        tr_mcc = matthews_corrcoef(tt, tp)\n        tr_rma = rule_macro_accuracy(tp, tt, b.get(\"rule\", None))\n\n        # ---- val ----\n        model.eval()\n        vloss = 0\n        vp, vt, vr = [], [], []\n        with torch.no_grad():\n            for b in va_loader:\n                b = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in b.items()\n                }\n                out = model(b[\"x\"])\n                loss = crit(out, b[\"y\"])\n                vloss += loss.item() * b[\"x\"].size(0)\n                vp += list((torch.sigmoid(out).cpu().numpy() > 0.5).astype(int))\n                vt += list(b[\"y\"].cpu().numpy())\n                if \"rule\" in b:\n                    vr += b[\"rule\"]\n        va_loss = vloss / len(dev_ds)\n        va_mcc = matthews_corrcoef(vt, vp)\n        va_rma = rule_macro_accuracy(vp, vt, vr if vr else None)\n\n        # record\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(va_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(tr_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(va_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_RMA\"].append(tr_rma)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_RMA\"].append(va_rma)\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(ep)\n\n        print(\n            f\"Epoch {ep}: validation_loss = {va_loss:.4f}  val_MCC={va_mcc:.3f}  val_RMA={va_rma:.3f}\"\n        )\n\n        if va_mcc > best_val:\n            best_val = va_mcc\n            best_state = model.state_dict()\n\n    return best_state, best_val\n\n\n# --------------------------- run sweep --------------------------------------\ncfg_hidden = [64, 128]\nbest_cfg = None\nbest_val = -1\nbest_state = None\nfor h in cfg_hidden:\n    print(f\"\\n=== training d_model={h} ===\")\n    state, val = train_cfg(h)\n    if val > best_val:\n        best_val = val\n        best_cfg = h\n        best_state = state\nprint(f\"\\nBest hidden size={best_cfg} with dev MCC={best_val:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"cfg\"][\"best_hidden\"] = best_cfg\n\n# --------------------------- test evaluation --------------------------------\ntest_loader = DataLoader(test_ds, batch_size=256, collate_fn=collate)\nmodel = SPRTransformer(len(all_chars), d_model=best_cfg).to(device)\nmodel.load_state_dict(best_state)\nmodel.eval()\ntp, tt, vr = [], [], []\nwith torch.no_grad():\n    for b in test_loader:\n        b = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in b.items()\n        }\n        logits = model(b[\"x\"])\n        tp += list((torch.sigmoid(logits).cpu().numpy() > 0.5).astype(int))\n        tt += list(b[\"y\"].cpu().numpy())\n        if \"rule\" in b:\n            vr += b[\"rule\"]\ntest_mcc = matthews_corrcoef(tt, tp)\ntest_rma = rule_macro_accuracy(tp, tt, vr if vr else None)\nprint(f\"Test MCC={test_mcc:.4f}  Test RMA={test_rma:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = tp\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = tt\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_RMA\"] = test_rma\n\n# ---------------------------- save ------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, math, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef, accuracy_score\n\n# ------------------------------------------------------------------- paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------- device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------------------------------------------------- experiment recorder\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\n            \"train_MCC\": [],\n            \"val_MCC\": [],\n            \"train_RMA\": [],\n            \"val_RMA\": [],\n            \"test_MCC\": None,\n            \"test_RMA\": None,\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"cfg\": {},\n    }\n}\n\n# ---------------------------------------------------------------- reproducibility\nseed = 1234\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n\n# ---------------------------------------------------------------- load dataset\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndata_path = pathlib.Path(\n    os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nif not data_path.exists():\n    raise RuntimeError(f\"SPR_BENCH not found at {data_path}\")\nds = load_spr_bench(data_path)\n\n# ---------------------------------------------------------------- vocabulary / encoding\nall_chars = sorted({c for s in ds[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i + 1 for i, c in enumerate(all_chars)}  # 0 = PAD\npad_id = 0\nmax_len = min(60, max(len(s) for s in ds[\"train\"][\"sequence\"]))\n\n\ndef encode(seq: str):\n    ids = [stoi.get(c, pad_id) for c in seq[:max_len]]\n    ids += [pad_id] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------- torch dataset\nclass SPRTorch(Dataset):\n    def __init__(self, hf_split):\n        self.X = [encode(s) for s in hf_split[\"sequence\"]]\n        self.y = hf_split[\"label\"]\n        self.rules = hf_split[\"rule\"] if \"rule\" in hf_split.column_names else None\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"x\": torch.tensor(self.X[idx], dtype=torch.long),\n            \"y\": torch.tensor(self.y[idx], dtype=torch.float32),\n        }\n        if self.rules is not None:\n            sample[\"rule\"] = self.rules[idx]\n        return sample\n\n\ntrain_ds, dev_ds, test_ds = (SPRTorch(ds[s]) for s in [\"train\", \"dev\", \"test\"])\n\n\ndef collate(batch):\n    xs = torch.stack([b[\"x\"] for b in batch])\n    ys = torch.stack([b[\"y\"] for b in batch])\n    out = {\"x\": xs, \"y\": ys}\n    if \"rule\" in batch[0]:\n        out[\"rule\"] = [b[\"rule\"] for b in batch]\n    return out\n\n\n# ---------------------------------------------------------------- model\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=max_len):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):  # x: (B,L,D)\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(self, vocab, d_model=64, nhead=4, nlayers=2, d_ff=128, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab + 1, d_model, padding_idx=pad_id)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, d_ff, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.head = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        mask = x == pad_id\n        h = self.emb(x)\n        h = self.pos(h)\n        h = self.encoder(h, src_key_padding_mask=mask)\n        h = h.mean(1)\n        return self.head(h).squeeze(1)\n\n\n# ---------------------------------------------------------------- metrics\ndef rule_macro_accuracy(preds, gts, rules=None):\n    if rules is None:\n        return accuracy_score(gts, preds)\n    groups = {}\n    for p, g, r in zip(preds, gts, rules):\n        groups.setdefault(r, {\"p\": [], \"g\": []})\n        groups[r][\"p\"].append(p)\n        groups[r][\"g\"].append(g)\n    accs = [accuracy_score(d[\"g\"], d[\"p\"]) for d in groups.values()]\n    return float(np.mean(accs))\n\n\n# ---------------------------------------------------------------- training routine (bug-fixed)\ndef train_cfg(hidden_size, epochs=8, lr=5e-4, batch_size=128):\n    model = SPRTransformer(len(all_chars), d_model=hidden_size).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=epochs * (len(train_ds) // batch_size)\n    )\n    criterion = nn.BCEWithLogitsLoss()\n\n    tr_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n    )\n    va_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n    best_val_mcc, best_state = -1, None\n\n    for epoch in range(1, epochs + 1):\n        # ----------------- train -----------------\n        model.train()\n        tot_loss, preds, gts, tr_rules = 0, [], [], []\n        for batch in tr_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            tot_loss += loss.item() * batch[\"x\"].size(0)\n            batch_pred = (torch.sigmoid(logits).detach().cpu().numpy() > 0.5).astype(\n                int\n            )\n            preds.extend(batch_pred)\n            gts.extend(batch[\"y\"].cpu().numpy())\n            if \"rule\" in batch:\n                tr_rules.extend(batch[\"rule\"])\n\n        train_loss = tot_loss / len(train_ds)\n        train_mcc = matthews_corrcoef(gts, preds)\n        train_rma = rule_macro_accuracy(preds, gts, tr_rules if tr_rules else None)\n\n        # ----------------- validation -----------------\n        model.eval()\n        val_loss, v_preds, v_gts, v_rules = 0, [], [], []\n        with torch.no_grad():\n            for batch in va_loader:\n                batch = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"x\"])\n                loss = criterion(logits, batch[\"y\"])\n                val_loss += loss.item() * batch[\"x\"].size(0)\n\n                batch_pred = (torch.sigmoid(logits).cpu().numpy() > 0.5).astype(int)\n                v_preds.extend(batch_pred)\n                v_gts.extend(batch[\"y\"].cpu().numpy())\n                if \"rule\" in batch:\n                    v_rules.extend(batch[\"rule\"])\n\n        val_loss /= len(dev_ds)\n        val_mcc = matthews_corrcoef(v_gts, v_preds)\n        val_rma = rule_macro_accuracy(v_preds, v_gts, v_rules if v_rules else None)\n\n        # ----------- bookkeeping -----------\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(train_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(val_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_RMA\"].append(train_rma)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_RMA\"].append(val_rma)\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n        print(\n            f\"Epoch {epoch}: val_loss={val_loss:.4f}  val_MCC={val_mcc:.3f}  val_RMA={val_rma:.3f}\"\n        )\n\n        if val_mcc > best_val_mcc:\n            best_val_mcc, best_state = val_mcc, model.state_dict()\n\n    return best_state, best_val_mcc\n\n\n# ---------------------------------------------------------------- hyper-parameter sweep\nhidden_choices = [64, 128]\nbest_hidden, best_state, best_val = None, None, -1\nfor h in hidden_choices:\n    print(f\"\\n=== Training model with d_model={h} ===\")\n    state, val = train_cfg(h)\n    if val > best_val:\n        best_val, best_hidden, best_state = val, h, state\nprint(f\"\\nBest model uses d_model={best_hidden}  (dev MCC={best_val:.4f})\")\nexperiment_data[\"SPR_BENCH\"][\"cfg\"][\"best_hidden\"] = best_hidden\n\n# ---------------------------------------------------------------- test evaluation\ntest_loader = DataLoader(test_ds, batch_size=256, collate_fn=collate)\nmodel = SPRTransformer(len(all_chars), d_model=best_hidden).to(device)\nmodel.load_state_dict(best_state)\nmodel.eval()\n\ntest_preds, test_gts, test_rules = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"x\"])\n        batch_pred = (torch.sigmoid(logits).cpu().numpy() > 0.5).astype(int)\n        test_preds.extend(batch_pred)\n        test_gts.extend(batch[\"y\"].cpu().numpy())\n        if \"rule\" in batch:\n            test_rules.extend(batch[\"rule\"])\n\ntest_mcc = matthews_corrcoef(test_gts, test_preds)\ntest_rma = rule_macro_accuracy(test_preds, test_gts, test_rules if test_rules else None)\nprint(f\"Test MCC={test_mcc:.4f}  Test RMA={test_rma:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_RMA\"] = test_rma\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\n# ---------------------------------------------------------------- save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, time, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# ---------------------------------------------------------------------------#\n# basic setup & bookkeeping\n# ---------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\nrng_seed = 2024\nrandom.seed(rng_seed)\nnp.random.seed(rng_seed)\ntorch.manual_seed(rng_seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------#\n# dataset helpers\n# ---------------------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        d[sp] = _l(f\"{sp}.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Loading real SPR_BENCH \u2026\")\n        return load_spr_bench(root)\n    print(\"Fallback: synthetic toy data\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for _ in range(n):\n            ln = random.randint(5, 12)\n            s = \"\".join(random.choice(syms) for _ in range(ln))\n            labs.append(int(s.count(\"A\") % 2 == 0))\n            seqs.append(s)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    d = DatasetDict()\n    d[\"train\"] = HFDataset.from_dict(synth(2000))\n    d[\"dev\"] = HFDataset.from_dict(synth(500))\n    d[\"test\"] = HFDataset.from_dict(synth(500))\n    return d\n\n\nspr = maybe_dataset()\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------#\n# vocabulary & encoding\n# ---------------------------------------------------------------------------#\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nPAD, CLS = 0, 1\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_sz = len(stoi) + 2\nmax_len = min(40, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +CLS\n\n\ndef encode_seq(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq[: max_len - 1]]\n    if len(ids) < max_len:\n        ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef count_vec(seq: str):\n    vec = np.zeros(len(vocab), dtype=np.float32)\n    for c in seq:\n        if c in stoi:\n            vec[stoi[c] - 2] += 1\n    if len(seq) > 0:\n        vec /= len(seq)\n    return vec\n\n\n# ---------------------------------------------------------------------------#\n# torch Dataset\n# ---------------------------------------------------------------------------#\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.sq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.sq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_seq(self.sq[idx]), dtype=torch.long),\n            \"cnt\": torch.tensor(count_vec(self.sq[idx]), dtype=torch.float32),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------------------------------------------------------------------#\n# hybrid model\n# ---------------------------------------------------------------------------#\nclass HybridTransformer(nn.Module):\n    def __init__(self, vocab_sz, emb=128, heads=8, layers=3, ff=256, dropout=0.1):\n        super().__init__()\n        self.emb_tok = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=heads,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.trf = nn.TransformerEncoder(enc_layer, num_layers=layers)\n        self.cnt_proj = nn.Sequential(\n            nn.Linear(len(vocab), emb), nn.ReLU(), nn.Dropout(dropout)\n        )\n        self.out = nn.Linear(emb * 2, 1)\n\n    def forward(self, x, cnt):\n        z = self.emb_tok(x) + self.pos[:, : x.size(1), :]\n        h = self.trf(z)[:, 0]  # CLS token\n        c = self.cnt_proj(cnt)\n        return self.out(torch.cat([h, c], dim=-1)).squeeze(1)\n\n\nmodel = HybridTransformer(vocab_sz=vocab_sz).to(device)\n\n\n# ---------------------------------------------------------------------------#\n# Rule-Macro Accuracy\n# ---------------------------------------------------------------------------#\ndef rule_macro_accuracy(preds, gts, ids):\n    bucket = {}\n    for p, g, i in zip(preds, gts, ids):\n        r = i.split(\"-\")[0]\n        corr, tot = bucket.get(r, (0, 0))\n        bucket[r] = (corr + int(p == g), tot + 1)\n    return np.mean([c / t for c, t in bucket.values()])\n\n\n# ---------------------------------------------------------------------------#\n# train / eval loops\n# ---------------------------------------------------------------------------#\ndef evaluate(dl):\n    model.eval()\n    all_logits, all_y, all_ids = [], [], []\n    loss_tot = 0\n    with torch.no_grad():\n        for batch in dl:\n            ids = batch[\"rid\"]\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"cnt\"])\n            loss = criterion(logit, batch[\"y\"])\n            loss_tot += loss.item() * batch[\"x\"].size(0)\n            all_logits.append(logit.cpu())\n            all_y.append(batch[\"y\"].cpu())\n            all_ids += ids\n    logits = torch.cat(all_logits)\n    y = torch.cat(all_y)\n    preds = (torch.sigmoid(logits) > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, all_ids)\n    return loss_tot / len(dl.dataset), acc, mcc, rma, preds, y_np\n\n\nbatch_size = 128\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=256)\ntest_dl = DataLoader(test_ds, batch_size=256)\n\n# label smoothing BCE\nsmooth_eps = 0.1\n\n\ndef smooth_targets(y):\n    return y * (1 - smooth_eps) + 0.5 * smooth_eps\n\n\ncriterion = lambda logits, targets: nn.BCEWithLogitsLoss()(\n    logits, smooth_targets(targets)\n)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    t0 = time.time()\n    loss_sum = 0\n    for batch in train_dl:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"], batch[\"cnt\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        loss_sum += loss.item() * batch[\"x\"].size(0)\n    scheduler.step()\n    train_loss = loss_sum / len(train_dl.dataset)\n    # quick last-batch metrics\n    with torch.no_grad():\n        pb = (torch.sigmoid(logits) > 0.5).int().cpu().numpy()\n        ty = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (pb == ty).mean()\n        tr_mcc = matthews_corrcoef(ty, pb)\n        tr_rma = rule_macro_accuracy(pb, ty, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: val_loss={val_loss:.4f}  val_acc={val_acc:.3f}  val_MCC={val_mcc:.3f}  val_RMA={val_rma:.3f}  ({time.time()-t0:.1f}s)\"\n    )\n\n# ---------------------------------------------------------------------------#\n# final evaluation\n# ---------------------------------------------------------------------------#\ntest_loss, test_acc, test_mcc, test_rma, preds, gts = evaluate(test_dl)\nprint(\"\\n=== Test ===\")\nprint(\n    f\"loss {test_loss:.4f} | acc {test_acc:.3f} | MCC {test_mcc:.3f} | RMA {test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, time, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\nfrom sklearn.metrics import matthews_corrcoef\n\n# working dir & bookkeeping\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# reproducibility\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------ data load\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict({s: _l(f\"{s}.csv\") for s in [\"train\", \"dev\", \"test\"]})\n\n\ndef maybe_data() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        return load_spr_bench(root)\n    print(\"Real data not found, using synthetic toy.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seq, l = [], []\n        for i in range(n):\n            s = \"\".join(random.choice(syms) for _ in range(random.randint(5, 12)))\n            seq.append(s)\n            l.append(int(s.count(\"A\") % 2 == 0))\n        return {\"id\": list(range(n)), \"sequence\": seq, \"label\": l}\n\n    return DatasetDict(\n        {\n            k: HFDataset.from_dict(synth(n))\n            for k, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]\n        }\n    )\n\n\ndset = maybe_data()\nprint(\"sizes:\", {k: len(v) for k, v in dset.items()})\n\n# ------------------------------------------------------------------ vocab / enc\nPAD, CLS, MASK = 0, 1, 2\nsym_set = sorted(set(\"\".join(dset[\"train\"][\"sequence\"])))\nstoi = {ch: i + 3 for i, ch in enumerate(sym_set)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 3\nmax_len = min(40, max(len(s) for s in dset[\"train\"][\"sequence\"])) + 1\n\n\ndef encode(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq[: max_len - 1]]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\n# ------------------------------------------------------------------ torch Datasets\nclass SPRCls(Dataset):\n    def __init__(self, hf):\n        self.seq, self.lab, self.ids = hf[\"sequence\"], hf[\"label\"], hf[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, i):\n        return {\n            \"x\": torch.tensor(encode(self.seq[i]), dtype=torch.long),\n            \"y\": torch.tensor(float(self.lab[i])),\n            \"rid\": str(self.ids[i]),\n        }\n\n\nclass SPRMLM(Dataset):\n    def __init__(self, hf, mlm_prob=0.15):\n        self.seq = hf[\"sequence\"]\n        self.prob = mlm_prob\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, i):\n        ids = encode(self.seq[i])\n        inp = ids.copy()\n        labels = [-100] * len(ids)\n        for p in range(1, len(ids)):  # skip CLS\n            if ids[p] == PAD or random.random() > self.prob:\n                continue\n            labels[p] = ids[p]\n            inp[p] = MASK\n        return {\n            \"x\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(labels, dtype=torch.long),\n        }\n\n\ntrain_cls, dev_cls, test_cls = (\n    SPRCls(dset[\"train\"]),\n    SPRCls(dset[\"dev\"]),\n    SPRCls(dset[\"test\"]),\n)\nmlm_data = SPRMLM(dset[\"train\"].select(range(len(dset[\"train\"]))).concat(dset[\"dev\"]))\n\n\n# ------------------------------------------------------------------ model\nclass Encoder(nn.Module):\n    def __init__(self, emb=96, layers=3, heads=8, ff=192):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb, nhead=heads, dim_feedforward=ff, dropout=0.1, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=layers)\n        self.cls_head = nn.Linear(emb, 1)\n        self.mlm_head = nn.Linear(emb, vocab_size)\n\n    def forward(self, x, task=\"cls\"):\n        h = self.embedding(x) + self.pos[:, : x.size(1)]\n        h = self.enc(h)\n        if task == \"cls\":\n            return self.cls_head(h[:, 0]).squeeze(1)\n        else:\n            return self.mlm_head(h)\n\n\nmodel = Encoder().to(device)\n\n\n# ------------------------------------------------------------------ helpers\ndef rule_macro_accuracy(preds, gts, ids):\n    bucket = {}\n    for p, g, i in zip(preds, gts, ids):\n        r = str(i).split(\"-\")[0]\n        c, t = bucket.get(r, (0, 0))\n        bucket[r] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in bucket.values()])\n\n\ndef eval_cls(loader, thr=0.5):\n    model.eval()\n    crit = nn.BCEWithLogitsLoss()\n    tot_loss = 0\n    logits_all = []\n    y_all = []\n    ids = []\n    with torch.no_grad():\n        for b in loader:\n            ids.extend(b[\"rid\"])\n            b = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in b.items()\n            }\n            logit = model(b[\"x\"], \"cls\")\n            loss = crit(logit, b[\"y\"])\n            tot_loss += loss.item() * b[\"x\"].size(0)\n            logits_all.append(logit.cpu())\n            y_all.append(b[\"y\"].cpu())\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (torch.sigmoid(logits) > thr).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, ids)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, logits.numpy(), y_np, ids\n\n\n# ------------------------------------------------------------------ pre-train MLM\nmlm_loader = DataLoader(mlm_data, batch_size=256, shuffle=True)\nmlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\nopt_mlm = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nmlm_epochs = 4\nprint(\"=== MLM pre-training ===\")\nfor ep in range(1, mlm_epochs + 1):\n    model.train()\n    tot = 0\n    n = 0\n    for b in mlm_loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        opt_mlm.zero_grad()\n        out = model(b[\"x\"], \"mlm\").permute(0, 2, 1)  # B, V, T\n        loss = mlm_loss_fn(out, b[\"labels\"])\n        loss.backward()\n        opt_mlm.step()\n        tot += loss.item() * b[\"x\"].size(0)\n        n += b[\"x\"].size(0)\n    print(f\"MLM epoch {ep}: loss {tot/n:.4f}\")\n\n# ------------------------------------------------------------------ fine-tune CLS\nbatch_size = 128\ntrain_loader = DataLoader(train_cls, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_cls, batch_size=256)\ntest_loader = DataLoader(test_cls, batch_size=256)\n\ncrit_cls = nn.BCEWithLogitsLoss()\nbase_lr = 5e-4\nopt_cls = torch.optim.Adam(model.parameters(), lr=base_lr, weight_decay=1e-5)\n\n\ndef cyc_lr(it, steps, base=base_lr):\n    return base * 0.5 * (1 + math.cos(math.pi * it / steps))\n\n\ncls_epochs = 6\nbest_val = 1e9\nbest_state = None\ntotal_steps = cls_epochs * len(train_loader)\nstep = 0\nfor ep in range(1, cls_epochs + 1):\n    model.train()\n    tr_loss = 0\n    for b in train_loader:\n        step += 1\n        for g in opt_cls.param_groups:\n            g[\"lr\"] = cyc_lr(step, total_steps)\n        b = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in b.items()\n        }\n        opt_cls.zero_grad()\n        logits = model(b[\"x\"], \"cls\")\n        # label smoothing  - y*0.9+0.05\n        y_smooth = b[\"y\"] * 0.9 + 0.05\n        loss = crit_cls(logits, y_smooth)\n        loss.backward()\n        opt_cls.step()\n        tr_loss += loss.item() * b[\"x\"].size(0)\n    tr_loss /= len(train_loader.dataset)\n    val_loss, val_acc, val_mcc, val_rma, *_ = eval_cls(dev_loader)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(ep)\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | acc {val_acc:.3f} | MCC {val_mcc:.3f} | RMA {val_rma:.3f}\"\n    )\n    if val_loss < best_val:\n        best_val = val_loss\n        best_state = model.state_dict()\n\n# restore best\nmodel.load_state_dict(best_state)\n\n# ------------------------------------------------------------------ threshold tuning on dev\n_, _, _, _, logits_dev, y_dev, _ = eval_cls(dev_loader, thr=0.5)\nprobs_dev = 1 / (1 + np.exp(-logits_dev))\nbest_thr, best_acc = 0.5, 0\nfor thr in np.linspace(0.2, 0.8, 25):\n    acc = ((probs_dev > thr) == y_dev).mean()\n    if acc > best_acc:\n        best_acc, best_thr = acc, thr\nprint(f\"Chosen threshold from dev: {best_thr:.3f}\")\n\n# ------------------------------------------------------------------ final test\ntest_loss, test_acc, test_mcc, test_rma, preds_logits, gts, ids = eval_cls(\n    test_loader, thr=best_thr\n)\nprint(\"\\n=== Test results ===\")\nprint(\n    f\"loss {test_loss:.4f} | acc {test_acc:.3f} | MCC {test_mcc:.3f} | RMA {test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"].update(\n    {\n        \"predictions\": (np.array(preds_logits) > best_thr).astype(int).tolist(),\n        \"ground_truth\": gts.tolist(),\n        \"test_metrics\": {\n            \"loss\": test_loss,\n            \"acc\": test_acc,\n            \"MCC\": test_mcc,\n            \"RMA\": test_rma,\n        },\n    }\n)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples\n[00:00, 148884.65 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 500 examples [00:00,\n109637.81 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 185950.70\nexamples/s]', '\\n', 'Splits:', ' ', \"dict_keys(['train', 'dev', 'test'])\", '\\n',\n'lr 3e-04 | epoch 1: train_loss=0.6937, val_loss=0.6924, val_MCC=0.0629', '\\n',\n'lr 3e-04 | epoch 2: train_loss=0.6918, val_loss=0.6907, val_MCC=0.2237', '\\n',\n'lr 3e-04 | epoch 3: train_loss=0.6903, val_loss=0.6890, val_MCC=0.2517', '\\n',\n'lr 3e-04 | epoch 4: train_loss=0.6880, val_loss=0.6862, val_MCC=0.2314', '\\n',\n'lr 3e-04 | epoch 5: train_loss=0.6849, val_loss=0.6821, val_MCC=0.2687', '\\n',\n'lr 5e-04 | epoch 1: train_loss=0.6911, val_loss=0.6887, val_MCC=0.2733', '\\n',\n'lr 5e-04 | epoch 2: train_loss=0.6873, val_loss=0.6843, val_MCC=0.2804', '\\n',\n'lr 5e-04 | epoch 3: train_loss=0.6824, val_loss=0.6779, val_MCC=0.2883', '\\n',\n'lr 5e-04 | epoch 4: train_loss=0.6737, val_loss=0.6652, val_MCC=0.2766', '\\n',\n'lr 5e-04 | epoch 5: train_loss=0.6585, val_loss=0.6542, val_MCC=0.2903', '\\n',\n'lr 1e-03 | epoch 1: train_loss=0.6913, val_loss=0.6869, val_MCC=0.1170', '\\n',\n'lr 1e-03 | epoch 2: train_loss=0.6826, val_loss=0.6742, val_MCC=0.2545', '\\n',\n'lr 1e-03 | epoch 3: train_loss=0.6641, val_loss=0.6562, val_MCC=0.2839', '\\n',\n'lr 1e-03 | epoch 4: train_loss=0.6576, val_loss=0.6568, val_MCC=0.2840', '\\n',\n'lr 1e-03 | epoch 5: train_loss=0.6522, val_loss=0.6551, val_MCC=0.3133', '\\n',\n'lr 2e-03 | epoch 1: train_loss=0.6896, val_loss=0.6843, val_MCC=0.2157', '\\n',\n'lr 2e-03 | epoch 2: train_loss=0.6689, val_loss=0.6637, val_MCC=0.2360', '\\n',\n'lr 2e-03 | epoch 3: train_loss=0.6541, val_loss=0.6788, val_MCC=0.2373', '\\n',\n'lr 2e-03 | epoch 4: train_loss=0.6530, val_loss=0.6552, val_MCC=0.3239', '\\n',\n'lr 2e-03 | epoch 5: train_loss=0.6490, val_loss=0.6467, val_MCC=0.3078', '\\n',\n'Best LR: 2e-03 with peak val_MCC=0.3239', '\\n', 'Test MCC with best lr:', ' ',\n'0.2854223755198158', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples\n[00:00, 83280.63 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 500 examples [00:00,\n87363.13 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 150625.01\nexamples/s]', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Epoch 1: val_loss = 0.6728 | val_acc = 0.582 | val_MCC = 0.212 |\nval_RMA = 0.582 (0.6s)', '\\n', 'Epoch 2: val_loss = 0.6401 | val_acc = 0.672 |\nval_MCC = 0.344 | val_RMA = 0.672 (0.2s)', '\\n', 'Epoch 3: val_loss = 0.6364 |\nval_acc = 0.672 | val_MCC = 0.344 | val_RMA = 0.672 (0.2s)', '\\n', 'Epoch 4:\nval_loss = 0.6292 | val_acc = 0.678 | val_MCC = 0.357 | val_RMA = 0.678 (0.2s)',\n'\\n', 'Epoch 5: val_loss = 0.6312 | val_acc = 0.678 | val_MCC = 0.356 | val_RMA\n= 0.678 (0.2s)', '\\n', 'Epoch 6: val_loss = 0.6286 | val_acc = 0.682 | val_MCC =\n0.364 | val_RMA = 0.682 (0.2s)', '\\n', '\\n=== Test results ===', '\\n', 'loss:\n0.6259 | acc: 0.694 | MCC: 0.387 | RMA: 0.694', '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 168676.26\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 101857.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 203606.99\nexamples/s]', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n221, in <module>\\n    tr_loss, tr_mcc, tr_rma = train(model, optimizer,\ncriterion, train_loader)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 162,\nin train\\n    for batch in loader:\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 701, in __next__\\n    data =\nself._next_data()\\n           ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 757, in _next_data\\n    data =\nself._dataset_fetcher.fetch(index)  # may raise StopIteration\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n~~~~~~~~~~~~^^^^^\\n  File \"runfile.py\", line 112, in __getitem__\\n    \"rule\":\ntorch.tensor(self.rule[idx]),\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: new(): invalid data type \\'str\\'\\n',\n'Execution time: 2 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH', '\\n', '\\rGenerating train\nsplit: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split: 2000\nexamples [00:00, 150898.67 examples/s]', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 500 examples\n[00:00, 105890.03 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 1000 examples [00:00,\n173068.04 examples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'lr 5e-04 | epoch 1: val_loss=0.6768  MCC=0.1528  RMA=0.5680', '\\n', 'lr 5e-04 |\nepoch 2: val_loss=0.6530  MCC=0.3060  RMA=0.6520', '\\n', 'lr 5e-04 | epoch 3:\nval_loss=0.6579  MCC=0.3286  RMA=0.6640', '\\n', 'lr 5e-04 | epoch 4:\nval_loss=0.6559  MCC=0.2682  RMA=0.6320', '\\n', 'lr 5e-04 | epoch 5:\nval_loss=0.6561  MCC=0.2652  RMA=0.6300', '\\n', 'lr 5e-04 | epoch 6:\nval_loss=0.6629  MCC=0.2549  RMA=0.6240', '\\n', 'lr 1e-03 | epoch 1:\nval_loss=0.6600  MCC=0.2404  RMA=0.6160', '\\n', 'lr 1e-03 | epoch 2:\nval_loss=0.6565  MCC=0.3320  RMA=0.6660', '\\n', 'lr 1e-03 | epoch 3:\nval_loss=0.6533  MCC=0.3327  RMA=0.6660', '\\n', 'lr 1e-03 | epoch 4:\nval_loss=0.6573  MCC=0.3225  RMA=0.6600', '\\n', 'lr 1e-03 | epoch 5:\nval_loss=0.6551  MCC=0.3040  RMA=0.6520', '\\n', 'lr 1e-03 | epoch 6:\nval_loss=0.6533  MCC=0.3067  RMA=0.6520', '\\n', 'Best LR=1e-03 with dev\nMCC=0.3327', '\\n', 'Test MCC=0.3332, Rule-Macro Acc=0.6670', '\\n', 'Execution\ntime: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 139582.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 117336.32\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 173218.14\nexamples/s]', '\\n', '\\n=== training d_model=64 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6788\nval_MCC=0.201  val_RMA=0.596', '\\n', 'Epoch 2: validation_loss = 0.6629\nval_MCC=0.274  val_RMA=0.634', '\\n', 'Epoch 3: validation_loss = 0.6520\nval_MCC=0.325  val_RMA=0.662', '\\n', 'Epoch 4: validation_loss = 0.6481\nval_MCC=0.296  val_RMA=0.646', '\\n', 'Epoch 5: validation_loss = 0.6465\nval_MCC=0.322  val_RMA=0.660', '\\n', 'Epoch 6: validation_loss = 0.6460\nval_MCC=0.349  val_RMA=0.674', '\\n', 'Epoch 7: validation_loss = 0.6458\nval_MCC=0.361  val_RMA=0.680', '\\n', 'Epoch 8: validation_loss = 0.6455\nval_MCC=0.357  val_RMA=0.678', '\\n', '\\n=== training d_model=128 ===', '\\n',\n'Epoch 1: validation_loss = 0.6596  val_MCC=0.244  val_RMA=0.618', '\\n', 'Epoch\n2: validation_loss = 0.6597  val_MCC=0.267  val_RMA=0.632', '\\n', 'Epoch 3:\nvalidation_loss = 0.6684  val_MCC=0.232  val_RMA=0.610', '\\n', 'Epoch 4:\nvalidation_loss = 0.6507  val_MCC=0.298  val_RMA=0.648', '\\n', 'Epoch 5:\nvalidation_loss = 0.6545  val_MCC=0.317  val_RMA=0.658', '\\n', 'Epoch 6:\nvalidation_loss = 0.6537  val_MCC=0.329  val_RMA=0.664', '\\n', 'Epoch 7:\nvalidation_loss = 0.6524  val_MCC=0.329  val_RMA=0.664', '\\n', 'Epoch 8:\nvalidation_loss = 0.6524  val_MCC=0.329  val_RMA=0.664', '\\n', '\\nBest hidden\nsize=64 with dev MCC=0.3607', '\\n', 'Test MCC=0.3771  Test RMA=0.6890', '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training model with d_model=64 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6788\nval_MCC=0.197  val_RMA=0.594', '\\n', 'Epoch 2: val_loss=0.6630  val_MCC=0.274\nval_RMA=0.634', '\\n', 'Epoch 3: val_loss=0.6531  val_MCC=0.330  val_RMA=0.664',\n'\\n', 'Epoch 4: val_loss=0.6467  val_MCC=0.300  val_RMA=0.648', '\\n', 'Epoch 5:\nval_loss=0.6463  val_MCC=0.322  val_RMA=0.660', '\\n', 'Epoch 6: val_loss=0.6459\nval_MCC=0.341  val_RMA=0.670', '\\n', 'Epoch 7: val_loss=0.6465  val_MCC=0.365\nval_RMA=0.682', '\\n', 'Epoch 8: val_loss=0.6463  val_MCC=0.361  val_RMA=0.680',\n'\\n', '\\n=== Training model with d_model=128 ===', '\\n', 'Epoch 1:\nval_loss=0.6596  val_MCC=0.244  val_RMA=0.618', '\\n', 'Epoch 2: val_loss=0.6595\nval_MCC=0.267  val_RMA=0.632', '\\n', 'Epoch 3: val_loss=0.6680  val_MCC=0.240\nval_RMA=0.614', '\\n', 'Epoch 4: val_loss=0.6508  val_MCC=0.294  val_RMA=0.646',\n'\\n', 'Epoch 5: val_loss=0.6542  val_MCC=0.325  val_RMA=0.662', '\\n', 'Epoch 6:\nval_loss=0.6534  val_MCC=0.333  val_RMA=0.666', '\\n', 'Epoch 7: val_loss=0.6527\nval_MCC=0.329  val_RMA=0.664', '\\n', 'Epoch 8: val_loss=0.6527  val_MCC=0.329\nval_RMA=0.664', '\\n', '\\nBest model uses d_model=64  (dev MCC=0.3646)', '\\n',\n'Test MCC=0.3831  Test RMA=0.6920', '\\n', 'Execution time: 8 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found real SPR_BENCH at', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', 'Split sizes:', ' ',\n\"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1: validation_loss =\n0.6757 | acc=0.596 | MCC=0.205 | RMA=0.596', '\\n', 'Epoch 2: validation_loss =\n0.6548 | acc=0.638 | MCC=0.277 | RMA=0.638', '\\n', 'Epoch 3: validation_loss =\n0.6369 | acc=0.672 | MCC=0.346 | RMA=0.672', '\\n', 'Epoch 4: validation_loss =\n0.6409 | acc=0.676 | MCC=0.355 | RMA=0.676', '\\n', 'Epoch 5: validation_loss =\n0.6315 | acc=0.674 | MCC=0.348 | RMA=0.674', '\\n', 'Epoch 6: validation_loss =\n0.6271 | acc=0.680 | MCC=0.361 | RMA=0.680', '\\n', 'Epoch 7: validation_loss =\n0.6244 | acc=0.692 | MCC=0.384 | RMA=0.692', '\\n', 'Epoch 8: validation_loss =\n0.6244 | acc=0.690 | MCC=0.380 | RMA=0.690', '\\n', '\\n===== TEST RESULTS =====',\n'\\n', 'loss=0.6205 | acc=0.699 | MCC=0.397 | RMA=0.699', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH \u2026', '\\n', \"{'train': 2000,\n'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1: val_loss=0.6812  val_acc=0.642\nval_MCC=0.297  val_RMA=0.642  (0.9s)', '\\n', 'Epoch 2: val_loss=0.6716\nval_acc=0.596  val_MCC=0.226  val_RMA=0.596  (0.6s)', '\\n', 'Epoch 3:\nval_loss=0.6455  val_acc=0.678  val_MCC=0.357  val_RMA=0.678  (0.6s)', '\\n',\n'Epoch 4: val_loss=0.6475  val_acc=0.670  val_MCC=0.344  val_RMA=0.670  (0.6s)',\n'\\n', 'Epoch 5: val_loss=0.6452  val_acc=0.678  val_MCC=0.357  val_RMA=0.678\n(0.6s)', '\\n', 'Epoch 6: val_loss=0.6475  val_acc=0.682  val_MCC=0.364\nval_RMA=0.682  (0.6s)', '\\n', 'Epoch 7: val_loss=0.6484  val_acc=0.674\nval_MCC=0.349  val_RMA=0.674  (0.6s)', '\\n', 'Epoch 8: val_loss=0.6459\nval_acc=0.684  val_MCC=0.368  val_RMA=0.684  (0.6s)', '\\n', '\\n=== Test ===',\n'\\n', 'loss 0.6416 | acc 0.685 | MCC 0.369 | RMA 0.685', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'sizes:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n127, in <module>\\n    mlm_data =\nSPRMLM(dset[\"train\"].select(range(len(dset[\"train\"]))).concat(dset[\"dev\"]))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nAttributeError:\n\\'Dataset\\' object has no attribute \\'concat\\'\\n', 'Execution time: a second\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found real SPR_BENCH at', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', 'Split sizes:', ' ',\n\"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1: validation_loss =\n0.6757 | acc=0.596 | MCC=0.205 | RMA=0.596', '\\n', 'Epoch 2: validation_loss =\n0.6548 | acc=0.638 | MCC=0.277 | RMA=0.638', '\\n', 'Epoch 3: validation_loss =\n0.6369 | acc=0.672 | MCC=0.346 | RMA=0.672', '\\n', 'Epoch 4: validation_loss =\n0.6409 | acc=0.676 | MCC=0.355 | RMA=0.676', '\\n', 'Epoch 5: validation_loss =\n0.6315 | acc=0.674 | MCC=0.348 | RMA=0.674', '\\n', 'Epoch 6: validation_loss =\n0.6271 | acc=0.680 | MCC=0.361 | RMA=0.680', '\\n', 'Epoch 7: validation_loss =\n0.6244 | acc=0.692 | MCC=0.384 | RMA=0.692', '\\n', 'Epoch 8: validation_loss =\n0.6244 | acc=0.690 | MCC=0.380 | RMA=0.690', '\\n', '\\n===== TEST RESULTS =====',\n'\\n', 'loss=0.6205 | acc=0.699 | MCC=0.397 | RMA=0.699', '\\n', 'Execution time:\n9 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found real SPR_BENCH at', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', 'Split sizes:', ' ',\n\"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1: validation_loss =\n0.6757 | acc=0.596 | MCC=0.205 | RMA=0.596', '\\n', 'Epoch 2: validation_loss =\n0.6548 | acc=0.638 | MCC=0.277 | RMA=0.638', '\\n', 'Epoch 3: validation_loss =\n0.6369 | acc=0.672 | MCC=0.346 | RMA=0.672', '\\n', 'Epoch 4: validation_loss =\n0.6409 | acc=0.676 | MCC=0.355 | RMA=0.676', '\\n', 'Epoch 5: validation_loss =\n0.6315 | acc=0.674 | MCC=0.348 | RMA=0.674', '\\n', 'Epoch 6: validation_loss =\n0.6271 | acc=0.680 | MCC=0.361 | RMA=0.680', '\\n', 'Epoch 7: validation_loss =\n0.6244 | acc=0.692 | MCC=0.384 | RMA=0.692', '\\n', 'Epoch 8: validation_loss =\n0.6244 | acc=0.690 | MCC=0.380 | RMA=0.690', '\\n', '\\n===== TEST RESULTS =====',\n'\\n', 'loss=0.6205 | acc=0.699 | MCC=0.397 | RMA=0.699', '\\n', 'Execution time:\n8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found real SPR_BENCH at', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', 'Split sizes:', ' ',\n\"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1: validation_loss =\n0.6757 | acc=0.596 | MCC=0.205 | RMA=0.596', '\\n', 'Epoch 2: validation_loss =\n0.6548 | acc=0.638 | MCC=0.277 | RMA=0.638', '\\n', 'Epoch 3: validation_loss =\n0.6369 | acc=0.672 | MCC=0.346 | RMA=0.672', '\\n', 'Epoch 4: validation_loss =\n0.6409 | acc=0.676 | MCC=0.355 | RMA=0.676', '\\n', 'Epoch 5: validation_loss =\n0.6315 | acc=0.674 | MCC=0.348 | RMA=0.674', '\\n', 'Epoch 6: validation_loss =\n0.6271 | acc=0.680 | MCC=0.361 | RMA=0.680', '\\n', 'Epoch 7: validation_loss =\n0.6244 | acc=0.692 | MCC=0.384 | RMA=0.692', '\\n', 'Epoch 8: validation_loss =\n0.6244 | acc=0.690 | MCC=0.380 | RMA=0.690', '\\n', '\\n===== TEST RESULTS =====',\n'\\n', 'loss=0.6205 | acc=0.699 | MCC=0.397 | RMA=0.699', '\\n', 'Execution time:\n8 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "The execution failed due to a TypeError in the __getitem__ method of the\nSPRTorch dataset class. Specifically, the 'rule' field is being converted to a\ntorch tensor, but the 'rule' field contains string data, which is not a valid\ndata type for torch tensor conversion.   To fix this issue, ensure that the\n'rule' field contains numerical values or modify the data type handling to\nappropriately process string data. One possible fix is to encode the string data\ninto numerical representations before creating the tensor.", "", "", "", "", "", "The error occurs because the `Dataset` object from the HuggingFace `datasets`\nlibrary does not have a `concat` method. Instead, you should use the\n`concatenate_datasets` function from the `datasets` library to concatenate\nmultiple datasets. Replace the line:  ```python mlm_data =\nSPRMLM(dset[\"train\"].select(range(len(dset[\"train\"]))).concat(dset[\"dev\"])) ```\nwith:  ```python from datasets import concatenate_datasets mlm_data =\nSPRMLM(concatenate_datasets([dset[\"train\"].select(range(len(dset[\"train\"]))),\ndset[\"dev\"]])) ```  This will resolve the issue and allow the script to proceed.", "", "", "", ""], "exc_type": [null, null, "TypeError", null, null, null, null, null, "AttributeError", null, null, null, null], "exc_info": [null, null, {"args": ["new(): invalid data type 'str'"]}, null, null, null, null, null, {"args": ["'Dataset' object has no attribute 'concat'"], "name": "concat", "obj": "Dataset({\n    features: ['id', 'sequence', 'label'],\n    num_rows: 2000\n})"}, null, null, null, null], "exc_stack": [null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 221, "<module>", "tr_loss, tr_mcc, tr_rma = train(model, optimizer, criterion, train_loader)"], ["runfile.py", 162, "train", "for batch in loader:"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 701, "__next__", "data = self._next_data()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 757, "_next_data", "data = self._dataset_fetcher.fetch(index)  # may raise StopIteration"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "fetch", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "<listcomp>", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["runfile.py", 112, "__getitem__", "\"rule\": torch.tensor(self.rule[idx]),"]], null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 127, "<module>", "mlm_data = SPRMLM(dset[\"train\"].select(range(len(dset[\"train\"]))).concat(dset[\"dev\"]))"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value of the model on the training dataset.", "data": [{"dataset_name": "Training dataset", "final_value": 0.649, "best_value": 0.649}]}, {"metric_name": "training MCC", "lower_is_better": false, "description": "The Matthews correlation coefficient for the training dataset.", "data": [{"dataset_name": "Training dataset", "final_value": 0.2699, "best_value": 0.2699}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the model on the validation dataset.", "data": [{"dataset_name": "Validation dataset", "final_value": 0.6467, "best_value": 0.6467}]}, {"metric_name": "validation MCC", "lower_is_better": false, "description": "The Matthews correlation coefficient for the validation dataset.", "data": [{"dataset_name": "Validation dataset", "final_value": 0.3239, "best_value": 0.3239}]}, {"metric_name": "test MCC", "lower_is_better": false, "description": "The Matthews correlation coefficient for the test dataset.", "data": [{"dataset_name": "Test dataset", "final_value": 0.2854, "best_value": 0.2854}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "Measures the error of the model. Lower values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.6204, "best_value": 0.6204}, {"dataset_name": "validation", "final_value": 0.6286, "best_value": 0.6286}, {"dataset_name": "test", "final_value": 0.6259, "best_value": 0.6259}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions. Higher values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.6875, "best_value": 0.6875}, {"dataset_name": "validation", "final_value": 0.682, "best_value": 0.682}, {"dataset_name": "test", "final_value": 0.694, "best_value": 0.694}]}, {"metric_name": "MCC", "lower_is_better": false, "description": "Matthews correlation coefficient, a balanced measure for binary classifications. Higher values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.3849, "best_value": 0.3849}, {"dataset_name": "validation", "final_value": 0.3643, "best_value": 0.3643}, {"dataset_name": "test", "final_value": 0.3872, "best_value": 0.3872}]}, {"metric_name": "RMA", "lower_is_better": false, "description": "Root mean accuracy, another measure of prediction accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.6875, "best_value": 0.6875}, {"dataset_name": "validation", "final_value": 0.682, "best_value": 0.682}, {"dataset_name": "test", "final_value": 0.694, "best_value": 0.694}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "MCC", "lower_is_better": false, "description": "Matthews correlation coefficient, a metric for evaluating classification models.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3332, "best_value": 0.3327}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss value, measures the error in prediction.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6329, "best_value": 0.653}]}, {"metric_name": "rule-macro accuracy", "lower_is_better": false, "description": "Macro-averaged accuracy for rule-based predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.667, "best_value": 0.666}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6251595959663391, "best_value": 0.6251595959663391}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6459335322380066, "best_value": 0.6459335322380066}]}, {"metric_name": "training MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3732892016928529, "best_value": 0.3732892016928529}]}, {"metric_name": "validation MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3646283130086287, "best_value": 0.3646283130086287}]}, {"metric_name": "training RMA", "lower_is_better": false, "description": "The RMA (Root Mean Accuracy) during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.686, "best_value": 0.686}]}, {"metric_name": "validation RMA", "lower_is_better": false, "description": "The RMA (Root Mean Accuracy) during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.682, "best_value": 0.682}]}, {"metric_name": "test MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3831281844486896, "best_value": 0.3831281844486896}]}, {"metric_name": "test RMA", "lower_is_better": false, "description": "The RMA (Root Mean Accuracy) during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "Measures the error between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6205, "best_value": 0.6205}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Proportion of correctly predicted instances.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}, {"metric_name": "MCC", "lower_is_better": false, "description": "Matthews correlation coefficient, a balanced measure for binary classifications.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.397, "best_value": 0.397}]}, {"metric_name": "RMA", "lower_is_better": false, "description": "Root Mean Accuracy, an alternative accuracy metric.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.635549, "best_value": 0.635549}]}, {"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "training MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.493049, "best_value": 0.493049}]}, {"metric_name": "training RMA", "lower_is_better": false, "description": "The RMA metric during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.645914, "best_value": 0.645914}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}]}, {"metric_name": "validation MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.368052, "best_value": 0.368052}]}, {"metric_name": "validation RMA", "lower_is_better": false, "description": "The RMA metric during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.64161, "best_value": 0.64161}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.685, "best_value": 0.685}]}, {"metric_name": "test MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.369295, "best_value": 0.369295}]}, {"metric_name": "test RMA", "lower_is_better": false, "description": "The RMA metric during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.685, "best_value": 0.685}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "The loss value indicates the error in prediction. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6205, "best_value": 0.6205}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "The accuracy of the model, representing the proportion of correct predictions. Higher values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}, {"metric_name": "MCC", "lower_is_better": false, "description": "The Matthews Correlation Coefficient (MCC) is a measure of the quality of binary classifications.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.397, "best_value": 0.397}]}, {"metric_name": "RMA", "lower_is_better": false, "description": "The RMA metric measures the reliability of predictions. Higher values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "Measures the error of the model's predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6205, "best_value": 0.6205}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Proportion of correct predictions made by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}, {"metric_name": "MCC", "lower_is_better": false, "description": "Matthews correlation coefficient, used for evaluating binary classifications.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.397, "best_value": 0.397}]}, {"metric_name": "RMA", "lower_is_better": false, "description": "Root Mean Accuracy, a measure of model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "Measures how well the model's predictions match the actual outcomes. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6205, "best_value": 0.6205}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions made by the model. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}, {"metric_name": "MCC", "lower_is_better": false, "description": "Matthews Correlation Coefficient, a balanced measure for binary classification. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.397, "best_value": 0.397}]}, {"metric_name": "RMA", "lower_is_better": false, "description": "Root Mean Accuracy, a metric for evaluating model accuracy, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699, "best_value": 0.699}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, true, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/loss_curve.png", "../../logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/mcc_curve.png", "../../logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_best_lr_2e-03_loss.png", "../../logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_best_lr_2e-03_mcc.png", "../../logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_lr_sweep_mcc.png", "../../logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_pred_hist.png"], [], ["../../logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_rma_curves.png", "../../logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_loss.png", "../../logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_mcc.png", "../../logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_rma.png", "../../logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_pred_distribution.png"], ["../../logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_pred_histogram.png"], ["../../logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_prediction_hist.png"], [], ["../../logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_pred_histogram.png"], ["../../logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_pred_histogram.png"], ["../../logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_pred_histogram.png"], ["../../logs/0-run/experiment_results/seed_aggregation_ebc6609305304dd0a17876b0f79d0152/spr_bench_aggregated_loss_curves.png"]], "plot_paths": [["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/mcc_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_best_lr_2e-03_loss.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_best_lr_2e-03_mcc.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_lr_sweep_mcc.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_accuracy_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_pred_hist.png"], [], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_rma_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_confusion_matrix.png"], [], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_loss.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_mcc.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_rma.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_pred_distribution.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_accuracy_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_pred_histogram.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_accuracy_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_prediction_hist.png"], [], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_accuracy_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_pred_histogram.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_accuracy_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_pred_histogram.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_accuracy_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_mcc_curves.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_confusion_matrix.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_pred_histogram.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ebc6609305304dd0a17876b0f79d0152/spr_bench_aggregated_loss_curves.png"]], "plot_analyses": [[{"analysis": "The loss curves show a steady decline in both training and validation loss over the epochs, indicating that the model is learning effectively. However, the validation loss exhibits a slight increase at epoch 3 before decreasing again, which could suggest minor overfitting at that point. This behavior stabilizes towards the later epochs, showing improved generalization.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/loss_curve.png"}, {"analysis": "The validation MCC curve demonstrates a consistent improvement over the epochs, peaking at around epoch 4 before slightly declining. This indicates that the model's ability to classify correctly improves with training but may slightly overfit or fail to generalize optimally after the peak.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/mcc_curve.png"}, {"analysis": "The BCE loss curves for training and validation align well with the earlier loss curve. Both show a general downward trend, though the validation curve exhibits a temporary increase at epoch 3. This consistency reaffirms the earlier observation of effective learning with minor overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_best_lr_2e-03_loss.png"}, {"analysis": "The MCC curves for training and validation show a steady increase over the epochs, with validation MCC peaking earlier than training MCC. This pattern suggests that the model is improving its classification capability but may require additional regularization to maintain performance without overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_best_lr_2e-03_mcc.png"}, {"analysis": "The learning rate sweep plot appears to be blank, which might indicate an issue with the data or visualization. No insights can be drawn from this plot as it lacks meaningful information.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_lr_sweep_mcc.png"}, {"analysis": "The confusion matrix reveals the classification performance on the test set. The model shows a higher number of correct predictions for the positive class (370) than for the negative class (273). However, there is a noticeable number of false positives (213) and false negatives (144), indicating room for improvement in balancing precision and recall.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3490730a40454337b9855835b70c4ba2_proc_3333223/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curves indicate a steady decrease in both training and validation loss over the epochs. The training loss decreases more sharply than the validation loss, suggesting good convergence but potential overfitting as the training loss is lower than the validation loss from epoch 2 onwards. The validation loss plateaus after epoch 4, indicating diminishing returns with further training.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show a consistent improvement in both training and validation accuracy over the epochs. The training accuracy exceeds the validation accuracy, which mirrors the trends in the loss curves and suggests potential overfitting. However, the validation accuracy stabilizes near 70%, approaching the SOTA baseline.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_accuracy_curves.png"}, {"analysis": "The MCC curves reveal steady improvement in both training and validation MCC over the epochs. The training MCC surpasses the validation MCC, which aligns with the observed overfitting trends. The validation MCC stabilizes after epoch 4, indicating that further training does not significantly improve performance on the validation set.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_mcc_curves.png"}, {"analysis": "The confusion matrix shows that the model performs reasonably well, with true positives and true negatives significantly outnumbering false positives and false negatives. However, there is a noticeable number of false positives and false negatives, indicating room for improvement in classification performance, particularly in distinguishing between positive and negative sequences.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_confusion_matrix.png"}, {"analysis": "The prediction distribution plot shows a clear separation between predicted true negatives and true positives. However, some overlap exists, suggesting that certain sequences are challenging for the model to classify correctly. This overlap corresponds to the false positives and false negatives observed in the confusion matrix.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_499aef4c5a5742cbb7b4993cdacf4b48_proc_3338338/spr_bench_pred_hist.png"}], [], [{"analysis": "The BCE loss curve shows a steady decrease in training loss across epochs, indicating effective learning by the model. However, the validation loss exhibits some fluctuations, suggesting that the model might be overfitting to the training data. The increasing trend in validation loss towards the end is concerning and highlights the need for regularization or early stopping.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_loss_curves.png"}, {"analysis": "The MCC curve displays a consistent improvement in the training metric, with the Matthews Correlation Coefficient steadily increasing. However, the validation MCC plateaus after an initial rise, which could indicate a limitation in the model's ability to generalize to unseen data. The gap between training and validation MCC suggests potential overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_mcc_curves.png"}, {"analysis": "The Rule-Macro Accuracy curve indicates that the training accuracy improves significantly over epochs, but the validation accuracy shows a smaller improvement and stagnates after a few epochs. This disparity between training and validation performance reinforces the observation of overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_rma_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well on the test set, with a higher number of true positives (370) and true negatives (297). However, the false positives (189) and false negatives (144) are non-negligible, indicating areas for improvement in handling both positive and negative classes effectively. The imbalance between true positives and false negatives suggests a need to refine the model's sensitivity to positive cases.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_bbac867dabc0475dbb6d116c006b7ef2_proc_3338340/spr_bench_confusion_matrix.png"}], [], [{"analysis": "The BCE loss curves show a steady decrease in loss for both the training and validation sets over eight epochs. The training loss decreases more sharply, suggesting that the model is learning effectively during training. However, the validation loss flattens around epoch 5, indicating a potential stabilization in generalization performance. There is no significant overfitting observed, as the validation loss does not increase towards the end.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_loss.png"}, {"analysis": "The MCC curves indicate that the model's performance in terms of correlation between predictions and true labels improves consistently over epochs. Both training and validation MCC values converge, with validation performance slightly lagging behind but showing a steady upward trend. This suggests that the model is learning meaningful patterns without overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_mcc.png"}, {"analysis": "The Rule-Macro-Accuracy (RMA) curves highlight a consistent improvement in accuracy for both training and validation sets. The upward trend in validation RMA suggests that the model generalizes well to unseen data. The convergence of training and validation accuracy towards the later epochs indicates good model stability.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_rma.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well in distinguishing between positive and negative classes, with 321 true negatives, 371 true positives, and relatively balanced misclassifications (165 false negatives and 143 false positives). This indicates a balanced performance across both classes, though there is room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_confusion_matrix.png"}, {"analysis": "The prediction distribution plot shows a clear separation between true negatives and true positives, with a higher count of correctly predicted positive samples. This suggests that the model has a strong ability to identify positive samples but might need additional refinement to improve its handling of negative samples.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3fc93d442e6e42afa3f6a4af0dcdf4e3_proc_3338341/spr_bench_pred_distribution.png"}], [{"analysis": "The loss curves show a consistent decrease in both the training and validation loss over the epochs, indicating that the model is learning effectively. However, the gap between the training and validation loss is small, suggesting minimal overfitting. The convergence of the curves towards the end implies that the model is approaching its optimal performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves reveal an upward trend for both training and validation accuracy, with the training accuracy showing some fluctuations. These fluctuations might indicate that the model is sensitive to certain patterns in the data. The validation accuracy stabilizes and comes close to the SOTA benchmark of 70%, demonstrating promising generalization.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_accuracy_curves.png"}, {"analysis": "The MCC (Matthews Correlation Coefficient) curves align with the accuracy trends, showing improvement over the epochs. The MCC for validation stabilizes, indicating that the model is achieving a balanced performance across classes. The fluctuations in the training MCC suggest areas for potential refinement in model training.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_mcc_curves.png"}, {"analysis": "The confusion matrix indicates that the model performs reasonably well on the test set, with a higher count of true positives (370) and true negatives (329). However, there are notable false positives (157) and false negatives (144), suggesting room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_confusion_matrix.png"}, {"analysis": "The prediction distribution plot shows a skew towards correct classifications for both true positives and true negatives. The distribution suggests that the model is confident in its predictions for most cases, but there might be some borderline cases that could benefit from further tuning or additional features.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_pred_histogram.png"}], [{"analysis": "The loss curves indicate that the model is learning effectively during training, as the training loss decreases consistently over the epochs. The validation loss also decreases initially, achieving a minimum around epoch 4 before starting to fluctuate slightly. This suggests that the model is learning but may be beginning to overfit slightly after epoch 4. Further regularization techniques or early stopping might help stabilize the validation performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show that training accuracy increases significantly, peaking at around epoch 2, before showing some fluctuations. Validation accuracy improves steadily and stabilizes after epoch 4, indicating that the model generalizes reasonably well to unseen data. However, the fluctuations in training accuracy might suggest overfitting or instability in the training process.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_accuracy_curves.png"}, {"analysis": "The MCC (Matthews Correlation Coefficient) curves show a similar trend to the accuracy curves, with training MCC peaking early and then fluctuating, while validation MCC improves steadily and stabilizes. This indicates that the model is reasonably balanced in its predictions but might benefit from additional tuning to reduce fluctuations in training performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_mcc_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well, with a balanced distribution of true positives and true negatives. However, there are noticeable false positives (161) and false negatives (154), suggesting that the model could improve its precision and recall. Adjusting the decision threshold or using techniques like cost-sensitive learning could help address these issues.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_confusion_matrix.png"}, {"analysis": "The prediction distribution plot shows a balanced distribution of true positives and true negatives, indicating that the model does not heavily favor one class over the other. This is a positive sign for the model's robustness in handling class imbalance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ee3c35864ef344c7b9546cbb9ae6f0aa_proc_3338340/spr_bench_prediction_hist.png"}], [], [{"analysis": "The loss curves indicate that the model is effectively learning during training. Both training and validation loss decrease over epochs, with validation loss being consistently lower than training loss. This suggests that the model is not overfitting and is generalizing well to unseen data. The plateau in loss reduction at later epochs may indicate that further improvements could require additional model tuning or architectural changes.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show steady improvement in both training and validation accuracy over epochs. However, there is a noticeable fluctuation in training accuracy at certain epochs, which might indicate instability in training or sensitivity to the learning rate. Validation accuracy stabilizes around 70%, suggesting that the model is nearing the SOTA benchmark but may require further optimization to surpass it.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_accuracy_curves.png"}, {"analysis": "The MCC curves demonstrate a similar trend to the accuracy curves, with steady improvement in both training and validation MCC over epochs. The MCC values for validation are consistently lower than those for training, indicating room for improvement in the model's ability to handle the complexities of the SPR task. The fluctuations in training MCC also highlight potential instability in the model's learning process.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_mcc_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well in distinguishing between positive and negative classes, with 329 true negatives and 370 true positives. However, there are notable numbers of false negatives (144) and false positives (157), indicating that the model struggles with certain edge cases or ambiguous sequences. Addressing these errors could significantly enhance performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_confusion_matrix.png"}, {"analysis": "The prediction distribution plot shows a balanced distribution of true negatives and true positives, which aligns with the confusion matrix. However, the overlap in predicted class probabilities for true negatives and true positives suggests that the model's confidence in its predictions could be improved. Refining the decision boundary or incorporating additional features might help reduce this overlap.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_pred_histogram.png"}], [{"analysis": "The BCE loss curves indicate a consistent decrease in loss for both the training and validation datasets across epochs. The validation loss closely follows the training loss, suggesting that the model is learning effectively without significant overfitting. The slight convergence towards the end suggests that the model could benefit from additional epochs or fine-tuning to achieve further improvements.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show a steady increase in both training and validation accuracy over the epochs. However, the training accuracy exhibits some oscillations, which could indicate sensitivity to updates or potential overfitting in certain epochs. The validation accuracy plateaus around 70%, which aligns with the SOTA benchmark, indicating competitive performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_accuracy_curves.png"}, {"analysis": "The Matthews Correlation Coefficient (MCC) curves reveal a similar trend to the accuracy curves, with the validation MCC steadily improving and plateauing around 0.4. This suggests that the model achieves a moderate level of agreement between predictions and actual labels, but there is room for improvement in handling imbalanced or complex rule-based sequences.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_mcc_curves.png"}, {"analysis": "The confusion matrix shows that the model correctly predicts the majority of both positive and negative classes, with 370 true positives and 329 true negatives. However, there are notable false positives (157) and false negatives (144), indicating areas where the model struggles to differentiate between classes. This highlights the need for further refinement in feature extraction or rule representation.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_confusion_matrix.png"}, {"analysis": "The prediction distribution plot shows a balanced distribution of true negatives and true positives. However, the overlap in prediction scores suggests that the model's confidence in distinguishing between classes could be improved. Calibration techniques or more discriminative features might enhance class separation.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_pred_histogram.png"}], [{"analysis": "This plot shows the BCE loss curves for both the training and validation datasets. Both curves decrease steadily, indicating that the model is learning effectively. The validation loss consistently remains lower than the training loss, which suggests that the model is not overfitting. However, the convergence is slow, and the gap between training and validation loss should be monitored in further experiments to ensure robustness.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_loss_curves.png"}, {"analysis": "This plot depicts the accuracy curves for the training and validation datasets. The training accuracy increases rapidly, peaking and then fluctuating slightly, while the validation accuracy stabilizes at around 70%. The fluctuation in training accuracy might indicate overfitting or instability in the learning process. The validation accuracy reaching 70% matches the SOTA baseline, which is promising, but further fine-tuning might be needed to surpass it.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_accuracy_curves.png"}, {"analysis": "This plot shows the Matthews Correlation Coefficient (MCC) curves for the training and validation datasets. The MCC for training shows sharp fluctuations after an initial increase, while the validation MCC stabilizes at a value around 0.4. This indicates that the model is capturing some of the underlying relationships but might still require improvements to handle the complexities of the SPR task effectively.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_mcc_curves.png"}, {"analysis": "The confusion matrix reveals the classification performance on the test set. The model achieves a reasonable balance between true positives (370) and true negatives (329), but there is a noticeable number of false positives (157) and false negatives (144). This suggests that the model is moderately effective but struggles with certain edge cases or complex rules. Further analysis of these misclassified instances could provide insights for model improvement.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_confusion_matrix.png"}, {"analysis": "This plot illustrates the distribution of predictions for true negatives and true positives. The separation between these distributions indicates that the model is reasonably confident in its predictions. However, there is some overlap, which could lead to misclassifications. Enhancing the model's ability to discriminate between positive and negative classes could improve overall performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_pred_histogram.png"}], []], "vlm_feedback_summary": ["The experiment results show effective learning and steady improvement in\nclassification performance, as evidenced by the loss and MCC curves. Minor\noverfitting is observed but stabilizes towards the later epochs. The confusion\nmatrix highlights the model's strengths and weaknesses in handling positive and\nnegative classes, suggesting areas for further refinement. One plot (learning\nrate sweep) lacks data and should be revisited for insights.", "The plots collectively indicate that the model achieves reasonable performance,\nwith validation metrics approaching the SOTA baseline. However, there are signs\nof overfitting, as evidenced by the gap between training and validation metrics.\nThe confusion matrix and prediction distribution highlight areas where\nclassification accuracy can be improved, particularly for challenging sequences.", "[]", "The results highlight potential overfitting issues and suggest a need for\nfurther improvements in generalization. The model demonstrates promising\nperformance, but validation metrics indicate room for optimization.\nRegularization techniques or alternative architectures could help address the\nobserved limitations.", "[]", "The experimental results demonstrate steady learning and generalization on the\nSPR_BENCH task. The loss, MCC, and RMA curves indicate consistent improvement\nwithout signs of overfitting. The confusion matrix and prediction distribution\nhighlight balanced performance across classes, though there is room for further\noptimization, particularly in reducing misclassifications.", "The plots provide valuable insights into the model's performance, indicating\nsteady learning and generalization. The accuracy and MCC metrics are promising,\napproaching the SOTA benchmark. However, there is room for improvement in\nreducing misclassifications and addressing fluctuations in training metrics.", "The experimental results show that the model is learning effectively, with\nreasonable generalization to validation data. However, there are indications of\npotential overfitting and instability in training. The confusion matrix and\nprediction distribution suggest balanced predictions but highlight areas for\nimprovement in precision and recall. Further tuning and regularization could\nenhance the model's performance.", "[]", "The plots suggest that the model is learning effectively and nearing SOTA\nperformance, but there are areas for improvement. The loss curves indicate good\ngeneralization, the accuracy and MCC curves show steady progress with some\ninstability, and the confusion matrix and prediction distribution highlight\nspecific areas where the model's performance can be refined.", "The plots indicate that the model performs competitively with the SOTA\nbenchmark, achieving comparable accuracy and MCC. However, there are areas for\nimprovement, such as reducing false positives and false negatives, refining\nfeature extraction, and improving class separation. The results are promising,\nbut further experimentation is needed to achieve superior performance.", "The plots indicate that the model is learning effectively but faces challenges\nin achieving stability and surpassing the SOTA baseline. The validation accuracy\nand MCC suggest moderate success, while the confusion matrix and prediction\ndistributions highlight areas for improvement in handling complex rules and edge\ncases. Further fine-tuning and analysis of misclassified instances are\nrecommended.", "[]"], "exec_time": [4.32969069480896, 4.709977865219116, 2.7459914684295654, 4.086806774139404, 7.816161870956421, 8.601496458053589, 7.307880640029907, 7.753565788269043, 1.3289141654968262, 9.956794023513794, 8.661718606948853, 8.208805322647095, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["['ds_name']"], [], ["[]"], [], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"All datasets processed without errors\"]"], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nspr_data = experiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"]\nbest_lr = spr_data[\"best_lr\"]\nbest_rec = spr_data[f\"{best_lr:.0e}\"]\n\n\n# ---------- helper for confused counts ----------\ndef confusion_counts(y_true, y_pred):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_true, y_pred))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_true, y_pred))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_true, y_pred))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_true, y_pred))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    plt.figure()\n    epochs = best_rec[\"epochs\"]\n    plt.plot(epochs, best_rec[\"losses\"][\"train\"], label=\"Train\")\n    plt.plot(epochs, best_rec[\"losses\"][\"val\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.legend()\n    fname = f\"spr_bench_best_lr_{best_lr:.0e}_loss.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) MCC curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, best_rec[\"metrics\"][\"train_MCC\"], label=\"Train\")\n    plt.plot(epochs, best_rec[\"metrics\"][\"val_MCC\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MCC\")\n    plt.legend()\n    fname = f\"spr_bench_best_lr_{best_lr:.0e}_mcc.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 3) LR sweep summary ----------\ntry:\n    plt.figure()\n    lrs = []\n    peak_mcc = []\n    for lr_key, rec in spr_data.items():\n        if not lr_key.endswith(\"e\"):  # skip aux keys\n            continue\n        lrs.append(lr_key)\n        peak_mcc.append(max(rec[\"metrics\"][\"val_MCC\"]))\n    plt.bar(lrs, peak_mcc, color=\"skyblue\")\n    plt.title(\"SPR_BENCH Learning-Rate Sweep\\nPeak Validation MCC per LR\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Peak Val MCC\")\n    plt.xticks(rotation=45)\n    fname = \"spr_bench_lr_sweep_mcc.png\"\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating LR sweep plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    y_true = spr_data[\"ground_truth\"]\n    y_pred = spr_data[\"predictions\"]\n    cm = confusion_counts(y_true, y_pred)\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(\n        \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n    )\n    plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n    plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    fname = \"spr_bench_confusion_matrix.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Logit histogram ----------\ntry:\n    # logits were not saved; reconstruct approximate logits from predictions as 0/1\n    preds = np.array(y_pred)\n    trues = np.array(y_true)\n    plt.figure()\n    plt.hist(preds[trues == 0], bins=2, alpha=0.7, label=\"True Negatives\")\n    plt.hist(preds[trues == 1], bins=2, alpha=0.7, label=\"True Positives\")\n    plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n    plt.xlabel(\"Predicted Class\")\n    plt.ylabel(\"Count\")\n    plt.legend()\n    fname = \"spr_bench_pred_distribution.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- load data ------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\n\n# ------------------- helper ------------------- #\ndef confusion_counts(y_true, y_pred):\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ------------------- plotting loop ------------------- #\nfor ds_name, rec in experiment_data.items():\n    epochs = rec.get(\"epochs\", [])\n    losses_tr = rec[\"losses\"][\"train\"]\n    losses_val = rec[\"losses\"][\"val\"]\n    acc_tr = [m[\"acc\"] for m in rec[\"metrics\"][\"train\"]]\n    acc_val = [m[\"acc\"] for m in rec[\"metrics\"][\"val\"]]\n    mcc_tr = [m[\"MCC\"] for m in rec[\"metrics\"][\"train\"]]\n    mcc_val = [m[\"MCC\"] for m in rec[\"metrics\"][\"val\"]]\n    y_pred = np.array(rec.get(\"predictions\", []))\n    y_true = np.array(rec.get(\"ground_truth\", []))\n\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, losses_tr, label=\"Train\")\n        plt.plot(epochs, losses_val, label=\"Validation\")\n        plt.title(f\"{ds_name} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = f\"{ds_name.lower()}_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) Accuracy curves\n    try:\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ds_name.lower()}_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating acc plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) MCC curves\n    try:\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(f\"{ds_name} MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MCC\")\n        plt.legend()\n        fname = f\"{ds_name.lower()}_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MCC plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix\n    try:\n        if y_true.size and y_pred.size:\n            cm = confusion_counts(y_true, y_pred)\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n            plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n            for i in range(2):\n                for j in range(2):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = f\"{ds_name.lower()}_confusion_matrix.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CM plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 5) Prediction histogram\n    try:\n        if y_true.size and y_pred.size:\n            plt.figure()\n            plt.hist(y_pred[y_true == 0], bins=2, alpha=0.7, label=\"True Negatives\")\n            plt.hist(y_pred[y_true == 1], bins=2, alpha=0.7, label=\"True Positives\")\n            plt.title(\n                f\"{ds_name} Prediction Distribution\\nLeft: True Neg, Right: True Pos\"\n            )\n            plt.xlabel(\"Predicted Class\")\n            plt.ylabel(\"Count\")\n            plt.legend()\n            fname = f\"{ds_name.lower()}_pred_hist.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating hist plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Print final test metrics\n    test_metrics = rec.get(\"test_metrics\", {})\n    if test_metrics:\n        print(\n            f\"{ds_name} test metrics:\",\n            {k: round(v, 4) for k, v in test_metrics.items()},\n        )\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nepochs = spr.get(\"epochs\", list(range(len(spr[\"losses\"][\"train\"]))))\n\n\n# ---------- helper ----------\ndef confusion_counts(y_true, y_pred):\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) BCE loss ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, spr[\"losses\"][\"train\"], label=\"Train\")\n    plt.plot(epochs, spr[\"losses\"][\"val\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.legend()\n    fname = \"spr_bench_loss_curves.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) MCC ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, spr[\"metrics\"][\"train\"], label=\"Train\")\n    plt.plot(epochs, spr[\"metrics\"][\"val\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Corr Coef\")\n    plt.legend()\n    fname = \"spr_bench_mcc_curves.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 3) RMA ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, spr[\"RMA\"][\"train\"], label=\"Train\")\n    plt.plot(epochs, spr[\"RMA\"][\"val\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH Rule-Macro Accuracy\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RMA\")\n    plt.legend()\n    fname = \"spr_bench_rma_curves.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating RMA plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    y_true = np.array(spr[\"ground_truth\"])\n    y_pred = np.array(spr[\"predictions\"])\n    cm = confusion_counts(y_true, y_pred)\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(\n        \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n    )\n    plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n    plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    fname = \"spr_bench_confusion_matrix.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    plt.figure()\n    plt.hist(y_pred[y_true == 0], bins=2, alpha=0.7, label=\"True Negatives\")\n    plt.hist(y_pred[y_true == 1], bins=2, alpha=0.7, label=\"True Positives\")\n    plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n    plt.xlabel(\"Predicted Class\")\n    plt.ylabel(\"Count\")\n    plt.legend()\n    fname = \"spr_bench_prediction_histogram.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating prediction histogram: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nspr = experiment_data.get(\"SPR_BENCH\", {})\nepochs = spr.get(\"epochs\", [])\n\n\n# -------- helper --------\ndef confusion_counts(y_true, y_pred):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_true, y_pred))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_true, y_pred))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_true, y_pred))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_true, y_pred))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# -------- 1) Loss curves --------\ntry:\n    plt.figure()\n    plt.plot(epochs, spr[\"losses\"][\"train\"], label=\"Train\")\n    plt.plot(epochs, spr[\"losses\"][\"val\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.legend()\n    fname = \"spr_bench_loss.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# -------- 2) MCC curves --------\ntry:\n    plt.figure()\n    plt.plot(epochs, spr[\"metrics\"][\"train_MCC\"], label=\"Train\")\n    plt.plot(epochs, spr[\"metrics\"][\"val_MCC\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MCC\")\n    plt.legend()\n    fname = \"spr_bench_mcc.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# -------- 3) Rule-Macro-Accuracy curves --------\ntry:\n    plt.figure()\n    plt.plot(epochs, spr[\"metrics\"][\"train_RMA\"], label=\"Train\")\n    plt.plot(epochs, spr[\"metrics\"][\"val_RMA\"], label=\"Validation\")\n    plt.title(\"SPR_BENCH Rule-Macro-Accuracy\\nLeft: Train, Right: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RMA\")\n    plt.legend()\n    fname = \"spr_bench_rma.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating RMA plot: {e}\")\n    plt.close()\n\n# -------- 4) Confusion matrix --------\ntry:\n    y_true = spr[\"ground_truth\"]\n    y_pred = spr[\"predictions\"]\n    cm = confusion_counts(y_true, y_pred)\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(\"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\")\n    plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n    plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n    fname = \"spr_bench_confusion_matrix.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# -------- 5) Prediction distribution --------\ntry:\n    preds = np.array(y_pred)\n    trues = np.array(y_true)\n    plt.figure()\n    plt.hist(preds[trues == 0], bins=2, alpha=0.7, label=\"True Negatives\")\n    plt.hist(preds[trues == 1], bins=2, alpha=0.7, label=\"True Positives\")\n    plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n    plt.xlabel(\"Predicted Class\")\n    plt.ylabel(\"Count\")\n    plt.legend()\n    fname = \"spr_bench_pred_distribution.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\n\ndef confusion_counts(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- iterate over datasets ----------\nfor dset_name, rec in experiment_data.items():\n    epochs = np.array(rec[\"epochs\"])\n    tr_loss = np.array(rec[\"losses\"][\"train\"])\n    val_loss = np.array(rec[\"losses\"][\"val\"])\n    tr_acc = np.array([m[\"acc\"] for m in rec[\"metrics\"][\"train\"]])\n    val_acc = np.array([m[\"acc\"] for m in rec[\"metrics\"][\"val\"]])\n    tr_mcc = np.array([m[\"MCC\"] for m in rec[\"metrics\"][\"train\"]])\n    val_mcc = np.array([m[\"MCC\"] for m in rec[\"metrics\"][\"val\"]])\n    y_pred = np.array(rec.get(\"predictions\", []))\n    y_true = np.array(rec.get(\"ground_truth\", []))\n\n    # optional epoch thinning for readability\n    if len(epochs) > 10:\n        idx = np.linspace(0, len(epochs) - 1, 10, dtype=int)\n    else:\n        idx = slice(None)\n\n    # 1) Loss\n    try:\n        plt.figure()\n        plt.plot(epochs[idx], tr_loss[idx], label=\"Train\")\n        plt.plot(epochs[idx], val_loss[idx], label=\"Validation\")\n        plt.title(f\"{dset_name} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = f\"{dset_name.lower()}_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {dset_name}: {e}\")\n        plt.close()\n\n    # 2) Accuracy\n    try:\n        plt.figure()\n        plt.plot(epochs[idx], tr_acc[idx], label=\"Train\")\n        plt.plot(epochs[idx], val_acc[idx], label=\"Validation\")\n        plt.title(f\"{dset_name} Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{dset_name.lower()}_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {dset_name}: {e}\")\n        plt.close()\n\n    # 3) MCC\n    try:\n        plt.figure()\n        plt.plot(epochs[idx], tr_mcc[idx], label=\"Train\")\n        plt.plot(epochs[idx], val_mcc[idx], label=\"Validation\")\n        plt.title(f\"{dset_name} MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MCC\")\n        plt.legend()\n        fname = f\"{dset_name.lower()}_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting MCC for {dset_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix\n    try:\n        if y_true.size and y_pred.size:\n            cm = confusion_counts(y_true, y_pred)\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(f\"{dset_name} Confusion Matrix\\nLeft: Ground Truth, Right: Preds\")\n            plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n            plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n            for i in range(2):\n                for j in range(2):\n                    plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n            fname = f\"{dset_name.lower()}_confusion_matrix.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion for {dset_name}: {e}\")\n        plt.close()\n\n    # 5) Prediction histogram\n    try:\n        if y_true.size and y_pred.size:\n            plt.figure()\n            plt.hist(y_pred[y_true == 0], bins=2, alpha=0.7, label=\"True Neg\")\n            plt.hist(y_pred[y_true == 1], bins=2, alpha=0.7, label=\"True Pos\")\n            plt.title(\n                f\"{dset_name} Prediction Distribution\\nLeft: True Neg, Right: True Pos\"\n            )\n            plt.xlabel(\"Predicted Class\")\n            plt.ylabel(\"Count\")\n            plt.legend()\n            fname = f\"{dset_name.lower()}_prediction_hist.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting histogram for {dset_name}: {e}\")\n        plt.close()\n\n    # ----- print summary metrics -----\n    if \"test_metrics\" in rec:\n        tm = rec[\"test_metrics\"]\n        print(\n            f\"{dset_name} test -> loss:{tm['loss']:.4f}  acc:{tm['acc']:.3f}  MCC:{tm['MCC']:.3f}  RMA:{tm['RMA']:.3f}\"\n        )\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & constants ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Experiment data (relative to $AI_SCIENTIST_ROOT)\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/experiment_data.npy\",\n]\n\n# ---------------- load every run ----------------\nall_runs = []\nfor p in experiment_data_path_list:\n    try:\n        full_p = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_p, allow_pickle=True).item()\n        if \"SPR_BENCH\" in exp:\n            all_runs.append(exp[\"SPR_BENCH\"])\n        else:\n            print(f\"Warning: SPR_BENCH entry missing in {p}\")\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif len(all_runs) == 0:\n    raise SystemExit(\"No valid runs found \u2013 aborting.\")\n\nn_runs = len(all_runs)\nprint(f\"Loaded {n_runs} SPR_BENCH run(s).\")\n\n\n# ---------------- helper to collect & align ----------------\ndef stack_metric(path, subkey):\n    \"\"\"Return epochs, 2-D array (runs\u00d7epochs), or ([],[]) if nothing found.\"\"\"\n    series = []\n    for run in all_runs:\n        arr = np.array(run[path].get(subkey, [])) if path in run else np.array([])\n        series.append(arr)\n    min_len = min(len(a) for a in series)\n    if min_len == 0:\n        return np.array([]), np.array([[]])\n    trimmed = np.stack([a[:min_len] for a in series])  # runs \u00d7 epochs\n    epochs = np.array(all_runs[0][\"epochs\"][:min_len])\n    return epochs, trimmed\n\n\n# ---------------- aggregate curves ----------------\nplots = [\n    (\n        \"Loss Curves\",\n        (\"losses\", \"train\"),\n        (\"losses\", \"val\"),\n        \"BCE Loss\",\n        \"spr_bench_aggregated_loss_curves.png\",\n    ),\n    (\n        \"Accuracy Curves\",\n        (\"metrics\", \"train\", \"acc\"),\n        (\"metrics\", \"val\", \"acc\"),\n        \"Accuracy\",\n        \"spr_bench_aggregated_accuracy_curves.png\",\n    ),\n    (\n        \"MCC Curves\",\n        (\"metrics\", \"train\", \"MCC\"),\n        (\"metrics\", \"val\", \"MCC\"),\n        \"Matthews CorrCoef\",\n        \"spr_bench_aggregated_mcc_curves.png\",\n    ),\n]\n\n\ndef retrieve(run_dict, keys):\n    \"\"\"Nested get with default {}\u2192[] to avoid KeyErrors.\"\"\"\n    d = run_dict\n    for k in keys:\n        d = d.get(k, {})\n    return d\n\n\n# plotting loop --------------------------------------------------------------\nfor title, train_path, val_path, ylabel, fname in plots:\n    try:\n        # unpack keys: (\"losses\",\"train\") etc.\n        if len(train_path) == 2:\n            t_epochs, train_mat = stack_metric(train_path[0], train_path[1])\n            v_epochs, val_mat = stack_metric(val_path[0], val_path[1])\n        else:  # metrics / train / acc\n            t_epochs, train_mat = stack_metric(train_path[0], train_path[2])\n            v_epochs, val_mat = stack_metric(val_path[0], val_path[2])\n\n        if t_epochs.size and v_epochs.size:\n            # train\n            train_mean = np.nanmean(train_mat, axis=0)\n            train_sem = np.nanstd(train_mat, axis=0, ddof=1) / np.sqrt(n_runs)\n            # val\n            val_mean = np.nanmean(val_mat, axis=0)\n            val_sem = np.nanstd(val_mat, axis=0, ddof=1) / np.sqrt(n_runs)\n\n            plt.figure()\n            plt.plot(t_epochs, train_mean, label=\"Train \u03bc\")\n            plt.fill_between(\n                t_epochs,\n                train_mean - train_sem,\n                train_mean + train_sem,\n                alpha=0.3,\n                label=\"Train \u00b1SEM\",\n            )\n            plt.plot(v_epochs, val_mean, label=\"Val \u03bc\")\n            plt.fill_between(\n                v_epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                alpha=0.3,\n                label=\"Val \u00b1SEM\",\n            )\n            plt.title(f\"SPR_BENCH {title}\\nMean \u00b1 SEM across {n_runs} runs\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(ylabel)\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, fname))\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot {fname}: {e}\")\n        plt.close()\n\n# ---------------- aggregate test metrics ----------------\ntry:\n    test_metrics_keys = set().union(\n        *(run.get(\"test_metrics\", {}).keys() for run in all_runs)\n    )\n    if test_metrics_keys:\n        print(\"\\n===== AGGREGATED TEST METRICS =====\")\n        for key in sorted(test_metrics_keys):\n            vals = [run.get(\"test_metrics\", {}).get(key, np.nan) for run in all_runs]\n            vals = np.array(vals, dtype=float)\n            mean = np.nanmean(vals)\n            std = np.nanstd(vals, ddof=1)\n            print(f\"{key}: {mean:.4f} \u00b1 {std:.4f} (n={n_runs})\")\nexcept Exception as e:\n    print(f\"Error aggregating test metrics: {e}\")\n"], "plot_plan": ["We will first load experiment_data.npy, identify the best-learning-rate record and gather the sweep statistics.  \nFive plots will be generated, each enclosed in its own try-except so failures do not crash the script and every figure is closed.  \n1) Loss curves (train/val) for the best LR; 2) MCC curves (train/val) for the best LR; 3) Bar chart comparing peak validation-MCC across all tested learning rates; 4) Confusion-matrix heat-map on the test split for the best LR; 5) Histogram showing the distribution of model logits on the test split, split by ground-truth label.  \nAll figures will be titled with the dataset name \u201cSPR_BENCH\u201d and a clear subtitle, and saved under descriptive filenames in working_dir.  \nWe cap at these five figures to respect the plotting-frequency guideline.  \nPlots use only data already stored in experiment_data.npy; no synthetic values are introduced.  \nThe code begins with the mandated imports and working_dir definition, ensures working_dir exists, and prints a brief confirmation when each figure is saved.", "The solution simply loads the saved `experiment_data.npy`, loops through every dataset stored inside, and extracts per-epoch losses and metrics as well as final test predictions. For each dataset it creates at most five standard visualizations: (1) train/validation loss curves, (2) accuracy curves, (3) MCC curves, (4) a confusion-matrix heatmap on the test set, and (5) a histogram of predicted classes split by ground-truth label. Each figure is wrapped in its own try-except block, includes an informative title and subtitle, is saved to `working/` with a descriptive filename, and is properly closed afterward. No synthetic data are generated\u2014the code only uses what is found in the NumPy file. Finally, the script prints out the stored test metrics so the user can see quantitative results alongside the figures. This adheres to all formatting, naming, and safety guidelines given.", null, "We first load the saved numpy dictionary, pull out the SPR_BENCH entry and read the lists of epochs, losses, MCC scores, RMA scores, predictions and ground-truth labels.  Five independent try-except blocks are then used to draw: (1) BCE loss curves, (2) MCC curves, (3) rule-macro accuracy curves, (4) a 2\u00d72 confusion-matrix heat-map on the test set, and (5) a simple histogram showing how often each class was predicted on the test set.  Each block finishes by saving the figure into the working directory with a descriptive name and then closing the figure, so resources are always released even when an exception occurs.  Titles explicitly mention the dataset and give a \u201cLeft: \u2026, Right: \u2026\u201d style subtitle where appropriate, following the guidelines.  All information plotted comes directly from experiment_data.npy\u2014nothing is fabricated.  No more than one figure per metric is created, staying well below the five-figure limit for similar plots.  Consistent naming (\u201cspr_bench_[metric].png\u201d) is used for every saved file.  The code is kept compact and uses only standard matplotlib and numpy.  Finally, the script prints a short confirmation message after each successful save so users can quickly verify that visuals were generated.", null, "We first load experiment_data.npy, extract the SPR_BENCH entry and pull out epoch index, loss, MCC and RMA arrays plus test-set predictions/ground-truth.  Five plots are produced in separate try-except blocks: 1) training/validation loss curve, 2) training/validation MCC curve, 3) training/validation Rule-Macro-Accuracy, 4) test-set confusion matrix, and 5) histogram of predicted classes conditioned on ground truth.  Each figure is titled, saved with descriptive file names that include the dataset (\u201cspr_bench\u201d) and plot type, and the figure is always closed afterward.  A helper confusion_counts function builds the confusion matrix.  All plotting stays within the available data; no extra epochs or logits are fabricated, and at most these five figures are generated.  The code starts with the required imports, creates working_dir, loads the NumPy dict, and then executes the plotting logic, printing a confirmation or error for every figure.  This satisfies the guideline requirements on structure, protective error handling, and clear labelling while remaining concise.", "We first load experiment_data.npy and grab the SPR_BENCH entry, extracting the epoch list, train/validation losses, and per-epoch metric dictionaries.  From the metric dicts we build NumPy arrays for accuracy and MCC so that we can draw paired curves for train vs. validation.  We then create up to five figures: (1) loss curves, (2) accuracy curves, (3) MCC curves, (4) a confusion-matrix heat-map on the test set, and (5) a simple histogram comparing the class-frequency of predictions conditioned on the ground-truth label.  Each figure is wrapped in its own try-except so that failure in one plot does not stop the others, and every figure is closed after saving.  File names combine the dataset name and plot type for clarity, and titles always include a brief subtitle explaining the axes or comparison (e.g. \u201cLeft: Train, Right: Validation\u201d).  We only plot data that actually exists in the loaded dictionary, using len checks where necessary.  Finally, we print out the saved test-set metrics (loss, acc, MCC, RMA) so the user can quickly verify performance without opening the plots.  All output images are written to the pre-defined working directory.", "We will first load the saved numpy dictionary, iterate over every dataset present, and for each one pull out epochs, losses, metrics, predictions and ground-truth.  We then generate at most five figures per dataset: (1) train/val loss, (2) train/val accuracy, (3) train/val MCC, (4) confusion matrix on test, and (5) histogram of predicted classes split by true label.  Every figure is wrapped in its own try/except so an error in one plot does not stop the rest.  If more than ~10 epochs were run we subsample to at most 5 equally-spaced x-ticks to keep figures readable.  Descriptive titles and filenames include the dataset name and plot type, and all figures are saved to the \u2018working\u2019 directory then closed.  Finally, we print the test metrics that were stored during training so the user can confirm they match the visualisations.  The code is self-contained, uses only matplotlib and numpy, and never fabricates data\u2014everything comes straight from experiment_data.npy.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["LEARNING_RATE", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["Below is a minimal script that immediately loads the saved NumPy file, extracts\nthe relevant information for the learning-rate run that achieved the highest\nvalidation MCC, and prints the final (or best) metric values for the training,\nvalidation, and test splits. It follows the directory, naming, and output-format\nrules specified in the task.", "The script will directly load `experiment_data.npy` from the `working`\ndirectory, iterate over every stored dataset (e.g., `SPR_BENCH`), and retrieve\nthe last\u2010epoch values for training and validation losses as well as accuracies,\nMCCs, and RMAs. It will also read the saved test metrics. For each dataset, it\nprints the dataset name first, followed by clearly labelled metric values such\nas \u201cfinal train loss\u201d or \u201ctest accuracy.\u201d All code is placed at the top level so\nit runs immediately when the file is executed.", "", "The script will load the numpy file from the \u201cworking\u201d directory, loop over\nevery dataset stored inside, and then compute either the final\u2010epoch value or\nthe best value (min for losses, max for scores) for every tracked metric.\nExplicit, descriptive names (e.g., \u201cfinal training MCC\u201d or \u201cbest validation\nloss\u201d) are used when printing. The code runs immediately at import time and does\nnot rely on any special entry point.", "The script loads the saved numpy dictionary, locates each dataset entry, and\nthen prints the final (or best, when appropriate) value for every recorded\nmetric with explicit, descriptive labels. This fulfills the requirement to\nreport only summary values and avoids any plotting or guarded-main boilerplate.", "The script first locates and loads the saved numpy \u201cexperiment_data\u201d dictionary,\nthen extracts the relevant losses and metric lists for the \u201cSPR_BENCH\u201d\nexperiment.  For list-type entries (training/validation), it computes the\noptimum value \u2014 minimum for losses and maximum for scores.  Single-value test\nmetrics are taken as-is.  Finally, it prints the dataset name followed by\nclearly labelled, human-readable metric summaries.", "The script will load the NumPy file from the working directory, unpack the\nstored dictionary, and iterate over every dataset key (e.g., \u201cSPR_BENCH\u201d).   For\neach dataset it will:   1. Select the last epoch entry for training metrics and\nlosses (final values).   2. Locate the epoch with the highest validation\naccuracy, returning its metrics and corresponding loss (best values).   3. Read\nthe saved test-set metrics (single final evaluation).   Finally, the script\nprints the dataset name followed by clearly labeled metric lines, complying with\nthe formatting rules and avoiding any plotting.", "The script will directly load the NumPy file from the \u201cworking\u201d directory,\nunpack it into a Python dictionary, and then iterate through every dataset it\ncontains (here only \u201cSPR_BENCH\u201d).   For each dataset, it retrieves the final\nepoch\u2019s training and validation metrics (accuracy, MCC, RMA) and losses, along\nwith the stored test metrics.   Each metric is printed with an explicit, self-\ndescribing label such as \u201ctraining accuracy\u201d or \u201cvalidation loss,\u201d preceded by\nthe dataset name.   No figures are generated, and execution starts immediately\nwithout an `if __name__ == \"__main__\":` guard.", "", "The script will load the NumPy file from the working directory, unpack the\nstored dictionary, and iterate over every dataset key (e.g., \u201cSPR_BENCH\u201d).   For\neach dataset it will:   1. Select the last epoch entry for training metrics and\nlosses (final values).   2. Locate the epoch with the highest validation\naccuracy, returning its metrics and corresponding loss (best values).   3. Read\nthe saved test-set metrics (single final evaluation).   Finally, the script\nprints the dataset name followed by clearly labeled metric lines, complying with\nthe formatting rules and avoiding any plotting.", "The script will load the NumPy file from the working directory, unpack the\nstored dictionary, and iterate over every dataset key (e.g., \u201cSPR_BENCH\u201d).   For\neach dataset it will:   1. Select the last epoch entry for training metrics and\nlosses (final values).   2. Locate the epoch with the highest validation\naccuracy, returning its metrics and corresponding loss (best values).   3. Read\nthe saved test-set metrics (single final evaluation).   Finally, the script\nprints the dataset name followed by clearly labeled metric lines, complying with\nthe formatting rules and avoiding any plotting.", "The script will load the NumPy file from the working directory, unpack the\nstored dictionary, and iterate over every dataset key (e.g., \u201cSPR_BENCH\u201d).   For\neach dataset it will:   1. Select the last epoch entry for training metrics and\nlosses (final values).   2. Locate the epoch with the highest validation\naccuracy, returning its metrics and corresponding loss (best values).   3. Read\nthe saved test-set metrics (single final evaluation).   Finally, the script\nprints the dataset name followed by clearly labeled metric lines, complying with\nthe formatting rules and avoiding any plotting.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------------\n# 0. Locate and load the saved experiment results\n# ---------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------------\n# 1. Navigate to the SPR_BENCH section and identify the best-learning-rate run\n# ---------------------------------------------------------------------------\nspr_results = experiment_data[\"LEARNING_RATE\"][\"SPR_BENCH\"]\nbest_lr = spr_results[\"best_lr\"]  # stored as float\nbest_key = f\"{best_lr:.0e}\"  # e.g. \"3e-04\"\nbest_record = spr_results[best_key]  # dict with metrics\n\n# ---------------------------------------------------------------------------\n# 2. Extract the required metric values\n# ---------------------------------------------------------------------------\n# Training metrics (take final epoch)\nfinal_train_loss = best_record[\"losses\"][\"train\"][-1]\nfinal_train_mcc = best_record[\"metrics\"][\"train_MCC\"][-1]\n\n# Validation metrics (loss: final epoch, MCC: best across epochs)\nfinal_val_loss = best_record[\"losses\"][\"val\"][-1]\nbest_val_mcc = max(best_record[\"metrics\"][\"val_MCC\"])\n\n# Test metric (single value)\ntest_mcc = spr_results[\"test_MCC\"]\n\n# ---------------------------------------------------------------------------\n# 3. Print the results with explicit dataset/metric names\n# ---------------------------------------------------------------------------\nprint(\"Training dataset\")\nprint(f\"final training loss: {final_train_loss:.4f}\")\nprint(f\"final training MCC:  {final_train_mcc:.4f}\\n\")\n\nprint(\"Validation dataset\")\nprint(f\"final validation loss: {final_val_loss:.4f}\")\nprint(f\"best validation MCC:  {best_val_mcc:.4f}\\n\")\n\nprint(\"Test dataset\")\nprint(f\"test MCC: {test_mcc:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------------\n# Load experiment data\n# ---------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------------\n# Helper to format a float safely\n# ---------------------------------------------------------------------------\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------------\n# Iterate over datasets and print metrics\n# ---------------------------------------------------------------------------\nfor ds_name, ds in experiment_data.items():\n    print(f\"\\n{ds_name}\")  # Dataset header\n\n    # --- training / validation losses -------------------------------------------------\n    train_losses = ds.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(\"final train loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"final validation loss:\", fmt(val_losses[-1]))\n\n    # --- training / validation metrics ------------------------------------------------\n    train_mets = ds.get(\"metrics\", {}).get(\"train\", [])\n    val_mets = ds.get(\"metrics\", {}).get(\"val\", [])\n    if train_mets:\n        last_train = train_mets[-1]\n        print(\"final train accuracy:\", fmt(last_train.get(\"acc\")))\n        print(\"final train MCC:\", fmt(last_train.get(\"MCC\")))\n        print(\"final train RMA:\", fmt(last_train.get(\"RMA\")))\n    if val_mets:\n        last_val = val_mets[-1]\n        print(\"final validation accuracy:\", fmt(last_val.get(\"acc\")))\n        print(\"final validation MCC:\", fmt(last_val.get(\"MCC\")))\n        print(\"final validation RMA:\", fmt(last_val.get(\"RMA\")))\n\n    # --- test metrics -----------------------------------------------------------------\n    test_mets = ds.get(\"test_metrics\", {})\n    if test_mets:\n        print(\"test loss:\", fmt(test_mets.get(\"loss\")))\n        print(\"test accuracy:\", fmt(test_mets.get(\"acc\")))\n        print(\"test MCC:\", fmt(test_mets.get(\"MCC\")))\n        print(\"test RMA:\", fmt(test_mets.get(\"RMA\")))\n", "", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the saved experiment data\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to safely fetch best / final values\n# ---------------------------------------------------------------------\ndef _safe_best(values, fn):\n    return fn(values) if values else None\n\n\ndef _safe_final(values):\n    return values[-1] if values else None\n\n\n# ---------------------------------------------------------------------\n# Iterate through every dataset stored in the file\n# ---------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- learning rate ------------------------------------------------\n    best_lr = data.get(\"best_lr\")\n    if best_lr is not None:\n        print(f\"  best learning rate: {best_lr:.0e}\")\n\n    # --- MCC metrics --------------------------------------------------\n    final_train_mcc = _safe_final(data[\"metrics\"][\"train\"])\n    best_val_mcc = _safe_best(data[\"metrics\"][\"val\"], max)\n    test_mcc = data.get(\"test_MCC\")\n\n    if final_train_mcc is not None:\n        print(f\"  final training MCC: {final_train_mcc:.4f}\")\n    if best_val_mcc is not None:\n        print(f\"  best validation MCC: {best_val_mcc:.4f}\")\n    if test_mcc is not None:\n        print(f\"  test MCC: {test_mcc:.4f}\")\n\n    # --- Loss values --------------------------------------------------\n    final_train_loss = _safe_final(data[\"losses\"][\"train\"])\n    best_val_loss = _safe_best(data[\"losses\"][\"val\"], min)\n\n    if final_train_loss is not None:\n        print(f\"  final training loss: {final_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"  best validation loss: {best_val_loss:.4f}\")\n\n    # --- Rule-Macro Accuracy (RMA) ------------------------------------\n    final_train_rma = _safe_final(data[\"RMA\"][\"train\"])\n    best_val_rma = _safe_best(data[\"RMA\"][\"val\"], max)\n    test_rma = data.get(\"test_RMA\")\n\n    if final_train_rma is not None:\n        print(f\"  final training rule-macro accuracy: {final_train_rma:.4f}\")\n    if best_val_rma is not None:\n        print(f\"  best validation rule-macro accuracy: {best_val_rma:.4f}\")\n    if test_rma is not None:\n        print(f\"  test rule-macro accuracy: {test_rma:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------------------------\n# Locate and load experiment results\n# -------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------------------------\n# Helper functions\n# -------------------------------------------------------------------------\ndef _last(lst, default=None):\n    \"\"\"Return the last element of a list or a default value if the list is empty.\"\"\"\n    return lst[-1] if lst else default\n\n\ndef _best(lst, mode=\"max\", default=None):\n    \"\"\"Return best value (max or min) from a list or a default.\"\"\"\n    if not lst:\n        return default\n    return max(lst) if mode == \"max\" else min(lst)\n\n\n# -------------------------------------------------------------------------\n# Iterate through the stored datasets and print metrics\n# -------------------------------------------------------------------------\nfor dataset_name, dataset_info in experiment_data.items():\n    print(dataset_name)  # Dataset header\n\n    metrics = dataset_info.get(\"metrics\", {})\n    losses = dataset_info.get(\"losses\", {})\n\n    # Training / validation losses\n    if \"train\" in losses:\n        print(f\"training loss: { _last(losses['train']):.4f }\")\n    if \"val\" in losses:\n        print(f\"validation loss: { _last(losses['val']):.4f }\")\n\n    # Matthews correlation coefficient (MCC)\n    if \"train_MCC\" in metrics:\n        print(\n            f\"training Matthews correlation coefficient: { _last(metrics['train_MCC']):.4f }\"\n        )\n    if \"val_MCC\" in metrics:\n        print(\n            f\"validation Matthews correlation coefficient: { _best(metrics['val_MCC'], mode='max'):.4f }\"\n        )\n    if \"test_MCC\" in metrics:\n        print(f\"test Matthews correlation coefficient: { metrics['test_MCC']:.4f }\")\n\n    # Rule-macro accuracy (RMA)\n    if \"train_RMA\" in metrics:\n        print(f\"training rule-macro accuracy: { _last(metrics['train_RMA']):.4f }\")\n    if \"val_RMA\" in metrics:\n        print(\n            f\"validation rule-macro accuracy: { _best(metrics['val_RMA'], mode='max'):.4f }\"\n        )\n    if \"test_RMA\" in metrics:\n        print(f\"test rule-macro accuracy: { metrics['test_RMA']:.4f }\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------- locate and load file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------- helper: pick best value\ndef _best(values, higher_is_better=True):\n    if not isinstance(values, (list, tuple)):\n        return values\n    if higher_is_better:\n        return max(values)\n    return min(values)\n\n\n# ------------------------------------------------------------------- extract and print metrics\nfor dataset_name, content in experiment_data.items():  # only \"SPR_BENCH\" here\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # losses\n    best_train_loss = _best(content[\"losses\"][\"train\"], higher_is_better=False)\n    best_val_loss = _best(content[\"losses\"][\"val\"], higher_is_better=False)\n    print(f\"best training loss: {best_train_loss}\")\n    print(f\"best validation loss: {best_val_loss}\")\n\n    # MCC\n    best_train_mcc = _best(content[\"metrics\"][\"train_MCC\"])\n    best_val_mcc = _best(content[\"metrics\"][\"val_MCC\"])\n    print(f\"best training MCC: {best_train_mcc}\")\n    print(f\"best validation MCC: {best_val_mcc}\")\n\n    # Rule-macro accuracy (RMA)\n    best_train_rma = _best(content[\"metrics\"][\"train_RMA\"])\n    best_val_rma = _best(content[\"metrics\"][\"val_RMA\"])\n    print(f\"best training RMA: {best_train_rma}\")\n    print(f\"best validation RMA: {best_val_rma}\")\n\n    # test metrics (single values)\n    print(f\"test MCC: {content['metrics']['test_MCC']}\")\n    print(f\"test RMA: {content['metrics']['test_RMA']}\")\n", "import os\nimport numpy as np\n\n# ---------- load data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\ndef print_metrics():\n    for ds_name, ds_dict in experiment_data.items():\n        print(ds_name)  # dataset header\n\n        # ---------- losses ----------\n        train_losses = ds_dict[\"losses\"][\"train\"]\n        val_losses = ds_dict[\"losses\"][\"val\"]\n\n        final_train_loss = train_losses[-1] if train_losses else None\n        best_val_idx = int(np.argmin(val_losses)) if val_losses else -1\n        best_val_loss = val_losses[best_val_idx] if val_losses else None\n\n        # ---------- training metrics (final epoch) ----------\n        final_train_metrics = (\n            ds_dict[\"metrics\"][\"train\"][-1] if ds_dict[\"metrics\"][\"train\"] else {}\n        )\n        # ---------- validation metrics (best accuracy) ----------\n        val_metrics_list = ds_dict[\"metrics\"][\"val\"]\n        if val_metrics_list:\n            val_accs = [m[\"acc\"] for m in val_metrics_list]\n            best_val_idx = int(np.argmax(val_accs))\n            best_val_metrics = val_metrics_list[best_val_idx]\n        else:\n            best_val_metrics = {}\n\n        # ---------- test metrics ----------\n        test_metrics = ds_dict.get(\"test_metrics\", {})\n\n        # ----- print all -----\n        if final_train_loss is not None:\n            print(f\"train loss: {final_train_loss:.4f}\")\n        if final_train_metrics:\n            print(f\"train accuracy: {final_train_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"train MCC: {final_train_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"train RMA: {final_train_metrics.get('RMA', float('nan')):.3f}\")\n\n        if best_val_loss is not None:\n            print(f\"validation loss: {best_val_loss:.4f}\")\n        if best_val_metrics:\n            print(\n                f\"validation accuracy: {best_val_metrics.get('acc', float('nan')):.3f}\"\n            )\n            print(f\"validation MCC: {best_val_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"validation RMA: {best_val_metrics.get('RMA', float('nan')):.3f}\")\n\n        if test_metrics:\n            print(f\"test loss: {test_metrics.get('loss', float('nan')):.4f}\")\n            print(f\"test accuracy: {test_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"test MCC: {test_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"test RMA: {test_metrics.get('RMA', float('nan')):.3f}\")\n        print()  # blank line between datasets\n\n\nprint_metrics()\n", "import os\nimport numpy as np\n\n\n# --------------------------------------------------------------------------- #\n# Helper to fetch final (last recorded) metric from a list\n# --------------------------------------------------------------------------- #\ndef last(lst, default=None):\n    \"\"\"Return the last element of a list or a default value.\"\"\"\n    return lst[-1] if lst else default\n\n\n# --------------------------------------------------------------------------- #\n# Load experiment data\n# --------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# --------------------------------------------------------------------------- #\n# Iterate through datasets and print final metrics\n# --------------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # Dataset header\n\n    # Final training metrics\n    train_metrics_final = last(data.get(\"metrics\", {}).get(\"train\", []), {})\n    train_loss_final = last(data.get(\"losses\", {}).get(\"train\", []), None)\n\n    if train_loss_final is not None:\n        print(f\"training loss: {train_loss_final:.6f}\")\n    for k, v in train_metrics_final.items():\n        print(f\"training {k}: {v:.6f}\")\n\n    # Final validation metrics\n    val_metrics_final = last(data.get(\"metrics\", {}).get(\"val\", []), {})\n    val_loss_final = last(data.get(\"losses\", {}).get(\"val\", []), None)\n\n    if val_loss_final is not None:\n        print(f\"validation loss: {val_loss_final:.6f}\")\n    for k, v in val_metrics_final.items():\n        print(f\"validation {k}: {v:.6f}\")\n\n    # Test metrics (single snapshot)\n    test_metrics = data.get(\"test_metrics\", {})\n    for k, v in test_metrics.items():\n        metric_name = {\n            \"loss\": \"test loss\",\n            \"acc\": \"test accuracy\",\n            \"MCC\": \"test MCC\",\n            \"RMA\": \"test RMA\",\n        }.get(k, f\"test {k}\")\n        print(f\"{metric_name}: {v:.6f}\")\n", "", "import os\nimport numpy as np\n\n# ---------- load data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\ndef print_metrics():\n    for ds_name, ds_dict in experiment_data.items():\n        print(ds_name)  # dataset header\n\n        # ---------- losses ----------\n        train_losses = ds_dict[\"losses\"][\"train\"]\n        val_losses = ds_dict[\"losses\"][\"val\"]\n\n        final_train_loss = train_losses[-1] if train_losses else None\n        best_val_idx = int(np.argmin(val_losses)) if val_losses else -1\n        best_val_loss = val_losses[best_val_idx] if val_losses else None\n\n        # ---------- training metrics (final epoch) ----------\n        final_train_metrics = (\n            ds_dict[\"metrics\"][\"train\"][-1] if ds_dict[\"metrics\"][\"train\"] else {}\n        )\n        # ---------- validation metrics (best accuracy) ----------\n        val_metrics_list = ds_dict[\"metrics\"][\"val\"]\n        if val_metrics_list:\n            val_accs = [m[\"acc\"] for m in val_metrics_list]\n            best_val_idx = int(np.argmax(val_accs))\n            best_val_metrics = val_metrics_list[best_val_idx]\n        else:\n            best_val_metrics = {}\n\n        # ---------- test metrics ----------\n        test_metrics = ds_dict.get(\"test_metrics\", {})\n\n        # ----- print all -----\n        if final_train_loss is not None:\n            print(f\"train loss: {final_train_loss:.4f}\")\n        if final_train_metrics:\n            print(f\"train accuracy: {final_train_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"train MCC: {final_train_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"train RMA: {final_train_metrics.get('RMA', float('nan')):.3f}\")\n\n        if best_val_loss is not None:\n            print(f\"validation loss: {best_val_loss:.4f}\")\n        if best_val_metrics:\n            print(\n                f\"validation accuracy: {best_val_metrics.get('acc', float('nan')):.3f}\"\n            )\n            print(f\"validation MCC: {best_val_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"validation RMA: {best_val_metrics.get('RMA', float('nan')):.3f}\")\n\n        if test_metrics:\n            print(f\"test loss: {test_metrics.get('loss', float('nan')):.4f}\")\n            print(f\"test accuracy: {test_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"test MCC: {test_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"test RMA: {test_metrics.get('RMA', float('nan')):.3f}\")\n        print()  # blank line between datasets\n\n\nprint_metrics()\n", "import os\nimport numpy as np\n\n# ---------- load data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\ndef print_metrics():\n    for ds_name, ds_dict in experiment_data.items():\n        print(ds_name)  # dataset header\n\n        # ---------- losses ----------\n        train_losses = ds_dict[\"losses\"][\"train\"]\n        val_losses = ds_dict[\"losses\"][\"val\"]\n\n        final_train_loss = train_losses[-1] if train_losses else None\n        best_val_idx = int(np.argmin(val_losses)) if val_losses else -1\n        best_val_loss = val_losses[best_val_idx] if val_losses else None\n\n        # ---------- training metrics (final epoch) ----------\n        final_train_metrics = (\n            ds_dict[\"metrics\"][\"train\"][-1] if ds_dict[\"metrics\"][\"train\"] else {}\n        )\n        # ---------- validation metrics (best accuracy) ----------\n        val_metrics_list = ds_dict[\"metrics\"][\"val\"]\n        if val_metrics_list:\n            val_accs = [m[\"acc\"] for m in val_metrics_list]\n            best_val_idx = int(np.argmax(val_accs))\n            best_val_metrics = val_metrics_list[best_val_idx]\n        else:\n            best_val_metrics = {}\n\n        # ---------- test metrics ----------\n        test_metrics = ds_dict.get(\"test_metrics\", {})\n\n        # ----- print all -----\n        if final_train_loss is not None:\n            print(f\"train loss: {final_train_loss:.4f}\")\n        if final_train_metrics:\n            print(f\"train accuracy: {final_train_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"train MCC: {final_train_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"train RMA: {final_train_metrics.get('RMA', float('nan')):.3f}\")\n\n        if best_val_loss is not None:\n            print(f\"validation loss: {best_val_loss:.4f}\")\n        if best_val_metrics:\n            print(\n                f\"validation accuracy: {best_val_metrics.get('acc', float('nan')):.3f}\"\n            )\n            print(f\"validation MCC: {best_val_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"validation RMA: {best_val_metrics.get('RMA', float('nan')):.3f}\")\n\n        if test_metrics:\n            print(f\"test loss: {test_metrics.get('loss', float('nan')):.4f}\")\n            print(f\"test accuracy: {test_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"test MCC: {test_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"test RMA: {test_metrics.get('RMA', float('nan')):.3f}\")\n        print()  # blank line between datasets\n\n\nprint_metrics()\n", "import os\nimport numpy as np\n\n# ---------- load data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\ndef print_metrics():\n    for ds_name, ds_dict in experiment_data.items():\n        print(ds_name)  # dataset header\n\n        # ---------- losses ----------\n        train_losses = ds_dict[\"losses\"][\"train\"]\n        val_losses = ds_dict[\"losses\"][\"val\"]\n\n        final_train_loss = train_losses[-1] if train_losses else None\n        best_val_idx = int(np.argmin(val_losses)) if val_losses else -1\n        best_val_loss = val_losses[best_val_idx] if val_losses else None\n\n        # ---------- training metrics (final epoch) ----------\n        final_train_metrics = (\n            ds_dict[\"metrics\"][\"train\"][-1] if ds_dict[\"metrics\"][\"train\"] else {}\n        )\n        # ---------- validation metrics (best accuracy) ----------\n        val_metrics_list = ds_dict[\"metrics\"][\"val\"]\n        if val_metrics_list:\n            val_accs = [m[\"acc\"] for m in val_metrics_list]\n            best_val_idx = int(np.argmax(val_accs))\n            best_val_metrics = val_metrics_list[best_val_idx]\n        else:\n            best_val_metrics = {}\n\n        # ---------- test metrics ----------\n        test_metrics = ds_dict.get(\"test_metrics\", {})\n\n        # ----- print all -----\n        if final_train_loss is not None:\n            print(f\"train loss: {final_train_loss:.4f}\")\n        if final_train_metrics:\n            print(f\"train accuracy: {final_train_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"train MCC: {final_train_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"train RMA: {final_train_metrics.get('RMA', float('nan')):.3f}\")\n\n        if best_val_loss is not None:\n            print(f\"validation loss: {best_val_loss:.4f}\")\n        if best_val_metrics:\n            print(\n                f\"validation accuracy: {best_val_metrics.get('acc', float('nan')):.3f}\"\n            )\n            print(f\"validation MCC: {best_val_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"validation RMA: {best_val_metrics.get('RMA', float('nan')):.3f}\")\n\n        if test_metrics:\n            print(f\"test loss: {test_metrics.get('loss', float('nan')):.4f}\")\n            print(f\"test accuracy: {test_metrics.get('acc', float('nan')):.3f}\")\n            print(f\"test MCC: {test_metrics.get('MCC', float('nan')):.3f}\")\n            print(f\"test RMA: {test_metrics.get('RMA', float('nan')):.3f}\")\n        print()  # blank line between datasets\n\n\nprint_metrics()\n", ""], "parse_term_out": ["['Training dataset', '\\n', 'final training loss: 0.6490', '\\n', 'final training\nMCC:  0.2699\\n', '\\n', 'Validation dataset', '\\n', 'final validation loss:\n0.6467', '\\n', 'best validation MCC:  0.3239\\n', '\\n', 'Test dataset', '\\n',\n'test MCC: 0.2854', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nSPR_BENCH', '\\n', 'final train loss:', ' ', '0.6204', '\\n', 'final\nvalidation loss:', ' ', '0.6286', '\\n', 'final train accuracy:', ' ', '0.6875',\n'\\n', 'final train MCC:', ' ', '0.3849', '\\n', 'final train RMA:', ' ',\n'0.6875', '\\n', 'final validation accuracy:', ' ', '0.6820', '\\n', 'final\nvalidation MCC:', ' ', '0.3643', '\\n', 'final validation RMA:', ' ', '0.6820',\n'\\n', 'test loss:', ' ', '0.6259', '\\n', 'test accuracy:', ' ', '0.6940', '\\n',\n'test MCC:', ' ', '0.3872', '\\n', 'test RMA:', ' ', '0.6940', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH', '\\n', '  best learning rate: 1e-03', '\\n', '  final training MCC:\n0.3542', '\\n', '  best validation MCC: 0.3327', '\\n', '  test MCC: 0.3332',\n'\\n', '  final training loss: 0.6329', '\\n', '  best validation loss: 0.6530',\n'\\n', '  final training rule-macro accuracy: 0.6765', '\\n', '  best validation\nrule-macro accuracy: 0.6660', '\\n', '  test rule-macro accuracy: 0.6670', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\",\nline 38, in <module>\\n    print(f\"training loss: { _last(losses[\\'train\\']):.4f\n}\")\\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError:\nInvalid format specifier \\'.4f \\' for object of type \\'float\\'\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best training loss: 0.6251595959663391', '\\n',\n'best validation loss: 0.6459335322380066', '\\n', 'best training MCC:\n0.3732892016928529', '\\n', 'best validation MCC: 0.3646283130086287', '\\n',\n'best training RMA: 0.686', '\\n', 'best validation RMA: 0.682', '\\n', 'test MCC:\n0.3831281844486896', '\\n', 'test RMA: 0.692', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train loss: 0.6211', '\\n', 'train accuracy: 0.675', '\\n',\n'train MCC: 0.350', '\\n', 'train RMA: 0.675', '\\n', 'validation loss: 0.6244',\n'\\n', 'validation accuracy: 0.692', '\\n', 'validation MCC: 0.384', '\\n',\n'validation RMA: 0.692', '\\n', 'test loss: 0.6205', '\\n', 'test accuracy:\n0.699', '\\n', 'test MCC: 0.397', '\\n', 'test RMA: 0.699', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training loss: 0.635549', '\\n', 'training acc: 0.750000',\n'\\n', 'training MCC: 0.493049', '\\n', 'training RMA: 0.750000', '\\n',\n'validation loss: 0.645914', '\\n', 'validation acc: 0.684000', '\\n', 'validation\nMCC: 0.368052', '\\n', 'validation RMA: 0.684000', '\\n', 'test loss: 0.641610',\n'\\n', 'test accuracy: 0.685000', '\\n', 'test MCC: 0.369295', '\\n', 'test RMA:\n0.685000', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH', '\\n', 'train loss: 0.6211', '\\n', 'train accuracy: 0.675', '\\n',\n'train MCC: 0.350', '\\n', 'train RMA: 0.675', '\\n', 'validation loss: 0.6244',\n'\\n', 'validation accuracy: 0.692', '\\n', 'validation MCC: 0.384', '\\n',\n'validation RMA: 0.692', '\\n', 'test loss: 0.6205', '\\n', 'test accuracy:\n0.699', '\\n', 'test MCC: 0.397', '\\n', 'test RMA: 0.699', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train loss: 0.6211', '\\n', 'train accuracy: 0.675', '\\n',\n'train MCC: 0.350', '\\n', 'train RMA: 0.675', '\\n', 'validation loss: 0.6244',\n'\\n', 'validation accuracy: 0.692', '\\n', 'validation MCC: 0.384', '\\n',\n'validation RMA: 0.692', '\\n', 'test loss: 0.6205', '\\n', 'test accuracy:\n0.699', '\\n', 'test MCC: 0.397', '\\n', 'test RMA: 0.699', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train loss: 0.6211', '\\n', 'train accuracy: 0.675', '\\n',\n'train MCC: 0.350', '\\n', 'train RMA: 0.675', '\\n', 'validation loss: 0.6244',\n'\\n', 'validation accuracy: 0.692', '\\n', 'validation MCC: 0.384', '\\n',\n'validation RMA: 0.692', '\\n', 'test loss: 0.6205', '\\n', 'test accuracy:\n0.699', '\\n', 'test MCC: 0.397', '\\n', 'test RMA: 0.699', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, "ValueError", null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, {"args": ["Invalid format specifier '.4f ' for object of type 'float'"]}, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 38, "<module>", "print(f\"training loss: { _last(losses['train']):.4f }\")"]], null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
