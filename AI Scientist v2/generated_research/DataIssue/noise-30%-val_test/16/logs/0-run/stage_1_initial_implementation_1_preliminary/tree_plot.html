<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[1, 7], [1, 6], [1, 4], [1, 5]], "layout": [[0.14285714285714285, 0.0], [0.42857142857142855, 0.0], [0.7142857142857143, 0.0], [1.0, 0.0], [0.0, 1.0], [0.2857142857142857, 1.0], [0.5714285714285714, 1.0], [0.8571428571428571, 1.0]], "plan": ["We will build a minimal yet solid baseline that treats each symbolic character\nas a token, embeds it, runs a single-layer Bi-LSTM, and predicts the\nacceptability label with a linear head.  A small vocabulary is induced from the\ntraining split, padding is handled in the collate-fn, and packed sequences let\nthe model ignore padded positions.  We train for only a few epochs with Adam,\nlog validation loss and Matthews Correlation Coefficient (MCC) every epoch, and\nfinally report MCC on the hidden test set.  All metrics/losses/predictions are\nstored in a structured experiment_data dict and dumped to the ./working folder.\nThe code respects the mandatory GPU/CPU handling rules, keeps execution outside\nany if-main guard, and runs end-to-end in a single file.", "We start with a compact character-level baseline: every input sequence is\ntokenised into individual symbols, mapped to integer IDs, padded/truncated to a\nfixed length and fed into a lightweight neural network (Embedding \u2192 Bi-LSTM \u2192\nmean-pool \u2192 Linear).  The dataset is loaded from the local SPR_BENCH folder via\nthe provided helper; if the folder is absent we fall back to a small synthetic\ndataset so the script always runs.  Training uses BCEWithLogitsLoss and Adam;\nafter each epoch we compute validation loss and Matthews Correlation Coefficient\n(MCC) and store them in the experiment_data dict.  All tensors and the model are\nmoved to GPU when available, and metrics/losses are saved as .npy files for\nlater analysis; a simple loss/MCC curve is also plotted to the working\ndirectory.  This gives a fully-functional end\u2013to-end pipeline\u2014data loading,\ntraining, evaluation and visualisation\u2014forming a solid baseline that we can\niterate on in subsequent experiments.  The code obeys all device-handling,\nsaving and execution constraints specified in the instructions and finishes in a\nfew minutes on modest hardware.", "A first solid baseline treats each symbolic sequence simply as a string of\ncharacters and learns to classify accept / reject with a lightweight neural\nnetwork.  We load train / dev / test splits via the provided utility, build a\nvocabulary from the training symbols, and encode every sequence as an integer\nlist.  A small PyTorch model \u2013 character embedding, one-layer GRU, and a sigmoid\nclassifier \u2013 is trained with BCE-with-logits loss.  Batches are padded on-the-\nfly and everything is pushed to GPU if available, satisfying the device-handling\nrequirements.  After every epoch we compute Matthews Correlation Coefficient on\nthe dev set and print the validation loss; MCC as well as losses are tracked\ninside an experiment_data dictionary that is finally saved with np.save.  We\nalso save dev-set predictions and a line plot of MCC across epochs into the\nworking directory for quick inspection.  Although simple, this end-to-end\npipeline establishes a reproducible reference point and ensures we can load\ndata, train, evaluate and visualise results before exploring more sophisticated\nSPR reasoning models.", "We will treat each SPR symbol sequence as a simple character string, build a\ncharacter-level vocabulary from the training split, and encode every sequence as\na list of integer IDs with 0 reserved for padding.  A compact PyTorch model will\nserve as the baseline: an Embedding layer (dim = 64, padding_idx = 0) feeds a\nsingle-layer bidirectional GRU (hidden = 128) whose outputs are average-pooled\nacross time and passed to a linear layer that produces class logits.  The model\nis trained for a few epochs (default = 5) with Adam and Cross-Entropy (binary or\nmulti-class detected automatically).  After every epoch we compute validation\nloss and the Matthews Correlation Coefficient (MCC) on the dev split, printing\nand storing them.  At the end we evaluate on the held-out test set, save all\nlosses/metrics/predictions/ground-truth into \u201cexperiment_data.npy\u201d, and plot the\nlearning curves into the working directory.  The script honours GPU\navailability, moves all tensors/models appropriately, pads batches with a custom\ncollate_fn, and will fall back to a tiny synthetic dataset if the real benchmark\ncannot be located so the code always runs.  This provides a clean, reproducible\nbaseline upon which more sophisticated SPR reasoning models can later be built.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- experiment data container ----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_mcc\": [], \"val_mcc\": [], \"test_mcc\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": {\"val\": [], \"test\": []},\n        \"ground_truth\": {\"val\": [], \"test\": []},\n        \"epochs\": [],\n    }\n}\n\n# ---------------- device ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- dataset loading helper (from prompt) ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# adjust the path as necessary\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_PATH\", \"./SPR_BENCH\"))\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocabulary build ----------------\ntrain_sequences = spr[\"train\"][\"sequence\"]\nvocab = {\"<pad>\": 0}\nfor seq in train_sequences:\n    for ch in seq:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq):\n    return [vocab[ch] for ch in seq]\n\n\n# -------------- custom torch dataset ---------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        # ensure int labels\n        self.labels = [int(l) for l in hf_split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = torch.tensor([len(item[\"ids\"]) for item in batch], dtype=torch.long)\n    max_len = lengths.max().item()\n    ids_padded = torch.zeros(len(batch), max_len, dtype=torch.long)  # PAD=0\n    for i, item in enumerate(batch):\n        ids_padded[i, : len(item[\"ids\"])] = item[\"ids\"]\n    labels = torch.stack([item[\"label\"] for item in batch])\n    return {\n        \"ids\": ids_padded.to(device),\n        \"lengths\": lengths.to(device),\n        \"labels\": labels.to(device),\n    }\n\n\n# -------------- dataloaders ----------------\nBATCH_SZ = 128\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=BATCH_SZ, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=BATCH_SZ, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), batch_size=BATCH_SZ, shuffle=False, collate_fn=collate\n)\n\n\n# -------------- model ----------------\nclass SequenceClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, num_classes)\n\n    def forward(self, ids, lengths):\n        emb = self.emb(ids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (h_n, _) = self.lstm(packed)\n        h_final = torch.cat([h_n[-2], h_n[-1]], dim=1)\n        return self.fc(h_final)\n\n\nmodel = SequenceClassifier(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# -------------- training loop ----------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss, preds, gts = 0.0, [], []\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"ids\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    train_loss = epoch_loss / len(train_loader.dataset)\n    train_mcc = matthews_corrcoef(gts, preds)\n\n    # validation\n    model.eval()\n    val_loss, val_preds, val_gts = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            logits = model(batch[\"ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            val_preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            val_gts.extend(batch[\"labels\"].cpu().tolist())\n    val_loss = val_loss / len(val_loader.dataset)\n    val_mcc = matthews_corrcoef(val_gts, val_preds)\n\n    # logging\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_mcc\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_mcc\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"predictions\"][\"val\"] = val_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"val\"] = val_gts\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_MCC = {val_mcc:.4f}\")\n\n# -------------- test evaluation --------------\nmodel.eval()\ntest_preds, test_gts = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        logits = model(batch[\"ids\"], batch[\"lengths\"])\n        test_preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        test_gts.extend(batch[\"labels\"].cpu().tolist())\ntest_mcc = matthews_corrcoef(test_gts, test_preds)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_mcc\"].append(test_mcc)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_gts\nprint(f\"\\nTest MCC: {test_mcc:.4f}\")\n\n# -------------- persist experiment data -------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Experiment data saved to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, time, json, math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\n\n# --- working dir -------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# --- device ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --- helper to load SPR_BENCH -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef maybe_load_real_dataset() -> DatasetDict:\n    env_path = os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    root = pathlib.Path(env_path)\n    if root.exists():\n        print(f\"Loading real SPR_BENCH from {root}\")\n        return load_spr_bench(root)\n    # ----------------- fallback synthetic data --------------------------\n    print(\"Real dataset not found, generating synthetic toy dataset...\")\n\n    def synth_split(n):\n        seqs, labels = [], []\n        syms = list(\"ABCDEFGH\")\n        for _ in range(n):\n            length = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(length))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule on 'A'\n            seqs.append(seq)\n            labels.append(label)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    dset = DatasetDict()\n    dset[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": []}, split=\"train\"\n    )  # dummy init\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        tmp = synth_split(n)\n        dset[split] = load_dataset(\"json\", data_files={\"train\": []}, split=\"train\")\n        dset[split] = dset[split].add_item(tmp)\n    # HuggingFace Dataset concat hack \u2013 simpler: convert manually:\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        d = synth_split(n)\n        dset[split] = load_dataset(\"csv\", data_files={\"train\": []}, split=\"train\")\n    # Easier: build by hand into Dataset.from_dict\n    from datasets import Dataset as HFDataset\n\n    dset = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        dset[split] = HFDataset.from_dict(synth_split(n))\n    return dset\n\n\nspr_bench = maybe_load_real_dataset()\nprint(\"Loaded splits:\", spr_bench.keys())\n\n# --- vocabulary --------------------------------------------------------------\nall_text = \"\".join(spr_bench[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nitos = {i: ch for ch, i in stoi.items()}\npad_idx = 0\nmax_len = min(40, max(len(s) for s in spr_bench[\"train\"][\"sequence\"]))\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\n# --- PyTorch dataset ---------------------------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_dataset):\n        self.seqs = hf_dataset[\"sequence\"]\n        self.labels = hf_dataset[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorch(spr_bench[\"train\"])\nval_ds = SPRTorch(spr_bench[\"dev\"])\ntest_ds = SPRTorch(spr_bench[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n\n# --- model -------------------------------------------------------------------\nclass CharBiLSTM(nn.Module):\n    def __init__(self, vocab_size, emb_dim=32, hidden=64):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size + 1, emb_dim, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)  # (B, L, E)\n        out, _ = self.lstm(emb)  # (B, L, 2H)\n        pooled = out.mean(dim=1)  # simple mean pool\n        logits = self.fc(pooled).squeeze(1)  # (B,)\n        return logits\n\n\nmodel = CharBiLSTM(len(vocab)).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --- training loop -----------------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss, preds, truths = 0.0, [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"x\"].size(0)\n        preds.extend(torch.sigmoid(logits).detach().cpu().numpy() > 0.5)\n        truths.extend(batch[\"y\"].detach().cpu().numpy())\n    train_loss = running_loss / len(train_ds)\n    train_mcc = matthews_corrcoef(truths, preds)\n\n    # ---- validation ----\n    model.eval()\n    val_loss_term, v_preds, v_truths = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            val_loss_term += loss.item() * batch[\"x\"].size(0)\n            v_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n            v_truths.extend(batch[\"y\"].cpu().numpy())\n    val_loss = val_loss_term / len(val_ds)\n    val_mcc = matthews_corrcoef(v_truths, v_preds)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_MCC={val_mcc:.4f}\"\n    )\n\n# --- final evaluation on test ------------------------------------------------\ntest_loader = DataLoader(test_ds, batch_size=256)\nmodel.eval()\nt_preds, t_truths = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"x\"])\n        t_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n        t_truths.extend(batch[\"y\"].cpu().numpy())\ntest_mcc = matthews_corrcoef(t_truths, t_preds)\nprint(f\"Test MCC: {test_mcc:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = t_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = t_truths\n\n# --- save metrics ------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --- visualisation -----------------------------------------------------------\nepochs_range = experiment_data[\"SPR_BENCH\"][\"epochs\"]\ntrain_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"]\nval_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"]\nplt.figure()\nplt.plot(epochs_range, train_loss, label=\"train_loss\")\nplt.plot(epochs_range, val_loss, label=\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nval_mccs = experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"]\nplt.figure()\nplt.plot(epochs_range, val_mccs, label=\"val_MCC\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"MCC\")\nplt.legend()\nplt.title(\"Validation MCC\")\nplt.savefig(os.path.join(working_dir, \"mcc_curve.png\"))\n", "import os, pathlib, time, json, math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import ClassLabel\nfrom sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\n\n# ---------- required working directory ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data container ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_mcc\": [], \"val_mcc\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------- load dataset ----------\nfrom SPR import load_spr_bench\n\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_DATA_DIR\", \"SPR_BENCH\"))\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Cannot find SPR_BENCH data at {DATA_PATH}. \"\n        \"Either place it there or set SPR_DATA_DIR env.\"\n    )\ndset = load_spr_bench(DATA_PATH)\nprint(\"Loaded dataset splits:\", list(dset.keys()))\n\n\n# Ensure label column is int\ndef _ensure_int(example):\n    example[\"label\"] = int(example[\"label\"])\n    return example\n\n\nfor split in dset.keys():\n    dset[split] = dset[split].map(_ensure_int)\n\n# ---------- build vocabulary ----------\nall_text = \"\".join(dset[\"train\"][\"sequence\"])\nvocab = sorted(list(set(all_text)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}  # reserve 0 for PAD\nitos = {i: c for c, i in stoi.items()}\nvocab_size = len(stoi) + 1\nprint(f\"Vocab size: {vocab_size-1}\")\n\n\ndef encode(seq):\n    return [stoi[ch] for ch in seq]\n\n\n# ---------- PyTorch Dataset wrapper ----------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx][\"sequence\"]\n        lab = self.data[idx][\"label\"]\n        return torch.tensor(encode(seq), dtype=torch.long), torch.tensor(\n            lab, dtype=torch.float\n        )\n\n\ndef collate_fn(batch):\n    seqs, labs = zip(*batch)\n    seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n    labs = torch.stack(labs)\n    return {\"seqs\": seqs_padded, \"lengths\": lengths, \"labels\": labs}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(dset[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n)\nval_loader = DataLoader(\n    SPRTorchDataset(dset[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(dset[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\n\n\n# ---------- model ----------\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_sz, embed_dim=32, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, 1)\n\n    def forward(self, x, lengths):\n        emb = self.embed(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h_n = self.gru(packed)\n        logits = self.fc(h_n.squeeze(0))\n        return logits.squeeze(1)\n\n\nmodel = CharGRU(vocab_size).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------- training loop ----------\nepochs = 10\n\n\ndef evaluate(loader):\n    model.eval()\n    preds, gts, losses = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"seqs\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"labels\"])\n            losses.append(loss.item())\n            preds.append(torch.sigmoid(logits).cpu())\n            gts.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    gts = torch.cat(gts).numpy()\n    mcc = matthews_corrcoef(gts, (preds > 0.5).astype(int))\n    return np.mean(losses), mcc, preds, gts\n\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    epoch_losses = []\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"seqs\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_losses.append(loss.item())\n\n    train_loss = np.mean(epoch_losses)\n    val_loss, val_mcc, _, _ = evaluate(val_loader)\n    train_loss_eval, train_mcc, _, _ = evaluate(train_loader)  # quick train MCC\n    print(\n        f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} \"\n        f\"val_loss={val_loss:.4f}  val_MCC={val_mcc:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_mcc\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_mcc\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ---------- final evaluation on test ----------\ntest_loss, test_mcc, test_preds, test_gts = evaluate(test_loader)\nprint(f\"Test set -- loss: {test_loss:.4f}, MCC: {test_mcc:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_mcc\"] = test_mcc\n\n# ---------- save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- visualisation ----------\nfig_path = os.path.join(working_dir, \"mcc_over_epochs.png\")\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_mcc\"],\n    label=\"Val MCC\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_mcc\"],\n    label=\"Train MCC\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MCC\")\nplt.title(\"MCC over epochs\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(fig_path)\nprint(f\"Saved MCC plot to {fig_path}\")\n", "import os, pathlib, random, math, time, json, itertools\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\n\n# ---------------------------------------------------------------------\n# mandatory working directory & experiment data container\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------------------------------------------------------------------\n# GPU / CPU handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# SPR dataset loader (from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# ---------------------------------------------------------------------\n# try loading real benchmark; otherwise make synthetic toy data\ndef get_dataset():\n    possible_paths = [\n        pathlib.Path(\"SPR_BENCH\"),\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\"),\n    ]\n    for p in possible_paths:\n        if (p / \"train.csv\").exists():\n            print(f\"Loading dataset from {p}\")\n            return load_spr_bench(p)\n    print(\"WARNING: SPR_BENCH not found, creating synthetic dataset for demo.\")\n\n    def make_split(n):\n        ids = list(range(n))\n        seqs = [\n            \"\".join(random.choice(list(\"ABCDEF\")) for _ in range(random.randint(5, 15)))\n            for _ in ids\n        ]\n        labels = [int(sum(ord(c) for c in s) % 2 == 0) for s in seqs]\n        return {\"id\": ids, \"sequence\": seqs, \"label\": labels}\n\n    d = DatasetDict()\n    d[\"train\"] = load_dataset(\n        \"json\",\n        data_files={\"train\": [json.dumps(x) for x in [make_split(5000)]]},\n        split=\"train\",\n    )\n    d[\"dev\"] = load_dataset(\n        \"json\",\n        data_files={\"train\": [json.dumps(x) for x in [make_split(1000)]]},\n        split=\"train\",\n    )\n    d[\"test\"] = load_dataset(\n        \"json\",\n        data_files={\"train\": [json.dumps(x) for x in [make_split(1000)]]},\n        split=\"train\",\n    )\n    return d\n\n\nspr = get_dataset()\n\n\n# ---------------------------------------------------------------------\n# Build vocabulary\ndef build_vocab(dataset):\n    chars = set()\n    for s in dataset[\"sequence\"]:\n        chars.update(set(s))\n    vocab = {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 is PAD\n    vocab[\"<PAD>\"] = 0\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size = {len(vocab)}\")\n\n\n# ---------------------------------------------------------------------\n# Torch dataset\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = [int(x) for x in hf_split[\"label\"]]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, s):\n        return [self.vocab[ch] for ch in s]\n\n    def __getitem__(self, idx):\n        seq = torch.tensor(self.encode(self.seqs[idx]), dtype=torch.long)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\"seq\": seq, \"label\": label}\n\n\ndef collate_fn(batch):\n    seqs = [b[\"seq\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n    max_len = max(lengths)\n    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\"seq\": padded, \"lengths\": lengths, \"label\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], vocab),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"], vocab),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(spr[\"test\"], vocab),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Number of classes: {num_classes}\")\n\n\n# ---------------------------------------------------------------------\n# Model\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, num_classes)\n\n    def forward(self, x, lengths):\n        emb = self.embed(x)  # [B,L,E]\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        out, _ = self.gru(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        out = out * mask  # zero-out paddings\n        mean = out.sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(mean)\n\n\nmodel = SPRClassifier(len(vocab), num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss() if num_classes > 2 else nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------------------------\ndef epoch_run(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, y_true, y_pred = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"seq\"], batch[\"lengths\"])\n            if num_classes > 2:\n                loss = criterion(logits, batch[\"label\"])\n                pred = torch.argmax(logits, dim=1)\n            else:\n                labels = batch[\"label\"].float().unsqueeze(1)\n                loss = criterion(logits, labels)\n                pred = (torch.sigmoid(logits) > 0.5).long().squeeze(1)\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"seq\"].size(0)\n            y_true.extend(batch[\"label\"].cpu().numpy().tolist())\n            y_pred.extend(pred.detach().cpu().numpy().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    mcc = matthews_corrcoef(y_true, y_pred) if len(set(y_true)) > 1 else 0.0\n    return avg_loss, mcc, y_true, y_pred\n\n\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_mcc, _, _ = epoch_run(train_loader, train=True)\n    val_loss, val_mcc, _, _ = epoch_run(dev_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n        f\"train_MCC={train_mcc:.3f}, val_MCC={val_mcc:.3f}\"\n    )\n\n# ---------------------------------------------------------------------\n# Final test evaluation\ntest_loss, test_mcc, y_true_test, y_pred_test = epoch_run(test_loader, train=False)\nprint(f\"\\nTEST  : loss={test_loss:.4f},  MCC={test_mcc:.3f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred_test\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true_test\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_MCC\"] = test_mcc\n\n# ---------------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------------------------------------------------------------------\n# Plotting learning curves\nplt.figure(figsize=(6, 4))\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"],\n    label=\"train loss\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"],\n    label=\"val loss\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"), dpi=150)\n\nplt.figure(figsize=(6, 4))\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"],\n    label=\"train MCC\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"],\n    label=\"val MCC\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MCC\")\nplt.legend()\nplt.title(\"MCC curve\")\nplt.savefig(os.path.join(working_dir, \"mcc_curve.png\"), dpi=150)\nprint(f\"Plots saved in {working_dir}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, time, json, math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\n\n# --- working dir -------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# --- device ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --- helper to load SPR_BENCH -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef maybe_load_real_dataset() -> DatasetDict:\n    env_path = os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    root = pathlib.Path(env_path)\n    if root.exists():\n        print(f\"Loading real SPR_BENCH from {root}\")\n        return load_spr_bench(root)\n    # ----------------- fallback synthetic data --------------------------\n    print(\"Real dataset not found, generating synthetic toy dataset...\")\n\n    def synth_split(n):\n        seqs, labels = [], []\n        syms = list(\"ABCDEFGH\")\n        for _ in range(n):\n            length = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(length))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule on 'A'\n            seqs.append(seq)\n            labels.append(label)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    dset = DatasetDict()\n    dset[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": []}, split=\"train\"\n    )  # dummy init\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        tmp = synth_split(n)\n        dset[split] = load_dataset(\"json\", data_files={\"train\": []}, split=\"train\")\n        dset[split] = dset[split].add_item(tmp)\n    # HuggingFace Dataset concat hack \u2013 simpler: convert manually:\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        d = synth_split(n)\n        dset[split] = load_dataset(\"csv\", data_files={\"train\": []}, split=\"train\")\n    # Easier: build by hand into Dataset.from_dict\n    from datasets import Dataset as HFDataset\n\n    dset = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        dset[split] = HFDataset.from_dict(synth_split(n))\n    return dset\n\n\nspr_bench = maybe_load_real_dataset()\nprint(\"Loaded splits:\", spr_bench.keys())\n\n# --- vocabulary --------------------------------------------------------------\nall_text = \"\".join(spr_bench[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nitos = {i: ch for ch, i in stoi.items()}\npad_idx = 0\nmax_len = min(40, max(len(s) for s in spr_bench[\"train\"][\"sequence\"]))\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\n# --- PyTorch dataset ---------------------------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_dataset):\n        self.seqs = hf_dataset[\"sequence\"]\n        self.labels = hf_dataset[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorch(spr_bench[\"train\"])\nval_ds = SPRTorch(spr_bench[\"dev\"])\ntest_ds = SPRTorch(spr_bench[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n\n# --- model -------------------------------------------------------------------\nclass CharBiLSTM(nn.Module):\n    def __init__(self, vocab_size, emb_dim=32, hidden=64):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size + 1, emb_dim, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)  # (B, L, E)\n        out, _ = self.lstm(emb)  # (B, L, 2H)\n        pooled = out.mean(dim=1)  # simple mean pool\n        logits = self.fc(pooled).squeeze(1)  # (B,)\n        return logits\n\n\nmodel = CharBiLSTM(len(vocab)).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --- training loop -----------------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss, preds, truths = 0.0, [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"x\"].size(0)\n        preds.extend(torch.sigmoid(logits).detach().cpu().numpy() > 0.5)\n        truths.extend(batch[\"y\"].detach().cpu().numpy())\n    train_loss = running_loss / len(train_ds)\n    train_mcc = matthews_corrcoef(truths, preds)\n\n    # ---- validation ----\n    model.eval()\n    val_loss_term, v_preds, v_truths = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            val_loss_term += loss.item() * batch[\"x\"].size(0)\n            v_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n            v_truths.extend(batch[\"y\"].cpu().numpy())\n    val_loss = val_loss_term / len(val_ds)\n    val_mcc = matthews_corrcoef(v_truths, v_preds)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_MCC={val_mcc:.4f}\"\n    )\n\n# --- final evaluation on test ------------------------------------------------\ntest_loader = DataLoader(test_ds, batch_size=256)\nmodel.eval()\nt_preds, t_truths = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"x\"])\n        t_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n        t_truths.extend(batch[\"y\"].cpu().numpy())\ntest_mcc = matthews_corrcoef(t_truths, t_preds)\nprint(f\"Test MCC: {test_mcc:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = t_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = t_truths\n\n# --- save metrics ------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --- visualisation -----------------------------------------------------------\nepochs_range = experiment_data[\"SPR_BENCH\"][\"epochs\"]\ntrain_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"]\nval_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"]\nplt.figure()\nplt.plot(epochs_range, train_loss, label=\"train_loss\")\nplt.plot(epochs_range, val_loss, label=\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nval_mccs = experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"]\nplt.figure()\nplt.plot(epochs_range, val_mccs, label=\"val_MCC\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"MCC\")\nplt.legend()\nplt.title(\"Validation MCC\")\nplt.savefig(os.path.join(working_dir, \"mcc_curve.png\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, time, json, math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\n\n# --- working dir -------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# --- device ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --- helper to load SPR_BENCH -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef maybe_load_real_dataset() -> DatasetDict:\n    env_path = os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    root = pathlib.Path(env_path)\n    if root.exists():\n        print(f\"Loading real SPR_BENCH from {root}\")\n        return load_spr_bench(root)\n    # ----------------- fallback synthetic data --------------------------\n    print(\"Real dataset not found, generating synthetic toy dataset...\")\n\n    def synth_split(n):\n        seqs, labels = [], []\n        syms = list(\"ABCDEFGH\")\n        for _ in range(n):\n            length = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(length))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule on 'A'\n            seqs.append(seq)\n            labels.append(label)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    dset = DatasetDict()\n    dset[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": []}, split=\"train\"\n    )  # dummy init\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        tmp = synth_split(n)\n        dset[split] = load_dataset(\"json\", data_files={\"train\": []}, split=\"train\")\n        dset[split] = dset[split].add_item(tmp)\n    # HuggingFace Dataset concat hack \u2013 simpler: convert manually:\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        d = synth_split(n)\n        dset[split] = load_dataset(\"csv\", data_files={\"train\": []}, split=\"train\")\n    # Easier: build by hand into Dataset.from_dict\n    from datasets import Dataset as HFDataset\n\n    dset = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        dset[split] = HFDataset.from_dict(synth_split(n))\n    return dset\n\n\nspr_bench = maybe_load_real_dataset()\nprint(\"Loaded splits:\", spr_bench.keys())\n\n# --- vocabulary --------------------------------------------------------------\nall_text = \"\".join(spr_bench[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nitos = {i: ch for ch, i in stoi.items()}\npad_idx = 0\nmax_len = min(40, max(len(s) for s in spr_bench[\"train\"][\"sequence\"]))\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\n# --- PyTorch dataset ---------------------------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_dataset):\n        self.seqs = hf_dataset[\"sequence\"]\n        self.labels = hf_dataset[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorch(spr_bench[\"train\"])\nval_ds = SPRTorch(spr_bench[\"dev\"])\ntest_ds = SPRTorch(spr_bench[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n\n# --- model -------------------------------------------------------------------\nclass CharBiLSTM(nn.Module):\n    def __init__(self, vocab_size, emb_dim=32, hidden=64):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size + 1, emb_dim, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)  # (B, L, E)\n        out, _ = self.lstm(emb)  # (B, L, 2H)\n        pooled = out.mean(dim=1)  # simple mean pool\n        logits = self.fc(pooled).squeeze(1)  # (B,)\n        return logits\n\n\nmodel = CharBiLSTM(len(vocab)).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --- training loop -----------------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss, preds, truths = 0.0, [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"x\"].size(0)\n        preds.extend(torch.sigmoid(logits).detach().cpu().numpy() > 0.5)\n        truths.extend(batch[\"y\"].detach().cpu().numpy())\n    train_loss = running_loss / len(train_ds)\n    train_mcc = matthews_corrcoef(truths, preds)\n\n    # ---- validation ----\n    model.eval()\n    val_loss_term, v_preds, v_truths = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            val_loss_term += loss.item() * batch[\"x\"].size(0)\n            v_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n            v_truths.extend(batch[\"y\"].cpu().numpy())\n    val_loss = val_loss_term / len(val_ds)\n    val_mcc = matthews_corrcoef(v_truths, v_preds)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_MCC={val_mcc:.4f}\"\n    )\n\n# --- final evaluation on test ------------------------------------------------\ntest_loader = DataLoader(test_ds, batch_size=256)\nmodel.eval()\nt_preds, t_truths = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"x\"])\n        t_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n        t_truths.extend(batch[\"y\"].cpu().numpy())\ntest_mcc = matthews_corrcoef(t_truths, t_preds)\nprint(f\"Test MCC: {test_mcc:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = t_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = t_truths\n\n# --- save metrics ------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --- visualisation -----------------------------------------------------------\nepochs_range = experiment_data[\"SPR_BENCH\"][\"epochs\"]\ntrain_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"]\nval_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"]\nplt.figure()\nplt.plot(epochs_range, train_loss, label=\"train_loss\")\nplt.plot(epochs_range, val_loss, label=\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nval_mccs = experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"]\nplt.figure()\nplt.plot(epochs_range, val_mccs, label=\"val_MCC\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"MCC\")\nplt.legend()\nplt.title(\"Validation MCC\")\nplt.savefig(os.path.join(working_dir, \"mcc_curve.png\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, time, json, math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\n\n# --- working dir -------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_MCC\": [], \"val_MCC\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# --- device ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --- helper to load SPR_BENCH -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef maybe_load_real_dataset() -> DatasetDict:\n    env_path = os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    root = pathlib.Path(env_path)\n    if root.exists():\n        print(f\"Loading real SPR_BENCH from {root}\")\n        return load_spr_bench(root)\n    # ----------------- fallback synthetic data --------------------------\n    print(\"Real dataset not found, generating synthetic toy dataset...\")\n\n    def synth_split(n):\n        seqs, labels = [], []\n        syms = list(\"ABCDEFGH\")\n        for _ in range(n):\n            length = random.randint(5, 12)\n            seq = \"\".join(random.choice(syms) for _ in range(length))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule on 'A'\n            seqs.append(seq)\n            labels.append(label)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    dset = DatasetDict()\n    dset[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": []}, split=\"train\"\n    )  # dummy init\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        tmp = synth_split(n)\n        dset[split] = load_dataset(\"json\", data_files={\"train\": []}, split=\"train\")\n        dset[split] = dset[split].add_item(tmp)\n    # HuggingFace Dataset concat hack \u2013 simpler: convert manually:\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        d = synth_split(n)\n        dset[split] = load_dataset(\"csv\", data_files={\"train\": []}, split=\"train\")\n    # Easier: build by hand into Dataset.from_dict\n    from datasets import Dataset as HFDataset\n\n    dset = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 500), (\"test\", 500)]:\n        dset[split] = HFDataset.from_dict(synth_split(n))\n    return dset\n\n\nspr_bench = maybe_load_real_dataset()\nprint(\"Loaded splits:\", spr_bench.keys())\n\n# --- vocabulary --------------------------------------------------------------\nall_text = \"\".join(spr_bench[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nitos = {i: ch for ch, i in stoi.items()}\npad_idx = 0\nmax_len = min(40, max(len(s) for s in spr_bench[\"train\"][\"sequence\"]))\n\n\ndef encode(seq):\n    ids = [stoi.get(c, 0) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\n# --- PyTorch dataset ---------------------------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_dataset):\n        self.seqs = hf_dataset[\"sequence\"]\n        self.labels = hf_dataset[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorch(spr_bench[\"train\"])\nval_ds = SPRTorch(spr_bench[\"dev\"])\ntest_ds = SPRTorch(spr_bench[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n\n# --- model -------------------------------------------------------------------\nclass CharBiLSTM(nn.Module):\n    def __init__(self, vocab_size, emb_dim=32, hidden=64):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size + 1, emb_dim, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)  # (B, L, E)\n        out, _ = self.lstm(emb)  # (B, L, 2H)\n        pooled = out.mean(dim=1)  # simple mean pool\n        logits = self.fc(pooled).squeeze(1)  # (B,)\n        return logits\n\n\nmodel = CharBiLSTM(len(vocab)).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --- training loop -----------------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss, preds, truths = 0.0, [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"x\"].size(0)\n        preds.extend(torch.sigmoid(logits).detach().cpu().numpy() > 0.5)\n        truths.extend(batch[\"y\"].detach().cpu().numpy())\n    train_loss = running_loss / len(train_ds)\n    train_mcc = matthews_corrcoef(truths, preds)\n\n    # ---- validation ----\n    model.eval()\n    val_loss_term, v_preds, v_truths = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            val_loss_term += loss.item() * batch[\"x\"].size(0)\n            v_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n            v_truths.extend(batch[\"y\"].cpu().numpy())\n    val_loss = val_loss_term / len(val_ds)\n    val_mcc = matthews_corrcoef(v_truths, v_preds)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_MCC\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"].append(val_mcc)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_MCC={val_mcc:.4f}\"\n    )\n\n# --- final evaluation on test ------------------------------------------------\ntest_loader = DataLoader(test_ds, batch_size=256)\nmodel.eval()\nt_preds, t_truths = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"x\"])\n        t_preds.extend(torch.sigmoid(logits).cpu().numpy() > 0.5)\n        t_truths.extend(batch[\"y\"].cpu().numpy())\ntest_mcc = matthews_corrcoef(t_truths, t_preds)\nprint(f\"Test MCC: {test_mcc:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_MCC\"] = test_mcc\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = t_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = t_truths\n\n# --- save metrics ------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --- visualisation -----------------------------------------------------------\nepochs_range = experiment_data[\"SPR_BENCH\"][\"epochs\"]\ntrain_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"]\nval_loss = experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"]\nplt.figure()\nplt.plot(epochs_range, train_loss, label=\"train_loss\")\nplt.plot(epochs_range, val_loss, label=\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nval_mccs = experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_MCC\"]\nplt.figure()\nplt.plot(epochs_range, val_mccs, label=\"val_MCC\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"MCC\")\nplt.legend()\nplt.title(\"Validation MCC\")\nplt.savefig(os.path.join(working_dir, \"mcc_curve.png\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 47, in <module>\\n    spr = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 39, in load_spr_bench\\n\ndset[\"train\"] = _load(\"train.csv\")\\n                    ^^^^^^^^^^^^^^^^^^\\n\nFile \"runfile.py\", line 31, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n55_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 128289.72\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 120505.20\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 149258.18\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train', 'dev',\n'test'])\", '\\n', 'Epoch 1: train_loss=0.6937, val_loss=0.6898, val_MCC=0.2113',\n'\\n', 'Epoch 2: train_loss=0.6862, val_loss=0.6816, val_MCC=0.2432', '\\n',\n'Epoch 3: train_loss=0.6678, val_loss=0.6784, val_MCC=0.1925', '\\n', 'Epoch 4:\ntrain_loss=0.6644, val_loss=0.6616, val_MCC=0.2843', '\\n', 'Epoch 5:\ntrain_loss=0.6608, val_loss=0.6618, val_MCC=0.2525', '\\n', 'Test MCC: 0.2614',\n'\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 31, in <module>\\n    from SPR import\nload_spr_bench\\nModuleNotFoundError: No module named \\'SPR\\'\\n', 'Execution\ntime: a second seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading dataset from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 158769.91\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 92065.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 141532.11\nexamples/s]', '\\n', 'Vocab size = 10', '\\n', 'Number of classes: 2', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 225, in\n<module>\\n    train_loss, train_mcc, _, _ = epoch_run(train_loader,\ntrain=True)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 209, in\nepoch_run\\n    loss = criterion(logits, labels)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/loss.py\", line 819, in forward\\n    return\nF.binary_cross_entropy_with_logits(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/functional.py\", line 3624, in\nbinary_cross_entropy_with_logits\\n    raise ValueError(\\nValueError: Target size\n(torch.Size([128, 1])) must be the same as input size (torch.Size([128, 2]))\\n',\n'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 191792.22\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 136409.00\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 225864.51\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train', 'dev',\n'test'])\", '\\n', 'Epoch 1: train_loss=0.6908, val_loss=0.6852, val_MCC=0.2815',\n'\\n', 'Epoch 2: train_loss=0.6799, val_loss=0.6699, val_MCC=0.2395', '\\n',\n'Epoch 3: train_loss=0.6643, val_loss=0.6549, val_MCC=0.2488', '\\n', 'Epoch 4:\ntrain_loss=0.6542, val_loss=0.6569, val_MCC=0.2471', '\\n', 'Epoch 5:\ntrain_loss=0.6516, val_loss=0.6538, val_MCC=0.2880', '\\n', 'Test MCC: 0.2495',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 174287.01\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 127509.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 154925.72\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train', 'dev',\n'test'])\", '\\n', 'Epoch 1: train_loss=0.6912, val_loss=0.6853, val_MCC=0.2136',\n'\\n', 'Epoch 2: train_loss=0.6800, val_loss=0.6709, val_MCC=0.2816', '\\n',\n'Epoch 3: train_loss=0.6644, val_loss=0.6602, val_MCC=0.2591', '\\n', 'Epoch 4:\ntrain_loss=0.6509, val_loss=0.6605, val_MCC=0.2800', '\\n', 'Epoch 5:\ntrain_loss=0.6478, val_loss=0.6548, val_MCC=0.2801', '\\n', 'Test MCC: 0.2815',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train',\n'dev', 'test'])\", '\\n', 'Epoch 1: train_loss=0.6889, val_loss=0.6837,\nval_MCC=0.2225', '\\n', 'Epoch 2: train_loss=0.6790, val_loss=0.6668,\nval_MCC=0.2474', '\\n', 'Epoch 3: train_loss=0.6626, val_loss=0.6590,\nval_MCC=0.2520', '\\n', 'Epoch 4: train_loss=0.6568, val_loss=0.6583,\nval_MCC=0.2513', '\\n', 'Epoch 5: train_loss=0.6519, val_loss=0.6592,\nval_MCC=0.2566', '\\n', 'Test MCC: 0.2435', '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", ""], "analysis": ["The execution failed due to a FileNotFoundError. The code attempted to load the\ndataset from a path '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-\n47-55_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH/train.csv', but the file does not exist at this location. To fix\nthis, ensure the dataset files ('train.csv', 'dev.csv', 'test.csv') are\ncorrectly placed in the specified directory. Alternatively, update the DATA_PATH\nvariable to point to the correct dataset location.", "", "The execution failed due to a missing module error: 'ModuleNotFoundError: No\nmodule named 'SPR''. This indicates that the Python interpreter could not find\nthe 'SPR' module, which is required for the script to run. To fix this issue,\nensure that the 'SPR.py' file is in the Python path or the current working\ndirectory. Alternatively, you can add the directory containing 'SPR.py' to the\nPython path or directly specify its location using `sys.path.append`.", "The execution failed due to a mismatch in tensor sizes when calculating the loss\nduring training. Specifically, the target tensor size was (128, 1) while the\ninput tensor size was (128, 2). This issue arises because the model outputs\nlogits of size (batch_size, num_classes), but the labels are expected to have\nthe same shape. Since the task is binary classification, the labels should be\none-dimensional or the logits should be reduced to a single value per sample. To\nfix this, the logits tensor should be adjusted to output a single value per\nsample (e.g., using logits[:, 1] or logits.squeeze(-1)) when using\nBCEWithLogitsLoss for binary classification.", "The training script executed successfully, and the output indicates that the\nsystem is functioning as intended. The model was trained for 5 epochs, and both\ntraining and validation losses decreased over time, though the improvement in\nMatthews Correlation Coefficient (MCC) was modest. The final test MCC was\n0.2495, which suggests that the model's performance can be improved further with\nbetter tuning or architectural changes. There are no apparent bugs in the\nimplementation.", "The training script executed successfully without any errors or bugs. The model\nwas trained on the SPR_BENCH dataset and achieved a Test MCC of 0.2815 after 5\nepochs. The loss curves and validation MCC were also saved for further analysis.\nWhile the MCC values are low, this might be due to the simplicity of the model\nor the nature of the dataset rather than a bug in the code.", "The training script executed successfully without any errors or bugs. The model\nwas trained on the SPR_BENCH dataset, achieving a Test MCC of 0.2435. While the\nMCC score indicates that the model's performance could be improved, this is not\na bug but rather a reflection of the model's current capability and the\ncomplexity of the task. The implementation appears to be functionally correct,\nand the results are consistent with expectations for a preliminary\nimplementation.", ""], "exc_type": ["FileNotFoundError", null, "ModuleNotFoundError", "ValueError", null, null, null, null], "exc_info": [{"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-1/SPR_BENCH/train.csv'"]}, null, {"args": ["No module named 'SPR'"], "name": "SPR", "msg": "No module named 'SPR'"}, {"args": ["Target size (torch.Size([128, 1])) must be the same as input size (torch.Size([128, 2]))"]}, null, null, null, null], "exc_stack": [[["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 47, "<module>", "spr = load_spr_bench(DATA_PATH)"], ["runfile.py", 39, "load_spr_bench", "dset[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 31, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 31, "<module>", "from SPR import load_spr_bench"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 225, "<module>", "train_loss, train_mcc, _, _ = epoch_run(train_loader, train=True)"], ["runfile.py", 209, "epoch_run", "loss = criterion(logits, labels)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/loss.py", 819, "forward", "return F.binary_cross_entropy_with_logits("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/functional.py", 3624, "binary_cross_entropy_with_logits", "raise ValueError("]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6608, "best_value": 0.6608}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6616, "best_value": 0.6616}]}, {"metric_name": "Matthews correlation coefficient", "lower_is_better": false, "description": "A metric for the quality of binary classifications.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2614, "best_value": 0.2843}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6516, "best_value": 0.6516}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6538, "best_value": 0.6538}]}, {"metric_name": "training Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient calculated during training. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2817, "best_value": 0.2817}]}, {"metric_name": "validation Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient calculated during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.288, "best_value": 0.288}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient calculated on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2495, "best_value": 0.2495}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Minimum loss value during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6478, "best_value": 0.6478}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Minimum loss value during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6548, "best_value": 0.6548}]}, {"metric_name": "training Matthews correlation coefficient", "lower_is_better": false, "description": "Maximum Matthews correlation coefficient during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3161, "best_value": 0.3161}]}, {"metric_name": "validation Matthews correlation coefficient", "lower_is_better": false, "description": "Maximum Matthews correlation coefficient during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2816, "best_value": 0.2816}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "Final Matthews correlation coefficient on the test data", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2815, "best_value": 0.2815}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6519, "best_value": 0.6519}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6583, "best_value": 0.6583}]}, {"metric_name": "training Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2858, "best_value": 0.2858}]}, {"metric_name": "validation Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2566, "best_value": 0.2566}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient during test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2435, "best_value": 0.2435}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/loss_curve.png", "../../logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/mcc_curve.png", "../../logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_MCC_curve.png", "../../logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_confusion_matrix.png"], [], [], ["../../logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/loss_curve.png", "../../logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/mcc_curve.png", "../../logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_MCC_curve.png", "../../logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/loss_curve.png", "../../logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/mcc_curve.png", "../../logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_MCC_curve.png", "../../logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/loss_curve.png", "../../logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/mcc_curve.png", "../../logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_MCC_curve.png", "../../logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_47f94f06368c41dbb149869da3e49dc4/SPR_BENCH_agg_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_47f94f06368c41dbb149869da3e49dc4/SPR_BENCH_agg_MCC_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_47f94f06368c41dbb149869da3e49dc4/SPR_BENCH_agg_confusion_matrix.png"]], "plot_paths": [[], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/mcc_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_MCC_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_confusion_matrix.png"], [], [], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/mcc_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_MCC_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/mcc_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_MCC_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/mcc_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_MCC_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_47f94f06368c41dbb149869da3e49dc4/SPR_BENCH_agg_loss_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_47f94f06368c41dbb149869da3e49dc4/SPR_BENCH_agg_MCC_curve.png", "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_47f94f06368c41dbb149869da3e49dc4/SPR_BENCH_agg_confusion_matrix.png"]], "plot_analyses": [[], [{"analysis": "The loss curve shows a consistent decrease in both training and validation loss over the epochs, indicating that the model is learning effectively. The validation loss is slightly lower than the training loss towards the later epochs, which might suggest a slight overfitting or better generalization on the validation set.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/loss_curve.png"}, {"analysis": "The validation MCC (Matthews Correlation Coefficient) curve shows fluctuations, with a significant drop at epoch 3 followed by a sharp increase at epoch 4. This indicates that the model's performance on the validation set is inconsistent and may need further tuning or additional regularization to stabilize.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/mcc_curve.png"}, {"analysis": "This loss curve mirrors the first one, showing a steady reduction in both training and validation loss. The consistent downward trend suggests that the model is learning effectively. However, the gap between training and validation loss should be monitored to avoid overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_loss_curve.png"}, {"analysis": "The MCC curve for both training and validation shows an upward trend overall, indicating improving performance. However, the fluctuations in validation MCC suggest that the model's generalization capability is not yet stable and may require additional optimization or regularization techniques.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_MCC_curve.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's performance on the test set. It shows 312 true positives, 318 true negatives, 168 false positives, and 202 false negatives. The relatively high false negative count indicates that the model struggles more with identifying positive cases accurately, which could be addressed through rebalancing the dataset or using different loss functions.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_70f6ed064d4443649c173ff39fe038eb_proc_3327622/SPR_BENCH_confusion_matrix.png"}], [], [], [{"analysis": "The plot displays the loss curves for both training and validation sets over five epochs. The training loss decreases steadily, indicating that the model is learning from the training data. The validation loss also decreases initially but plateaus and slightly increases in later epochs, suggesting potential overfitting as the model starts to memorize the training data.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/loss_curve.png"}, {"analysis": "This plot shows the validation Matthews Correlation Coefficient (MCC) over five epochs. The MCC initially decreases after the first epoch but then increases significantly, indicating an improvement in the model's ability to correctly classify sequences governed by the rules as training progresses. However, the initial drop suggests that the model may struggle with generalization early in training.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/mcc_curve.png"}, {"analysis": "The plot demonstrates the loss curves for training and validation sets specific to the SPR_BENCH dataset. Both curves show a decreasing trend, with the validation loss slightly higher than the training loss. This trend suggests that the model is learning effectively, but the slight divergence between the two curves indicates a possible overfitting issue.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot illustrates the Matthews Correlation Coefficient (MCC) for both training and validation sets. The training MCC increases steadily, while the validation MCC fluctuates but shows an upward trend overall. This indicates that the model is improving its classification ability, although the fluctuations in validation MCC suggest some instability in generalization.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_MCC_curve.png"}, {"analysis": "The confusion matrix for the test set shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The high TP and TN counts indicate good classification performance, but the presence of a considerable number of FP and FN shows that there is room for improvement in the model's precision and recall.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss curves over epochs. Both losses decrease steadily, indicating that the model is learning effectively. The validation loss is closely following the training loss, suggesting minimal overfitting and good generalization performance. However, the relatively small difference between the two losses indicates that further optimization might be needed to reduce both losses further.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/loss_curve.png"}, {"analysis": "This plot depicts the Matthews Correlation Coefficient (MCC) for the validation set over epochs. The MCC increases significantly in the first two epochs, then stabilizes, indicating that the model's predictive performance improves initially and then plateaus. This suggests that the model is learning meaningful patterns, but further tuning or additional data might be required to achieve higher MCC values.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/mcc_curve.png"}, {"analysis": "This plot reiterates the loss curves for the SPR_BENCH dataset, showing a similar trend of decreasing training and validation losses. The alignment between the two curves indicates that the model is not overfitting and is generalizing well to the validation set. The steady decline in loss suggests that the model is successfully learning from the data.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot shows the Matthews Correlation Coefficient (MCC) for both training and validation sets over epochs. The training MCC increases steadily, while the validation MCC initially rises, then stabilizes. The close alignment between the two curves indicates that the model generalizes well, but the plateau in validation MCC suggests that further improvements in model architecture or training strategy may be needed to enhance performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_MCC_curve.png"}, {"analysis": "This confusion matrix provides a breakdown of the model's performance on the test set. The number of true positives (TP) and true negatives (TN) is relatively high, indicating that the model is correctly classifying many instances. However, the false positives (FP) and false negatives (FN) are also notable, suggesting room for improvement in reducing misclassifications. Balancing precision and recall could help address these issues.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the loss curves for both training and validation datasets over five epochs. The training loss consistently decreases, indicating that the model is learning from the data. The validation loss also decreases initially but starts to plateau and slightly increase around epoch 4, suggesting possible overfitting or a need for further tuning of hyperparameters.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/loss_curve.png"}, {"analysis": "This plot illustrates the Matthews Correlation Coefficient (MCC) for validation data over five epochs. The MCC improves steadily, demonstrating that the model's predictions are becoming more aligned with the true labels. However, the relatively low MCC values indicate that there is still significant room for improvement in model performance.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/mcc_curve.png"}, {"analysis": "This plot is another representation of the loss curves for the training and validation datasets over five epochs. Both curves show a downward trend, with the validation loss plateauing around epoch 4. This reinforces the observation of potential overfitting.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot presents the MCC curves for both training and validation datasets over five epochs. The MCC for the training data improves steadily, while the validation MCC shows a similar trend but plateaus slightly after epoch 3. The gap between the training and validation MCCs suggests that the model might not generalize perfectly to unseen data.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_MCC_curve.png"}, {"analysis": "This confusion matrix provides a breakdown of the model's performance on the test dataset. The model has a high number of true positives (395) and true negatives (225), but also a significant number of false positives (261) and false negatives (119). This indicates that while the model is making correct predictions, it is also prone to both types of errors, which could be addressed through further optimization or balancing of the dataset.", "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["[]", "The provided plots indicate that the model is learning effectively, as seen from\nthe decreasing loss curves. However, the fluctuations in the validation MCC and\nthe high false negatives in the confusion matrix suggest areas for improvement,\nsuch as stabilizing generalization performance and addressing class imbalances.", "[]", "[]", "The plots indicate that the model is learning effectively but shows signs of\noverfitting and instability in validation performance. The confusion matrix\nhighlights areas for improvement in precision and recall.", "The plots indicate that the model is learning effectively, with both loss and\nMCC metrics showing improvement over epochs. The loss curves suggest good\ngeneralization, and the MCC curves demonstrate meaningful learning, though\nfurther optimization might be needed to enhance performance. The confusion\nmatrix highlights reasonable classification accuracy but points to areas for\nreducing misclassification errors.", "The plots provide a comprehensive view of the model's performance during\ntraining and testing. The loss curves suggest that the model is learning\neffectively, though potential overfitting is observed. The MCC curves indicate\nsteady improvement but highlight a need for better generalization. The confusion\nmatrix reveals areas for optimization in reducing false positives and false\nnegatives.", "[]"], "exec_time": [1.2372732162475586, 3.2222611904144287, 1.0702440738677979, 3.4236559867858887, 4.971063137054443, 4.099795341491699, 4.228419780731201, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], ["[]"], [], [], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --- setup -------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nepochs = data[\"epochs\"]\ntrain_loss = data[\"losses\"][\"train\"]\nval_loss = data[\"losses\"][\"val\"]\ntrain_mcc = data[\"metrics\"][\"train_MCC\"]\nval_mcc = data[\"metrics\"][\"val_MCC\"]\ntest_pred = np.array(data[\"predictions\"])\ntest_true = np.array(data[\"ground_truth\"])\n\n# 1) loss curve ---------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) MCC curve ----------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_mcc, label=\"Train MCC\")\n    plt.plot(epochs, val_mcc, label=\"Validation MCC\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Corr. Coef.\")\n    plt.title(\"SPR_BENCH \u2013 MCC Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_MCC_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curve: {e}\")\n    plt.close()\n\n# 3) Test confusion matrix (bar) ---------------------------------------------\ntry:\n    tp = int(((test_pred == 1) & (test_true == 1)).sum())\n    tn = int(((test_pred == 0) & (test_true == 0)).sum())\n    fp = int(((test_pred == 1) & (test_true == 0)).sum())\n    fn = int(((test_pred == 0) & (test_true == 1)).sum())\n    counts = [tp, tn, fp, fn]\n    labels = [\"TP\", \"TN\", \"FP\", \"FN\"]\n    plt.figure()\n    plt.bar(labels, counts, color=[\"green\", \"blue\", \"orange\", \"red\"])\n    for idx, c in enumerate(counts):\n        plt.text(idx, c + 0.5, str(c), ha=\"center\")\n    plt.ylabel(\"Count\")\n    plt.title(\"SPR_BENCH \u2013 Test Confusion Matrix\")\n    save_path = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# quick console confirmation\nprint(\"Plotted epochs:\", epochs)\nprint(\"Train loss:\", train_loss)\nprint(\"Val loss:\", val_loss)\nprint(\"Train MCC:\", train_mcc)\nprint(\"Val MCC:\", val_mcc)\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --- setup -------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nepochs = data[\"epochs\"]\ntrain_loss = data[\"losses\"][\"train\"]\nval_loss = data[\"losses\"][\"val\"]\ntrain_mcc = data[\"metrics\"][\"train_MCC\"]\nval_mcc = data[\"metrics\"][\"val_MCC\"]\ntest_pred = np.array(data[\"predictions\"])\ntest_true = np.array(data[\"ground_truth\"])\n\n# 1) loss curve ---------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) MCC curve ----------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_mcc, label=\"Train MCC\")\n    plt.plot(epochs, val_mcc, label=\"Validation MCC\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Corr. Coef.\")\n    plt.title(\"SPR_BENCH \u2013 MCC Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_MCC_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curve: {e}\")\n    plt.close()\n\n# 3) Test confusion matrix (bar) ---------------------------------------------\ntry:\n    tp = int(((test_pred == 1) & (test_true == 1)).sum())\n    tn = int(((test_pred == 0) & (test_true == 0)).sum())\n    fp = int(((test_pred == 1) & (test_true == 0)).sum())\n    fn = int(((test_pred == 0) & (test_true == 1)).sum())\n    counts = [tp, tn, fp, fn]\n    labels = [\"TP\", \"TN\", \"FP\", \"FN\"]\n    plt.figure()\n    plt.bar(labels, counts, color=[\"green\", \"blue\", \"orange\", \"red\"])\n    for idx, c in enumerate(counts):\n        plt.text(idx, c + 0.5, str(c), ha=\"center\")\n    plt.ylabel(\"Count\")\n    plt.title(\"SPR_BENCH \u2013 Test Confusion Matrix\")\n    save_path = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# quick console confirmation\nprint(\"Plotted epochs:\", epochs)\nprint(\"Train loss:\", train_loss)\nprint(\"Val loss:\", val_loss)\nprint(\"Train MCC:\", train_mcc)\nprint(\"Val MCC:\", val_mcc)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --- setup -------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nepochs = data[\"epochs\"]\ntrain_loss = data[\"losses\"][\"train\"]\nval_loss = data[\"losses\"][\"val\"]\ntrain_mcc = data[\"metrics\"][\"train_MCC\"]\nval_mcc = data[\"metrics\"][\"val_MCC\"]\ntest_pred = np.array(data[\"predictions\"])\ntest_true = np.array(data[\"ground_truth\"])\n\n# 1) loss curve ---------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) MCC curve ----------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_mcc, label=\"Train MCC\")\n    plt.plot(epochs, val_mcc, label=\"Validation MCC\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Corr. Coef.\")\n    plt.title(\"SPR_BENCH \u2013 MCC Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_MCC_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curve: {e}\")\n    plt.close()\n\n# 3) Test confusion matrix (bar) ---------------------------------------------\ntry:\n    tp = int(((test_pred == 1) & (test_true == 1)).sum())\n    tn = int(((test_pred == 0) & (test_true == 0)).sum())\n    fp = int(((test_pred == 1) & (test_true == 0)).sum())\n    fn = int(((test_pred == 0) & (test_true == 1)).sum())\n    counts = [tp, tn, fp, fn]\n    labels = [\"TP\", \"TN\", \"FP\", \"FN\"]\n    plt.figure()\n    plt.bar(labels, counts, color=[\"green\", \"blue\", \"orange\", \"red\"])\n    for idx, c in enumerate(counts):\n        plt.text(idx, c + 0.5, str(c), ha=\"center\")\n    plt.ylabel(\"Count\")\n    plt.title(\"SPR_BENCH \u2013 Test Confusion Matrix\")\n    save_path = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# quick console confirmation\nprint(\"Plotted epochs:\", epochs)\nprint(\"Train loss:\", train_loss)\nprint(\"Val loss:\", val_loss)\nprint(\"Train MCC:\", train_mcc)\nprint(\"Val MCC:\", val_mcc)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --- setup -------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nepochs = data[\"epochs\"]\ntrain_loss = data[\"losses\"][\"train\"]\nval_loss = data[\"losses\"][\"val\"]\ntrain_mcc = data[\"metrics\"][\"train_MCC\"]\nval_mcc = data[\"metrics\"][\"val_MCC\"]\ntest_pred = np.array(data[\"predictions\"])\ntest_true = np.array(data[\"ground_truth\"])\n\n# 1) loss curve ---------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) MCC curve ----------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_mcc, label=\"Train MCC\")\n    plt.plot(epochs, val_mcc, label=\"Validation MCC\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Corr. Coef.\")\n    plt.title(\"SPR_BENCH \u2013 MCC Curve\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_MCC_curve.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curve: {e}\")\n    plt.close()\n\n# 3) Test confusion matrix (bar) ---------------------------------------------\ntry:\n    tp = int(((test_pred == 1) & (test_true == 1)).sum())\n    tn = int(((test_pred == 0) & (test_true == 0)).sum())\n    fp = int(((test_pred == 1) & (test_true == 0)).sum())\n    fn = int(((test_pred == 0) & (test_true == 1)).sum())\n    counts = [tp, tn, fp, fn]\n    labels = [\"TP\", \"TN\", \"FP\", \"FN\"]\n    plt.figure()\n    plt.bar(labels, counts, color=[\"green\", \"blue\", \"orange\", \"red\"])\n    for idx, c in enumerate(counts):\n        plt.text(idx, c + 0.5, str(c), ha=\"center\")\n    plt.ylabel(\"Count\")\n    plt.title(\"SPR_BENCH \u2013 Test Confusion Matrix\")\n    save_path = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# quick console confirmation\nprint(\"Plotted epochs:\", epochs)\nprint(\"Train loss:\", train_loss)\nprint(\"Val loss:\", val_loss)\nprint(\"Train MCC:\", train_mcc)\nprint(\"Val MCC:\", val_mcc)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport math\n\n# ------------------------------------------------------------------ setup ---\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- add here every experiment_data.npy path that was provided -------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3b0e5fc9f3d0490db9fe94540ade17f9_proc_3327621/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0756859685e545839334dda4058b0025_proc_3327623/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_efa10bfdb5d14392acfe951e5d3c67d4_proc_3327624/experiment_data.npy\",\n]\n\n# ----------------------------------------------------------------- loading --\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp_d = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_d)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif len(all_experiment_data) == 0:\n    raise SystemExit(\"No experiment data could be loaded.\")\n\n# -------------------------------------------------------------- aggregation --\n# discover shared dataset names\ndataset_names = set.intersection(*[set(ed.keys()) for ed in all_experiment_data])\n\nfor dset in dataset_names:\n    # Gather per-run tensors\n    loss_train_runs, loss_val_runs = [], []\n    mcc_train_runs, mcc_val_runs = [], []\n    epochs_ref = None\n    cm_counts = []  # TP,TN,FP,FN per run\n\n    for run_idx, ed in enumerate(all_experiment_data):\n        try:\n            data = ed[dset]\n            epochs = np.asarray(data[\"epochs\"])\n            if epochs_ref is None:\n                epochs_ref = epochs\n            # only keep runs that have identical epoch length\n            if len(epochs) != len(epochs_ref):\n                print(f\"Skipping run {run_idx} for {dset}: mismatched epochs.\")\n                continue\n            loss_train_runs.append(np.asarray(data[\"losses\"][\"train\"]))\n            loss_val_runs.append(np.asarray(data[\"losses\"][\"val\"]))\n            mcc_train_runs.append(np.asarray(data[\"metrics\"][\"train_MCC\"]))\n            mcc_val_runs.append(np.asarray(data[\"metrics\"][\"val_MCC\"]))\n\n            # confusion matrix\n            preds = np.asarray(data[\"predictions\"])\n            gts = np.asarray(data[\"ground_truth\"])\n            tp = int(((preds == 1) & (gts == 1)).sum())\n            tn = int(((preds == 0) & (gts == 0)).sum())\n            fp = int(((preds == 1) & (gts == 0)).sum())\n            fn = int(((preds == 0) & (gts == 1)).sum())\n            cm_counts.append([tp, tn, fp, fn])\n        except Exception as e:\n            print(f\"Run {run_idx} missing keys for {dset}: {e}\")\n\n    # convert to arrays\n    def stack_and_stats(lst):\n        arr = np.stack(lst, axis=0)  # shape (runs, epochs)\n        mean = arr.mean(axis=0)\n        sem = (\n            arr.std(axis=0, ddof=1) / math.sqrt(arr.shape[0])\n            if arr.shape[0] > 1\n            else np.zeros_like(mean)\n        )\n        return mean, sem\n\n    if len(loss_train_runs) == 0:\n        print(f\"No complete runs for {dset}, skipping plotting.\")\n        continue\n\n    m_train, s_train = stack_and_stats(loss_train_runs)\n    m_val, s_val = stack_and_stats(loss_val_runs)\n    m_mcc_tr, s_mcc_tr = stack_and_stats(mcc_train_runs)\n    m_mcc_val, s_mcc_val = stack_and_stats(mcc_val_runs)\n\n    # ----------------------------------------------------------- plot losses\n    try:\n        plt.figure()\n        plt.plot(epochs_ref, m_train, label=\"Train Loss mean\")\n        plt.fill_between(\n            epochs_ref,\n            m_train - s_train,\n            m_train + s_train,\n            alpha=0.25,\n            label=\"Train Loss \u00b1 SEM\",\n        )\n        plt.plot(epochs_ref, m_val, label=\"Val Loss mean\")\n        plt.fill_between(\n            epochs_ref, m_val - s_val, m_val + s_val, alpha=0.25, label=\"Val Loss \u00b1 SEM\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.title(\n            f\"{dset} \u2013 Aggregated Loss Curve\\n(Mean \u00b1 SEM over {len(loss_train_runs)} runs)\"\n        )\n        plt.legend()\n        save_path = os.path.join(working_dir, f\"{dset}_agg_loss_curve.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss for {dset}: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------- plot MCC\n    try:\n        plt.figure()\n        plt.plot(epochs_ref, m_mcc_tr, label=\"Train MCC mean\")\n        plt.fill_between(\n            epochs_ref,\n            m_mcc_tr - s_mcc_tr,\n            m_mcc_tr + s_mcc_tr,\n            alpha=0.25,\n            label=\"Train MCC \u00b1 SEM\",\n        )\n        plt.plot(epochs_ref, m_mcc_val, label=\"Val MCC mean\")\n        plt.fill_between(\n            epochs_ref,\n            m_mcc_val - s_mcc_val,\n            m_mcc_val + s_mcc_val,\n            alpha=0.25,\n            label=\"Val MCC \u00b1 SEM\",\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews Corr. Coef.\")\n        plt.title(\n            f\"{dset} \u2013 Aggregated MCC Curve\\n(Mean \u00b1 SEM over {len(mcc_train_runs)} runs)\"\n        )\n        plt.legend()\n        save_path = os.path.join(working_dir, f\"{dset}_agg_MCC_curve.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated MCC for {dset}: {e}\")\n        plt.close()\n\n    # --------------------------------------- aggregated confusion-matrix bar\n    try:\n        cm_arr = np.asarray(cm_counts)  # shape (runs, 4)\n        cm_mean = cm_arr.mean(axis=0)\n        cm_sem = (\n            cm_arr.std(axis=0, ddof=1) / math.sqrt(cm_arr.shape[0])\n            if cm_arr.shape[0] > 1\n            else np.zeros_like(cm_mean)\n        )\n        labels = [\"TP\", \"TN\", \"FP\", \"FN\"]\n        plt.figure()\n        plt.bar(\n            labels,\n            cm_mean,\n            yerr=cm_sem,\n            capsize=5,\n            color=[\"green\", \"blue\", \"orange\", \"red\"],\n        )\n        for idx, val in enumerate(cm_mean):\n            plt.text(idx, val + max(cm_sem[idx], 1e-3), f\"{val:.1f}\", ha=\"center\")\n        plt.ylabel(\"Count (mean)\")\n        plt.title(\n            f\"{dset} \u2013 Aggregated Confusion Matrix\\n(Mean \u00b1 SEM over {cm_arr.shape[0]} runs)\"\n        )\n        save_path = os.path.join(working_dir, f\"{dset}_agg_confusion_matrix.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ----------------------------------------- console summary (final epoch)\n    final_val_mcc_mean = m_mcc_val[-1]\n    final_val_mcc_sem = s_mcc_val[-1]\n    print(\n        f\"{dset}: final-epoch Val MCC = {final_val_mcc_mean:.4f} \u00b1 {final_val_mcc_sem:.4f} (mean \u00b1 SEM, N={len(mcc_val_runs)})\"\n    )\n"], "plot_plan": [null, "We first load the serialized experiment_data.npy file and extract the lists of epochs, losses, MCC scores and the test\u2010set predictions/labels belonging to the \u201cSPR_BENCH\u201d entry.  Three straightforward visualisations are produced.  (1) A line plot of train vs. validation loss across epochs allows quick inspection of convergence and over-fitting.  (2) A second line plot juxtaposes train and validation MCC to trace the evolution of discriminative performance.  (3) Using the boolean test predictions and ground-truth labels we aggregate TP, TN, FP, FN counts and draw a small bar chart that conveys the final confusion matrix.  Each figure is generated inside its own try-except block, saved under a descriptive filename in working_dir and closed immediately afterward, fully complying with the plotting guidelines.  No synthetic data are fabricated\u2014everything comes directly from the stored experiment dictionary.  The script is short, standard matplotlib is employed without custom styles, and an informative title is added to every figure.  Finally, the script prints the arrays it plotted so that the user can quickly confirm which metric corresponds to which curve.", null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["", "This script loads the saved experiment data, walks through every dataset entry,\nand prints a concise summary of the key results. For each dataset it reports the\nminimum training loss, minimum validation loss, maximum training MCC, maximum\nvalidation MCC, and the final test MCC. Values are clearly labeled so they can\nbe read unambiguously.", "", "", "This script loads the saved experiment data, walks through every dataset entry,\nand prints a concise summary of the key results. For each dataset it reports the\nminimum training loss, minimum validation loss, maximum training MCC, maximum\nvalidation MCC, and the final test MCC. Values are clearly labeled so they can\nbe read unambiguously.", "This script loads the saved experiment data, walks through every dataset entry,\nand prints a concise summary of the key results. For each dataset it reports the\nminimum training loss, minimum validation loss, maximum training MCC, maximum\nvalidation MCC, and the final test MCC. Values are clearly labeled so they can\nbe read unambiguously.", "This script loads the saved experiment data, walks through every dataset entry,\nand prints a concise summary of the key results. For each dataset it reports the\nminimum training loss, minimum validation loss, maximum training MCC, maximum\nvalidation MCC, and the final test MCC. Values are clearly labeled so they can\nbe read unambiguously.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Load experiment data\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\ndef best_loss(loss_list):\n    \"\"\"Return the minimum loss from a list of losses.\"\"\"\n    return float(np.min(loss_list)) if loss_list else None\n\n\ndef best_mcc(mcc_list):\n    \"\"\"Return the maximum MCC from a list of MCC scores.\"\"\"\n    return float(np.max(mcc_list)) if mcc_list else None\n\n\n# -----------------------------------------------------------------------------\n# Print metrics for every dataset\n# -----------------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # Losses\n    train_loss_best = best_loss(data[\"losses\"].get(\"train\", []))\n    val_loss_best = best_loss(data[\"losses\"].get(\"val\", []))\n\n    # MCC metrics\n    train_mcc_best = best_mcc(data[\"metrics\"].get(\"train_MCC\", []))\n    val_mcc_best = best_mcc(data[\"metrics\"].get(\"val_MCC\", []))\n    test_mcc_final = data[\"metrics\"].get(\"test_MCC\", None)\n\n    # Print with explicit labels\n    if train_loss_best is not None:\n        print(f\"  training loss (minimum): {train_loss_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"  validation loss (minimum): {val_loss_best:.4f}\")\n    if train_mcc_best is not None:\n        print(\n            f\"  training Matthews correlation coefficient (maximum): {train_mcc_best:.4f}\"\n        )\n    if val_mcc_best is not None:\n        print(\n            f\"  validation Matthews correlation coefficient (maximum): {val_mcc_best:.4f}\"\n        )\n    if test_mcc_final is not None:\n        print(f\"  test Matthews correlation coefficient (final): {test_mcc_final:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Load experiment data\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\ndef best_loss(loss_list):\n    \"\"\"Return the minimum loss from a list of losses.\"\"\"\n    return float(np.min(loss_list)) if loss_list else None\n\n\ndef best_mcc(mcc_list):\n    \"\"\"Return the maximum MCC from a list of MCC scores.\"\"\"\n    return float(np.max(mcc_list)) if mcc_list else None\n\n\n# -----------------------------------------------------------------------------\n# Print metrics for every dataset\n# -----------------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # Losses\n    train_loss_best = best_loss(data[\"losses\"].get(\"train\", []))\n    val_loss_best = best_loss(data[\"losses\"].get(\"val\", []))\n\n    # MCC metrics\n    train_mcc_best = best_mcc(data[\"metrics\"].get(\"train_MCC\", []))\n    val_mcc_best = best_mcc(data[\"metrics\"].get(\"val_MCC\", []))\n    test_mcc_final = data[\"metrics\"].get(\"test_MCC\", None)\n\n    # Print with explicit labels\n    if train_loss_best is not None:\n        print(f\"  training loss (minimum): {train_loss_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"  validation loss (minimum): {val_loss_best:.4f}\")\n    if train_mcc_best is not None:\n        print(\n            f\"  training Matthews correlation coefficient (maximum): {train_mcc_best:.4f}\"\n        )\n    if val_mcc_best is not None:\n        print(\n            f\"  validation Matthews correlation coefficient (maximum): {val_mcc_best:.4f}\"\n        )\n    if test_mcc_final is not None:\n        print(f\"  test Matthews correlation coefficient (final): {test_mcc_final:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Load experiment data\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\ndef best_loss(loss_list):\n    \"\"\"Return the minimum loss from a list of losses.\"\"\"\n    return float(np.min(loss_list)) if loss_list else None\n\n\ndef best_mcc(mcc_list):\n    \"\"\"Return the maximum MCC from a list of MCC scores.\"\"\"\n    return float(np.max(mcc_list)) if mcc_list else None\n\n\n# -----------------------------------------------------------------------------\n# Print metrics for every dataset\n# -----------------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # Losses\n    train_loss_best = best_loss(data[\"losses\"].get(\"train\", []))\n    val_loss_best = best_loss(data[\"losses\"].get(\"val\", []))\n\n    # MCC metrics\n    train_mcc_best = best_mcc(data[\"metrics\"].get(\"train_MCC\", []))\n    val_mcc_best = best_mcc(data[\"metrics\"].get(\"val_MCC\", []))\n    test_mcc_final = data[\"metrics\"].get(\"test_MCC\", None)\n\n    # Print with explicit labels\n    if train_loss_best is not None:\n        print(f\"  training loss (minimum): {train_loss_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"  validation loss (minimum): {val_loss_best:.4f}\")\n    if train_mcc_best is not None:\n        print(\n            f\"  training Matthews correlation coefficient (maximum): {train_mcc_best:.4f}\"\n        )\n    if val_mcc_best is not None:\n        print(\n            f\"  validation Matthews correlation coefficient (maximum): {val_mcc_best:.4f}\"\n        )\n    if test_mcc_final is not None:\n        print(f\"  test Matthews correlation coefficient (final): {test_mcc_final:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Load experiment data\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\ndef best_loss(loss_list):\n    \"\"\"Return the minimum loss from a list of losses.\"\"\"\n    return float(np.min(loss_list)) if loss_list else None\n\n\ndef best_mcc(mcc_list):\n    \"\"\"Return the maximum MCC from a list of MCC scores.\"\"\"\n    return float(np.max(mcc_list)) if mcc_list else None\n\n\n# -----------------------------------------------------------------------------\n# Print metrics for every dataset\n# -----------------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # Losses\n    train_loss_best = best_loss(data[\"losses\"].get(\"train\", []))\n    val_loss_best = best_loss(data[\"losses\"].get(\"val\", []))\n\n    # MCC metrics\n    train_mcc_best = best_mcc(data[\"metrics\"].get(\"train_MCC\", []))\n    val_mcc_best = best_mcc(data[\"metrics\"].get(\"val_MCC\", []))\n    test_mcc_final = data[\"metrics\"].get(\"test_MCC\", None)\n\n    # Print with explicit labels\n    if train_loss_best is not None:\n        print(f\"  training loss (minimum): {train_loss_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"  validation loss (minimum): {val_loss_best:.4f}\")\n    if train_mcc_best is not None:\n        print(\n            f\"  training Matthews correlation coefficient (maximum): {train_mcc_best:.4f}\"\n        )\n    if val_mcc_best is not None:\n        print(\n            f\"  validation Matthews correlation coefficient (maximum): {val_mcc_best:.4f}\"\n        )\n    if test_mcc_final is not None:\n        print(f\"  test Matthews correlation coefficient (final): {test_mcc_final:.4f}\")\n", ""], "parse_term_out": ["", "['SPR_BENCH:', '\\n', '  training loss (minimum): 0.6608', '\\n', '  validation\nloss (minimum): 0.6616', '\\n', '  training Matthews correlation coefficient\n(maximum): 0.2827', '\\n', '  validation Matthews correlation coefficient\n(maximum): 0.2843', '\\n', '  test Matthews correlation coefficient (final):\n0.2614', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "", "['SPR_BENCH:', '\\n', '  training loss (minimum): 0.6516', '\\n', '  validation\nloss (minimum): 0.6538', '\\n', '  training Matthews correlation coefficient\n(maximum): 0.2817', '\\n', '  validation Matthews correlation coefficient\n(maximum): 0.2880', '\\n', '  test Matthews correlation coefficient (final):\n0.2495', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  training loss (minimum): 0.6478', '\\n', '  validation\nloss (minimum): 0.6548', '\\n', '  training Matthews correlation coefficient\n(maximum): 0.3161', '\\n', '  validation Matthews correlation coefficient\n(maximum): 0.2816', '\\n', '  test Matthews correlation coefficient (final):\n0.2815', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  training loss (minimum): 0.6519', '\\n', '  validation\nloss (minimum): 0.6583', '\\n', '  training Matthews correlation coefficient\n(maximum): 0.2858', '\\n', '  validation Matthews correlation coefficient\n(maximum): 0.2566', '\\n', '  test Matthews correlation coefficient (final):\n0.2435', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
