{
  "best node": {
    "overall_plan": "The research plan involves a progressive transition from hyperparameter tuning of a Bi-LSTM model to employing advanced Transformer architectures for sequence classification or rule-based inference. Initially, learning rate optimization was emphasized to improve model performance, using metrics such as the Matthews Correlation Coefficient (MCC). This foundational work set the stage for adopting a lightweight Transformer encoder, incorporating innovations like a learnable [CLS] token, learned positional embeddings, and a character-level vocabulary to capture more complex input features. The plan further extends this baseline Transformer with explicit symbolic-count features, aiming to better capture global rule cues that pure sequence models might learn slowly. By providing normalized counts of symbols and sequence length, this hybrid approach seeks to enhance Rule-Macro Accuracy and overall accuracy. Additional strategies like label-smoothing and an aggressive One-Cycle learning-rate schedule are employed to improve generalization. The experimental setup is designed for rapid training and includes contingency measures like fallback to a synthetic dataset, ensuring robustness and reproducibility throughout the research process.",
    "analysis": "",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "loss",
            "lower_is_better": true,
            "description": "Measures the error between predicted and actual values.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.6205,
                "best_value": 0.6205
              }
            ]
          },
          {
            "metric_name": "accuracy",
            "lower_is_better": false,
            "description": "Proportion of correctly predicted instances.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.699,
                "best_value": 0.699
              }
            ]
          },
          {
            "metric_name": "MCC",
            "lower_is_better": false,
            "description": "Matthews correlation coefficient, a balanced measure for binary classifications.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.397,
                "best_value": 0.397
              }
            ]
          },
          {
            "metric_name": "RMA",
            "lower_is_better": false,
            "description": "Root Mean Accuracy, an alternative accuracy metric.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.699,
                "best_value": 0.699
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "import os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "plot_plan": "We first load experiment_data.npy and grab the SPR_BENCH entry, extracting the epoch list, train/validation losses, and per-epoch metric dictionaries.  From the metric dicts we build NumPy arrays for accuracy and MCC so that we can draw paired curves for train vs. validation.  We then create up to five figures: (1) loss curves, (2) accuracy curves, (3) MCC curves, (4) a confusion-matrix heat-map on the test set, and (5) a simple histogram comparing the class-frequency of predictions conditioned on the ground-truth label.  Each figure is wrapped in its own try-except so that failure in one plot does not stop the others, and every figure is closed after saving.  File names combine the dataset name and plot type for clarity, and titles always include a brief subtitle explaining the axes or comparison (e.g. \u201cLeft: Train, Right: Validation\u201d).  We only plot data that actually exists in the loaded dictionary, using len checks where necessary.  Finally, we print out the saved test-set metrics (loss, acc, MCC, RMA) so the user can quickly verify performance without opening the plots.  All output images are written to the pre-defined working directory.",
    "plot_analyses": [
      {
        "analysis": "The loss curves show a consistent decrease in both the training and validation loss over the epochs, indicating that the model is learning effectively. However, the gap between the training and validation loss is small, suggesting minimal overfitting. The convergence of the curves towards the end implies that the model is approaching its optimal performance.",
        "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_loss_curves.png"
      },
      {
        "analysis": "The accuracy curves reveal an upward trend for both training and validation accuracy, with the training accuracy showing some fluctuations. These fluctuations might indicate that the model is sensitive to certain patterns in the data. The validation accuracy stabilizes and comes close to the SOTA benchmark of 70%, demonstrating promising generalization.",
        "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_accuracy_curves.png"
      },
      {
        "analysis": "The MCC (Matthews Correlation Coefficient) curves align with the accuracy trends, showing improvement over the epochs. The MCC for validation stabilizes, indicating that the model is achieving a balanced performance across classes. The fluctuations in the training MCC suggest areas for potential refinement in model training.",
        "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_mcc_curves.png"
      },
      {
        "analysis": "The confusion matrix indicates that the model performs reasonably well on the test set, with a higher count of true positives (370) and true negatives (329). However, there are notable false positives (157) and false negatives (144), suggesting room for improvement in reducing misclassifications.",
        "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_confusion_matrix.png"
      },
      {
        "analysis": "The prediction distribution plot shows a skew towards correct classifications for both true positives and true negatives. The distribution suggests that the model is confident in its predictions for most cases, but there might be some borderline cases that could benefit from further tuning or additional features.",
        "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_pred_histogram.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_loss_curves.png",
      "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_accuracy_curves.png",
      "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_mcc_curves.png",
      "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_confusion_matrix.png",
      "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/spr_bench_pred_histogram.png"
    ],
    "vlm_feedback_summary": "The plots provide valuable insights into the model's performance, indicating steady learning and generalization. The accuracy and MCC metrics are promising, approaching the SOTA benchmark. However, there is room for improvement in reducing misclassifications and addressing fluctuations in training metrics.",
    "exp_results_dir": "experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339",
    "exp_results_npy_files": [
      "experiment_results/experiment_93f8f3c806cf498b8ff964af0581a522_proc_3338339/experiment_data.npy"
    ]
  },
  "best node with different seeds": [
    {
      "overall_plan": "The research plan involves a progression from hyperparameter tuning of a Bi-LSTM model to implementing advanced Transformer architectures for sequence classification or rule-based inference. Initially, the focus was on optimizing the learning rate to improve performance metrics like the Matthews Correlation Coefficient (MCC). This laid the groundwork for adopting a lightweight Transformer encoder with innovations such as a learnable [CLS] token, learned positional embeddings, and a character-level vocabulary to capture complex input features. The plan extends this baseline Transformer with symbolic-count features to enhance Rule-Macro Accuracy and overall accuracy, by incorporating normalized counts of symbols and sequence length to better capture global rule cues. Strategies like label-smoothing and an aggressive One-Cycle learning-rate schedule are employed for improved generalization. The experimental setup is designed for rapid training and includes fallback measures like a synthetic dataset for robustness and reproducibility. The current plan, labeled as a 'Seed node,' suggests an initial stage or reset point, potentially introducing new foundational ideas or methodologies that will be explored in conjunction with the established strategies.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "loss",
              "lower_is_better": true,
              "description": "Measures how well the model's predictions match the actual outcomes. Lower values indicate better performance.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.6205,
                  "best_value": 0.6205
                }
              ]
            },
            {
              "metric_name": "accuracy",
              "lower_is_better": false,
              "description": "Measures the proportion of correct predictions made by the model. Higher values indicate better performance.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.699,
                  "best_value": 0.699
                }
              ]
            },
            {
              "metric_name": "MCC",
              "lower_is_better": false,
              "description": "Matthews Correlation Coefficient, a balanced measure for binary classification. Higher values indicate better performance.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.397,
                  "best_value": 0.397
                }
              ]
            },
            {
              "metric_name": "RMA",
              "lower_is_better": false,
              "description": "Root Mean Accuracy, a metric for evaluating model accuracy, where higher values indicate better performance.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.699,
                  "best_value": 0.699
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
      "plot_analyses": [
        {
          "analysis": "This plot shows the BCE loss curves for both the training and validation datasets. Both curves decrease steadily, indicating that the model is learning effectively. The validation loss consistently remains lower than the training loss, which suggests that the model is not overfitting. However, the convergence is slow, and the gap between training and validation loss should be monitored in further experiments to ensure robustness.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_loss_curves.png"
        },
        {
          "analysis": "This plot depicts the accuracy curves for the training and validation datasets. The training accuracy increases rapidly, peaking and then fluctuating slightly, while the validation accuracy stabilizes at around 70%. The fluctuation in training accuracy might indicate overfitting or instability in the learning process. The validation accuracy reaching 70% matches the SOTA baseline, which is promising, but further fine-tuning might be needed to surpass it.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_accuracy_curves.png"
        },
        {
          "analysis": "This plot shows the Matthews Correlation Coefficient (MCC) curves for the training and validation datasets. The MCC for training shows sharp fluctuations after an initial increase, while the validation MCC stabilizes at a value around 0.4. This indicates that the model is capturing some of the underlying relationships but might still require improvements to handle the complexities of the SPR task effectively.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_mcc_curves.png"
        },
        {
          "analysis": "The confusion matrix reveals the classification performance on the test set. The model achieves a reasonable balance between true positives (370) and true negatives (329), but there is a noticeable number of false positives (157) and false negatives (144). This suggests that the model is moderately effective but struggles with certain edge cases or complex rules. Further analysis of these misclassified instances could provide insights for model improvement.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_confusion_matrix.png"
        },
        {
          "analysis": "This plot illustrates the distribution of predictions for true negatives and true positives. The separation between these distributions indicates that the model is reasonably confident in its predictions. However, there is some overlap, which could lead to misclassifications. Enhancing the model's ability to discriminate between positive and negative classes could improve overall performance.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_pred_histogram.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_loss_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_accuracy_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_mcc_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_confusion_matrix.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/spr_bench_pred_histogram.png"
      ],
      "vlm_feedback_summary": "The plots indicate that the model is learning effectively but faces challenges in achieving stability and surpassing the SOTA baseline. The validation accuracy and MCC suggest moderate success, while the confusion matrix and prediction distributions highlight areas for improvement in handling complex rules and edge cases. Further fine-tuning and analysis of misclassified instances are recommended.",
      "exp_results_dir": "experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341",
      "exp_results_npy_files": [
        "experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/experiment_data.npy"
      ]
    },
    {
      "overall_plan": "The overall research plan involves transitioning from hyperparameter tuning of a Bi-LSTM model to employing advanced Transformer architectures for sequence classification or rule-based inference. Initially, learning rate optimization using metrics like the Matthews Correlation Coefficient (MCC) was emphasized to improve model performance. This foundational work set the stage for adopting a lightweight Transformer encoder, incorporating innovations such as a learnable [CLS] token, learned positional embeddings, and a character-level vocabulary to capture more complex input features. The plan further extends this baseline Transformer with explicit symbolic-count features, aiming to better capture global rule cues that pure sequence models might learn slowly. By providing normalized counts of symbols and sequence length, this hybrid approach seeks to enhance Rule-Macro Accuracy and overall accuracy. Additional strategies like label-smoothing and an aggressive One-Cycle learning-rate schedule are employed to improve generalization. The experimental setup is designed for rapid training and includes contingency measures like fallback to a synthetic dataset, ensuring robustness and reproducibility throughout the research process. The current plan, described as a 'Seed node,' does not alter these objectives, maintaining the focus on the extensive strategies from the previous plan.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "loss",
              "lower_is_better": true,
              "description": "The loss value indicates the error in prediction. Lower values are better.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.6205,
                  "best_value": 0.6205
                }
              ]
            },
            {
              "metric_name": "accuracy",
              "lower_is_better": false,
              "description": "The accuracy of the model, representing the proportion of correct predictions. Higher values are better.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.699,
                  "best_value": 0.699
                }
              ]
            },
            {
              "metric_name": "MCC",
              "lower_is_better": false,
              "description": "The Matthews Correlation Coefficient (MCC) is a measure of the quality of binary classifications.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.397,
                  "best_value": 0.397
                }
              ]
            },
            {
              "metric_name": "RMA",
              "lower_is_better": false,
              "description": "The RMA metric measures the reliability of predictions. Higher values are better.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.699,
                  "best_value": 0.699
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
      "plot_analyses": [
        {
          "analysis": "The loss curves indicate that the model is effectively learning during training. Both training and validation loss decrease over epochs, with validation loss being consistently lower than training loss. This suggests that the model is not overfitting and is generalizing well to unseen data. The plateau in loss reduction at later epochs may indicate that further improvements could require additional model tuning or architectural changes.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_loss_curves.png"
        },
        {
          "analysis": "The accuracy curves show steady improvement in both training and validation accuracy over epochs. However, there is a noticeable fluctuation in training accuracy at certain epochs, which might indicate instability in training or sensitivity to the learning rate. Validation accuracy stabilizes around 70%, suggesting that the model is nearing the SOTA benchmark but may require further optimization to surpass it.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_accuracy_curves.png"
        },
        {
          "analysis": "The MCC curves demonstrate a similar trend to the accuracy curves, with steady improvement in both training and validation MCC over epochs. The MCC values for validation are consistently lower than those for training, indicating room for improvement in the model's ability to handle the complexities of the SPR task. The fluctuations in training MCC also highlight potential instability in the model's learning process.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_mcc_curves.png"
        },
        {
          "analysis": "The confusion matrix reveals that the model performs reasonably well in distinguishing between positive and negative classes, with 329 true negatives and 370 true positives. However, there are notable numbers of false negatives (144) and false positives (157), indicating that the model struggles with certain edge cases or ambiguous sequences. Addressing these errors could significantly enhance performance.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_confusion_matrix.png"
        },
        {
          "analysis": "The prediction distribution plot shows a balanced distribution of true negatives and true positives, which aligns with the confusion matrix. However, the overlap in predicted class probabilities for true negatives and true positives suggests that the model's confidence in its predictions could be improved. Refining the decision boundary or incorporating additional features might help reduce this overlap.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_pred_histogram.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_loss_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_accuracy_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_mcc_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_confusion_matrix.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/spr_bench_pred_histogram.png"
      ],
      "vlm_feedback_summary": "The plots suggest that the model is learning effectively and nearing SOTA performance, but there are areas for improvement. The loss curves indicate good generalization, the accuracy and MCC curves show steady progress with some instability, and the confusion matrix and prediction distribution highlight specific areas where the model's performance can be refined.",
      "exp_results_dir": "experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338",
      "exp_results_npy_files": [
        "experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/experiment_data.npy"
      ]
    },
    {
      "overall_plan": "The overall research plan involves transitioning from hyperparameter tuning of Bi-LSTM models to employing advanced Transformer architectures for sequence classification or rule-based inference. The focus includes optimizing learning rates to improve model performance, incorporating a lightweight Transformer encoder with innovations like a learnable [CLS] token, learned positional embeddings, and a character-level vocabulary to capture complex input features. The plan extends this baseline with explicit symbolic-count features to enhance global rule cue capture, aiming to improve Rule-Macro Accuracy and overall accuracy. Additional strategies include label-smoothing and an aggressive One-Cycle learning-rate schedule to enhance generalization. The experimental setup is designed for rapid training with contingency measures like fallback to a synthetic dataset to ensure robustness and reproducibility. The current plan as a 'Seed node' indicates the initiation of a new research phase, possibly laying the groundwork for future developments building on the established framework.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "loss",
              "lower_is_better": true,
              "description": "Measures the error of the model's predictions.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.6205,
                  "best_value": 0.6205
                }
              ]
            },
            {
              "metric_name": "accuracy",
              "lower_is_better": false,
              "description": "Proportion of correct predictions made by the model.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.699,
                  "best_value": 0.699
                }
              ]
            },
            {
              "metric_name": "MCC",
              "lower_is_better": false,
              "description": "Matthews correlation coefficient, used for evaluating binary classifications.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.397,
                  "best_value": 0.397
                }
              ]
            },
            {
              "metric_name": "RMA",
              "lower_is_better": false,
              "description": "Root Mean Accuracy, a measure of model performance.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.699,
                  "best_value": 0.699
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import load_dataset, DatasetDict, Dataset as HFDataset\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 boiler-plate dirs / bookkeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nseed = 2024\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 device handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset loading helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(\"train.csv\")\n    d[\"dev\"] = _l(\"dev.csv\")\n    d[\"test\"] = _l(\"test.csv\")\n    return d\n\n\ndef maybe_dataset() -> DatasetDict:\n    root = pathlib.Path(\n        os.getenv(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if root.exists():\n        print(\"Found real SPR_BENCH at\", root)\n        return load_spr_bench(root)\n    print(\"\u26a0\ufe0f  SPR_BENCH not found \u2013 generating toy synthetic data.\")\n    syms = list(\"ABCDEFGH\")\n\n    def synth(n):\n        seqs, labs = [], []\n        for i in range(n):\n            ln = random.randint(5, 15)\n            seq = \"\".join(random.choice(syms) for _ in range(ln))\n            lab = int(seq.count(\"A\") % 2 == 0)\n            seqs.append(seq)\n            labs.append(lab)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    dd = DatasetDict()\n    for split, n in [(\"train\", 3000), (\"dev\", 800), (\"test\", 800)]:\n        dd[split] = HFDataset.from_dict(synth(n))\n    return dd\n\n\nspr = maybe_dataset()\nprint(\"Split sizes:\", {k: len(v) for k, v in spr.items()})\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tokenisation utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nPAD, CLS = 0, 1\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 2 for i, ch in enumerate(vocab)}  # reserve 0/1\nitos = {i: ch for ch, i in enumerate([\"<pad>\", \"<cls>\"] + vocab)}\nvocab_size = len(stoi) + 2\n\nmax_len = min(48, max(len(s) for s in spr[\"train\"][\"sequence\"])) + 1  # +1 CLS\n\n\ndef encode_tokens(seq: str):\n    ids = [CLS] + [stoi.get(c, PAD) for c in seq][: max_len - 1]\n    ids += [PAD] * (max_len - len(ids))\n    return ids[:max_len]\n\n\ndef encode_counts(seq: str):\n    vec = np.zeros(len(vocab) + 1, dtype=np.float32)  # symbol counts\n    for ch in seq:\n        if ch in stoi:\n            vec[stoi[ch] - 2] += 1.0\n    vec[:-1] /= max(len(seq), 1)  # normalised counts\n    vec[-1] = len(seq) / max_len  # length fraction\n    return vec\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 torch Dataset class \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass SPRTorch(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.ids = hf_ds[\"id\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_tokens(self.seq[idx]), dtype=torch.long),\n            \"feat\": torch.tensor(encode_counts(self.seq[idx])),\n            \"y\": torch.tensor(self.lab[idx], dtype=torch.float32),\n            \"rid\": str(self.ids[idx]),\n        }\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hybrid model definition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nclass CountAwareTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_sz: int,\n        emb=64,\n        nhead=8,\n        nlayers=2,\n        ff=128,\n        extra_dim=0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=PAD)\n        self.pos = nn.Parameter(torch.randn(1, max_len, emb))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb,\n            nhead=nhead,\n            dim_feedforward=ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.feat_proj = nn.Linear(extra_dim, emb)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb * 2, emb), nn.ReLU(), nn.Dropout(dropout), nn.Linear(emb, 1)\n        )\n\n    def forward(self, tok, feats):\n        h = self.emb(tok) + self.pos[:, : tok.size(1), :]\n        h = self.transformer(h)\n        cls = h[:, 0]\n        f = self.feat_proj(feats)\n        cat = torch.cat([cls, f], dim=-1)\n        return self.classifier(cat).squeeze(1)\n\n\nmodel = CountAwareTransformer(\n    vocab_size,\n    emb=96,\n    nhead=8,\n    nlayers=3,\n    ff=256,\n    extra_dim=len(vocab) + 1,\n    dropout=0.15,\n).to(device)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ndef rule_macro_accuracy(preds, gts, ids):\n    d = {}\n    for p, g, i in zip(preds, gts, ids):\n        key = str(i).split(\"-\")[0]\n        c, t = d.get(key, (0, 0))\n        d[key] = (c + int(p == g), t + 1)\n    return np.mean([c / t for c, t in d.values()]) if d else 0.0\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, logits_all, y_all, id_all = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"rid\"]\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logit = model(batch[\"x\"], batch[\"feat\"])\n            y = batch[\"y\"]\n            loss = criterion(logit, y)\n            tot_loss += loss.item() * y.size(0)\n            logits_all.append(logit.sigmoid().cpu())\n            y_all.append(y.cpu())\n            id_all += ids\n    logits = torch.cat(logits_all)\n    y = torch.cat(y_all)\n    preds = (logits > 0.5).int().numpy()\n    y_np = y.int().numpy()\n    acc = (preds == y_np).mean()\n    mcc = matthews_corrcoef(y_np, preds) if len(np.unique(y_np)) > 1 else 0.0\n    rma = rule_macro_accuracy(preds, y_np, id_all)\n    return tot_loss / len(loader.dataset), acc, mcc, rma, preds, y_np, id_all\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 data loaders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 loss, optimiser, scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nlabel_smooth = 0.04\n\n\ndef smooth_labels(y):\n    return y * (1 - label_smooth) + 0.5 * label_smooth\n\n\ncriterion = lambda logits, y: nn.BCEWithLogitsLoss()(logits, smooth_labels(y))\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\ntotal_steps = len(train_loader) * 8\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=3e-3, total_steps=total_steps\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss_sum = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"x\"], batch[\"feat\"])\n        loss = criterion(out, batch[\"y\"])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        tr_loss_sum += loss.item() * batch[\"y\"].size(0)\n    train_loss = tr_loss_sum / len(train_loader.dataset)\n    with torch.no_grad():\n        p = (out.sigmoid() > 0.5).int().cpu().numpy()\n        yb = batch[\"y\"].cpu().int().numpy()\n        tr_acc = (p == yb).mean()\n        tr_mcc = matthews_corrcoef(yb, p) if len(np.unique(yb)) > 1 else 0.0\n        tr_rma = rule_macro_accuracy(p, yb, batch[\"rid\"])\n    val_loss, val_acc, val_mcc, val_rma, *_ = evaluate(val_loader)\n\n    # bookkeeping\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"MCC\": tr_mcc, \"RMA\": tr_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"MCC\": val_mcc, \"RMA\": val_rma}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.3f} | MCC={val_mcc:.3f} | RMA={val_rma:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 final test evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\ntest_loss, test_acc, test_mcc, test_rma, preds, gts, ids = evaluate(test_loader)\nprint(\"\\n===== TEST RESULTS =====\")\nprint(\n    f\"loss={test_loss:.4f} | acc={test_acc:.3f} | MCC={test_mcc:.3f} | RMA={test_rma:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts.tolist()\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"MCC\": test_mcc,\n    \"RMA\": test_rma,\n}\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH entry not found in experiment_data.npy\")\n    raise SystemExit\n\nd = experiment_data[\"SPR_BENCH\"]\nepochs = np.array(d.get(\"epochs\", []))\n\nloss_tr = np.array(d[\"losses\"].get(\"train\", []))\nloss_val = np.array(d[\"losses\"].get(\"val\", []))\n\n\ndef _metric_arr(lst, key):\n    return np.array([m.get(key, np.nan) for m in lst])\n\n\nacc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"acc\")\nacc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"acc\")\nmcc_tr = _metric_arr(d[\"metrics\"].get(\"train\", []), \"MCC\")\nmcc_val = _metric_arr(d[\"metrics\"].get(\"val\", []), \"MCC\")\n\ny_true = d.get(\"ground_truth\", [])\ny_pred = d.get(\"predictions\", [])\n\n\n# ---------- helper ----------\ndef confusion_counts(y_t, y_p):\n    tp = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    tn = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    fp = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_t, y_p))\n    fn = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_t, y_p))\n    return np.array([[tn, fp], [fn, tp]])\n\n\n# ---------- 1) Loss curves ----------\ntry:\n    if len(epochs) and len(loss_tr) and len(loss_val):\n        plt.figure()\n        plt.plot(epochs, loss_tr, label=\"Train\")\n        plt.plot(epochs, loss_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = \"spr_bench_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- 2) Accuracy curves ----------\ntry:\n    if len(epochs) and len(acc_tr) and len(acc_val):\n        plt.figure()\n        plt.plot(epochs, acc_tr, label=\"Train\")\n        plt.plot(epochs, acc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = \"spr_bench_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- 3) MCC curves ----------\ntry:\n    if len(epochs) and len(mcc_tr) and len(mcc_val):\n        plt.figure()\n        plt.plot(epochs, mcc_tr, label=\"Train\")\n        plt.plot(epochs, mcc_val, label=\"Validation\")\n        plt.title(\"SPR_BENCH MCC Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews CorrCoef\")\n        plt.legend()\n        fname = \"spr_bench_mcc_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC plot: {e}\")\n    plt.close()\n\n# ---------- 4) Confusion matrix ----------\ntry:\n    if len(y_true) and len(y_pred):\n        cm = confusion_counts(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH Confusion Matrix on Test\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks([0, 1], [\"Neg\", \"Pos\"])\n        plt.yticks([0, 1], [\"Neg\", \"Pos\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n        fname = \"spr_bench_confusion_matrix.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- 5) Prediction histogram ----------\ntry:\n    if len(y_true) and len(y_pred):\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        plt.figure()\n        plt.hist(\n            y_pred[y_true == 0],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Negatives\",\n        )\n        plt.hist(\n            y_pred[y_true == 1],\n            bins=np.arange(-0.5, 2),\n            alpha=0.7,\n            label=\"True Positives\",\n        )\n        plt.title(\"SPR_BENCH Prediction Distribution\\nLeft: True Neg, Right: True Pos\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = \"spr_bench_pred_histogram.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\nif \"test_metrics\" in d:\n    print(\"\\n===== TEST METRICS =====\")\n    for k, v in d[\"test_metrics\"].items():\n        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
      "plot_analyses": [
        {
          "analysis": "The BCE loss curves indicate a consistent decrease in loss for both the training and validation datasets across epochs. The validation loss closely follows the training loss, suggesting that the model is learning effectively without significant overfitting. The slight convergence towards the end suggests that the model could benefit from additional epochs or fine-tuning to achieve further improvements.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_loss_curves.png"
        },
        {
          "analysis": "The accuracy curves show a steady increase in both training and validation accuracy over the epochs. However, the training accuracy exhibits some oscillations, which could indicate sensitivity to updates or potential overfitting in certain epochs. The validation accuracy plateaus around 70%, which aligns with the SOTA benchmark, indicating competitive performance.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_accuracy_curves.png"
        },
        {
          "analysis": "The Matthews Correlation Coefficient (MCC) curves reveal a similar trend to the accuracy curves, with the validation MCC steadily improving and plateauing around 0.4. This suggests that the model achieves a moderate level of agreement between predictions and actual labels, but there is room for improvement in handling imbalanced or complex rule-based sequences.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_mcc_curves.png"
        },
        {
          "analysis": "The confusion matrix shows that the model correctly predicts the majority of both positive and negative classes, with 370 true positives and 329 true negatives. However, there are notable false positives (157) and false negatives (144), indicating areas where the model struggles to differentiate between classes. This highlights the need for further refinement in feature extraction or rule representation.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_confusion_matrix.png"
        },
        {
          "analysis": "The prediction distribution plot shows a balanced distribution of true negatives and true positives. However, the overlap in prediction scores suggests that the model's confidence in distinguishing between classes could be improved. Calibration techniques or more discriminative features might enhance class separation.",
          "plot_path": "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_pred_histogram.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_loss_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_accuracy_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_mcc_curves.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_confusion_matrix.png",
        "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/spr_bench_pred_histogram.png"
      ],
      "vlm_feedback_summary": "The plots indicate that the model performs competitively with the SOTA benchmark, achieving comparable accuracy and MCC. However, there are areas for improvement, such as reducing false positives and false negatives, refining feature extraction, and improving class separation. The results are promising, but further experimentation is needed to achieve superior performance.",
      "exp_results_dir": "experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340",
      "exp_results_npy_files": [
        "experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/experiment_data.npy"
      ]
    }
  ],
  "aggregated results of nodes with different seeds": {
    "overall_plan": "The research plan involves a strategic transition from hyperparameter tuning of a Bi-LSTM model to employing advanced Transformer architectures for sequence classification, with an emphasis on improving model performance using metrics like the Matthews Correlation Coefficient (MCC). This foundational work included the development of a lightweight Transformer encoder with innovations such as a learnable [CLS] token, learned positional embeddings, and a character-level vocabulary aimed at capturing complex input features. The plan extends to include explicit symbolic-count features for better capturing global rule cues, using normalized counts of symbols and sequence length to enhance Rule-Macro Accuracy and overall accuracy. Additional strategies like label-smoothing and a One-Cycle learning-rate schedule are employed for improved generalization. A robust experimental setup ensures rapid training and includes contingency measures such as a synthetic dataset fallback. Currently, the plan involves aggregating results from multiple seeds to assess model stability and variance, ensuring consistent and reliable findings. This comprehensive approach balances innovative model development with rigorous evaluation and result verification.",
    "metric": {
      "value": null,
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "# plotting aggregation code",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & constants ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Experiment data (relative to $AI_SCIENTIST_ROOT)\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_3404b88306e94a93bb791755a8841632_proc_3338338/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ad156518f1b945e7aa57e43a6ebb5ff5_proc_3338340/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_cdd18529ee024526a1fab484c64aba23_proc_3338341/experiment_data.npy\",\n]\n\n# ---------------- load every run ----------------\nall_runs = []\nfor p in experiment_data_path_list:\n    try:\n        full_p = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_p, allow_pickle=True).item()\n        if \"SPR_BENCH\" in exp:\n            all_runs.append(exp[\"SPR_BENCH\"])\n        else:\n            print(f\"Warning: SPR_BENCH entry missing in {p}\")\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif len(all_runs) == 0:\n    raise SystemExit(\"No valid runs found \u2013 aborting.\")\n\nn_runs = len(all_runs)\nprint(f\"Loaded {n_runs} SPR_BENCH run(s).\")\n\n\n# ---------------- helper to collect & align ----------------\ndef stack_metric(path, subkey):\n    \"\"\"Return epochs, 2-D array (runs\u00d7epochs), or ([],[]) if nothing found.\"\"\"\n    series = []\n    for run in all_runs:\n        arr = np.array(run[path].get(subkey, [])) if path in run else np.array([])\n        series.append(arr)\n    min_len = min(len(a) for a in series)\n    if min_len == 0:\n        return np.array([]), np.array([[]])\n    trimmed = np.stack([a[:min_len] for a in series])  # runs \u00d7 epochs\n    epochs = np.array(all_runs[0][\"epochs\"][:min_len])\n    return epochs, trimmed\n\n\n# ---------------- aggregate curves ----------------\nplots = [\n    (\n        \"Loss Curves\",\n        (\"losses\", \"train\"),\n        (\"losses\", \"val\"),\n        \"BCE Loss\",\n        \"spr_bench_aggregated_loss_curves.png\",\n    ),\n    (\n        \"Accuracy Curves\",\n        (\"metrics\", \"train\", \"acc\"),\n        (\"metrics\", \"val\", \"acc\"),\n        \"Accuracy\",\n        \"spr_bench_aggregated_accuracy_curves.png\",\n    ),\n    (\n        \"MCC Curves\",\n        (\"metrics\", \"train\", \"MCC\"),\n        (\"metrics\", \"val\", \"MCC\"),\n        \"Matthews CorrCoef\",\n        \"spr_bench_aggregated_mcc_curves.png\",\n    ),\n]\n\n\ndef retrieve(run_dict, keys):\n    \"\"\"Nested get with default {}\u2192[] to avoid KeyErrors.\"\"\"\n    d = run_dict\n    for k in keys:\n        d = d.get(k, {})\n    return d\n\n\n# plotting loop --------------------------------------------------------------\nfor title, train_path, val_path, ylabel, fname in plots:\n    try:\n        # unpack keys: (\"losses\",\"train\") etc.\n        if len(train_path) == 2:\n            t_epochs, train_mat = stack_metric(train_path[0], train_path[1])\n            v_epochs, val_mat = stack_metric(val_path[0], val_path[1])\n        else:  # metrics / train / acc\n            t_epochs, train_mat = stack_metric(train_path[0], train_path[2])\n            v_epochs, val_mat = stack_metric(val_path[0], val_path[2])\n\n        if t_epochs.size and v_epochs.size:\n            # train\n            train_mean = np.nanmean(train_mat, axis=0)\n            train_sem = np.nanstd(train_mat, axis=0, ddof=1) / np.sqrt(n_runs)\n            # val\n            val_mean = np.nanmean(val_mat, axis=0)\n            val_sem = np.nanstd(val_mat, axis=0, ddof=1) / np.sqrt(n_runs)\n\n            plt.figure()\n            plt.plot(t_epochs, train_mean, label=\"Train \u03bc\")\n            plt.fill_between(\n                t_epochs,\n                train_mean - train_sem,\n                train_mean + train_sem,\n                alpha=0.3,\n                label=\"Train \u00b1SEM\",\n            )\n            plt.plot(v_epochs, val_mean, label=\"Val \u03bc\")\n            plt.fill_between(\n                v_epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                alpha=0.3,\n                label=\"Val \u00b1SEM\",\n            )\n            plt.title(f\"SPR_BENCH {title}\\nMean \u00b1 SEM across {n_runs} runs\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(ylabel)\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, fname))\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot {fname}: {e}\")\n        plt.close()\n\n# ---------------- aggregate test metrics ----------------\ntry:\n    test_metrics_keys = set().union(\n        *(run.get(\"test_metrics\", {}).keys() for run in all_runs)\n    )\n    if test_metrics_keys:\n        print(\"\\n===== AGGREGATED TEST METRICS =====\")\n        for key in sorted(test_metrics_keys):\n            vals = [run.get(\"test_metrics\", {}).get(key, np.nan) for run in all_runs]\n            vals = np.array(vals, dtype=float)\n            mean = np.nanmean(vals)\n            std = np.nanstd(vals, ddof=1)\n            print(f\"{key}: {mean:.4f} \u00b1 {std:.4f} (n={n_runs})\")\nexcept Exception as e:\n    print(f\"Error aggregating test metrics: {e}\")\n",
    "plot_analyses": [],
    "plot_paths": [
      "experiments/2025-08-17_18-47-55_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ebc6609305304dd0a17876b0f79d0152/spr_bench_aggregated_loss_curves.png"
    ],
    "vlm_feedback_summary": [],
    "exp_results_dir": "experiment_results/seed_aggregation_ebc6609305304dd0a17876b0f79d0152",
    "exp_results_npy_files": []
  }
}