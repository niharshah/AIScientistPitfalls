<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 4], [0, 3], [0, 8], [0, 6], [0, 7], [0, 2], [2, 5], [7, 11], [7, 10], [7, 9], [7, 12]], "layout": [[0.46153846153846156, 0.0], [0.0, 0.5], [0.15384615384615385, 0.5], [0.3076923076923077, 0.5], [0.46153846153846156, 0.5], [0.15384615384615385, 1.0], [0.6153846153846154, 0.5], [0.7692307692307693, 0.5], [0.9230769230769231, 0.5], [0.5384615384615384, 1.0], [0.6923076923076923, 1.0], [0.8461538461538461, 1.0], [1.0, 1.0]], "plan": ["We frame SPR sequence classification as a simple character-level sequence\nproblem, converting each abstract symbol to an integer and padding to a fixed\nlength.  A small bidirectional GRU with an embedding layer is trained using\nBCEWithLogitsLoss, with the official \u201cdev\u201d split used for validation and early\nstopping.  Macro-F1 is computed each epoch to monitor rule-reasoning accuracy,\nand loss/F1 curves are stored for later analysis.  All tensors and the model are\nmoved to GPU when available; the optimizer is created only after the model is on\nthe chosen device.  Metrics, losses, predictions, and ground truth are stored in\na hierarchical experiment_data dict and saved as a NumPy file, while a learning-\ncurve figure is written to the ./working directory.  The code automatically\nfalls back to a tiny synthetic dataset if SPR_BENCH is not found, ensuring the\nscript is fully self-contained and executable in under 30 minutes.  This\nprovides a clean, reproducible baseline that we will refine with more advanced\narchitectures in later stages.", "Hyperparam tuning name: num_epochs. Below solution extends training up to 30\nepochs and adds an early-stopping mechanism (patience = 5) so the run halts once\nvalidation loss stops improving. All metrics/losses are tracked and saved in the\nrequested experiment_data structure.", "Hyperparam tuning name: learning_rate. The solution sweeps over the learning-\nrate grid {3e-4, 5e-4, 1e-3, 2e-3}.   For every rate it: (1) trains a fresh\nmodel for five epochs, (2) logs train/validation loss and macro-F1, and (3)\nstores everything in an experiment_data structure under the key \"learning_rate\".\nAfter the sweep the rate with the best validation macro-F1 is re-chosen, the\nmodel is retrained, and the test macro-F1 plus predictions/labels are saved.\nAll arrays are stored with np.save(\"experiment_data.npy\") so subsequent\nanalysis/plots are easy.", "Hyperparam tuning name: batch_size. We iterate over the candidate batch sizes\n(32, 64, 256).   For each value we build a DataLoader with that batch_size,\ntrain a freshly-initialized CharBiGRU for a fixed number of epochs, evaluate on\nvalidation and test splits, collect losses/F1 and save loss-curves.   Results\nfor every batch size are stored inside experiment_data under the top-key\n\u201cbatch_size\u201d, then serialized to experiment_data.npy.", "Hyperparam tuning name: emb_dim (embedding dimension). We will grid-search\ncharacter embedding sizes (32, 64, 128, 256).   For every dimension we: (1)\nbuild a fresh CharBiGRU, (2) train 5 epochs, (3) record train/validation losses\nand macro-F1, (4) keep the model achieving the highest dev F1.   All results are\nstored in a nested experiment_data dictionary under the top-level key \"emb_dim\",\nthen saved to experiment_data.npy.   After tuning, the best model is evaluated\non the test split and its predictions/labels are saved as well.", "The bug occurs when trying to concatenate two HuggingFace `Dataset` objects with\nthe \u201c+\u201d operator.   Instead, use `datasets.concatenate_datasets([ds1, ds2])`,\nwhich correctly merges datasets while preserving column types and features.\nThe revised script below replaces that line, adds the required device handling,\nkeeps hyper-parameter tuning unchanged, and stores all metrics and artefacts in\nthe prescribed `working/` directory.", "Hyperparam tuning name: hidden_size. We grid-search the GRU hidden dimension\nover {64, 128, 256, 512}.   For each size we (1) build a fresh bi-GRU\nclassifier, (2) train it for a few epochs, (3) store per-epoch losses and F1\nscores, and (4) keep the model that yields the best dev F1.   All statistics are\ncollected in the experiment_data dict under the key \"hidden_size\" (then by\nactual size), saved to experiment_data.npy, and the best model is finally\nevaluated on the test set.", "Hyperparam tuning name: dropout_rate. Here\u2019s the plan: we wrap the original\npipeline in a loop over four dropout probabilities (0.0, 0.1, 0.3, 0.5).   A new\nCharBiGRU class now accepts dropout_rate and applies nn.Dropout after the bi-GRU\nhidden state.   For every rate we (re)-initialize the model, train for 5 epochs,\ntrack per-epoch losses/F1, evaluate on the dev and test splits, and store\neverything inside experiment_data[\u2018dropout_rate\u2019][rate].   Finally we save the\nwhole dictionary to experiment_data.npy and also emit one loss-curve PNG per\nrate.", "Hyperparam tuning name: weight_decay. We sweep five weight-decay coefficients\n(0, 1e-6, 1e-5, 1e-4, 1e-3).   For each value we (1) re-instantiate the BiGRU,\n(2) train 5 epochs with Adam using that weight-decay, (3) record per-epoch\ntrain/validation losses and macro-F1, and (4) evaluate on the test split.   All\nresults are stored in an experiment_data dictionary under the top-level key\n\u201cweight_decay\u201d, indexed by the numeric value, and finally saved to\nworking/experiment_data.npy.   The model obtaining the highest dev macro-F1 is\nkept and its test macro-F1 is printed.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict\n\n# ---------------- GPU / device ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef get_dataset() -> DatasetDict:\n    possible = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for p in possible:\n        if (p / \"train.csv\").exists():\n            print(f\"Loading real SPR_BENCH from {p}\")\n            return load_spr_bench(p)\n    # --------- synthetic fallback ------------\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows = []\n        shapes = \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)  # even # of A -> label 1\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = to_ds(synth(2000))\n    dset[\"dev\"] = to_ds(synth(500))\n    dset[\"test\"] = to_ds(synth(500))\n    return dset\n\n\nspr = get_dataset()\n\n# -------------- vocabulary -------------------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(list(set(all_text)))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 is PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nprint(f\"Vocab size: {vocab_size-1}\")\n\nmax_len = min(100, max(len(s) for s in spr[\"train\"][\"sequence\"]))  # cap at 100\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq = hf_split[\"sequence\"]\n        self.y = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(encode(self.seq[idx]), dtype=torch.long)\n        y = torch.tensor(int(self.y[idx]), dtype=torch.float)\n        return {\"input_ids\": x, \"label\": y}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------- model ------------------------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int = 64, hid: int = 128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.rnn(emb)\n        h = torch.cat([h[0], h[1]], dim=1)\n        logits = self.fc(h).squeeze(1)\n        return logits\n\n\nmodel = CharBiGRU(vocab_size).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# -------------- tracking ---------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# -------------- training loop ---------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_losses = []\n    tr_outputs = []\n    tr_labels = []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        tr_losses.append(loss.item())\n        preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy()\n        tr_outputs.extend(preds)\n        tr_labels.extend(batch[\"label\"].long().cpu().numpy())\n    train_f1 = f1_score(tr_labels, tr_outputs, average=\"macro\")\n    val_losses = []\n    val_outputs = []\n    val_labels = []\n    model.eval()\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            val_losses.append(loss.item())\n            preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy()\n            val_outputs.extend(preds)\n            val_labels.extend(batch[\"label\"].long().cpu().numpy())\n    val_f1 = f1_score(val_labels, val_outputs, average=\"macro\")\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {np.mean(val_losses):.4f}, val_macro_f1 = {val_f1:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(np.mean(tr_losses))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(np.mean(val_losses))\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# -------------- test evaluation --------------\nmodel.eval()\ntest_preds = []\ntest_labels = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy()\n        test_preds.extend(preds)\n        test_labels.extend(batch[\"label\"].long().cpu().numpy())\ntest_macro_f1 = f1_score(test_labels, test_preds, average=\"macro\")\nprint(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# -------------- visualization ----------------\nplt.figure(figsize=(6, 4))\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"],\n    label=\"train_loss\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"],\n    label=\"val_loss\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"SPR_BENCH_loss_curve\")\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\nfrom typing import List\n\n# ---------- saving / folders ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- load SPR_BENCH or synth ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef get_dataset() -> DatasetDict:\n    possible = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for p in possible:\n        if (p / \"train.csv\").exists():\n            print(f\"Loading real SPR_BENCH from {p}\")\n            return load_spr_bench(p)\n\n    # --- synthetic fallback ---\n    def synth(n):\n        rows, shapes = [], \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n            )\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n    return DatasetDict(\n        train=to_ds(synth(2000)), dev=to_ds(synth(500)), test=to_ds(synth(500))\n    )\n\n\nspr = get_dataset()\n\n# ---------- vocab ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nvocab_size, max_len = len(stoi) + 1, min(100, max(map(len, spr[\"train\"][\"sequence\"])))\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq, self.y = split[\"sequence\"], split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int = 64, hid: int = 128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat((h[0], h[1]), 1)\n        return self.fc(h).squeeze(1)\n\n\nmodel = CharBiGRU(vocab_size).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------- experiment tracking ----------\nexperiment_data = {\n    \"num_epochs\": {  # hyperparameter tuning type\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\ntrack = experiment_data[\"num_epochs\"][\"SPR_BENCH\"]\n\n# ---------- training w/ early stopping ----------\nmax_epochs, patience = 30, 5\nbest_val, bad_epochs = float(\"inf\"), 0\nfor epoch in range(1, max_epochs + 1):\n    # ---- train ----\n    model.train()\n    tr_loss, tr_pred, tr_lab = [], [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        tr_loss.append(loss.item())\n        tr_pred.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n        tr_lab.extend(batch[\"label\"].long().cpu().numpy())\n    train_f1 = f1_score(tr_lab, tr_pred, average=\"macro\")\n\n    # ---- validation ----\n    model.eval()\n    va_loss, va_pred, va_lab = [], [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            va_loss.append(criterion(logits, batch[\"label\"]).item())\n            va_pred.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            va_lab.extend(batch[\"label\"].long().cpu().numpy())\n    val_loss = np.mean(va_loss)\n    val_f1 = f1_score(va_lab, va_pred, average=\"macro\")\n\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, val_macro_f1={val_f1:.4f}\")\n    # ---- logging ----\n    track[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n    track[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n    track[\"losses\"][\"train\"].append(np.mean(tr_loss))\n    track[\"losses\"][\"val\"].append(val_loss)\n    track[\"epochs\"].append(epoch)\n\n    # ---- early stopping ----\n    if val_loss < best_val - 1e-4:\n        best_val, bad_epochs = val_loss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= patience:\n        print(f\"Early stopping at epoch {epoch}\")\n        break\n\n# ---------- test evaluation ----------\nmodel.eval()\ntest_pred, test_lab = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        test_pred.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n        test_lab.extend(batch[\"label\"].long().cpu().numpy())\ntest_macro_f1 = f1_score(test_lab, test_pred, average=\"macro\")\nprint(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n\ntrack[\"predictions\"], track[\"ground_truth\"] = test_pred, test_lab\n\n# ---------- save artifacts ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\nplt.figure(figsize=(6, 4))\nplt.plot(track[\"epochs\"], track[\"losses\"][\"train\"], label=\"train_loss\")\nplt.plot(track[\"epochs\"], track[\"losses\"][\"val\"], label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"SPR_BENCH_loss_curve\")\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict\n\n# ---------- experiment data container ----------\nexperiment_data = {\n    \"learning_rate\": {\n        \"SPR_BENCH\": {\n            \"per_lr\": {},  # filled later\n            \"best_lr\": None,\n            \"test_macro_f1\": None,\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- seed ----------\ndef set_seed(s=42):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n\n\nset_seed()\n\n\n# ---------- dataset helpers ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n    # fallback synthetic\n    print(\"SPR_BENCH not found \u2013 creating synthetic toy dataset\")\n\n    def synth(n):\n        rows = []\n        shapes = \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n            )\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = to_ds(synth(2000))\n    d[\"dev\"] = to_ds(synth(500))\n    d[\"test\"] = to_ds(synth(500))\n    return d\n\n\nspr = get_dataset()\n\n# ---------- vocabulary ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(list(set(all_text)))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.y = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\nbatch_size = 128\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.rnn(emb)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ---------- training / evaluation helpers ----------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    if train_mode:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, labels = [], [], []\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss.append(loss.item())\n            preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            labels.extend(batch[\"label\"].long().cpu().numpy())\n    macro_f1 = f1_score(labels, preds, average=\"macro\")\n    return np.mean(total_loss), macro_f1\n\n\ndef train_for_lr(lr, epochs=5):\n    print(f\"\\n--- Training with lr={lr} ---\")\n    model = CharBiGRU(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    tr_loader = DataLoader(\n        SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n    )\n    val_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\n    metrics = {\"train_macro_f1\": [], \"val_macro_f1\": []}\n    losses = {\"train\": [], \"val\": []}\n    for ep in range(1, epochs + 1):\n        tr_loss, tr_f1 = run_epoch(model, tr_loader, criterion, optimizer)\n        val_loss, val_f1 = run_epoch(model, val_loader, criterion)\n        metrics[\"train_macro_f1\"].append(tr_f1)\n        metrics[\"val_macro_f1\"].append(val_f1)\n        losses[\"train\"].append(tr_loss)\n        losses[\"val\"].append(val_loss)\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f} val_macro_f1={val_f1:.4f}\")\n    return model, metrics, losses\n\n\n# ---------- hyperparameter sweep ----------\nlr_grid = [3e-4, 5e-4, 1e-3, 2e-3]\nbest_val_f1 = -1\nbest_lr = None\nbest_state = None\nfor lr in lr_grid:\n    model, metrics, losses = train_for_lr(lr, epochs=5)\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"per_lr\"][lr] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n    }\n    val_last = metrics[\"val_macro_f1\"][-1]\n    if val_last > best_val_f1:\n        best_val_f1 = val_last\n        best_lr = lr\n        best_state = model.state_dict()\n\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_lr\"] = best_lr\nprint(f\"\\nBest lr by val_macro_f1: {best_lr} ({best_val_f1:.4f})\")\n\n# ---------- retrain on train+dev with best lr, then test ----------\ncomb_train = DatasetDict()\ncomb_train[\"train\"] = spr[\"train\"].flatten_indices() + spr[\"dev\"].flatten_indices()\ntrain_loader = DataLoader(\n    SPRDataset(comb_train[\"train\"]), batch_size=batch_size, shuffle=True\n)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\nbest_model = CharBiGRU(vocab_size).to(device)\nbest_model.load_state_dict(\n    best_state\n)  # start from tuned weights (already trained on train only)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(best_model.parameters(), lr=best_lr)\n# fine-tune 2 more epochs on combined set\nfor _ in range(2):\n    run_epoch(best_model, train_loader, criterion, optimizer)\n\n# test evaluation\nbest_model.eval()\ntest_preds, test_labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = best_model(batch[\"input_ids\"])\n        test_preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n        test_labels.extend(batch[\"label\"].long().cpu().numpy())\ntest_macro_f1 = f1_score(test_labels, test_preds, average=\"macro\")\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"test_macro_f1\"] = test_macro_f1\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nprint(f\"Test Macro-F1 with lr={best_lr}: {test_macro_f1:.4f}\")\n\n# ---------- save experiment data ----------\nos.makedirs(\"working\", exist_ok=True)\nnp.save(\"experiment_data.npy\", experiment_data)\n\n# ---------- optional: plot loss curves for best lr ----------\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nplt.figure(figsize=(6, 4))\nlr_losses = experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"per_lr\"][best_lr][\"losses\"]\nplt.plot(lr_losses[\"train\"], label=\"train_loss\")\nplt.plot(lr_losses[\"val\"], label=\"val_loss\")\nplt.title(f\"Loss curve (lr={best_lr})\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"working/best_lr_loss_curve.png\")\nplt.close()\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict\n\n# ---------- paths / saving ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- load (real or synthetic) SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(f\"Loading real SPR_BENCH from {p}\")\n            return load_spr_bench(p)\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows = []\n        shapes = \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n            )\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = to_ds(synth(2000))\n    d[\"dev\"] = to_ds(synth(500))\n    d[\"test\"] = to_ds(synth(500))\n    return d\n\n\nspr = get_dataset()\n\n# ---------- vocab / encoding ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(list(set(all_text)))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nprint(\"Vocab size\", vocab_size - 1)\nmax_len = min(100, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq = hf_split[\"sequence\"]\n        self.y = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int = 64, hid: int = 128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ---------- experiment data ----------\nexperiment_data = {\"batch_size\": {}}\n\n\ndef run_experiment(bs: int, epochs: int = 5, lr: float = 1e-3):\n    print(f\"\\n=== Training with batch_size={bs} ===\")\n    train_loader = DataLoader(SPRDataset(spr[\"train\"]), batch_size=bs, shuffle=True)\n    val_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=bs)\n    test_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=bs)\n\n    model = CharBiGRU(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=lr)\n\n    record = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        tr_losses = []\n        tr_preds = []\n        tr_lbls = []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optim.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optim.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            tr_lbls.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_lbls, tr_preds, average=\"macro\")\n\n        # validation\n        model.eval()\n        val_losses = []\n        val_preds = []\n        val_lbls = []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_losses.append(criterion(logits, batch[\"label\"]).item())\n                val_preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n                val_lbls.extend(batch[\"label\"].long().cpu().numpy())\n        val_f1 = f1_score(val_lbls, val_preds, average=\"macro\")\n        print(f\"Epoch {epoch}: val_loss {np.mean(val_losses):.4f}, val_f1 {val_f1:.4f}\")\n\n        record[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        record[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        record[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        record[\"losses\"][\"val\"].append(np.mean(val_losses))\n        record[\"epochs\"].append(epoch)\n\n    # test\n    model.eval()\n    test_preds = []\n    test_lbls = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            test_preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            test_lbls.extend(batch[\"label\"].long().cpu().numpy())\n    record[\"predictions\"] = test_preds\n    record[\"ground_truth\"] = test_lbls\n    test_f1 = f1_score(test_lbls, test_preds, average=\"macro\")\n    print(f\"Test Macro-F1 for bs={bs}: {test_f1:.4f}\")\n\n    # plot\n    plt.figure()\n    plt.plot(record[\"epochs\"], record[\"losses\"][\"train\"], label=\"train_loss\")\n    plt.plot(record[\"epochs\"], record[\"losses\"][\"val\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(f\"Loss curve bs={bs}\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_loss_curve_bs{bs}.png\"))\n    plt.close()\n\n    return record\n\n\nfor bs in [32, 64, 256]:\n    experiment_data[\"batch_size\"][f\"SPR_BENCH_bs{bs}\"] = run_experiment(bs)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# -------------------------- misc / seeds --------------------------\ntorch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n# -------------------------- saving dir ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------- device --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------------- data ----------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_file: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    ds = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        ds[split] = _load(f\"{split}.csv\")\n    return ds\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n\n    # synthetic fallback if real data missing\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows = []\n        shapes = \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_hfds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = to_hfds(synth(2000))\n    dset[\"dev\"] = to_hfds(synth(500))\n    dset[\"test\"] = to_hfds(synth(500))\n    return dset\n\n\nspr = get_dataset()\n\n# -------------------------- vocab ---------------------------------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(list(set(all_text)))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nprint(\"Vocab size:\", vocab_size - 1)\n\nmax_len = min(100, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef encode(seq: str):\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq = hf_split[\"sequence\"]\n        self.y = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(encode(self.seq[idx]), dtype=torch.long)\n        y = torch.tensor(int(self.y[idx]), dtype=torch.float)\n        return {\"input_ids\": x, \"label\": y}\n\n\n# -------------------------- model ---------------------------------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int = 64, hid: int = 128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.rnn(emb)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# -------------------- hyperparameter tuning -----------------------\nembed_dims = [32, 64, 128, 256]\nbatch_size, lr, epochs = 128, 1e-3, 5\n\nexperiment_data = {\"emb_dim\": {}}\nbest_val_f1, best_state, best_dim = -1, None, None\n\nfor emb_dim in embed_dims:\n    print(f\"\\n=== Training with emb_dim={emb_dim} ===\")\n    train_loader = DataLoader(\n        SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n    )\n    val_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\n    test_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\n    model = CharBiGRU(vocab_size, emb_dim=emb_dim).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    logs = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n\n    for ep in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_loss, tr_preds, tr_lbls = [], [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            tr_loss.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).cpu().int().numpy())\n            tr_lbls.extend(batch[\"label\"].cpu().int().numpy())\n        train_f1 = f1_score(tr_lbls, tr_preds, average=\"macro\")\n\n        # ---- validation ----\n        model.eval()\n        val_loss, val_preds, val_lbls = [], [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                val_loss.append(loss.item())\n                val_preds.extend((torch.sigmoid(logits) > 0.5).cpu().int().numpy())\n                val_lbls.extend(batch[\"label\"].cpu().int().numpy())\n        val_f1 = f1_score(val_lbls, val_preds, average=\"macro\")\n        print(f\"Epoch {ep}: val_loss={np.mean(val_loss):.4f}  val_F1={val_f1:.4f}\")\n\n        # ---- log ----\n        logs[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        logs[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        logs[\"losses\"][\"train\"].append(np.mean(tr_loss))\n        logs[\"losses\"][\"val\"].append(np.mean(val_loss))\n        logs[\"epochs\"].append(ep)\n\n        # save best across epochs for this emb_dim\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_state = model.state_dict()\n            best_dim = emb_dim\n\n    experiment_data[\"emb_dim\"][emb_dim] = logs\n\nprint(f\"\\nBest emb_dim={best_dim} with dev F1={best_val_f1:.4f}\")\n\n# -------------------- test with best model ------------------------\ntrain_loader = DataLoader(SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\nbest_model = CharBiGRU(vocab_size, emb_dim=best_dim).to(device)\nbest_model.load_state_dict(best_state)\nbest_model.eval()\n\ntest_preds, test_lbls = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = best_model(batch[\"input_ids\"])\n        test_preds.extend((torch.sigmoid(logits) > 0.5).cpu().int().numpy())\n        test_lbls.extend(batch[\"label\"].cpu().int().numpy())\n\ntest_f1 = f1_score(test_lbls, test_preds, average=\"macro\")\nprint(f\"Test Macro-F1 (best emb_dim={best_dim}): {test_f1:.4f}\")\n\nexperiment_data[\"emb_dim\"][best_dim][\"predictions\"] = test_preds\nexperiment_data[\"emb_dim\"][best_dim][\"ground_truth\"] = test_lbls\nexperiment_data[\"emb_dim\"][best_dim][\"test_macro_f1\"] = test_f1\n\n# -------------------- save & plot -------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# plot loss curves per emb_dim\nplt.figure(figsize=(7, 4))\nfor emb_dim, logs in experiment_data[\"emb_dim\"].items():\n    plt.plot(logs[\"epochs\"], logs[\"losses\"][\"val\"], label=f\"emb={emb_dim}\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Val Loss\")\nplt.legend()\nplt.title(\"Validation Loss vs Epochs (different emb_dim)\")\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"val_loss_emb_dim.png\"))\nplt.close()\n", "import os, random, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict, concatenate_datasets\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict\n\n# -------------------------------------------------\n# mandatory working dir and device setup\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------\n# experiment data container\n# -------------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"per_lr\": {},  # lr -> {'metrics':\u2026, 'losses':\u2026}\n        \"best_lr\": None,\n        \"test_macro_f1\": None,\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------\n# deterministic seeding\n# -------------------------------------------------\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed()\n\n\n# -------------------------------------------------\n# dataset helpers\n# -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=os.path.join(working_dir, \".cache_dsets\"),\n        )\n\n    dset = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        dset[sp] = _load(f\"{sp}.csv\")\n    return dset\n\n\ndef get_dataset() -> DatasetDict:\n    # search a couple of likely locations\n    candidate_roots = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for root in candidate_roots:\n        if (root / \"train.csv\").exists():\n            print(f\"Loading real SPR_BENCH from {root}\")\n            return load_spr_bench(root)\n\n    # ---------- synthetic fallback ----------\n    print(\"SPR_BENCH not found \u2013 creating synthetic toy dataset\")\n\n    def _synth(n_rows):\n        rows = []\n        shapes = \"ABCD\"\n        for i in range(n_rows):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            # toy rule: even # of 'A' \u2192 label 1 else 0\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n            )\n        return rows\n\n    def _to_ds(rows):\n        # write rows to disk-less json dataset\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _to_ds(_synth(2000))\n    dset[\"dev\"] = _to_ds(_synth(500))\n    dset[\"test\"] = _to_ds(_synth(500))\n    return dset\n\n\nspr = get_dataset()\n\n# -------------------------------------------------\n# vocabulary and encoding utilities\n# -------------------------------------------------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(list(set(all_text)))\nstoi = {ch: idx + 1 for idx, ch in enumerate(vocab)}  # 0 reserved for PAD/UNK\nitos = {idx: ch for ch, idx in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.labels[idx]), dtype=torch.float),\n        }\n\n\n# -------------------------------------------------\n# model definition\n# -------------------------------------------------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int = 64, hid: int = 128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        x = self.emb(x)\n        _, h = self.rnn(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# -------------------------------------------------\n# helpers for training / evaluation\n# -------------------------------------------------\ndef run_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n\n    epoch_loss, preds, labels = [], [], []\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            # move to device\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss.append(loss.item())\n            preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            labels.extend(batch[\"label\"].long().cpu().numpy())\n    macro_f1 = f1_score(labels, preds, average=\"macro\")\n    return float(np.mean(epoch_loss)), macro_f1\n\n\n# -------------------------------------------------\n# hyper-parameter sweep (learning-rate)\n# -------------------------------------------------\nbatch_size = 128\nlr_grid = [3e-4, 5e-4, 1e-3, 2e-3]\nbest_val_f1, best_lr, best_state = -1.0, None, None\n\nfor lr in lr_grid:\n    print(f\"\\n--- Training with lr={lr} ---\")\n    model = CharBiGRU(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n\n    tr_loader = DataLoader(\n        SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n    )\n    val_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\n\n    metrics = {\"train_macro_F1\": [], \"val_macro_F1\": []}\n    losses = {\"train\": [], \"val\": []}\n\n    for epoch in range(1, 6):  # 5 epochs\n        tr_loss, tr_f1 = run_epoch(model, tr_loader, criterion, optimizer)\n        val_loss, val_f1 = run_epoch(model, val_loader, criterion)\n        metrics[\"train_macro_F1\"].append(tr_f1)\n        metrics[\"val_macro_F1\"].append(val_f1)\n        losses[\"train\"].append(tr_loss)\n        losses[\"val\"].append(val_loss)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f} \"\n            f\"macro_F1 = {val_f1:.4f}\"\n        )\n\n    experiment_data[\"SPR_BENCH\"][\"per_lr\"][lr] = {\"metrics\": metrics, \"losses\": losses}\n    # track best\n    if metrics[\"val_macro_F1\"][-1] > best_val_f1:\n        best_val_f1 = metrics[\"val_macro_F1\"][-1]\n        best_lr = lr\n        best_state = model.state_dict()\n\nexperiment_data[\"SPR_BENCH\"][\"best_lr\"] = best_lr\nprint(f\"\\nBest lr by val_macro_F1: {best_lr} ({best_val_f1:.4f})\")\n\n# -------------------------------------------------\n# retrain on (train + dev) with best learning rate\n# -------------------------------------------------\ncombined_train_ds = concatenate_datasets([spr[\"train\"], spr[\"dev\"]])\ntrain_loader = DataLoader(\n    SPRDataset(combined_train_ds), batch_size=batch_size, shuffle=True\n)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\nbest_model = CharBiGRU(vocab_size).to(device)\nbest_model.load_state_dict(best_state)  # warm-start\noptimizer = torch.optim.Adam(best_model.parameters(), lr=best_lr)\ncriterion = nn.BCEWithLogitsLoss()\n\nprint(\"\\nFine-tuning on combined train+dev (2 epochs)\")\nfor epoch in range(1, 3):\n    ft_loss, ft_f1 = run_epoch(best_model, train_loader, criterion, optimizer)\n    print(\n        f\"Fine-tune Epoch {epoch}: train_loss = {ft_loss:.4f} \"\n        f\"macro_F1 = {ft_f1:.4f}\"\n    )\n\n# -------------------------------------------------\n# final test evaluation\n# -------------------------------------------------\nbest_model.eval()\ntest_preds, test_labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = best_model(batch[\"input_ids\"])\n        test_preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n        test_labels.extend(batch[\"label\"].long().cpu().numpy())\n\ntest_macro_f1 = f1_score(test_labels, test_preds, average=\"macro\")\nexperiment_data[\"SPR_BENCH\"][\"test_macro_f1\"] = test_macro_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nprint(f\"\\nTest Macro-F1 with lr={best_lr}: {test_macro_f1:.4f}\")\n\n# -------------------------------------------------\n# save artefacts\n# -------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# optional: plot loss curve for best lr\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nplt.figure(figsize=(6, 4))\nbest_losses = experiment_data[\"SPR_BENCH\"][\"per_lr\"][best_lr][\"losses\"]\nplt.plot(best_losses[\"train\"], label=\"train_loss\")\nplt.plot(best_losses[\"val\"], label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BCE Loss\")\nplt.title(f\"Loss curve (lr={best_lr})\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"best_lr_loss_curve.png\"))\nplt.close()\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\nfrom typing import List\n\n# ------------- folders & meta -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"hidden_size\": {}}  # container required by guideline\n\n# ------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------- data loading --------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    ds = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        ds[split] = _load(f\"{split}.csv\")\n    return ds\n\n\ndef get_dataset() -> DatasetDict:\n    # search local real dataset\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n    # otherwise create synthetic toy dataset\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows, shapes = [], \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n            )\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        to_ds(synth(2000)),\n        to_ds(synth(500)),\n        to_ds(synth(500)),\n    )\n    return dset\n\n\nspr = get_dataset()\n\n# ------------- vocab ----------------------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 reserved for PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nprint(\"Vocab size:\", vocab_size - 1)\nmax_len = min(100, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq, self.y = split[\"sequence\"], split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ------------- model ----------------------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ------------- hyper-parameter sweep ------\nhidden_sizes = [64, 128, 256, 512]\nnum_epochs = 5\nbest_val_f1, best_state, best_hid = 0.0, None, None\n\nfor hid in hidden_sizes:\n    print(f\"\\n=== Training with hidden_size={hid} ===\")\n    # containers for this run\n    exp_run = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n    }\n    # model, optimiser\n    model = CharBiGRU(vocab_size, emb_dim=64, hid=hid).to(device)\n    crit = nn.BCEWithLogitsLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_loss, tr_pred, tr_gold = [], [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            opt.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"label\"])\n            loss.backward()\n            opt.step()\n            tr_loss.append(loss.item())\n            tr_pred.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            tr_gold.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_gold, tr_pred, average=\"macro\")\n\n        # ---- validate ----\n        model.eval()\n        val_loss, val_pred, val_gold = [], [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_loss.append(crit(logits, batch[\"label\"]).item())\n                val_pred.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n                val_gold.extend(batch[\"label\"].long().cpu().numpy())\n        val_f1 = f1_score(val_gold, val_pred, average=\"macro\")\n\n        print(\n            f\"  Epoch {epoch}: val_loss={np.mean(val_loss):.4f} val_macro_f1={val_f1:.4f}\"\n        )\n\n        # store stats\n        exp_run[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        exp_run[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        exp_run[\"losses\"][\"train\"].append(np.mean(tr_loss))\n        exp_run[\"losses\"][\"val\"].append(np.mean(val_loss))\n        exp_run[\"epochs\"].append(epoch)\n\n    # save run info\n    experiment_data[\"hidden_size\"][str(hid)] = exp_run\n\n    # keep best\n    if exp_run[\"metrics\"][\"val_macro_f1\"][-1] > best_val_f1:\n        best_val_f1 = exp_run[\"metrics\"][\"val_macro_f1\"][-1]\n        best_state = model.state_dict()\n        best_hid = hid\n\nprint(f\"\\nBest hidden_size={best_hid} with dev macro-F1={best_val_f1:.4f}\")\n\n# ------------- test evaluation with best ---\nbest_model = CharBiGRU(vocab_size, emb_dim=64, hid=best_hid).to(device)\nbest_model.load_state_dict(best_state)\nbest_model.eval()\ntest_pred, test_gold = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = best_model(batch[\"input_ids\"])\n        test_pred.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n        test_gold.extend(batch[\"label\"].long().cpu().numpy())\ntest_macro_f1 = f1_score(test_gold, test_pred, average=\"macro\")\nprint(f\"Test Macro-F1 (best model): {test_macro_f1:.4f}\")\n\nexperiment_data[\"hidden_size\"][\"best_model\"] = {\n    \"hidden_size\": best_hid,\n    \"test_macro_f1\": test_macro_f1,\n    \"predictions\": np.array(test_pred),\n    \"ground_truth\": np.array(test_gold),\n}\n\n# ------------- save & plot -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# loss curves for each hid\nplt.figure(figsize=(8, 5))\nfor hid in hidden_sizes:\n    plt.plot(\n        experiment_data[\"hidden_size\"][str(hid)][\"epochs\"],\n        experiment_data[\"hidden_size\"][str(hid)][\"losses\"][\"val\"],\n        label=f\"hid{hid}\",\n    )\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Val loss\")\nplt.title(\"Val loss per hidden size\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"val_loss_hidden_size.png\"))\nplt.close()\n", "import os, pathlib, random\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\n# ---------- dirs ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- data ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):  # helper to read csv\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n\n    # ---------- synthetic fallback ----------\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows, shapes = [], \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        to_ds(synth(2000)),\n        to_ds(synth(500)),\n        to_ds(synth(500)),\n    )\n    return d\n\n\nspr = get_dataset()\n\n# ---------- vocab ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 is PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(map(len, spr[\"train\"][\"sequence\"])))\n\n\ndef encode(seq: str):\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq, self.y = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\ntrain_loader = lambda s: DataLoader(\n    SPRDataset(spr[s]), batch_size=128, shuffle=(s == \"train\")\n)\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128, dropout=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat([h[0], h[1]], 1)\n        h = self.drop(h)\n        return self.fc(h).squeeze(1)\n\n\n# ---------- experiment store ----------\nexperiment_data = {\"dropout_rate\": {}}\n\n# ---------- hyper-parameter sweep ----------\ndropout_grid = [0.0, 0.1, 0.3, 0.5]\nepochs = 5\nfor rate in dropout_grid:\n    print(f\"\\n=== Training with dropout_rate={rate} ===\")\n    model = CharBiGRU(vocab_size, dropout=rate).to(device)\n    crit = nn.BCEWithLogitsLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    rec = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # ---- epoch loop ----\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tr_loss, tr_preds, tr_labels = [], [], []\n        for batch in train_loader(\"train\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            opt.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"label\"])\n            loss.backward()\n            opt.step()\n            tr_loss.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tr_labels.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n\n        # validate\n        model.eval()\n        val_loss, val_preds, val_labels = [], [], []\n        with torch.no_grad():\n            for batch in train_loader(\"dev\"):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_loss.append(crit(logits, batch[\"label\"]).item())\n                val_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n                val_labels.extend(batch[\"label\"].long().cpu().numpy())\n        val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n\n        print(\n            f\"Epoch {ep} | train_loss={np.mean(tr_loss):.4f} val_loss={np.mean(val_loss):.4f} \"\n            f\"train_F1={train_f1:.3f} val_F1={val_f1:.3f}\"\n        )\n\n        rec[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        rec[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        rec[\"losses\"][\"train\"].append(np.mean(tr_loss))\n        rec[\"losses\"][\"val\"].append(np.mean(val_loss))\n        rec[\"epochs\"].append(ep)\n\n    # ---- test ----\n    model.eval()\n    tst_preds, tst_labels = [], []\n    with torch.no_grad():\n        for batch in train_loader(\"test\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tst_labels.extend(batch[\"label\"].long().cpu().numpy())\n    test_f1 = f1_score(tst_labels, tst_preds, average=\"macro\")\n    print(f\"Test Macro-F1 (dropout={rate}): {test_f1:.4f}\")\n\n    rec[\"predictions\"], rec[\"ground_truth\"] = tst_preds, tst_labels\n    rec[\"test_macro_f1\"] = test_f1\n    experiment_data[\"dropout_rate\"][rate] = rec\n\n    # plot per-rate loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"train\"], label=\"train\")\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"val\"], label=\"val\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.title(f\"Loss curve (dropout={rate})\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"loss_curve_dropout_{rate}.png\"))\n    plt.close()\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\nfrom typing import List\n\n# -------------------- paths / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------- data loading ---------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    possible = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for p in possible:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n\n    # ---------- synthetic fallback ----------\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n\n    def build_rows(n):\n        shapes = \"ABCD\"\n        rows = []\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def rows_to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = rows_to_ds(build_rows(2000))\n    d[\"dev\"] = rows_to_ds(build_rows(500))\n    d[\"test\"] = rows_to_ds(build_rows(500))\n    return d\n\n\nspr = get_dataset()\n\n# -------------------- vocab ----------------------------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 = PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(map(len, spr[\"train\"][\"sequence\"])))\n\n\ndef encode(seq: str) -> List[int]:\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq = hf_split[\"sequence\"]\n        self.y = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_loader = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model ----------------------------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.rnn(emb)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ------------------ experiment tracking ----------------\nexperiment_data = {\"weight_decay\": {\"SPR_BENCH\": {}}}  # will fill per weight_decay\n\nweight_decays = [0.0, 1e-6, 1e-5, 1e-4, 1e-3]\nepochs = 5\nbest_val_f1, best_state, best_wd = -1, None, None\n\n\ndef evaluate(model, loader):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            labels.extend(batch[\"label\"].long().cpu().numpy())\n    return f1_score(labels, preds, average=\"macro\"), preds, labels\n\n\n# ------------------- hyper-parameter sweep --------------\ncriterion = nn.BCEWithLogitsLoss()\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay = {wd} ===\")\n    # init containers\n    exp_rec = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][str(wd)] = exp_rec\n    # model / optim\n    model = CharBiGRU(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    # epoch loop\n    for ep in range(1, epochs + 1):\n        model.train()\n        tr_losses, tr_preds, tr_labels = [], [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            tr_losses.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n            tr_labels.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n        val_f1, _, _ = evaluate(model, dev_loader)\n        exp_rec[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        exp_rec[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        exp_rec[\"losses\"][\"train\"].append(np.mean(tr_losses))\n        exp_rec[\"losses\"][\"val\"].append(0)  # will fill below\n        exp_rec[\"epochs\"].append(ep)\n        # quick val loss for record\n        with torch.no_grad():\n            val_losses = []\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                val_losses.append(\n                    criterion(model(batch[\"input_ids\"]), batch[\"label\"]).item()\n                )\n        exp_rec[\"losses\"][\"val\"][-1] = np.mean(val_losses)\n        print(f\"  Epoch {ep}: train_f1={train_f1:.4f}  val_f1={val_f1:.4f}\")\n    # test eval\n    test_f1, t_preds, t_labels = evaluate(model, test_loader)\n    exp_rec[\"predictions\"] = t_preds\n    exp_rec[\"ground_truth\"] = t_labels\n    print(f\"  --> weight_decay {wd} Test macro-F1: {test_f1:.4f}\")\n    # track best on dev\n    if exp_rec[\"metrics\"][\"val_macro_f1\"][-1] > best_val_f1:\n        best_val_f1 = exp_rec[\"metrics\"][\"val_macro_f1\"][-1]\n        best_state = model.state_dict()\n        best_wd = wd\n\n# ------------------- best model report ------------------\nprint(f\"\\nBest weight_decay based on dev set: {best_wd}  (val_f1={best_val_f1:.4f})\")\nbest_model = CharBiGRU(vocab_size).to(device)\nbest_model.load_state_dict(best_state)\nbest_test_f1, _, _ = evaluate(best_model, test_loader)\nprint(f\"Test Macro-F1 of best model: {best_test_f1:.4f}\")\n\n# ------------------- save data & plot -------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n\n# (Optional) loss curve for best wd\nrec = experiment_data[\"weight_decay\"][\"SPR_BENCH\"][str(best_wd)]\nplt.figure(figsize=(6, 4))\nplt.plot(rec[\"epochs\"], rec[\"losses\"][\"train\"], label=\"train_loss\")\nplt.plot(rec[\"epochs\"], rec[\"losses\"][\"val\"], label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(f\"Loss curve wd={best_wd}\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, f\"loss_curve_wd_{best_wd}.png\"))\nplt.close()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\n# ---------- dirs ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- data ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):  # helper to read csv\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n\n    # ---------- synthetic fallback ----------\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows, shapes = [], \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        to_ds(synth(2000)),\n        to_ds(synth(500)),\n        to_ds(synth(500)),\n    )\n    return d\n\n\nspr = get_dataset()\n\n# ---------- vocab ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 is PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(map(len, spr[\"train\"][\"sequence\"])))\n\n\ndef encode(seq: str):\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq, self.y = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\ntrain_loader = lambda s: DataLoader(\n    SPRDataset(spr[s]), batch_size=128, shuffle=(s == \"train\")\n)\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128, dropout=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat([h[0], h[1]], 1)\n        h = self.drop(h)\n        return self.fc(h).squeeze(1)\n\n\n# ---------- experiment store ----------\nexperiment_data = {\"dropout_rate\": {}}\n\n# ---------- hyper-parameter sweep ----------\ndropout_grid = [0.0, 0.1, 0.3, 0.5]\nepochs = 5\nfor rate in dropout_grid:\n    print(f\"\\n=== Training with dropout_rate={rate} ===\")\n    model = CharBiGRU(vocab_size, dropout=rate).to(device)\n    crit = nn.BCEWithLogitsLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    rec = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # ---- epoch loop ----\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tr_loss, tr_preds, tr_labels = [], [], []\n        for batch in train_loader(\"train\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            opt.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"label\"])\n            loss.backward()\n            opt.step()\n            tr_loss.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tr_labels.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n\n        # validate\n        model.eval()\n        val_loss, val_preds, val_labels = [], [], []\n        with torch.no_grad():\n            for batch in train_loader(\"dev\"):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_loss.append(crit(logits, batch[\"label\"]).item())\n                val_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n                val_labels.extend(batch[\"label\"].long().cpu().numpy())\n        val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n\n        print(\n            f\"Epoch {ep} | train_loss={np.mean(tr_loss):.4f} val_loss={np.mean(val_loss):.4f} \"\n            f\"train_F1={train_f1:.3f} val_F1={val_f1:.3f}\"\n        )\n\n        rec[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        rec[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        rec[\"losses\"][\"train\"].append(np.mean(tr_loss))\n        rec[\"losses\"][\"val\"].append(np.mean(val_loss))\n        rec[\"epochs\"].append(ep)\n\n    # ---- test ----\n    model.eval()\n    tst_preds, tst_labels = [], []\n    with torch.no_grad():\n        for batch in train_loader(\"test\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tst_labels.extend(batch[\"label\"].long().cpu().numpy())\n    test_f1 = f1_score(tst_labels, tst_preds, average=\"macro\")\n    print(f\"Test Macro-F1 (dropout={rate}): {test_f1:.4f}\")\n\n    rec[\"predictions\"], rec[\"ground_truth\"] = tst_preds, tst_labels\n    rec[\"test_macro_f1\"] = test_f1\n    experiment_data[\"dropout_rate\"][rate] = rec\n\n    # plot per-rate loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"train\"], label=\"train\")\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"val\"], label=\"val\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.title(f\"Loss curve (dropout={rate})\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"loss_curve_dropout_{rate}.png\"))\n    plt.close()\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\n# ---------- dirs ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- data ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):  # helper to read csv\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n\n    # ---------- synthetic fallback ----------\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows, shapes = [], \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        to_ds(synth(2000)),\n        to_ds(synth(500)),\n        to_ds(synth(500)),\n    )\n    return d\n\n\nspr = get_dataset()\n\n# ---------- vocab ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 is PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(map(len, spr[\"train\"][\"sequence\"])))\n\n\ndef encode(seq: str):\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq, self.y = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\ntrain_loader = lambda s: DataLoader(\n    SPRDataset(spr[s]), batch_size=128, shuffle=(s == \"train\")\n)\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128, dropout=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat([h[0], h[1]], 1)\n        h = self.drop(h)\n        return self.fc(h).squeeze(1)\n\n\n# ---------- experiment store ----------\nexperiment_data = {\"dropout_rate\": {}}\n\n# ---------- hyper-parameter sweep ----------\ndropout_grid = [0.0, 0.1, 0.3, 0.5]\nepochs = 5\nfor rate in dropout_grid:\n    print(f\"\\n=== Training with dropout_rate={rate} ===\")\n    model = CharBiGRU(vocab_size, dropout=rate).to(device)\n    crit = nn.BCEWithLogitsLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    rec = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # ---- epoch loop ----\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tr_loss, tr_preds, tr_labels = [], [], []\n        for batch in train_loader(\"train\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            opt.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"label\"])\n            loss.backward()\n            opt.step()\n            tr_loss.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tr_labels.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n\n        # validate\n        model.eval()\n        val_loss, val_preds, val_labels = [], [], []\n        with torch.no_grad():\n            for batch in train_loader(\"dev\"):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_loss.append(crit(logits, batch[\"label\"]).item())\n                val_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n                val_labels.extend(batch[\"label\"].long().cpu().numpy())\n        val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n\n        print(\n            f\"Epoch {ep} | train_loss={np.mean(tr_loss):.4f} val_loss={np.mean(val_loss):.4f} \"\n            f\"train_F1={train_f1:.3f} val_F1={val_f1:.3f}\"\n        )\n\n        rec[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        rec[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        rec[\"losses\"][\"train\"].append(np.mean(tr_loss))\n        rec[\"losses\"][\"val\"].append(np.mean(val_loss))\n        rec[\"epochs\"].append(ep)\n\n    # ---- test ----\n    model.eval()\n    tst_preds, tst_labels = [], []\n    with torch.no_grad():\n        for batch in train_loader(\"test\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tst_labels.extend(batch[\"label\"].long().cpu().numpy())\n    test_f1 = f1_score(tst_labels, tst_preds, average=\"macro\")\n    print(f\"Test Macro-F1 (dropout={rate}): {test_f1:.4f}\")\n\n    rec[\"predictions\"], rec[\"ground_truth\"] = tst_preds, tst_labels\n    rec[\"test_macro_f1\"] = test_f1\n    experiment_data[\"dropout_rate\"][rate] = rec\n\n    # plot per-rate loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"train\"], label=\"train\")\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"val\"], label=\"val\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.title(f\"Loss curve (dropout={rate})\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"loss_curve_dropout_{rate}.png\"))\n    plt.close()\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\n# ---------- dirs ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- data ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):  # helper to read csv\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndef get_dataset() -> DatasetDict:\n    for p in [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]:\n        if (p / \"train.csv\").exists():\n            print(\"Loading real SPR_BENCH from\", p)\n            return load_spr_bench(p)\n\n    # ---------- synthetic fallback ----------\n    print(\"SPR_BENCH not found, creating synthetic toy dataset\")\n\n    def synth(n):\n        rows, shapes = [], \"ABCD\"\n        for i in range(n):\n            seq = \"\".join(random.choices(shapes, k=random.randint(5, 12)))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\n            \"json\", data_files={\"data\": rows}, field=\"data\", split=\"train\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        to_ds(synth(2000)),\n        to_ds(synth(500)),\n        to_ds(synth(500)),\n    )\n    return d\n\n\nspr = get_dataset()\n\n# ---------- vocab ----------\nall_text = \"\".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text))\nstoi = {ch: i + 1 for i, ch in enumerate(vocab)}  # 0 is PAD\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(stoi) + 1\nmax_len = min(100, max(map(len, spr[\"train\"][\"sequence\"])))\n\n\ndef encode(seq: str):\n    ids = [stoi.get(ch, 0) for ch in seq[:max_len]]\n    return ids + [0] * (max_len - len(ids))\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq, self.y = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seq[idx]), dtype=torch.long),\n            \"label\": torch.tensor(int(self.y[idx]), dtype=torch.float),\n        }\n\n\ntrain_loader = lambda s: DataLoader(\n    SPRDataset(spr[s]), batch_size=128, shuffle=(s == \"train\")\n)\n\n\n# ---------- model ----------\nclass CharBiGRU(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128, dropout=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hid * 2, 1)\n\n    def forward(self, x):\n        _, h = self.rnn(self.emb(x))\n        h = torch.cat([h[0], h[1]], 1)\n        h = self.drop(h)\n        return self.fc(h).squeeze(1)\n\n\n# ---------- experiment store ----------\nexperiment_data = {\"dropout_rate\": {}}\n\n# ---------- hyper-parameter sweep ----------\ndropout_grid = [0.0, 0.1, 0.3, 0.5]\nepochs = 5\nfor rate in dropout_grid:\n    print(f\"\\n=== Training with dropout_rate={rate} ===\")\n    model = CharBiGRU(vocab_size, dropout=rate).to(device)\n    crit = nn.BCEWithLogitsLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    rec = {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # ---- epoch loop ----\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tr_loss, tr_preds, tr_labels = [], [], []\n        for batch in train_loader(\"train\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            opt.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"label\"])\n            loss.backward()\n            opt.step()\n            tr_loss.append(loss.item())\n            tr_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tr_labels.extend(batch[\"label\"].long().cpu().numpy())\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n\n        # validate\n        model.eval()\n        val_loss, val_preds, val_labels = [], [], []\n        with torch.no_grad():\n            for batch in train_loader(\"dev\"):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_loss.append(crit(logits, batch[\"label\"]).item())\n                val_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n                val_labels.extend(batch[\"label\"].long().cpu().numpy())\n        val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n\n        print(\n            f\"Epoch {ep} | train_loss={np.mean(tr_loss):.4f} val_loss={np.mean(val_loss):.4f} \"\n            f\"train_F1={train_f1:.3f} val_F1={val_f1:.3f}\"\n        )\n\n        rec[\"metrics\"][\"train_macro_f1\"].append(train_f1)\n        rec[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        rec[\"losses\"][\"train\"].append(np.mean(tr_loss))\n        rec[\"losses\"][\"val\"].append(np.mean(val_loss))\n        rec[\"epochs\"].append(ep)\n\n    # ---- test ----\n    model.eval()\n    tst_preds, tst_labels = [], []\n    with torch.no_grad():\n        for batch in train_loader(\"test\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            tst_preds.extend((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            tst_labels.extend(batch[\"label\"].long().cpu().numpy())\n    test_f1 = f1_score(tst_labels, tst_preds, average=\"macro\")\n    print(f\"Test Macro-F1 (dropout={rate}): {test_f1:.4f}\")\n\n    rec[\"predictions\"], rec[\"ground_truth\"] = tst_preds, tst_labels\n    rec[\"test_macro_f1\"] = test_f1\n    experiment_data[\"dropout_rate\"][rate] = rec\n\n    # plot per-rate loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"train\"], label=\"train\")\n    plt.plot(rec[\"epochs\"], rec[\"losses\"][\"val\"], label=\"val\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.legend()\n    plt.title(f\"Loss curve (dropout={rate})\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"loss_curve_dropout_{rate}.png\"))\n    plt.close()\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 97701.00\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 86370.08\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 142882.10\nexamples/s]', '\\n', 'Vocab size: 9', '\\n', 'Epoch 1: validation_loss = 0.6445,\nval_macro_f1 = 0.6777', '\\n', 'Epoch 2: validation_loss = 0.6417, val_macro_f1 =\n0.6794', '\\n', 'Epoch 3: validation_loss = 0.6396, val_macro_f1 = 0.6771', '\\n',\n'Epoch 4: validation_loss = 0.6405, val_macro_f1 = 0.6815', '\\n', 'Epoch 5:\nvalidation_loss = 0.6487, val_macro_f1 = 0.6498', '\\n', 'Test Macro-F1: 0.6629',\n'\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 107279.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 105400.41\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 167792.30\nexamples/s]', '\\n', 'Epoch 1: val_loss=0.6445, val_macro_f1=0.6777', '\\n',\n'Epoch 2: val_loss=0.6417, val_macro_f1=0.6794', '\\n', 'Epoch 3:\nval_loss=0.6396, val_macro_f1=0.6771', '\\n', 'Epoch 4: val_loss=0.6405,\nval_macro_f1=0.6815', '\\n', 'Epoch 5: val_loss=0.6487, val_macro_f1=0.6498',\n'\\n', 'Epoch 6: val_loss=0.6424, val_macro_f1=0.6771', '\\n', 'Epoch 7:\nval_loss=0.6440, val_macro_f1=0.6771', '\\n', 'Epoch 8: val_loss=0.6532,\nval_macro_f1=0.6578', '\\n', 'Early stopping at epoch 8', '\\n', 'Test Macro-F1:\n0.6659', '\\n', 'Execution time: 6 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples\n[00:00, 161291.47 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 500 examples [00:00,\n117054.70 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 184259.72\nexamples/s]', '\\n', '\\n--- Training with lr=0.0003 ---', '\\n', 'Epoch 1:\nval_loss=0.6544 val_macro_f1=0.6520', '\\n', 'Epoch 2: val_loss=0.6434\nval_macro_f1=0.6707', '\\n', 'Epoch 3: val_loss=0.6383 val_macro_f1=0.6813',\n'\\n', 'Epoch 4: val_loss=0.6364 val_macro_f1=0.6809', '\\n', 'Epoch 5:\nval_loss=0.6360 val_macro_f1=0.6809', '\\n', '\\n--- Training with lr=0.0005 ---',\n'\\n', 'Epoch 1: val_loss=0.6451 val_macro_f1=0.6779', '\\n', 'Epoch 2:\nval_loss=0.6368 val_macro_f1=0.6788', '\\n', 'Epoch 3: val_loss=0.6348\nval_macro_f1=0.6831', '\\n', 'Epoch 4: val_loss=0.6353 val_macro_f1=0.6788',\n'\\n', 'Epoch 5: val_loss=0.6391 val_macro_f1=0.6833', '\\n', '\\n--- Training with\nlr=0.001 ---', '\\n', 'Epoch 1: val_loss=0.6472 val_macro_f1=0.6774', '\\n',\n'Epoch 2: val_loss=0.6392 val_macro_f1=0.6768', '\\n', 'Epoch 3: val_loss=0.6400\nval_macro_f1=0.6785', '\\n', 'Epoch 4: val_loss=0.6452 val_macro_f1=0.6640',\n'\\n', 'Epoch 5: val_loss=0.6770 val_macro_f1=0.6023', '\\n', '\\n--- Training with\nlr=0.002 ---', '\\n', 'Epoch 1: val_loss=0.6492 val_macro_f1=0.6698', '\\n',\n'Epoch 2: val_loss=0.6394 val_macro_f1=0.6751', '\\n', 'Epoch 3: val_loss=0.6450\nval_macro_f1=0.6788', '\\n', 'Epoch 4: val_loss=0.6447 val_macro_f1=0.6791',\n'\\n', 'Epoch 5: val_loss=0.6464 val_macro_f1=0.6793', '\\n', '\\nBest lr by\nval_macro_f1: 0.0005 (0.6833)', '\\n', '\\rFlattening the indices:   0%|\n| 0/2000 [00:00<?, ? examples/s]', '', '\\rFlattening the indices:\n100%|##########| 2000/2000 [00:00<00:00, 495136.82 examples/s]', '\\n',\n'\\rFlattening the indices:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rFlattening the indices: 100%|##########| 500/500 [00:00<00:00, 262144.00\nexamples/s]', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\",\nline 204, in <module>\\n    comb_train[\"train\"] = spr[\"train\"].flatten_indices()\n+ spr[\"dev\"].flatten_indices()\\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\nTypeError:\nunsupported operand type(s) for +: \\'Dataset\\' and \\'Dataset\\'\\n', 'Execution\ntime: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 132120.71\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 109689.42\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 190806.30\nexamples/s]', '\\n', 'Vocab size', ' ', '9', '\\n', '\\n=== Training with\nbatch_size=32 ===', '\\n', 'Epoch 1: val_loss 0.6521, val_f1 0.6493', '\\n',\n'Epoch 2: val_loss 0.6416, val_f1 0.6833', '\\n', 'Epoch 3: val_loss 0.6375,\nval_f1 0.6793', '\\n', 'Epoch 4: val_loss 0.6411, val_f1 0.6773', '\\n', 'Epoch 5:\nval_loss 0.6448, val_f1 0.6812', '\\n', 'Test Macro-F1 for bs=32: 0.6849', '\\n',\n'\\n=== Training with batch_size=64 ===', '\\n', 'Epoch 1: val_loss 0.6364, val_f1\n0.6788', '\\n', 'Epoch 2: val_loss 0.6458, val_f1 0.6660', '\\n', 'Epoch 3:\nval_loss 0.6390, val_f1 0.6677', '\\n', 'Epoch 4: val_loss 0.6463, val_f1\n0.6579', '\\n', 'Epoch 5: val_loss 0.6418, val_f1 0.6775', '\\n', 'Test Macro-F1\nfor bs=64: 0.6862', '\\n', '\\n=== Training with batch_size=256 ===', '\\n', 'Epoch\n1: val_loss 0.6541, val_f1 0.6277', '\\n', 'Epoch 2: val_loss 0.6468, val_f1\n0.6806', '\\n', 'Epoch 3: val_loss 0.6407, val_f1 0.6809', '\\n', 'Epoch 4:\nval_loss 0.6381, val_f1 0.6804', '\\n', 'Epoch 5: val_loss 0.6386, val_f1\n0.6773', '\\n', 'Test Macro-F1 for bs=256: 0.6842', '\\n', 'Execution time: 5\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples\n[00:00, 130436.12 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 500 examples [00:00,\n60984.99 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 117280.54\nexamples/s]', '\\n', 'Vocab size:', ' ', '9', '\\n', '\\n=== Training with\nemb_dim=32 ===', '\\n', 'Epoch 1: val_loss=0.6556  val_F1=0.6256', '\\n', 'Epoch\n2: val_loss=0.6439  val_F1=0.6796', '\\n', 'Epoch 3: val_loss=0.6410\nval_F1=0.6798', '\\n', 'Epoch 4: val_loss=0.6412  val_F1=0.6795', '\\n', 'Epoch 5:\nval_loss=0.6436  val_F1=0.6720', '\\n', '\\n=== Training with emb_dim=64 ===',\n'\\n', 'Epoch 1: val_loss=0.6428  val_F1=0.6656', '\\n', 'Epoch 2: val_loss=0.6411\nval_F1=0.6781', '\\n', 'Epoch 3: val_loss=0.6355  val_F1=0.6827', '\\n', 'Epoch 4:\nval_loss=0.6444  val_F1=0.6719', '\\n', 'Epoch 5: val_loss=0.6398\nval_F1=0.6793', '\\n', '\\n=== Training with emb_dim=128 ===', '\\n', 'Epoch 1:\nval_loss=0.6466  val_F1=0.6848', '\\n', 'Epoch 2: val_loss=0.6388\nval_F1=0.6769', '\\n', 'Epoch 3: val_loss=0.6542  val_F1=0.6516', '\\n', 'Epoch 4:\nval_loss=0.6393  val_F1=0.6772', '\\n', 'Epoch 5: val_loss=0.6501\nval_F1=0.6680', '\\n', '\\n=== Training with emb_dim=256 ===', '\\n', 'Epoch 1:\nval_loss=0.6396  val_F1=0.6753', '\\n', 'Epoch 2: val_loss=0.6444\nval_F1=0.6864', '\\n', 'Epoch 3: val_loss=0.6458  val_F1=0.6771', '\\n', 'Epoch 4:\nval_loss=0.6474  val_F1=0.6751', '\\n', 'Epoch 5: val_loss=0.6451\nval_F1=0.6754', '\\n', '\\nBest emb_dim=256 with dev F1=0.6864', '\\n', 'Test\nMacro-F1 (best emb_dim=256): 0.6890', '\\n', 'Execution time: 7 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loading real SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 118968.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 30913.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 186330.70\nexamples/s]', '\\n', '\\n--- Training with lr=0.0003 ---', '\\n', 'Epoch 1:\nvalidation_loss = 0.6544 macro_F1 = 0.6520', '\\n', 'Epoch 2: validation_loss =\n0.6434 macro_F1 = 0.6707', '\\n', 'Epoch 3: validation_loss = 0.6383 macro_F1 =\n0.6813', '\\n', 'Epoch 4: validation_loss = 0.6364 macro_F1 = 0.6809', '\\n',\n'Epoch 5: validation_loss = 0.6360 macro_F1 = 0.6809', '\\n', '\\n--- Training\nwith lr=0.0005 ---', '\\n', 'Epoch 1: validation_loss = 0.6451 macro_F1 =\n0.6779', '\\n', 'Epoch 2: validation_loss = 0.6368 macro_F1 = 0.6788', '\\n',\n'Epoch 3: validation_loss = 0.6348 macro_F1 = 0.6831', '\\n', 'Epoch 4:\nvalidation_loss = 0.6353 macro_F1 = 0.6788', '\\n', 'Epoch 5: validation_loss =\n0.6391 macro_F1 = 0.6833', '\\n', '\\n--- Training with lr=0.001 ---', '\\n',\n'Epoch 1: validation_loss = 0.6472 macro_F1 = 0.6774', '\\n', 'Epoch 2:\nvalidation_loss = 0.6392 macro_F1 = 0.6768', '\\n', 'Epoch 3: validation_loss =\n0.6400 macro_F1 = 0.6785', '\\n', 'Epoch 4: validation_loss = 0.6452 macro_F1 =\n0.6640', '\\n', 'Epoch 5: validation_loss = 0.6770 macro_F1 = 0.6023', '\\n',\n'\\n--- Training with lr=0.002 ---', '\\n', 'Epoch 1: validation_loss = 0.6492\nmacro_F1 = 0.6698', '\\n', 'Epoch 2: validation_loss = 0.6394 macro_F1 = 0.6751',\n'\\n', 'Epoch 3: validation_loss = 0.6450 macro_F1 = 0.6788', '\\n', 'Epoch 4:\nvalidation_loss = 0.6447 macro_F1 = 0.6791', '\\n', 'Epoch 5: validation_loss =\n0.6464 macro_F1 = 0.6793', '\\n', '\\nBest lr by val_macro_F1: 0.0005 (0.6833)',\n'\\n', '\\nFine-tuning on combined train+dev (2 epochs)', '\\n', 'Fine-tune Epoch\n1: train_loss = 0.6213 macro_F1 = 0.6794', '\\n', 'Fine-tune Epoch 2: train_loss\n= 0.6185 macro_F1 = 0.6811', '\\n', '\\nTest Macro-F1 with lr=0.0005: 0.6878',\n'\\n', 'Execution time: 10 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', 'Vocab size:', ' ', '9',\n'\\n', '\\n=== Training with hidden_size=64 ===', '\\n', '  Epoch 1:\nval_loss=0.6415 val_macro_f1=0.6659', '\\n', '  Epoch 2: val_loss=0.6410\nval_macro_f1=0.6811', '\\n', '  Epoch 3: val_loss=0.6368 val_macro_f1=0.6830',\n'\\n', '  Epoch 4: val_loss=0.6381 val_macro_f1=0.6836', '\\n', '  Epoch 5:\nval_loss=0.6387 val_macro_f1=0.6834', '\\n', '\\n=== Training with hidden_size=128\n===', '\\n', '  Epoch 1: val_loss=0.6470 val_macro_f1=0.6738', '\\n', '  Epoch 2:\nval_loss=0.6389 val_macro_f1=0.6812', '\\n', '  Epoch 3: val_loss=0.6392\nval_macro_f1=0.6796', '\\n', '  Epoch 4: val_loss=0.6436 val_macro_f1=0.6719',\n'\\n', '  Epoch 5: val_loss=0.6377 val_macro_f1=0.6773', '\\n', '\\n=== Training\nwith hidden_size=256 ===', '\\n', '  Epoch 1: val_loss=0.6449\nval_macro_f1=0.6801', '\\n', '  Epoch 2: val_loss=0.6405 val_macro_f1=0.6806',\n'\\n', '  Epoch 3: val_loss=0.6479 val_macro_f1=0.6693', '\\n', '  Epoch 4:\nval_loss=0.6396 val_macro_f1=0.6715', '\\n', '  Epoch 5: val_loss=0.6472\nval_macro_f1=0.6735', '\\n', '\\n=== Training with hidden_size=512 ===', '\\n', '\nEpoch 1: val_loss=0.6384 val_macro_f1=0.6806', '\\n', '  Epoch 2: val_loss=0.6442\nval_macro_f1=0.6798', '\\n', '  Epoch 3: val_loss=0.6442 val_macro_f1=0.6660',\n'\\n', '  Epoch 4: val_loss=0.6488 val_macro_f1=0.6698', '\\n', '  Epoch 5:\nval_loss=0.6442 val_macro_f1=0.6849', '\\n', '\\nBest hidden_size=512 with dev\nmacro-F1=0.6849', '\\n', 'Test Macro-F1 (best model): 0.6868', '\\n', 'Execution\ntime: 5 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\n=== Training with\ndropout_rate=0.0 ===', '\\n', 'Epoch 1 | train_loss=0.6536 val_loss=0.6445\ntrain_F1=0.628 val_F1=0.678', '\\n', 'Epoch 2 | train_loss=0.6328 val_loss=0.6417\ntrain_F1=0.679 val_F1=0.679', '\\n', 'Epoch 3 | train_loss=0.6217 val_loss=0.6396\ntrain_F1=0.680 val_F1=0.677', '\\n', 'Epoch 4 | train_loss=0.6205 val_loss=0.6405\ntrain_F1=0.683 val_F1=0.682', '\\n', 'Epoch 5 | train_loss=0.6196 val_loss=0.6487\ntrain_F1=0.677 val_F1=0.650', '\\n', 'Test Macro-F1 (dropout=0.0): 0.6629', '\\n',\n'\\n=== Training with dropout_rate=0.1 ===', '\\n', 'Epoch 1 | train_loss=0.6516\nval_loss=0.6440 train_F1=0.633 val_F1=0.678', '\\n', 'Epoch 2 | train_loss=0.6305\nval_loss=0.6402 train_F1=0.674 val_F1=0.685', '\\n', 'Epoch 3 | train_loss=0.6247\nval_loss=0.6381 train_F1=0.676 val_F1=0.681', '\\n', 'Epoch 4 | train_loss=0.6270\nval_loss=0.6434 train_F1=0.673 val_F1=0.680', '\\n', 'Epoch 5 | train_loss=0.6196\nval_loss=0.6417 train_F1=0.679 val_F1=0.677', '\\n', 'Test Macro-F1\n(dropout=0.1): 0.6824', '\\n', '\\n=== Training with dropout_rate=0.3 ===', '\\n',\n'Epoch 1 | train_loss=0.6598 val_loss=0.6477 train_F1=0.603 val_F1=0.668', '\\n',\n'Epoch 2 | train_loss=0.6306 val_loss=0.6449 train_F1=0.672 val_F1=0.662', '\\n',\n'Epoch 3 | train_loss=0.6229 val_loss=0.6467 train_F1=0.678 val_F1=0.676', '\\n',\n'Epoch 4 | train_loss=0.6276 val_loss=0.6393 train_F1=0.676 val_F1=0.679', '\\n',\n'Epoch 5 | train_loss=0.6213 val_loss=0.6385 train_F1=0.681 val_F1=0.678', '\\n',\n'Test Macro-F1 (dropout=0.3): 0.6883', '\\n', '\\n=== Training with\ndropout_rate=0.5 ===', '\\n', 'Epoch 1 | train_loss=0.6551 val_loss=0.6413\ntrain_F1=0.618 val_F1=0.674', '\\n', 'Epoch 2 | train_loss=0.6312 val_loss=0.6395\ntrain_F1=0.669 val_F1=0.683', '\\n', 'Epoch 3 | train_loss=0.6292 val_loss=0.6390\ntrain_F1=0.676 val_F1=0.680', '\\n', 'Epoch 4 | train_loss=0.6237 val_loss=0.6361\ntrain_F1=0.678 val_F1=0.679', '\\n', 'Epoch 5 | train_loss=0.6223 val_loss=0.6408\ntrain_F1=0.680 val_F1=0.675', '\\n', 'Test Macro-F1 (dropout=0.5): 0.6864', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\n=== Training with\nweight_decay = 0.0 ===', '\\n', '  Epoch 1: train_f1=0.6280  val_f1=0.6777',\n'\\n', '  Epoch 2: train_f1=0.6699  val_f1=0.6818', '\\n', '  Epoch 3:\ntrain_f1=0.6774  val_f1=0.6809', '\\n', '  Epoch 4: train_f1=0.6766\nval_f1=0.6700', '\\n', '  Epoch 5: train_f1=0.6799  val_f1=0.6793', '\\n', '  -->\nweight_decay 0.0 Test macro-F1: 0.6850', '\\n', '\\n=== Training with weight_decay\n= 1e-06 ===', '\\n', '  Epoch 1: train_f1=0.6065  val_f1=0.6830', '\\n', '  Epoch\n2: train_f1=0.6765  val_f1=0.6852', '\\n', '  Epoch 3: train_f1=0.6796\nval_f1=0.6829', '\\n', '  Epoch 4: train_f1=0.6790  val_f1=0.6757', '\\n', '\nEpoch 5: train_f1=0.6783  val_f1=0.6754', '\\n', '  --> weight_decay 1e-06 Test\nmacro-F1: 0.6851', '\\n', '\\n=== Training with weight_decay = 1e-05 ===', '\\n', '\nEpoch 1: train_f1=0.6140  val_f1=0.6497', '\\n', '  Epoch 2: train_f1=0.6763\nval_f1=0.6758', '\\n', '  Epoch 3: train_f1=0.6797  val_f1=0.6764', '\\n', '\nEpoch 4: train_f1=0.6771  val_f1=0.6793', '\\n', '  Epoch 5: train_f1=0.6821\nval_f1=0.6838', '\\n', '  --> weight_decay 1e-05 Test macro-F1: 0.6787', '\\n',\n'\\n=== Training with weight_decay = 0.0001 ===', '\\n', '  Epoch 1:\ntrain_f1=0.6228  val_f1=0.6639', '\\n', '  Epoch 2: train_f1=0.6715\nval_f1=0.6680', '\\n', '  Epoch 3: train_f1=0.6810  val_f1=0.6809', '\\n', '\nEpoch 4: train_f1=0.6803  val_f1=0.6796', '\\n', '  Epoch 5: train_f1=0.6798\nval_f1=0.6699', '\\n', '  --> weight_decay 0.0001 Test macro-F1: 0.6808', '\\n',\n'\\n=== Training with weight_decay = 0.001 ===', '\\n', '  Epoch 1:\ntrain_f1=0.6212  val_f1=0.6806', '\\n', '  Epoch 2: train_f1=0.6723\nval_f1=0.6822', '\\n', '  Epoch 3: train_f1=0.6739  val_f1=0.6785', '\\n', '\nEpoch 4: train_f1=0.6788  val_f1=0.6829', '\\n', '  Epoch 5: train_f1=0.6762\nval_f1=0.6700', '\\n', '  --> weight_decay 0.001 Test macro-F1: 0.6710', '\\n',\n'\\nBest weight_decay based on dev set: 1e-05  (val_f1=0.6838)', '\\n', 'Test\nMacro-F1 of best model: 0.6787', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\n=== Training with\ndropout_rate=0.0 ===', '\\n', 'Epoch 1 | train_loss=0.6463 val_loss=0.6456\ntrain_F1=0.629 val_F1=0.679', '\\n', 'Epoch 2 | train_loss=0.6313 val_loss=0.6410\ntrain_F1=0.683 val_F1=0.679', '\\n', 'Epoch 3 | train_loss=0.6203 val_loss=0.6477\ntrain_F1=0.682 val_F1=0.678', '\\n', 'Epoch 4 | train_loss=0.6204 val_loss=0.6394\ntrain_F1=0.681 val_F1=0.680', '\\n', 'Epoch 5 | train_loss=0.6155 val_loss=0.6420\ntrain_F1=0.684 val_F1=0.680', '\\n', 'Test Macro-F1 (dropout=0.0): 0.6874', '\\n',\n'\\n=== Training with dropout_rate=0.1 ===', '\\n', 'Epoch 1 | train_loss=0.6572\nval_loss=0.6450 train_F1=0.608 val_F1=0.662', '\\n', 'Epoch 2 | train_loss=0.6292\nval_loss=0.6369 train_F1=0.677 val_F1=0.675', '\\n', 'Epoch 3 | train_loss=0.6215\nval_loss=0.6385 train_F1=0.678 val_F1=0.680', '\\n', 'Epoch 4 | train_loss=0.6197\nval_loss=0.6428 train_F1=0.678 val_F1=0.679', '\\n', 'Epoch 5 | train_loss=0.6179\nval_loss=0.6456 train_F1=0.682 val_F1=0.678', '\\n', 'Test Macro-F1\n(dropout=0.1): 0.6810', '\\n', '\\n=== Training with dropout_rate=0.3 ===', '\\n',\n'Epoch 1 | train_loss=0.6590 val_loss=0.6551 train_F1=0.593 val_F1=0.627', '\\n',\n'Epoch 2 | train_loss=0.6355 val_loss=0.6400 train_F1=0.671 val_F1=0.679', '\\n',\n'Epoch 3 | train_loss=0.6262 val_loss=0.6466 train_F1=0.673 val_F1=0.658', '\\n',\n'Epoch 4 | train_loss=0.6211 val_loss=0.6399 train_F1=0.674 val_F1=0.685', '\\n',\n'Epoch 5 | train_loss=0.6205 val_loss=0.6400 train_F1=0.680 val_F1=0.684', '\\n',\n'Test Macro-F1 (dropout=0.3): 0.6842', '\\n', '\\n=== Training with\ndropout_rate=0.5 ===', '\\n', 'Epoch 1 | train_loss=0.6668 val_loss=0.6480\ntrain_F1=0.596 val_F1=0.652', '\\n', 'Epoch 2 | train_loss=0.6287 val_loss=0.6417\ntrain_F1=0.670 val_F1=0.685', '\\n', 'Epoch 3 | train_loss=0.6274 val_loss=0.6360\ntrain_F1=0.680 val_F1=0.679', '\\n', 'Epoch 4 | train_loss=0.6274 val_loss=0.6364\ntrain_F1=0.679 val_F1=0.679', '\\n', 'Epoch 5 | train_loss=0.6210 val_loss=0.6431\ntrain_F1=0.679 val_F1=0.668', '\\n', 'Test Macro-F1 (dropout=0.5): 0.6869', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\n=== Training with\ndropout_rate=0.0 ===', '\\n', 'Epoch 1 | train_loss=0.6561 val_loss=0.6419\ntrain_F1=0.618 val_F1=0.679', '\\n', 'Epoch 2 | train_loss=0.6328 val_loss=0.6367\ntrain_F1=0.671 val_F1=0.679', '\\n', 'Epoch 3 | train_loss=0.6282 val_loss=0.6427\ntrain_F1=0.678 val_F1=0.668', '\\n', 'Epoch 4 | train_loss=0.6270 val_loss=0.6459\ntrain_F1=0.675 val_F1=0.674', '\\n', 'Epoch 5 | train_loss=0.6205 val_loss=0.6373\ntrain_F1=0.679 val_F1=0.679', '\\n', 'Test Macro-F1 (dropout=0.0): 0.6858', '\\n',\n'\\n=== Training with dropout_rate=0.1 ===', '\\n', 'Epoch 1 | train_loss=0.6599\nval_loss=0.6463 train_F1=0.608 val_F1=0.665', '\\n', 'Epoch 2 | train_loss=0.6321\nval_loss=0.6399 train_F1=0.678 val_F1=0.677', '\\n', 'Epoch 3 | train_loss=0.6223\nval_loss=0.6394 train_F1=0.679 val_F1=0.683', '\\n', 'Epoch 4 | train_loss=0.6204\nval_loss=0.6445 train_F1=0.674 val_F1=0.674', '\\n', 'Epoch 5 | train_loss=0.6182\nval_loss=0.6395 train_F1=0.684 val_F1=0.679', '\\n', 'Test Macro-F1\n(dropout=0.1): 0.6882', '\\n', '\\n=== Training with dropout_rate=0.3 ===', '\\n',\n'Epoch 1 | train_loss=0.6545 val_loss=0.6393 train_F1=0.634 val_F1=0.679', '\\n',\n'Epoch 2 | train_loss=0.6312 val_loss=0.6342 train_F1=0.681 val_F1=0.681', '\\n',\n'Epoch 3 | train_loss=0.6249 val_loss=0.6387 train_F1=0.676 val_F1=0.687', '\\n',\n'Epoch 4 | train_loss=0.6230 val_loss=0.6381 train_F1=0.682 val_F1=0.681', '\\n',\n'Epoch 5 | train_loss=0.6185 val_loss=0.6386 train_F1=0.679 val_F1=0.679', '\\n',\n'Test Macro-F1 (dropout=0.3): 0.6824', '\\n', '\\n=== Training with\ndropout_rate=0.5 ===', '\\n', 'Epoch 1 | train_loss=0.6554 val_loss=0.6494\ntrain_F1=0.623 val_F1=0.663', '\\n', 'Epoch 2 | train_loss=0.6341 val_loss=0.6410\ntrain_F1=0.675 val_F1=0.689', '\\n', 'Epoch 3 | train_loss=0.6303 val_loss=0.6395\ntrain_F1=0.679 val_F1=0.675', '\\n', 'Epoch 4 | train_loss=0.6255 val_loss=0.6385\ntrain_F1=0.675 val_F1=0.681', '\\n', 'Epoch 5 | train_loss=0.6202 val_loss=0.6420\ntrain_F1=0.681 val_F1=0.683', '\\n', 'Test Macro-F1 (dropout=0.5): 0.6899', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loading real SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\n=== Training with\ndropout_rate=0.0 ===', '\\n', 'Epoch 1 | train_loss=0.6537 val_loss=0.6458\ntrain_F1=0.619 val_F1=0.672', '\\n', 'Epoch 2 | train_loss=0.6283 val_loss=0.6476\ntrain_F1=0.680 val_F1=0.678', '\\n', 'Epoch 3 | train_loss=0.6241 val_loss=0.6379\ntrain_F1=0.675 val_F1=0.679', '\\n', 'Epoch 4 | train_loss=0.6177 val_loss=0.6412\ntrain_F1=0.682 val_F1=0.679', '\\n', 'Epoch 5 | train_loss=0.6157 val_loss=0.6424\ntrain_F1=0.685 val_F1=0.670', '\\n', 'Test Macro-F1 (dropout=0.0): 0.6845', '\\n',\n'\\n=== Training with dropout_rate=0.1 ===', '\\n', 'Epoch 1 | train_loss=0.6516\nval_loss=0.6408 train_F1=0.641 val_F1=0.674', '\\n', 'Epoch 2 | train_loss=0.6320\nval_loss=0.6386 train_F1=0.673 val_F1=0.678', '\\n', 'Epoch 3 | train_loss=0.6251\nval_loss=0.6397 train_F1=0.673 val_F1=0.683', '\\n', 'Epoch 4 | train_loss=0.6203\nval_loss=0.6353 train_F1=0.679 val_F1=0.677', '\\n', 'Epoch 5 | train_loss=0.6165\nval_loss=0.6452 train_F1=0.677 val_F1=0.668', '\\n', 'Test Macro-F1\n(dropout=0.1): 0.6759', '\\n', '\\n=== Training with dropout_rate=0.3 ===', '\\n',\n'Epoch 1 | train_loss=0.6575 val_loss=0.6468 train_F1=0.632 val_F1=0.662', '\\n',\n'Epoch 2 | train_loss=0.6326 val_loss=0.6406 train_F1=0.676 val_F1=0.678', '\\n',\n'Epoch 3 | train_loss=0.6238 val_loss=0.6359 train_F1=0.681 val_F1=0.681', '\\n',\n'Epoch 4 | train_loss=0.6234 val_loss=0.6393 train_F1=0.680 val_F1=0.679', '\\n',\n'Epoch 5 | train_loss=0.6154 val_loss=0.6463 train_F1=0.685 val_F1=0.677', '\\n',\n'Test Macro-F1 (dropout=0.3): 0.6830', '\\n', '\\n=== Training with\ndropout_rate=0.5 ===', '\\n', 'Epoch 1 | train_loss=0.6560 val_loss=0.6453\ntrain_F1=0.617 val_F1=0.659', '\\n', 'Epoch 2 | train_loss=0.6311 val_loss=0.6361\ntrain_F1=0.670 val_F1=0.681', '\\n', 'Epoch 3 | train_loss=0.6275 val_loss=0.6379\ntrain_F1=0.670 val_F1=0.679', '\\n', 'Epoch 4 | train_loss=0.6234 val_loss=0.6397\ntrain_F1=0.678 val_F1=0.687', '\\n', 'Epoch 5 | train_loss=0.6229 val_loss=0.6393\ntrain_F1=0.675 val_F1=0.683', '\\n', 'Test Macro-F1 (dropout=0.5): 0.6872', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["", "The code executed successfully without any errors or bugs. The training and\nvalidation processes were completed, and early stopping was triggered at epoch 8\ndue to lack of improvement in validation loss. The final test macro-F1 score was\n0.6659, which is a reasonable outcome for this stage of experimentation. No\nissues were observed in the execution.", "The script failed due to a 'TypeError: unsupported operand type(s) for +:\n'Dataset' and 'Dataset'' when attempting to combine the 'train' and 'dev'\ndatasets. The issue lies in the line: 'comb_train[\"train\"] =\nspr[\"train\"].flatten_indices() + spr[\"dev\"].flatten_indices()'. The '+' operator\nis not supported for the 'Dataset' type. To fix this, use the\n'datasets.concatenate_datasets()' method from the Hugging Face datasets library\nto properly combine the two datasets. For example: 'comb_train[\"train\"] =\ndatasets.concatenate_datasets([spr[\"train\"], spr[\"dev\"]])'.", "The execution output indicates that the training script ran successfully without\nany errors. The script trained a character-level BiGRU model with three\ndifferent batch sizes (32, 64, and 256) and evaluated the model's performance\nusing the Macro-F1 metric on the validation and test sets. The results were\nconsistent, showing gradual improvement in validation loss and Macro-F1 over\nepochs. The final test Macro-F1 scores for the different batch sizes were\nreasonable and close to each other. No bugs or issues were observed in the\nexecution.", "", "The training script executed successfully without any errors or bugs. The best\nlearning rate was identified as 0.0005, and the model achieved a Test Macro-F1\nscore of 0.6878 after fine-tuning. The implementation and results align with the\nintended goals of the experiment.", "", "The training script executed successfully without any errors or bugs. The model\nwas trained with different dropout rates, and the outputs (losses, F1 scores)\nwere logged correctly. The results showed varying performance across dropout\nrates, and the experiment data was saved successfully. No issues were detected\nin the implementation or execution.", "", "The training script executed successfully without any bugs or errors. The\nexperiment tested the performance of a CharBiGRU model with different dropout\nrates (0.0, 0.1, 0.3, 0.5) on the SPR_BENCH dataset. The training, validation,\nand test results were clearly logged, and the model achieved reasonable\nperformance across all configurations. Additionally, the experiment data was\nsaved properly, and loss curves were plotted for each dropout rate. The\nimplementation is robust and provides useful insights into the impact of dropout\nrates on model performance.", "The execution output demonstrates that the training script ran successfully\nwithout any errors or bugs. The experiment involved a hyperparameter sweep over\ndifferent dropout rates (0.0, 0.1, 0.3, 0.5) to evaluate their effect on the\nmodel's performance. The training, validation, and test results were logged for\neach dropout rate, and the final test Macro-F1 scores were reported. The script\nalso saved the experiment data as 'experiment_data.npy' and plotted loss curves\nfor each dropout rate. The results indicate that the model achieved its highest\ntest Macro-F1 score of 0.6899 with a dropout rate of 0.5.", "The execution output indicates that the training script ran successfully without\nany errors or bugs. The script performed a hyperparameter sweep over different\ndropout rates and reported both training and validation losses, as well as F1\nscores, for each epoch. The test Macro-F1 scores were also calculated for each\ndropout rate and the results were saved to a file. The script achieved the\nhighest test Macro-F1 score of 0.6872 with a dropout rate of 0.5. Overall, the\nscript appears to have met its objectives for this stage of experimentation.", ""], "exc_type": [null, null, "TypeError", null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, {"args": ["unsupported operand type(s) for +: 'Dataset' and 'Dataset'"]}, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 204, "<module>", "comb_train[\"train\"] = spr[\"train\"].flatten_indices() + spr[\"dev\"].flatten_indices()"]], null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6831, "best_value": 0.6831}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6815, "best_value": 0.6815}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6196, "best_value": 0.6196}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6396, "best_value": 0.6396}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the test dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6629, "best_value": 0.6629}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6815, "best_value": 0.6815}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6103, "best_value": 0.6103}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6396, "best_value": 0.6396}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6659, "best_value": 0.6659}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The macro F1 score computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH_bs32", "final_value": 0.6748, "best_value": 0.6748}, {"dataset_name": "SPR_BENCH_bs64", "final_value": 0.6833, "best_value": 0.6833}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6776, "best_value": 0.6776}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH_bs32", "final_value": 0.6833, "best_value": 0.6833}, {"dataset_name": "SPR_BENCH_bs64", "final_value": 0.6788, "best_value": 0.6788}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6809, "best_value": 0.6809}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH_bs32", "final_value": 0.618, "best_value": 0.618}, {"dataset_name": "SPR_BENCH_bs64", "final_value": 0.6192, "best_value": 0.6192}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.619, "best_value": 0.619}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH_bs32", "final_value": 0.6375, "best_value": 0.6375}, {"dataset_name": "SPR_BENCH_bs64", "final_value": 0.6364, "best_value": 0.6364}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6381, "best_value": 0.6381}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score computed on the test dataset.", "data": [{"dataset_name": "SPR_BENCH_bs32", "final_value": 0.6849, "best_value": 0.6849}, {"dataset_name": "SPR_BENCH_bs64", "final_value": 0.6862, "best_value": 0.6862}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6842, "best_value": 0.6842}]}]}, {"metric_names": [{"metric_name": "Training macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the training dataset.", "data": [{"dataset_name": "Embedding dimension = 32", "final_value": 0.682, "best_value": 0.682}, {"dataset_name": "Embedding dimension = 64", "final_value": 0.6794, "best_value": 0.6794}, {"dataset_name": "Embedding dimension = 128", "final_value": 0.6825, "best_value": 0.6825}, {"dataset_name": "Embedding dimension = 256", "final_value": 0.6804, "best_value": 0.6804}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "Loss value for the training dataset.", "data": [{"dataset_name": "Embedding dimension = 32", "final_value": 0.6175, "best_value": 0.6175}, {"dataset_name": "Embedding dimension = 64", "final_value": 0.6151, "best_value": 0.6151}, {"dataset_name": "Embedding dimension = 128", "final_value": 0.6172, "best_value": 0.6172}, {"dataset_name": "Embedding dimension = 256", "final_value": 0.6153, "best_value": 0.6153}]}, {"metric_name": "Validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the validation dataset.", "data": [{"dataset_name": "Embedding dimension = 32", "final_value": 0.672, "best_value": 0.672}, {"dataset_name": "Embedding dimension = 64", "final_value": 0.6793, "best_value": 0.6793}, {"dataset_name": "Embedding dimension = 128", "final_value": 0.668, "best_value": 0.668}, {"dataset_name": "Embedding dimension = 256", "final_value": 0.6754, "best_value": 0.6754}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset.", "data": [{"dataset_name": "Embedding dimension = 32", "final_value": 0.6436, "best_value": 0.6436}, {"dataset_name": "Embedding dimension = 64", "final_value": 0.6398, "best_value": 0.6398}, {"dataset_name": "Embedding dimension = 128", "final_value": 0.6501, "best_value": 0.6501}, {"dataset_name": "Embedding dimension = 256", "final_value": 0.6451, "best_value": 0.6451}]}, {"metric_name": "Test macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the test dataset.", "data": [{"dataset_name": "Test dataset", "final_value": 0.689, "best_value": 0.689}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the balance between precision and recall, useful for imbalanced datasets.", "data": [{"dataset_name": "training", "final_value": 0.6805, "best_value": 0.6805}, {"dataset_name": "validation", "final_value": 0.6833, "best_value": 0.6833}, {"dataset_name": "test", "final_value": 0.6878, "best_value": 0.6878}]}, {"metric_name": "BCE loss", "lower_is_better": true, "description": "Binary Cross-Entropy loss, used for binary classification tasks to measure the difference between predicted and true labels.", "data": [{"dataset_name": "training", "final_value": 0.6184, "best_value": 0.6184}, {"dataset_name": "validation", "final_value": 0.6391, "best_value": 0.6391}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the training dataset", "data": [{"dataset_name": "hidden_size=64", "final_value": 0.6804, "best_value": 0.6804}, {"dataset_name": "hidden_size=128", "final_value": 0.6845, "best_value": 0.6845}, {"dataset_name": "hidden_size=256", "final_value": 0.6866, "best_value": 0.6866}, {"dataset_name": "hidden_size=512", "final_value": 0.6836, "best_value": 0.6836}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the validation dataset", "data": [{"dataset_name": "hidden_size=64", "final_value": 0.6834, "best_value": 0.6834}, {"dataset_name": "hidden_size=128", "final_value": 0.6773, "best_value": 0.6773}, {"dataset_name": "hidden_size=256", "final_value": 0.6735, "best_value": 0.6735}, {"dataset_name": "hidden_size=512", "final_value": 0.6849, "best_value": 0.6849}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "hidden_size=64", "final_value": 0.6193, "best_value": 0.6193}, {"dataset_name": "hidden_size=128", "final_value": 0.6168, "best_value": 0.6168}, {"dataset_name": "hidden_size=256", "final_value": 0.6172, "best_value": 0.6172}, {"dataset_name": "hidden_size=512", "final_value": 0.6149, "best_value": 0.6149}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "hidden_size=64", "final_value": 0.6387, "best_value": 0.6387}, {"dataset_name": "hidden_size=128", "final_value": 0.6377, "best_value": 0.6377}, {"dataset_name": "hidden_size=256", "final_value": 0.6472, "best_value": 0.6472}, {"dataset_name": "hidden_size=512", "final_value": 0.6442, "best_value": 0.6442}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the best model's test dataset", "data": [{"dataset_name": "best_model (hidden_size=512)", "final_value": 0.6868, "best_value": 0.6868}]}]}, {"metric_names": [{"metric_name": "Training macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the training dataset.", "data": [{"dataset_name": "Training dataset with dropout 0.0", "final_value": 0.6772, "best_value": 0.6772}, {"dataset_name": "Training dataset with dropout 0.1", "final_value": 0.6788, "best_value": 0.6788}, {"dataset_name": "Training dataset with dropout 0.3", "final_value": 0.681, "best_value": 0.681}, {"dataset_name": "Training dataset with dropout 0.5", "final_value": 0.6801, "best_value": 0.6801}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "Loss on the training dataset.", "data": [{"dataset_name": "Training dataset with dropout 0.0", "final_value": 0.6196, "best_value": 0.6196}, {"dataset_name": "Training dataset with dropout 0.1", "final_value": 0.6196, "best_value": 0.6196}, {"dataset_name": "Training dataset with dropout 0.3", "final_value": 0.6213, "best_value": 0.6213}, {"dataset_name": "Training dataset with dropout 0.5", "final_value": 0.6223, "best_value": 0.6223}]}, {"metric_name": "Validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the validation dataset.", "data": [{"dataset_name": "Validation dataset with dropout 0.0", "final_value": 0.6498, "best_value": 0.6498}, {"dataset_name": "Validation dataset with dropout 0.1", "final_value": 0.6768, "best_value": 0.6768}, {"dataset_name": "Validation dataset with dropout 0.3", "final_value": 0.6775, "best_value": 0.6775}, {"dataset_name": "Validation dataset with dropout 0.5", "final_value": 0.6748, "best_value": 0.6748}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Loss on the validation dataset.", "data": [{"dataset_name": "Validation dataset with dropout 0.0", "final_value": 0.6487, "best_value": 0.6487}, {"dataset_name": "Validation dataset with dropout 0.1", "final_value": 0.6417, "best_value": 0.6417}, {"dataset_name": "Validation dataset with dropout 0.3", "final_value": 0.6385, "best_value": 0.6385}, {"dataset_name": "Validation dataset with dropout 0.5", "final_value": 0.6408, "best_value": 0.6408}]}, {"metric_name": "Test macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the test dataset.", "data": [{"dataset_name": "Test dataset with dropout 0.0", "final_value": 0.6629, "best_value": 0.6629}, {"dataset_name": "Test dataset with dropout 0.1", "final_value": 0.6824, "best_value": 0.6824}, {"dataset_name": "Test dataset with dropout 0.3", "final_value": 0.6883, "best_value": 0.6883}, {"dataset_name": "Test dataset with dropout 0.5", "final_value": 0.6864, "best_value": 0.6864}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the balance between precision and recall for a dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6787, "best_value": 0.6838}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.644718, "best_value": 0.619674}]}]}, {"metric_names": [{"metric_name": "Training macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the training dataset, indicating the balance between precision and recall.", "data": [{"dataset_name": "Training dataset", "final_value": 0.6837, "best_value": 0.6837}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "The loss value for the training dataset, representing the error during training.", "data": [{"dataset_name": "Training dataset", "final_value": 0.6155, "best_value": 0.6155}]}, {"metric_name": "Validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the validation dataset, indicating the balance between precision and recall.", "data": [{"dataset_name": "Validation dataset", "final_value": 0.6797, "best_value": 0.6836}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset, representing the error during validation.", "data": [{"dataset_name": "Validation dataset", "final_value": 0.642, "best_value": 0.64}]}, {"metric_name": "Test macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the test dataset, indicating the balance between precision and recall.", "data": [{"dataset_name": "Test dataset", "final_value": 0.6869, "best_value": 0.6874}]}]}, {"metric_names": [{"metric_name": "Training macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the training dataset.", "data": [{"dataset_name": "Training dataset (dropout rate: 0.0)", "final_value": 0.6786, "best_value": 0.6786}, {"dataset_name": "Training dataset (dropout rate: 0.1)", "final_value": 0.6843, "best_value": 0.6843}, {"dataset_name": "Training dataset (dropout rate: 0.3)", "final_value": 0.6789, "best_value": 0.6789}, {"dataset_name": "Training dataset (dropout rate: 0.5)", "final_value": 0.6814, "best_value": 0.6814}]}, {"metric_name": "Validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the validation dataset.", "data": [{"dataset_name": "Validation dataset (dropout rate: 0.0)", "final_value": 0.6791, "best_value": 0.6791}, {"dataset_name": "Validation dataset (dropout rate: 0.1)", "final_value": 0.6794, "best_value": 0.6794}, {"dataset_name": "Validation dataset (dropout rate: 0.3)", "final_value": 0.6787, "best_value": 0.6787}, {"dataset_name": "Validation dataset (dropout rate: 0.5)", "final_value": 0.6833, "best_value": 0.6833}]}, {"metric_name": "Test macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the test dataset.", "data": [{"dataset_name": "Test dataset (dropout rate: 0.0)", "final_value": 0.6858, "best_value": 0.6858}, {"dataset_name": "Test dataset (dropout rate: 0.1)", "final_value": 0.6882, "best_value": 0.6882}, {"dataset_name": "Test dataset (dropout rate: 0.3)", "final_value": 0.6824, "best_value": 0.6824}, {"dataset_name": "Test dataset (dropout rate: 0.5)", "final_value": 0.6899, "best_value": 0.6899}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "Loss value for the training dataset.", "data": [{"dataset_name": "Training dataset (dropout rate: 0.0)", "final_value": 0.6205, "best_value": 0.6205}, {"dataset_name": "Training dataset (dropout rate: 0.1)", "final_value": 0.6182, "best_value": 0.6182}, {"dataset_name": "Training dataset (dropout rate: 0.3)", "final_value": 0.6185, "best_value": 0.6185}, {"dataset_name": "Training dataset (dropout rate: 0.5)", "final_value": 0.6202, "best_value": 0.6202}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset.", "data": [{"dataset_name": "Validation dataset (dropout rate: 0.0)", "final_value": 0.6373, "best_value": 0.6373}, {"dataset_name": "Validation dataset (dropout rate: 0.1)", "final_value": 0.6395, "best_value": 0.6395}, {"dataset_name": "Validation dataset (dropout rate: 0.3)", "final_value": 0.6386, "best_value": 0.6386}, {"dataset_name": "Validation dataset (dropout rate: 0.5)", "final_value": 0.642, "best_value": 0.642}]}]}, {"metric_names": [{"metric_name": "Training macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the training dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "Training dataset", "final_value": 0.6852, "best_value": 0.6852}, {"dataset_name": "Training dataset", "final_value": 0.677, "best_value": 0.677}, {"dataset_name": "Training dataset", "final_value": 0.6849, "best_value": 0.6849}, {"dataset_name": "Training dataset", "final_value": 0.6746, "best_value": 0.6746}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "The loss value for the training dataset, indicating the error in prediction.", "data": [{"dataset_name": "Training dataset", "final_value": 0.6157, "best_value": 0.6157}, {"dataset_name": "Training dataset", "final_value": 0.6165, "best_value": 0.6165}, {"dataset_name": "Training dataset", "final_value": 0.6154, "best_value": 0.6154}, {"dataset_name": "Training dataset", "final_value": 0.6229, "best_value": 0.6229}]}, {"metric_name": "Validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the validation dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "Validation dataset", "final_value": 0.6697, "best_value": 0.6697}, {"dataset_name": "Validation dataset", "final_value": 0.668, "best_value": 0.668}, {"dataset_name": "Validation dataset", "final_value": 0.6766, "best_value": 0.6766}, {"dataset_name": "Validation dataset", "final_value": 0.6834, "best_value": 0.6834}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset, indicating the error in prediction.", "data": [{"dataset_name": "Validation dataset", "final_value": 0.6424, "best_value": 0.6424}, {"dataset_name": "Validation dataset", "final_value": 0.6452, "best_value": 0.6452}, {"dataset_name": "Validation dataset", "final_value": 0.6463, "best_value": 0.6463}, {"dataset_name": "Validation dataset", "final_value": 0.6393, "best_value": 0.6393}]}, {"metric_name": "Test macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the test dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "Test dataset", "final_value": 0.6845, "best_value": 0.6845}, {"dataset_name": "Test dataset", "final_value": 0.6759, "best_value": 0.6759}, {"dataset_name": "Test dataset", "final_value": 0.683, "best_value": 0.683}, {"dataset_name": "Test dataset", "final_value": 0.6872, "best_value": 0.6872}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_loss_curve_replot.png", "../../logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_label_distribution.png"], [], ["../../logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs32.png", "../../logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs64.png", "../../logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs256.png", "../../logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curves_all_bs.png", "../../logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/val_loss_emb_dim.png", "../../logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb128_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb256_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/best_lr_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_best_lr_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_test_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/val_loss_hidden_size.png", "../../logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_val_loss_by_hidden_size.png", "../../logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_macro_f1_by_hidden_size.png", "../../logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_train_val_loss_best_hid_512.png", "../../logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_confusion_matrix_best_hid_512.png"], ["../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.5.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_test_F1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.5.png"], ["../../logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/loss_curve_wd_1e-05.png", "../../logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_train_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_val_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_test_macro_f1_bars.png", "../../logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_loss_curves_best_wd_1e-05.png"], ["../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.5.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_test_F1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.5.png"], ["../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.5.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_test_F1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.5.png"], ["../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.5.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_test_F1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.5.png"], ["../../logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_mean_test_F1_vs_dropout.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_macro_F1_dropout_0.0_avg.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_macro_F1_dropout_0.1_avg.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_macro_F1_dropout_0.3_avg.png"]], "plot_paths": [["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_loss_curve_replot.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_label_distribution.png"], [], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs32.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs64.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs256.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curves_all_bs.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/val_loss_emb_dim.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb32_train_val_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb64_train_val_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb128_train_val_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb256_train_val_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_test_f1_bar.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/best_lr_loss_curve.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_best_lr_loss_curve.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_test_confusion_matrix.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/val_loss_hidden_size.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_val_loss_by_hidden_size.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_macro_f1_by_hidden_size.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_train_val_loss_best_hid_512.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_confusion_matrix_best_hid_512.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.5.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_test_F1_vs_dropout.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.5.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/loss_curve_wd_1e-05.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_train_macro_f1_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_val_macro_f1_curves.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_test_macro_f1_bars.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_loss_curves_best_wd_1e-05.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.5.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_test_F1_vs_dropout.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.5.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.5.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_test_F1_vs_dropout.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.5.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.5.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_test_F1_vs_dropout.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.0.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.1.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.3.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/spr_bench_macro_F1_dropout_0.5.png"], ["experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_mean_test_F1_vs_dropout.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_macro_F1_dropout_0.0_avg.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_macro_F1_dropout_0.1_avg.png", "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4586c5e32441ea937f5b34d115e3e3/spr_bench_macro_F1_dropout_0.3_avg.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation loss over five epochs. The training loss decreases steadily, indicating that the model is learning from the training data. However, the validation loss initially decreases but starts to increase after the third epoch. This suggests potential overfitting, as the model is performing better on the training data but worse on the validation set after a certain point.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot shows the Macro-F1 scores for both training and validation sets across five epochs. The training Macro-F1 score improves significantly in the first two epochs and stabilizes afterwards. The validation Macro-F1 score follows a similar trend initially but starts to decline after epoch four. This further supports the observation of overfitting, as the model's performance on unseen data deteriorates while it continues to improve on the training data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's predictions on the test set. The model correctly classifies 342 instances of class 0 and 321 instances of class 1. However, it also misclassifies 144 instances of class 0 as class 1 and 193 instances of class 1 as class 0. This indicates a moderate level of performance, with room for improvement in reducing false positives and false negatives.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0084b574b61744079532e85f0e3f8feb_proc_3327802/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning from the training data. However, the validation loss initially decreases but then starts to increase after epoch 3, suggesting potential overfitting. This indicates that the model is not generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_loss_curve.png"}, {"analysis": "The pattern of the training and validation loss curves aligns with the earlier observation of overfitting. While the training loss continues to decrease, the validation loss increases after a few epochs, reaffirming the need for regularization techniques or early stopping to improve generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_loss_curve_replot.png"}, {"analysis": "The training macro-F1 score improves consistently over the epochs, showing that the model's performance on the training set is improving. However, the validation macro-F1 score fluctuates significantly, with a notable drop at epoch 5. This suggests that the model's ability to generalize is inconsistent and might require further tuning or additional validation techniques.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_f1_curve.png"}, {"analysis": "The label distribution plot shows a mismatch between the ground truth and predictions, particularly for Class 1, where the predictions are lower than the ground truth. This indicates that the model might be biased or underperforming for Class 1, which could be addressed by rebalancing the training data or using class-specific loss functions.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_744bb367019a4d7dae5fdb55626af4b9_proc_3330987/SPR_BENCH_label_distribution.png"}], [], [{"analysis": "The training loss decreases steadily across all epochs, indicating that the model is learning effectively. However, the validation loss initially decreases but starts to increase after the third epoch, suggesting potential overfitting. The batch size of 32 might be too small to generalize well, as indicated by the divergence in training and validation loss trends.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs32.png"}, {"analysis": "The training loss decreases consistently, showing effective learning. However, the validation loss fluctuates significantly, indicating instability in generalization. This could be due to the batch size of 64 being a moderate choice, leading to inconsistent updates or insufficient smoothing of gradients.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs64.png"}, {"analysis": "The training loss decreases sharply and consistently, showing effective learning. The validation loss also decreases steadily but plateaus after the third epoch. This suggests that the batch size of 256 provides stable generalization, as the validation loss does not exhibit significant fluctuations.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curve_bs256.png"}, {"analysis": "The aggregated loss curves across batch sizes show that larger batch sizes (256) lead to smoother and more stable training and validation loss trends, while smaller batch sizes (32, 64) result in fluctuating validation loss. This indicates that larger batch sizes might be better suited for this task, providing better generalization and stability.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_loss_curves_all_bs.png"}, {"analysis": "The final test Macro-F1 scores are very close across batch sizes, indicating that the model's performance is not significantly affected by the choice of batch size. However, the slight edge in stability and performance with larger batch sizes suggests that they might be a preferable choice for this task.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7aa6f8addb3f471caf4b444ae4f41e24_proc_3330989/SPR_BENCH_test_f1_bar.png"}], [{"analysis": "This plot compares validation loss across different embedding dimensions (32, 64, 128, 256) over five epochs. Embedding dimension 64 achieves the lowest validation loss consistently, suggesting it is the most effective for this task. Larger embedding dimensions (128, 256) show erratic behavior, indicating possible overfitting or instability in training. Embedding dimension 32 has higher loss values, which may indicate underfitting.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/val_loss_emb_dim.png"}, {"analysis": "For embedding dimension 32, training loss decreases steadily, indicating effective learning. However, validation loss plateaus and slightly increases after epoch 4, suggesting potential overfitting. Macro-F1 scores for training and validation improve initially but diverge slightly after epoch 4, confirming the overfitting trend.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb32_train_val_curves.png"}, {"analysis": "For embedding dimension 64, both training and validation losses decrease consistently, with validation loss showing minor fluctuations. Macro-F1 scores for both training and validation improve and converge by epoch 5, indicating stable and effective learning. This embedding dimension shows the best performance in terms of both loss and Macro-F1.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb64_train_val_curves.png"}, {"analysis": "For embedding dimension 128, training loss decreases steadily, but validation loss oscillates significantly, indicating instability. The Macro-F1 score for training improves steadily, while the validation Macro-F1 fluctuates, suggesting that this embedding dimension may not generalize well to validation data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb128_train_val_curves.png"}, {"analysis": "For embedding dimension 256, training loss decreases steadily, but validation loss shows a slight upward trend after epoch 3, indicating overfitting. Macro-F1 scores for training improve steadily, but validation Macro-F1 scores fluctuate and slightly decrease after epoch 3, confirming overfitting and instability at this embedding dimension.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_emb256_train_val_curves.png"}, {"analysis": "This summary plot shows the test Macro-F1 score for embedding dimension 256. The test Macro-F1 is approximately 0.7, which aligns with the observed trends in the training and validation phases. However, the lack of comparative results for other embedding dimensions limits the interpretability of this plot. Embedding dimension 256 may not be the optimal choice despite the high test Macro-F1 due to its instability during training.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_0a7e59e04f4f47e8a955d7444ce3be45_proc_3330990/spr_bench_test_f1_bar.png"}], [{"analysis": "This plot shows the loss curves for training and validation over epochs with a learning rate of 0.0005. The training loss decreases steadily, indicating that the model is learning effectively. However, the validation loss plateaus and slightly increases after the third epoch, suggesting potential overfitting or that the learning rate might not be optimal for further improvement. The gap between training and validation loss also indicates that the model's generalization can be improved.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/best_lr_loss_curve.png"}, {"analysis": "These plots compare the Macro-F1 scores for training and validation across different learning rates. For training, the learning rate of 0.002 initially achieves the highest Macro-F1 score but quickly plateaus, while lower learning rates (e.g., 0.0005) show more stable improvements over epochs. For validation, 0.0005 achieves the best performance, maintaining a high and stable Macro-F1 score, while higher learning rates (e.g., 0.001) exhibit a sharp drop after a few epochs. This suggests that 0.0005 is the most effective learning rate for balancing training progression and validation performance.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "This plot confirms the performance of the model with the best learning rate (0.0005). The training loss steadily decreases, while the validation loss remains relatively stable after the second epoch. The slight increase in validation loss after the third epoch indicates that the model might be starting to overfit, but the overall trend suggests that the chosen learning rate is effective for the task.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_best_lr_loss_curve.png"}, {"analysis": "The confusion matrix for the test set shows that the model achieves a good balance between True Positives (375) and True Negatives (314). However, there are notable False Positives (172) and False Negatives (139), indicating that the model struggles slightly more with distinguishing certain sequences. The relatively high number of False Positives suggests that the model might be biased toward predicting the positive class. Further tuning or adjustments to the decision threshold could help address this issue.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b0b3735d907e4a2ca66d964166d08c43_proc_3330988/SPR_BENCH_test_confusion_matrix.png"}], [{"analysis": "This plot compares validation loss across different hidden sizes (64, 128, 256, 512) over five epochs. The hidden size of 64 shows the most stable and consistent decrease in validation loss, indicating better generalization. In contrast, larger hidden sizes such as 256 and 512 show fluctuations, suggesting overfitting or instability in training dynamics at these configurations.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/val_loss_hidden_size.png"}, {"analysis": "This plot also examines validation loss across hidden sizes but provides a clearer comparison. Similar trends are observed, with smaller hidden sizes (64 and 128) demonstrating more stability. Hidden size 64 achieves the lowest validation loss by the final epoch, reinforcing its potential as an optimal choice for this setup.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_val_loss_by_hidden_size.png"}, {"analysis": "This plot evaluates validation Macro-F1 scores for different hidden sizes. Hidden size 64 consistently outperforms other configurations in terms of Macro-F1, maintaining higher scores throughout the epochs. Larger hidden sizes like 512 exhibit significant fluctuations, which may indicate overfitting or sensitivity to training noise.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_macro_f1_by_hidden_size.png"}, {"analysis": "This plot highlights the training and validation loss trends for the hidden size of 512. While training loss decreases steadily, validation loss initially increases before stabilizing. This divergence suggests overfitting, as the model performs better on training data but struggles to generalize to validation data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_train_val_loss_best_hid_512.png"}, {"analysis": "The confusion matrix shows the model's performance on the validation set for a binary classification task. True positives (375) and true negatives (313) are reasonably high, but the number of false positives (173) and false negatives (139) indicates room for improvement in both precision and recall. This could be addressed by further hyperparameter tuning or model regularization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_2083beb7b3a24f56b7461acb6d447901_proc_3330987/spr_bench_confusion_matrix_best_hid_512.png"}], [{"analysis": "The training loss decreases consistently over the epochs, indicating that the model is learning from the training data. However, the validation loss shows an upward trend after the third epoch, suggesting overfitting. The lack of dropout (0.0) may be a contributing factor to this overfitting behavior.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.0.png"}, {"analysis": "The training loss decreases steadily, and the validation loss remains relatively stable with a slight upward trend after the third epoch. The introduction of a small dropout (0.1) seems to have a minor regularization effect, reducing overfitting compared to no dropout.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.1.png"}, {"analysis": "The training loss continues to decrease, and the validation loss shows a slight decrease after the third epoch, indicating improved generalization. A dropout rate of 0.3 appears to strike a balance between learning and regularization, leading to better performance on the validation set.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.3.png"}, {"analysis": "The training loss decreases steadily, and the validation loss shows a slight upward trend after the third epoch. A higher dropout rate of 0.5 seems to introduce more regularization, but it does not significantly improve validation performance compared to a dropout of 0.3.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/loss_curve_dropout_0.5.png"}, {"analysis": "The Macro-F1 score improves as dropout increases from 0.0 to 0.3, with a peak at 0.3 and 0.5. This suggests that dropout is effective in improving generalization, with 0.3 being the optimal dropout rate for this setup.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_test_F1_vs_dropout.png"}, {"analysis": "The Macro-F1 score for training increases steadily, while the validation score peaks at epoch 3 and then declines. With no dropout (0.0), the model overfits to the training data, as evidenced by the divergence of training and validation scores in later epochs.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.0.png"}, {"analysis": "The Macro-F1 score for training increases steadily, and the validation score remains relatively stable, with a slight decline after epoch 3. A dropout rate of 0.1 introduces some regularization, reducing overfitting compared to no dropout.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.1.png"}, {"analysis": "The Macro-F1 score for training increases steadily, and the validation score shows a stable trend with minimal decline in later epochs. A dropout rate of 0.3 appears to provide the best balance between learning and regularization, leading to improved generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.3.png"}, {"analysis": "The Macro-F1 score for training increases steadily, while the validation score shows a slight decline after epoch 3. A dropout rate of 0.5 introduces significant regularization, but it does not improve validation performance compared to a dropout of 0.3.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_96255ae056f642059702c07062aaf285_proc_3330989/spr_bench_macro_F1_dropout_0.5.png"}], [{"analysis": "This plot shows the training and validation loss over five epochs for a weight decay of 1e-05. The training loss decreases steadily, indicating that the model is learning effectively. However, the validation loss does not decrease in tandem and instead fluctuates slightly, suggesting potential overfitting or that the model is not generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/loss_curve_wd_1e-05.png"}, {"analysis": "This plot illustrates the training Macro-F1 score across epochs for different weight decay values. All configurations show rapid improvement in Macro-F1 within the first two epochs, with diminishing returns in later epochs. The different weight decays converge to similar performance, with no significant advantage for any particular value.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_train_macro_f1_curves.png"}, {"analysis": "This plot shows the validation Macro-F1 score over epochs for various weight decay values. Unlike the training scores, the validation scores exhibit more variability, with some weight decay values showing a decline after initial improvement. This indicates that the choice of weight decay impacts generalization performance, with 1e-05 and 1e-06 showing relatively stable trends.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_val_macro_f1_curves.png"}, {"analysis": "This bar chart compares the final test Macro-F1 scores for different weight decay values. The scores are nearly identical across all configurations, suggesting that weight decay has minimal impact on the final test performance, or that the model's performance is robust to this hyperparameter.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_test_macro_f1_bars.png"}, {"analysis": "This plot repeats the loss curves for the best-performing weight decay (1e-05). The training loss decreases steadily, while the validation loss fluctuates but does not show a clear decreasing trend. This reinforces the earlier observation of potential overfitting or suboptimal generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4dc8c367eee445f6a2a760d5a6fdf65e_proc_3330990/SPR_BENCH_loss_curves_best_wd_1e-05.png"}], [{"analysis": "The loss curves for dropout=0.0 show that the training loss decreases steadily, indicating effective learning. However, the validation loss exhibits fluctuations and does not consistently decrease, suggesting potential overfitting or instability in generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.0.png"}, {"analysis": "The loss curves for dropout=0.1 show a similar trend to dropout=0.0, with steady training loss reduction. The validation loss shows less fluctuation compared to dropout=0.0, indicating slightly improved generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.1.png"}, {"analysis": "The loss curves for dropout=0.3 show steady training loss reduction, with validation loss exhibiting a slight upward trend after epoch 3. This may indicate overfitting starting to occur, despite the increased dropout rate.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.3.png"}, {"analysis": "The loss curves for dropout=0.5 show a significant reduction in training loss, but the validation loss starts to increase after epoch 3, indicating overfitting. The higher dropout rate does not seem to mitigate overfitting effectively in this case.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/loss_curve_dropout_0.5.png"}, {"analysis": "The bar chart of Macro-F1 scores across dropout rates reveals that the performance is relatively stable, with a slight decrease at dropout=0.1 and dropout=0.3. Dropout=0.0 and dropout=0.5 show the highest Macro-F1 scores, suggesting that these configurations may be more suitable for this task.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_test_F1_vs_dropout.png"}, {"analysis": "The Macro-F1 curves for dropout=0.0 show that training Macro-F1 increases sharply in the initial epochs and then stabilizes, while validation Macro-F1 fluctuates slightly but remains close to the training curve, indicating good generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.0.png"}, {"analysis": "The Macro-F1 curves for dropout=0.1 show that the validation Macro-F1 closely tracks the training Macro-F1, with both stabilizing after epoch 2. This suggests that the model generalizes well with this dropout rate.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.1.png"}, {"analysis": "The Macro-F1 curves for dropout=0.3 show that the validation Macro-F1 initially tracks the training Macro-F1 but starts to diverge slightly after epoch 3, indicating some overfitting.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.3.png"}, {"analysis": "The Macro-F1 curves for dropout=0.5 show that the validation Macro-F1 diverges more noticeably from the training Macro-F1 after epoch 3, confirming overfitting at this dropout level.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/spr_bench_macro_F1_dropout_0.5.png"}], [{"analysis": "The loss curves show that for dropout=0.0, the training loss decreases steadily, but the validation loss fluctuates and increases after epoch 3. This indicates potential overfitting. The model is fitting the training data well but struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.0.png"}, {"analysis": "With dropout=0.1, the training loss decreases steadily, and the validation loss stabilizes with less fluctuation compared to dropout=0.0. This suggests that a small dropout rate helps reduce overfitting and improves generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.1.png"}, {"analysis": "For dropout=0.3, the training loss decreases steadily, but the validation loss stabilizes at a higher level compared to dropout=0.1. This indicates that a higher dropout rate may be introducing too much regularization, slightly hindering the model's ability to fit the validation data.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.3.png"}, {"analysis": "At dropout=0.5, the training loss continues to decrease, but the validation loss shows an upward trend after epoch 4. This suggests that excessive dropout might be impairing the model's learning capacity, leading to underfitting.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/loss_curve_dropout_0.5.png"}, {"analysis": "The bar chart of Macro-F1 scores across dropout rates shows minimal variation, with scores hovering around 0.69 for all dropout values. This suggests that the choice of dropout rate has a limited impact on the overall performance metric, although slight differences in loss curves indicate potential trade-offs in generalization and overfitting.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_test_F1_vs_dropout.png"}, {"analysis": "The Macro-F1 curve for dropout=0.0 shows that the training performance improves steadily, but the validation performance plateaus and slightly diverges, indicating overfitting.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.0.png"}, {"analysis": "For dropout=0.1, the Macro-F1 curve shows a good balance between training and validation performance, with both curves converging towards the same value, indicating improved generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.1.png"}, {"analysis": "The Macro-F1 curve for dropout=0.3 shows that the validation performance initially surpasses the training performance, indicating regularization is working well early on, but the gap widens slightly, suggesting potential underfitting.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.3.png"}, {"analysis": "For dropout=0.5, the Macro-F1 curve shows a significant gap between training and validation performance, with the validation performance lagging behind, indicating underfitting due to excessive regularization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/spr_bench_macro_F1_dropout_0.5.png"}], [{"analysis": "The loss curves indicate that the training loss consistently decreases across epochs for all dropout rates, suggesting that the model is learning effectively during training. However, the validation loss shows varying trends. For dropout rates of 0.0 and 0.1, the validation loss initially decreases but begins to increase after reaching a minimum, suggesting potential overfitting. For higher dropout rates (0.3 and 0.5), the validation loss remains relatively stable or increases at a slower rate, indicating better generalization due to regularization effects of dropout.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.0.png"}, {"analysis": "The bar plot reveals that the Macro-F1 score on the test set is relatively stable across different dropout rates, with a slight improvement at a dropout rate of 0.5. This suggests that while dropout regularization does not drastically affect the overall performance, a higher dropout rate may slightly improve the model's ability to generalize.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.1.png"}, {"analysis": "The Macro-F1 curves for both training and validation show that the training performance consistently improves across epochs for all dropout rates. However, the validation performance plateaus or slightly decreases after reaching a peak, especially for lower dropout rates (0.0 and 0.1). Higher dropout rates (0.3 and 0.5) appear to maintain better validation performance over epochs, further supporting the observation that dropout improves generalization.", "plot_path": "experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/loss_curve_dropout_0.3.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model is learning effectively during the initial\nepochs but starts to overfit as training progresses. The confusion matrix\nhighlights areas for improvement in classification accuracy, particularly in\nreducing misclassifications for both classes.", "The results indicate that the model is overfitting, as evidenced by the\ndivergence between training and validation losses and the fluctuations in\nvalidation macro-F1 scores. The label distribution analysis highlights a\npotential bias in predictions, particularly for Class 1, suggesting the need for\nfurther tuning and adjustments to improve performance and generalization.", "[]", "The plots provide insights into the impact of batch size on training stability,\ngeneralization, and final performance. Larger batch sizes (256) show smoother\nloss trends and better generalization, while smaller batch sizes (32, 64)\nexhibit fluctuating validation loss. The final Macro-F1 scores are similar\nacross batch sizes, but larger batch sizes appear slightly more robust.", "The analysis highlights that embedding dimension 64 provides the most stable and\neffective performance across metrics, while other dimensions show signs of\noverfitting or instability. Validation loss and Macro-F1 trends suggest that\nembedding dimensions 128 and 256 are less reliable. Embedding dimension 32\nunderperforms, likely due to underfitting.", "The plots provide insights into the model's performance during training and\nvalidation with different learning rates. A learning rate of 0.0005 emerges as\nthe most effective, showing stable and high Macro-F1 scores on validation.\nHowever, there are signs of overfitting after the third epoch, and the confusion\nmatrix highlights areas for improvement in classification accuracy, particularly\nin reducing False Positives and False Negatives.", "The analysis reveals that smaller hidden sizes (especially 64) consistently\nachieve better generalization and stability across validation loss and Macro-F1\nmetrics. Larger hidden sizes, while potentially more expressive, exhibit\noverfitting and instability. The confusion matrix suggests a need to improve\nprecision and recall, likely through additional tuning or adjustments to the\ntraining process.", "The results indicate that dropout plays a critical role in regularizing the\nmodel and improving generalization. A dropout rate of 0.3 appears to be optimal,\nbalancing training performance and validation stability. Overfitting is evident\nin cases with low or no dropout, as seen in the divergence between training and\nvalidation metrics. The Macro-F1 score is a suitable evaluation metric for this\ntask, as it captures the balance between precision and recall across classes.", "The plots indicate that the model learns effectively during training, as\nevidenced by the decreasing training loss and increasing Macro-F1 scores.\nHowever, the validation loss and scores suggest potential overfitting or\nchallenges in generalization. Weight decay has a limited impact on final test\nperformance, although it does influence validation trends during training.\nFurther investigation into regularization techniques or model architecture\nadjustments may be warranted to improve generalization.", "The results indicate that lower dropout rates (0.0 and 0.1) lead to better\ngeneralization and higher Macro-F1 scores, while higher dropout rates (0.3 and\n0.5) show signs of overfitting and reduced performance. The model achieves its\nbest performance with no dropout or minimal dropout, suggesting that\nregularization through dropout may not be as effective for this task.", "The plots provide insights into the impact of dropout rates on model\nperformance. Lower dropout rates (e.g., 0.1) seem to strike a balance between\nreducing overfitting and maintaining generalization, while higher dropout rates\n(e.g., 0.5) lead to underfitting. The Macro-F1 scores show minimal variation\nacross dropout rates, suggesting robustness in the metric. However, the loss and\nMacro-F1 curves indicate trade-offs in generalization and learning capacity as\ndropout rates vary.", "The plots demonstrate the impact of dropout rates on model training and\ngeneralization. While lower dropout rates lead to potential overfitting, higher\ndropout rates improve validation performance and generalization. The Macro-F1\nscore remains stable across dropout rates, with a slight improvement at higher\ndropout levels.", "[]"], "exec_time": [3.501241445541382, 6.345742464065552, 4.331113815307617, 5.300836563110352, 7.469961404800415, 10.80608344078064, 5.387160062789917, 6.0286314487457275, 9.650336027145386, 10.560575246810913, 5.743812322616577, 5.319028854370117, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['experiment_data']"], ["[]"], [], ["['SPR_BENCH_bs256']"], ["['synthetic SPR_BENCH (emb=64)']"], ["[\"SPR_BENCH\"]"], ["[\"experiment_data\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"dropout=0.0\"", "\"dropout=0.1\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------------- plotting ------------------\nfor dset, info in experiment_data.items():\n    epochs = info.get(\"epochs\", [])\n    losses_tr = info.get(\"losses\", {}).get(\"train\", [])\n    losses_val = info.get(\"losses\", {}).get(\"val\", [])\n    f1_tr = info.get(\"metrics\", {}).get(\"train_macro_f1\", [])\n    f1_val = info.get(\"metrics\", {}).get(\"val_macro_f1\", [])\n    y_true = np.array(info.get(\"ground_truth\", []))\n    y_pred = np.array(info.get(\"predictions\", []))\n\n    # 1. Loss curve\n    try:\n        plt.figure()\n        plt.plot(epochs, losses_tr, label=\"Train\")\n        plt.plot(epochs, losses_val, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dset}: Train vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dset}_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {dset}: {e}\")\n        plt.close()\n\n    # 2. Macro-F1 curve\n    try:\n        plt.figure()\n        plt.plot(epochs, f1_tr, label=\"Train\")\n        plt.plot(epochs, f1_val, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dset}: Train vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dset}_macro_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve for {dset}: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix (only if preds exist)\n    try:\n        if y_true.size and y_pred.size:\n            cm = np.zeros((2, 2), dtype=int)\n            for t, p in zip(y_true, y_pred):\n                cm[int(t), int(p)] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            for i in range(2):\n                for j in range(2):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n            plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n            plt.title(f\"{dset}: Confusion Matrix (Test Set)\")\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    # Extract logged data\n    track = experiment_data[\"num_epochs\"][\"SPR_BENCH\"]\n    epochs = track[\"epochs\"]\n    train_loss, val_loss = track[\"losses\"][\"train\"], track[\"losses\"][\"val\"]\n    train_f1, val_f1 = (\n        track[\"metrics\"][\"train_macro_f1\"],\n        track[\"metrics\"][\"val_macro_f1\"],\n    )\n    preds, gts = np.array(track[\"predictions\"]), np.array(track[\"ground_truth\"])\n\n    # Compute evaluation metric\n    try:\n        test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n        print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n    except Exception as e:\n        print(f\"Error computing test F1: {e}\")\n\n    # ---------- Plot 1: Loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve_replot.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ---------- Plot 2: Macro-F1 curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ---------- Plot 3: Prediction vs Ground Truth counts ----------\n    try:\n        plt.figure()\n        width = 0.35\n        labels = [\"Class 0\", \"Class 1\"]\n        gt_counts = [np.sum(gts == 0), np.sum(gts == 1)]\n        pred_counts = [np.sum(preds == 0), np.sum(preds == 1)]\n        x = np.arange(len(labels))\n        plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        plt.xticks(x, labels)\n        plt.ylabel(\"Count\")\n        plt.title(\"SPR_BENCH: Label Distribution\u2014Ground Truth vs Predictions\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_label_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch arrays\ndef get_run(key):\n    run = experiment_data[\"batch_size\"][key]\n    return (\n        run[\"epochs\"],\n        run[\"losses\"][\"train\"],\n        run[\"losses\"][\"val\"],\n        run[\"metrics\"][\"train_macro_f1\"],\n        run[\"metrics\"][\"val_macro_f1\"],\n        run[\"predictions\"],\n        run[\"ground_truth\"],\n    )\n\n\nruns = {}\nfor bs in [32, 64, 256]:\n    k = f\"SPR_BENCH_bs{bs}\"\n    if \"batch_size\" in experiment_data and k in experiment_data[\"batch_size\"]:\n        runs[bs] = get_run(k)\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for bs, (ep, tr_loss, val_loss, *_) in runs.items():\n        plt.plot(ep, tr_loss, label=f\"train bs{bs}\")\n        plt.plot(ep, val_loss, \"--\", label=f\"val bs{bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Loss Curves across Batch Sizes\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves_all_bs.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve figure: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for bs, (ep, *_, tr_f1, val_f1, _) in runs.items():\n        plt.plot(ep, tr_f1, label=f\"train bs{bs}\")\n        plt.plot(ep, val_f1, \"--\", label=f\"val bs{bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Macro-F1 Curves across Batch Sizes\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves_all_bs.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve figure: {e}\")\n    plt.close()\n\n# 3) Final test Macro-F1 bar chart\ntry:\n    plt.figure(figsize=(5, 4))\n    bs_list, test_f1 = [], []\n    from sklearn.metrics import f1_score\n\n    for bs, (*_, preds, gts) in runs.items():\n        bs_list.append(str(bs))\n        test_f1.append(f1_score(gts, preds, average=\"macro\"))\n    plt.bar(bs_list, test_f1, color=\"skyblue\")\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Test Macro-F1 by Batch Size\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test F1 bar figure: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------ paths & data loading -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    emb_logs = experiment_data.get(\"emb_dim\", {})\n    # ------- per-embedding plots (max 4 -> still <=5) -------\n    for emb_dim, logs in emb_logs.items():\n        try:\n            epochs = logs.get(\"epochs\", [])\n            tr_loss = logs.get(\"losses\", {}).get(\"train\", [])\n            val_loss = logs.get(\"losses\", {}).get(\"val\", [])\n            tr_f1 = logs.get(\"metrics\", {}).get(\"train_macro_f1\", [])\n            val_f1 = logs.get(\"metrics\", {}).get(\"val_macro_f1\", [])\n\n            plt.figure(figsize=(10, 4))\n            # Left: Loss\n            plt.subplot(1, 2, 1)\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss\")\n            plt.legend()\n\n            # Right: Macro-F1\n            plt.subplot(1, 2, 2)\n            plt.plot(epochs, tr_f1, label=\"Train F1\")\n            plt.plot(epochs, val_f1, label=\"Val F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(\"Macro-F1\")\n            plt.legend()\n\n            plt.suptitle(\n                f\"Left: Loss, Right: Macro-F1 \u2014 synthetic SPR_BENCH (emb={emb_dim})\"\n            )\n            fname = f\"spr_bench_emb{emb_dim}_train_val_curves.png\"\n            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating curve plot for emb_dim={emb_dim}: {e}\")\n            plt.close()\n\n    # -------- aggregated bar chart of test Macro-F1 ----------\n    try:\n        dims, test_scores = [], []\n        for emb_dim, logs in emb_logs.items():\n            score = logs.get(\"test_macro_f1\")\n            if score is not None:\n                dims.append(str(emb_dim))\n                test_scores.append(score)\n\n        if test_scores:\n            plt.figure(figsize=(6, 4))\n            plt.bar(dims, test_scores, color=\"skyblue\")\n            plt.xlabel(\"Embedding Dimension\")\n            plt.ylabel(\"Test Macro-F1\")\n            plt.title(\"Test Macro-F1 by Embedding Dim \u2014 synthetic SPR_BENCH\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, \"spr_bench_test_f1_bar.png\"))\n            plt.close()\n            print(\"Test macro-F1 scores:\", dict(zip(dims, test_scores)))\n    except Exception as e:\n        print(f\"Error creating bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"SPR_BENCH\", {})\n\n# ----------------- Plot 1: macro-F1 curves -----------------\ntry:\n    per_lr = spr_data.get(\"per_lr\", {})\n    if per_lr:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        for lr, d in per_lr.items():\n            epochs = np.arange(1, len(d[\"metrics\"][\"train_macro_F1\"]) + 1)\n            axs[0].plot(epochs, d[\"metrics\"][\"train_macro_F1\"], label=f\"lr={lr}\")\n            axs[1].plot(epochs, d[\"metrics\"][\"val_macro_F1\"], label=f\"lr={lr}\")\n        for ax, split in zip(axs, [\"Train\", \"Validation\"]):\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Macro-F1\")\n            ax.set_title(f\"{split} Macro-F1\")\n            ax.legend()\n        plt.suptitle(\"SPR_BENCH: Macro-F1 over Epochs (Left: Train, Right: Val)\")\n        plt.tight_layout(rect=[0, 0, 1, 0.93])\n        fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# ----------------- Plot 2: best-lr loss curves -----------------\ntry:\n    best_lr = spr_data.get(\"best_lr\", None)\n    if best_lr is not None:\n        losses = spr_data[\"per_lr\"][best_lr][\"losses\"]\n        epochs = np.arange(1, len(losses[\"train\"]) + 1)\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, losses[\"train\"], label=\"Train\")\n        plt.plot(epochs, losses[\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.title(f\"SPR_BENCH: Loss Curve (best lr={best_lr})\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_best_lr_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------------- Plot 3: confusion matrix -----------------\ntry:\n    preds = np.array(spr_data.get(\"predictions\", []))\n    gts = np.array(spr_data.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        tp = np.sum((preds == 1) & (gts == 1))\n        tn = np.sum((preds == 0) & (gts == 0))\n        fp = np.sum((preds == 1) & (gts == 0))\n        fn = np.sum((preds == 0) & (gts == 1))\n        cm = np.array([[tn, fp], [fn, tp]])\n        plt.figure(figsize=(4, 4))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n        plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.title(\"SPR_BENCH: Test Confusion Matrix\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\nprint(\"Plotting complete. Files saved to:\", working_dir)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    hid_dict = experiment_data.get(\"hidden_size\", {})\n    best_info = hid_dict.get(\"best_model\", {})\n    best_hid = best_info.get(\"hidden_size\")\n    test_macro_f1 = best_info.get(\"test_macro_f1\")\n\n    print(f\"Best hidden size: {best_hid}\")\n    print(f\"Test Macro-F1: {test_macro_f1}\")\n\n    # collect list of hids (str) excluding the helper keys\n    hids = [k for k in hid_dict.keys() if k not in (\"best_model\",)]\n\n    # ---------- Plot 1: val loss per hid ----------\n    try:\n        plt.figure(figsize=(7, 4))\n        for hid in hids:\n            run = hid_dict[hid]\n            plt.plot(run[\"epochs\"], run[\"losses\"][\"val\"], label=f\"hid {hid}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Loss\")\n        plt.title(\"SPR_BENCH: Validation Loss over Epochs\\nComparing Hidden Sizes\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"spr_bench_val_loss_by_hidden_size.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val-loss plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 2: macro-F1 per hid ----------\n    try:\n        plt.figure(figsize=(7, 4))\n        for hid in hids:\n            run = hid_dict[hid]\n            plt.plot(run[\"epochs\"], run[\"metrics\"][\"val_macro_f1\"], label=f\"hid {hid}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Macro-F1\")\n        plt.title(\"SPR_BENCH: Validation Macro-F1 over Epochs\\nComparing Hidden Sizes\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"spr_bench_macro_f1_by_hidden_size.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 3: train vs val loss for best hid ----------\n    try:\n        if best_hid is not None:\n            run = hid_dict[str(best_hid)]\n            plt.figure(figsize=(7, 4))\n            plt.plot(run[\"epochs\"], run[\"losses\"][\"train\"], label=\"Train\")\n            plt.plot(run[\"epochs\"], run[\"losses\"][\"val\"], label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"SPR_BENCH: Train vs Val Loss\\nBest Hidden Size = {best_hid}\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(\n                working_dir, f\"spr_bench_train_val_loss_best_hid_{best_hid}.png\"\n            )\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating best-hid loss plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 4: confusion matrix ----------\n    try:\n        preds = best_info.get(\"predictions\")\n        gts = best_info.get(\"ground_truth\")\n        if preds is not None and gts is not None:\n            cm = np.zeros((2, 2), dtype=int)\n            for p, g in zip(preds, gts):\n                cm[int(g), int(p)] += 1\n            plt.figure(figsize=(4, 4))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            for i in range(2):\n                for j in range(2):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                \"SPR_BENCH: Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.tight_layout()\n            fname = os.path.join(\n                working_dir, f\"spr_bench_confusion_matrix_best_hid_{best_hid}.png\"\n            )\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndrop_dict = experiment_data.get(\"dropout_rate\", {})\ndropouts = sorted(drop_dict.keys())\n\n# -------- helper to find best dropout --------\nbest_rate, best_val = None, -1.0\nfor r in dropouts:\n    val_f1 = drop_dict[r][\"metrics\"][\"val_macro_f1\"][-1]  # last epoch val F1\n    if val_f1 > best_val:\n        best_val, best_rate = val_f1, r\nbest_test = drop_dict[best_rate][\"test_macro_f1\"] if best_rate is not None else None\n\n# -------- plot 1: test macro-F1 summary --------\ntry:\n    plt.figure(figsize=(6, 4))\n    test_scores = [drop_dict[r][\"test_macro_f1\"] for r in dropouts]\n    plt.bar([str(r) for r in dropouts], test_scores, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.xlabel(\"Dropout rate\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 vs Dropout\")\n    for i, v in enumerate(test_scores):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_F1_vs_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating summary plot: {e}\")\n    plt.close()\n\n# -------- per-dropout F1 curves (limit to 4 = total 5 plots) --------\nfor idx, r in enumerate(dropouts):\n    if idx >= 4:  # ensure at most 5 plots total (1 summary + 4 curves)\n        break\n    try:\n        rec = drop_dict[r]\n        epochs = rec[\"epochs\"]\n        tr_f1 = rec[\"metrics\"][\"train_macro_f1\"]\n        val_f1 = rec[\"metrics\"][\"val_macro_f1\"]\n\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, tr_f1, label=\"train\")\n        plt.plot(epochs, val_f1, label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.title(f\"SPR_BENCH Macro-F1 Curves (dropout={r})\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_macro_F1_dropout_{r}.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for dropout={r}: {e}\")\n        plt.close()\n\nprint(\n    f\"Best dropout rate: {best_rate} | Val-F1: {best_val:.4f} | Test-F1: {best_test:.4f}\"\n    if best_rate is not None\n    else \"No data found.\"\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    wd_dict = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wds = sorted(wd_dict.keys(), key=lambda x: float(x))\n    # ------------------------------------------------\n    # 1) train-F1 curves\n    try:\n        plt.figure(figsize=(6, 4))\n        for wd in wds:\n            rec = wd_dict[wd]\n            plt.plot(rec[\"epochs\"], rec[\"metrics\"][\"train_macro_f1\"], label=f\"wd={wd}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Training Macro-F1 vs Epoch (SPR_BENCH)\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_macro_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train-F1 plot: {e}\")\n        plt.close()\n    # ------------------------------------------------\n    # 2) val-F1 curves\n    try:\n        plt.figure(figsize=(6, 4))\n        for wd in wds:\n            rec = wd_dict[wd]\n            plt.plot(rec[\"epochs\"], rec[\"metrics\"][\"val_macro_f1\"], label=f\"wd={wd}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Validation Macro-F1 vs Epoch (SPR_BENCH)\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_macro_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val-F1 plot: {e}\")\n        plt.close()\n    # ------------------------------------------------\n    # 3) final test-F1 bar chart\n    try:\n        test_f1s = [\n            np.mean(wd_dict[wd][\"metrics\"][\"val_macro_f1\"][-1:]) * 0 for wd in wds\n        ]  # dummy init\n        # recover stored test scores (they are not in metrics, so rebuild)\n        test_f1s = []\n        for wd in wds:\n            preds = np.array(wd_dict[wd][\"predictions\"])\n            gt = np.array(wd_dict[wd][\"ground_truth\"])\n            # recompute macro-F1 just for plotting (SAFETY)\n            from sklearn.metrics import f1_score\n\n            test_f1 = f1_score(gt, preds, average=\"macro\") if preds.size else 0.0\n            test_f1s.append(test_f1)\n        plt.figure(figsize=(6, 4))\n        plt.bar(range(len(wds)), test_f1s, tick_label=wds)\n        plt.ylabel(\"Macro-F1\")\n        plt.xlabel(\"Weight Decay\")\n        plt.title(\"Final Test Macro-F1 vs Weight Decay (SPR_BENCH)\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_macro_f1_bars.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test-F1 bar chart: {e}\")\n        plt.close()\n    # ------------------------------------------------\n    # 4) loss curve for best run\n    try:\n        best_wd = max(wds, key=lambda wd: wd_dict[wd][\"metrics\"][\"val_macro_f1\"][-1])\n        rec = wd_dict[best_wd]\n        plt.figure(figsize=(6, 4))\n        plt.plot(rec[\"epochs\"], rec[\"losses\"][\"train\"], label=\"train_loss\")\n        plt.plot(rec[\"epochs\"], rec[\"losses\"][\"val\"], label=\"val_loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curves (SPR_BENCH, best wd={best_wd})\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(\n            working_dir, f\"SPR_BENCH_loss_curves_best_wd_{best_wd}.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n        # print best score\n        from sklearn.metrics import f1_score\n\n        preds = np.array(rec[\"predictions\"])\n        gt = np.array(rec[\"ground_truth\"])\n        best_test_f1 = f1_score(gt, preds, average=\"macro\") if preds.size else 0.0\n        print(f\"Best weight_decay={best_wd} | Test Macro-F1={best_test_f1:.4f}\")\n    except Exception as e:\n        print(f\"Error creating best-run loss curve: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndrop_dict = experiment_data.get(\"dropout_rate\", {})\ndropouts = sorted(drop_dict.keys())\n\n# -------- helper to find best dropout --------\nbest_rate, best_val = None, -1.0\nfor r in dropouts:\n    val_f1 = drop_dict[r][\"metrics\"][\"val_macro_f1\"][-1]  # last epoch val F1\n    if val_f1 > best_val:\n        best_val, best_rate = val_f1, r\nbest_test = drop_dict[best_rate][\"test_macro_f1\"] if best_rate is not None else None\n\n# -------- plot 1: test macro-F1 summary --------\ntry:\n    plt.figure(figsize=(6, 4))\n    test_scores = [drop_dict[r][\"test_macro_f1\"] for r in dropouts]\n    plt.bar([str(r) for r in dropouts], test_scores, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.xlabel(\"Dropout rate\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 vs Dropout\")\n    for i, v in enumerate(test_scores):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_F1_vs_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating summary plot: {e}\")\n    plt.close()\n\n# -------- per-dropout F1 curves (limit to 4 = total 5 plots) --------\nfor idx, r in enumerate(dropouts):\n    if idx >= 4:  # ensure at most 5 plots total (1 summary + 4 curves)\n        break\n    try:\n        rec = drop_dict[r]\n        epochs = rec[\"epochs\"]\n        tr_f1 = rec[\"metrics\"][\"train_macro_f1\"]\n        val_f1 = rec[\"metrics\"][\"val_macro_f1\"]\n\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, tr_f1, label=\"train\")\n        plt.plot(epochs, val_f1, label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.title(f\"SPR_BENCH Macro-F1 Curves (dropout={r})\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_macro_F1_dropout_{r}.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for dropout={r}: {e}\")\n        plt.close()\n\nprint(\n    f\"Best dropout rate: {best_rate} | Val-F1: {best_val:.4f} | Test-F1: {best_test:.4f}\"\n    if best_rate is not None\n    else \"No data found.\"\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndrop_dict = experiment_data.get(\"dropout_rate\", {})\ndropouts = sorted(drop_dict.keys())\n\n# -------- helper to find best dropout --------\nbest_rate, best_val = None, -1.0\nfor r in dropouts:\n    val_f1 = drop_dict[r][\"metrics\"][\"val_macro_f1\"][-1]  # last epoch val F1\n    if val_f1 > best_val:\n        best_val, best_rate = val_f1, r\nbest_test = drop_dict[best_rate][\"test_macro_f1\"] if best_rate is not None else None\n\n# -------- plot 1: test macro-F1 summary --------\ntry:\n    plt.figure(figsize=(6, 4))\n    test_scores = [drop_dict[r][\"test_macro_f1\"] for r in dropouts]\n    plt.bar([str(r) for r in dropouts], test_scores, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.xlabel(\"Dropout rate\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 vs Dropout\")\n    for i, v in enumerate(test_scores):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_F1_vs_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating summary plot: {e}\")\n    plt.close()\n\n# -------- per-dropout F1 curves (limit to 4 = total 5 plots) --------\nfor idx, r in enumerate(dropouts):\n    if idx >= 4:  # ensure at most 5 plots total (1 summary + 4 curves)\n        break\n    try:\n        rec = drop_dict[r]\n        epochs = rec[\"epochs\"]\n        tr_f1 = rec[\"metrics\"][\"train_macro_f1\"]\n        val_f1 = rec[\"metrics\"][\"val_macro_f1\"]\n\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, tr_f1, label=\"train\")\n        plt.plot(epochs, val_f1, label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.title(f\"SPR_BENCH Macro-F1 Curves (dropout={r})\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_macro_F1_dropout_{r}.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for dropout={r}: {e}\")\n        plt.close()\n\nprint(\n    f\"Best dropout rate: {best_rate} | Val-F1: {best_val:.4f} | Test-F1: {best_test:.4f}\"\n    if best_rate is not None\n    else \"No data found.\"\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndrop_dict = experiment_data.get(\"dropout_rate\", {})\ndropouts = sorted(drop_dict.keys())\n\n# -------- helper to find best dropout --------\nbest_rate, best_val = None, -1.0\nfor r in dropouts:\n    val_f1 = drop_dict[r][\"metrics\"][\"val_macro_f1\"][-1]  # last epoch val F1\n    if val_f1 > best_val:\n        best_val, best_rate = val_f1, r\nbest_test = drop_dict[best_rate][\"test_macro_f1\"] if best_rate is not None else None\n\n# -------- plot 1: test macro-F1 summary --------\ntry:\n    plt.figure(figsize=(6, 4))\n    test_scores = [drop_dict[r][\"test_macro_f1\"] for r in dropouts]\n    plt.bar([str(r) for r in dropouts], test_scores, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.xlabel(\"Dropout rate\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 vs Dropout\")\n    for i, v in enumerate(test_scores):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_F1_vs_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating summary plot: {e}\")\n    plt.close()\n\n# -------- per-dropout F1 curves (limit to 4 = total 5 plots) --------\nfor idx, r in enumerate(dropouts):\n    if idx >= 4:  # ensure at most 5 plots total (1 summary + 4 curves)\n        break\n    try:\n        rec = drop_dict[r]\n        epochs = rec[\"epochs\"]\n        tr_f1 = rec[\"metrics\"][\"train_macro_f1\"]\n        val_f1 = rec[\"metrics\"][\"val_macro_f1\"]\n\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, tr_f1, label=\"train\")\n        plt.plot(epochs, val_f1, label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.title(f\"SPR_BENCH Macro-F1 Curves (dropout={r})\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_macro_F1_dropout_{r}.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for dropout={r}: {e}\")\n        plt.close()\n\nprint(\n    f\"Best dropout rate: {best_rate} | Val-F1: {best_val:.4f} | Test-F1: {best_test:.4f}\"\n    if best_rate is not None\n    else \"No data found.\"\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport math\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------------------------------\n# 1. Load every experiment_data.npy that was listed\n# -------------------------------------------------------------------------\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_4132b97f5b3d4914a0fd75cf44368225_proc_3330988/experiment_data.npy\",\n        \"experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f461584c3de14c95a4fe092985d5a596_proc_3330989/experiment_data.npy\",\n        \"experiments/2025-08-17_18-48-09_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_60be6ebb88b242b1ba0754fe3f82a233_proc_3330987/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        edict = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(edict)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# -------------------------------------------------------------------------\n# 2. Gather metrics across runs\n# -------------------------------------------------------------------------\nfrom collections import defaultdict\n\ntest_f1_by_dropout = defaultdict(list)\ntrain_curves_by_dropout = defaultdict(list)\nval_curves_by_dropout = defaultdict(list)\nepochs_by_dropout = defaultdict(list)  # keep epoch vectors (assume identical per run)\n\nfor run in all_experiment_data:\n    drop_dict = run.get(\"dropout_rate\", {})\n    for d_rate, rec in drop_dict.items():\n        test_f1_by_dropout[d_rate].append(rec[\"test_macro_f1\"])\n        train_curves_by_dropout[d_rate].append(\n            np.array(rec[\"metrics\"][\"train_macro_f1\"])\n        )\n        val_curves_by_dropout[d_rate].append(np.array(rec[\"metrics\"][\"val_macro_f1\"]))\n        epochs_by_dropout[d_rate].append(np.array(rec[\"epochs\"]))\n\ndropouts = sorted(test_f1_by_dropout.keys())\n\n\n# helper to compute sem\ndef sem(arr):\n    n = len(arr)\n    return np.std(arr, ddof=1) / math.sqrt(n) if n > 1 else 0.0\n\n\n# -------------------------------------------------------------------------\n# 3. Plot aggregated Test Macro-F1 with error bars\n# -------------------------------------------------------------------------\ntry:\n    plt.figure(figsize=(7, 4))\n    means = [np.mean(test_f1_by_dropout[d]) for d in dropouts]\n    errs = [sem(test_f1_by_dropout[d]) for d in dropouts]\n    plt.bar(\n        [str(d) for d in dropouts],\n        means,\n        yerr=errs,\n        capsize=5,\n        color=\"skyblue\",\n        label=\"mean \u00b1 SEM\",\n    )\n    plt.ylabel(\"Macro-F1\")\n    plt.xlabel(\"Dropout rate\")\n    plt.title(\n        \"SPR_BENCH: Mean Test Macro-F1 vs Dropout (n={})\".format(\n            len(all_experiment_data)\n        )\n    )\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_mean_test_F1_vs_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated bar plot: {e}\")\n    plt.close()\n\n# -------------------------------------------------------------------------\n# 4. Plot averaged learning curves (\u22643 rates to keep \u22645 figures total)\n# -------------------------------------------------------------------------\nmax_curve_plots = 3\nfor idx, d in enumerate(dropouts[:max_curve_plots]):\n    try:\n        # Align epochs across runs (truncate to shortest)\n        curves_tr = train_curves_by_dropout[d]\n        curves_val = val_curves_by_dropout[d]\n        min_len = min(c.shape[0] for c in curves_tr + curves_val)\n        tr_stack = np.stack([c[:min_len] for c in curves_tr], axis=0)\n        val_stack = np.stack([c[:min_len] for c in curves_val], axis=0)\n        epochs = epochs_by_dropout[d][0][:min_len]  # take first run's epoch vector\n\n        tr_mean, tr_sem = tr_stack.mean(axis=0), tr_stack.std(\n            axis=0, ddof=1\n        ) / math.sqrt(tr_stack.shape[0])\n        val_mean, val_sem = val_stack.mean(axis=0), val_stack.std(\n            axis=0, ddof=1\n        ) / math.sqrt(val_stack.shape[0])\n\n        plt.figure(figsize=(7, 4))\n        plt.plot(epochs, tr_mean, label=\"Train mean\", color=\"tab:blue\")\n        plt.fill_between(\n            epochs, tr_mean - tr_sem, tr_mean + tr_sem, alpha=0.3, color=\"tab:blue\"\n        )\n        plt.plot(epochs, val_mean, label=\"Val mean\", color=\"tab:orange\")\n        plt.fill_between(\n            epochs,\n            val_mean - val_sem,\n            val_mean + val_sem,\n            alpha=0.3,\n            color=\"tab:orange\",\n        )\n\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.title(\n            f\"SPR_BENCH Macro-F1 Curves (dropout={d})\\nMean \u00b1 SEM across {tr_stack.shape[0]} runs\"\n        )\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_macro_F1_dropout_{d}_avg.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating averaged curve for dropout={d}: {e}\")\n        plt.close()\n\n# -------------------------------------------------------------------------\n# 5. Print out numeric summary\n# -------------------------------------------------------------------------\nfor d in dropouts:\n    print(\n        f\"Dropout {d}: mean Test-F1={np.mean(test_f1_by_dropout[d]):.4f}  \u00b1SEM={sem(test_f1_by_dropout[d]):.4f}\"\n    )\n"], "plot_plan": ["We load the saved experiment_data.npy, loop over contained datasets (here only \u201cSPR_BENCH\u201d), and create up to three standard plots that can be derived directly from what is stored: (1) train/val loss over epochs, (2) train/val macro-F1 over epochs, and (3) a confusion-matrix heat-map for the final test predictions.  Each plot is wrapped in its own try-except so partial failures are handled gracefully, and every figure is explicitly closed.  The filenames include the dataset name and plot type, all saved in working_dir.  We avoid creating any additional plots or simulating data, and keep code minimal while following the specified matplotlib and path conventions.", "We will first load experiment_data.npy from the working directory and extract the SPR_BENCH run under the \u201cnum_epochs\u201d sweep.  After computing the test macro-F1 from the stored predictions and ground-truth labels, we will generate three standard visualizations: (1) training vs. validation loss across epochs, (2) training vs. validation macro-F1 across epochs, and (3) a simple bar chart comparing the count of positive/negative ground-truth labels with the model\u2019s predictions (a lightweight alternative to a confusion matrix).  Each figure will be wrapped in its own try-except block, titled clearly, saved with descriptive file names, and the figure will be closed in all circumstances.  We restrict ourselves to the information already logged in experiment_data.npy, and plot at full epoch resolution because the run is short (<30 epochs).  All plots are saved into working_dir, and the code prints the computed test macro-F1 so the user can quickly verify performance.  The script uses only numpy, matplotlib, os, and sklearn.metrics for F1 calculation, adhering to the requirement of basic matplotlib usage and minimal dependencies.  This fulfills the visualization guidelines while remaining concise and robust.", null, "We first load experiment_data.npy and extract the three runs (SPR_BENCH_bs32/64/256).  For each run we pull the stored \u201cepochs\u201d, \u201closses\u201d (train/val), the \u201ctrain_macro_f1\u201d and \u201cval_macro_f1\u201d curves, and the final test\u2010set macro-F1 found at the end of the run.  We then create at most three concise figures: (1) one figure with the train/validation loss curves for the three batch sizes (solid vs dashed per run); (2) one figure with the train/validation macro-F1 curves; (3) a simple bar chart comparing the final test macro-F1 across the batch sizes.  Each figure is wrapped in its own try\u2013except block, saved into working_dir with descriptive filenames, given a clear title that includes the dataset name and the plot type, and finally closed.  No additional data are fabricated and no more than the requested five figures are produced.  Basic matplotlib is used and every figure is explicitly closed to free resources.  The code runs end-to-end in a few lines, following all plotting guidelines.", "We will load `experiment_data.npy`, extract the nested dictionary and iterate over each embedding dimension to visualise the logged history.  \nFor every `emb_dim` we create one figure with two side-by-side subplots: the left subplot shows training/validation loss across epochs, while the right subplot shows training/validation macro-F1; this gives users a quick diagnostic of convergence and over-fitting.  \nEach of these figures is wrapped in its own `try-except` block and saved with descriptive names that contain the dataset (`SPR_BENCH` if real, otherwise synthetic) and the embedding size.  \nTo stay below the five-figure limit we generate at most one figure per embedding dimension (there are four) and then add a single aggregated bar chart comparing the final test macro-F1 for all dimensions.  \nAll plots are given clear titles and subtitles (e.g. \u201cLeft: Loss, Right: Macro-F1 \u2014 synthetic SPR_BENCH\u201d).  \nFigures are closed explicitly after saving to free resources.  \nThe script respects the prescribed imports, constructs `working_dir` relative to the current path, and prints the collected test-set scores so users can confirm the numbers match the plots.", "We first load the saved experiment_data.npy to retrieve all recorded metrics.  \nThe script then creates up to three concise visualisations.  \n(1) For every learning-rate tried, we draw the macro-F1 evolution across epochs for both train and validation splits; the left subplot shows training curves and the right subplot shows validation curves, and all lrs are colour-coded.  \n(2) Using the best learning-rate indicated in the file, we plot its training/validation BCE loss curve so users can quickly judge over-/under-fitting.  \n(3) On the final test set we compute a 2\u00d72 confusion-matrix from the stored predictions and ground-truth labels and render it with imshow, annotating each cell for clarity.  \nEach figure is produced inside its own try-except guard, saved to working_dir with descriptive filenames, and immediately closed to free memory.  \nWe respect the \u201c\u22645 plots\u201d rule and make no assumptions about data that are not present.  \nThe code is self-contained, uses plain matplotlib, and prints a short confirmation once all plots are written.", "We first load the saved experiment_data.npy, extract the list of hidden sizes experimented with, and identify the best run metadata (hidden size and test macro-F1). Four visualizations are produced, each wrapped in an independent try-except block: (1) validation loss trajectories for every hidden size, (2) macro-F1 trajectories for every hidden size, (3) a detailed train-vs-validation loss curve for the best hidden size, and (4) a 2\u00d72 confusion-matrix heat-map comparing ground-truth versus model predictions on the test split. All plots are titled with the dataset name \u201cSPR_BENCH\u201d and descriptive subtitles; filenames also embed the dataset and plot type. To respect the five-figure limit we skip per-epoch sample plots. The script prints the best hidden size and its associated test macro-F1 before plotting, saves each figure into the working directory, closes figures in all cases, and never fabricates data\u2014everything comes directly from experiment_data.npy. The code relies only on numpy and matplotlib, implements a lightweight confusion-matrix routine, and gracefully handles missing files or malformed data.", "The solution first loads the previously saved experiment_data.npy, retrieves the per-dropout histories (losses, macro-F1, test macro-F1), and determines the best dropout rate by highest validation F1 on the final epoch. It then generates at most five plots: (i) a bar chart summarising test macro-F1 across dropout rates, and (ii-v) one line plot per dropout showing the training/validation macro-F1 curves over epochs. Each plot is wrapped in its own try-except block, saved with descriptive filenames inside the working directory, and the figures are closed afterwards. Finally, the code prints out the best dropout rate and its corresponding validation and test scores.", "The script loads the saved `experiment_data.npy`, extracts the per\u2010weight\u2010decay records for the SPR_BENCH dataset, and produces four diagnostic figures: training macro-F1 curves, validation macro-F1 curves, a bar chart of final test macro-F1 for each weight decay, and the loss curves of the best run. Each plot is generated inside its own try-except block, titled clearly, saved to `working_dir` with descriptive filenames, and the figure is closed to free memory. The code also prints the best weight-decay value together with its test macro-F1 so that the numbers in the plots can be cross-checked. Only information already present in `experiment_data.npy` is used, and at most four figures are created to stay below the five-figure guideline.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "batch_size", "emb_dim (embedding dimension)", null, "hidden_size", "dropout_rate", "weight_decay", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate the working directory, load the saved NumPy dictionary,\niterate over each dataset entry, compute the best (i.e., maximum for F1 scores\nand minimum for losses) or final epoch values as appropriate, and print them\nwith clear, explicit metric names. No plots are generated, and the code executes\nimmediately upon running.", "The script loads the stored NumPy dictionary, navigates its nested structure to\naccess each dataset\u2019s tracked metrics, identifies the best (max for F1, min for\nloss) or computes the test macro-F1 from saved predictions, and prints each\nvalue with clear, explicit names. It automatically handles any number of\ndatasets or hyper-parameter groups in the file and produces only textual\noutput\u2014no plots or extra files.", "", "The script will locate the \u201cworking\u201d directory, load the saved NumPy file, and\niterate over each stored sub-experiment (the different batch sizes).   For every\nsub-experiment it will extract:   \u2022 the final train macro-F1 score and train\nloss,   \u2022 the best (highest) validation macro-F1 score and the best (lowest)\nvalidation loss,   \u2022 the test macro-F1 score computed from the saved predictions\nand ground-truth labels.   It then prints the dataset name followed by these\nclearly-labeled metrics. No plots are generated and the code runs immediately at\nimport time.", "We first load the saved NumPy file, which stores a dictionary of logs for each\nembedding dimension.   For every embedding dimension we fetch the last (i.e.,\nfinal) entry of each recorded list to obtain the final training loss, training\nmacro-F1, validation loss, and validation macro-F1 scores.   If a test-set\nmacro-F1 is present (only for the best model) we report it as well.   Results\nare printed in a clear hierarchy: embedding dimension \u2192 dataset name \u2192 metric\nname with its value.", "The script will load the numpy file located in the working directory, convert it\nback to a Python dict, and iterate over every dataset entry it contains. For\neach dataset, it will first identify the learning-rate that achieved the best\nvalidation performance (already stored under \"best_lr\") and then fetch the\ncorresponding final epoch statistics. Finally, it prints clearly labelled\nmetrics\u2014training macro F1 score, validation macro F1 score, training BCE loss,\nvalidation BCE loss, the best learning rate used, and the test macro F1\nscore\u2014preceded by the dataset name. No plots are created and no special entry-\npoint guard is used; the script runs immediately when executed.", "Below is a small script that immediately loads the saved numpy file, loops\nthrough every stored experimental run (each hidden-size as well as the best\nmodel), and prints the final values of all available metrics with explicit,\nself-describing labels.", "The script will locate the saved experiment_data.npy in the working directory,\nread it into a Python dict, and iterate over every tested dropout rate.   For\neach rate, it reports the final-epoch training and validation macro-F1 scores\nand losses, as well as the test macro-F1 score.   Each group of numbers is\nclearly introduced by the dataset name (\u201cTraining dataset\u201d, \u201cValidation\ndataset\u201d, \u201cTest dataset\u201d) and by an explicit metric name (e.g., \u201cTraining macro\nF1 score\u201d).", "The script loads the saved experiment data, iterates over each weight-decay\nconfiguration, and selects the configuration whose last\u2010epoch validation\nmacro-F1 score is the highest. For this best configuration it extracts the\nfinal-epoch training macro-F1 score, validation macro-F1 score, training loss,\nvalidation loss, and recomputes the test macro-F1 score from the stored\npredictions and ground-truth labels. It then prints these metrics, clearly\nlabeled, under the dataset name. The code executes immediately at import time\nand does not rely on any special entry point.", "The script will locate the saved experiment_data.npy in the working directory,\nread it into a Python dict, and iterate over every tested dropout rate.   For\neach rate, it reports the final-epoch training and validation macro-F1 scores\nand losses, as well as the test macro-F1 score.   Each group of numbers is\nclearly introduced by the dataset name (\u201cTraining dataset\u201d, \u201cValidation\ndataset\u201d, \u201cTest dataset\u201d) and by an explicit metric name (e.g., \u201cTraining macro\nF1 score\u201d).", "The script will locate the saved experiment_data.npy in the working directory,\nread it into a Python dict, and iterate over every tested dropout rate.   For\neach rate, it reports the final-epoch training and validation macro-F1 scores\nand losses, as well as the test macro-F1 score.   Each group of numbers is\nclearly introduced by the dataset name (\u201cTraining dataset\u201d, \u201cValidation\ndataset\u201d, \u201cTest dataset\u201d) and by an explicit metric name (e.g., \u201cTraining macro\nF1 score\u201d).", "The script will locate the saved experiment_data.npy in the working directory,\nread it into a Python dict, and iterate over every tested dropout rate.   For\neach rate, it reports the final-epoch training and validation macro-F1 scores\nand losses, as well as the test macro-F1 score.   Each group of numbers is\nclearly introduced by the dataset name (\u201cTraining dataset\u201d, \u201cValidation\ndataset\u201d, \u201cTest dataset\u201d) and by an explicit metric name (e.g., \u201cTraining macro\nF1 score\u201d).", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------- locate and load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------- metric extraction --------------\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Epoch-wise arrays\n    train_f1_list = content[\"metrics\"].get(\"train_macro_f1\", [])\n    val_f1_list = content[\"metrics\"].get(\"val_macro_f1\", [])\n    train_loss_list = content[\"losses\"].get(\"train\", [])\n    val_loss_list = content[\"losses\"].get(\"val\", [])\n\n    # Best or final values\n    if train_f1_list:\n        best_train_f1 = max(train_f1_list)\n        print(f\"Best training macro F1 score: {best_train_f1:.4f}\")\n\n    if val_f1_list:\n        best_val_f1 = max(val_f1_list)\n        print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n\n    if train_loss_list:\n        best_train_loss = min(train_loss_list)\n        print(f\"Best training loss: {best_train_loss:.4f}\")\n\n    if val_loss_list:\n        best_val_loss = min(val_loss_list)\n        print(f\"Best validation loss: {best_val_loss:.4f}\")\n\n    # Test metric (only one value)\n    if \"predictions\" in content and \"ground_truth\" in content:\n        # If test macro F1 was stored directly (preferred)\n        test_f1 = content[\"metrics\"].get(\"test_macro_f1\")\n        if test_f1 is not None:\n            print(f\"Test macro F1 score: {test_f1:.4f}\")\n        else:\n            # Recompute if not stored\n            from sklearn.metrics import f1_score\n\n            preds = content[\"predictions\"]\n            gts = content[\"ground_truth\"]\n            if preds and gts:\n                test_f1_recalc = f1_score(gts, preds, average=\"macro\")\n                print(f\"Test macro F1 score: {test_f1_recalc:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# -------- locate and load experiment data --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------- helper functions --------\ndef best_value(values, larger_is_better=True):\n    \"\"\"Return best value from a list according to the optimisation direction.\"\"\"\n    return max(values) if larger_is_better else min(values)\n\n\ndef print_metric(name: str, value: float):\n    print(f\"{name}: {value:.4f}\")\n\n\n# -------- iterate over stored results --------\nfor hp_group, datasets in experiment_data.items():  # e.g. \"num_epochs\"\n    for dataset_name, data in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---- macro-F1 scores ----\n        train_f1_vals = data.get(\"metrics\", {}).get(\"train_macro_f1\", [])\n        val_f1_vals = data.get(\"metrics\", {}).get(\"val_macro_f1\", [])\n        if train_f1_vals:\n            print_metric(\n                \"Best training macro F1 score\",\n                best_value(train_f1_vals, larger_is_better=True),\n            )\n        if val_f1_vals:\n            print_metric(\n                \"Best validation macro F1 score\",\n                best_value(val_f1_vals, larger_is_better=True),\n            )\n\n        # ---- losses ----\n        train_loss_vals = data.get(\"losses\", {}).get(\"train\", [])\n        val_loss_vals = data.get(\"losses\", {}).get(\"val\", [])\n        if train_loss_vals:\n            print_metric(\n                \"Lowest training loss\",\n                best_value(train_loss_vals, larger_is_better=False),\n            )\n        if val_loss_vals:\n            print_metric(\n                \"Lowest validation loss\",\n                best_value(val_loss_vals, larger_is_better=False),\n            )\n\n        # ---- test set evaluation ----\n        preds = data.get(\"predictions\", [])\n        gts = data.get(\"ground_truth\", [])\n        if preds and gts:\n            test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n            print_metric(\"Test macro F1 score\", test_macro_f1)\n", "", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor ds_name, record in experiment_data[\"batch_size\"].items():\n    # metrics & losses are stored as lists across epochs\n    train_f1_final = record[\"metrics\"][\"train_macro_f1\"][-1]\n    val_f1_best = max(record[\"metrics\"][\"val_macro_f1\"])\n\n    train_loss_final = record[\"losses\"][\"train\"][-1]\n    val_loss_best = min(record[\"losses\"][\"val\"])\n\n    # compute test macro-F1 from stored predictions/labels\n    test_f1 = f1_score(record[\"ground_truth\"], record[\"predictions\"], average=\"macro\")\n\n    # ---------- printing ----------\n    print(f\"Dataset: {ds_name}\")\n    print(f\"Final train macro F1 score: {train_f1_final:.4f}\")\n    print(f\"Best validation macro F1 score: {val_f1_best:.4f}\")\n    print(f\"Final train loss: {train_loss_final:.4f}\")\n    print(f\"Best validation loss: {val_loss_best:.4f}\")\n    print(f\"Test macro F1 score: {test_f1:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# ------------------ Load experiment data ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfilepath = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(filepath, allow_pickle=True).item()\n\n\n# ------------------ Helper for printing -------------------\ndef print_metric(metric_name: str, value: float):\n    print(f\"    {metric_name}: {value:.4f}\")\n\n\n# ------------------ Iterate over embedding dims -----------\nfor emb_dim, logs in experiment_data.get(\"emb_dim\", {}).items():\n    print(f\"\\nEmbedding dimension = {emb_dim}\")\n\n    # -------- Train dataset --------\n    print(\"  Train dataset\")\n    train_f1 = logs[\"metrics\"][\"train_macro_f1\"][-1]\n    train_loss = logs[\"losses\"][\"train\"][-1]\n    print_metric(\"Training macro F1 score\", train_f1)\n    print_metric(\"Training loss\", train_loss)\n\n    # -------- Validation dataset --------\n    print(\"  Validation dataset\")\n    val_f1 = logs[\"metrics\"][\"val_macro_f1\"][-1]\n    val_loss = logs[\"losses\"][\"val\"][-1]\n    print_metric(\"Validation macro F1 score\", val_f1)\n    print_metric(\"Validation loss\", val_loss)\n\n    # -------- Test dataset (only for best model) --------\n    if \"test_macro_f1\" in logs:\n        print(\"  Test dataset\")\n        test_f1 = logs[\"test_macro_f1\"]\n        print_metric(\"Test macro F1 score\", test_f1)\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate and load the saved experiment dictionary\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# iterate over each dataset entry and print metrics\n# -------------------------------------------------\nfor dset_name, dset_dict in experiment_data.items():\n    # dataset header\n    print(f\"{dset_name}\")\n\n    # fetch best learning rate and corresponding records\n    best_lr = dset_dict.get(\"best_lr\")\n    lr_records = dset_dict[\"per_lr\"].get(best_lr, {})\n    metrics = lr_records.get(\"metrics\", {})\n    losses = lr_records.get(\"losses\", {})\n\n    # final epoch values\n    train_macro_f1 = metrics.get(\"train_macro_F1\", [None])[-1]\n    val_macro_f1 = metrics.get(\"val_macro_F1\", [None])[-1]\n    train_bce_loss = losses.get(\"train\", [None])[-1]\n    val_bce_loss = losses.get(\"val\", [None])[-1]\n    test_macro_f1 = dset_dict.get(\"test_macro_f1\")\n\n    # print with explicit labels\n    print(f\"best learning rate: {best_lr}\")\n    print(f\"training macro F1 score: {train_macro_f1:.4f}\")\n    print(f\"validation macro F1 score: {val_macro_f1:.4f}\")\n    print(f\"training BCE loss: {train_bce_loss:.4f}\")\n    print(f\"validation BCE loss: {val_bce_loss:.4f}\")\n    print(f\"test macro F1 score: {test_macro_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate and load the saved experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Iterate over every experimental run (each hidden size)\n# ------------------------------------------------------------------\nfor run_name, run_data in experiment_data[\"hidden_size\"].items():\n    # Handle the special \"best_model\" record separately\n    if run_name == \"best_model\":\n        best = run_data\n        print(f\"Dataset: best_model (hidden_size={best['hidden_size']})\")\n        print(f\"Test macro F1 score: {best['test_macro_f1']:.4f}\")\n        print()  # blank line for readability\n        continue\n\n    # Regular runs (key is the hidden size as a string, e.g., \"64\")\n    metrics = run_data[\"metrics\"]\n    losses = run_data[\"losses\"]\n\n    final_train_f1 = metrics[\"train_macro_f1\"][-1]\n    final_val_f1 = metrics[\"val_macro_f1\"][-1]\n    final_train_loss = losses[\"train\"][-1]\n    final_validation_loss = losses[\"val\"][-1]\n\n    print(f\"Dataset: hidden_size={run_name}\")\n    print(f\"Final train macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Final validation macro F1 score: {final_val_f1:.4f}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_validation_loss:.4f}\")\n    print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- display ----------\nfor dropout_rate, record in experiment_data.get(\"dropout_rate\", {}).items():\n    print(f\"\\n=== Results for dropout rate: {dropout_rate} ===\")\n\n    # Training metrics (final epoch)\n    print(\"Training dataset\")\n    final_train_f1 = record[\"metrics\"][\"train_macro_f1\"][-1]\n    final_train_loss = record[\"losses\"][\"train\"][-1]\n    print(f\"Training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Training loss: {final_train_loss:.4f}\")\n\n    # Validation metrics (final epoch)\n    print(\"Validation dataset\")\n    final_val_f1 = record[\"metrics\"][\"val_macro_f1\"][-1]\n    final_val_loss = record[\"losses\"][\"val\"][-1]\n    print(f\"Validation macro F1 score: {final_val_f1:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\")\n\n    # Test metrics\n    print(\"Test dataset\")\n    test_f1 = record[\"test_macro_f1\"]\n    print(f\"Test macro F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef final_value(lst):\n    return lst[-1] if lst else None\n\n\n# ---------- find best configuration ----------\nspr_dict = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\nbest_key, best_val_f1 = None, -1.0\nfor wd_str, rec in spr_dict.items():\n    val_f1_last = final_value(rec[\"metrics\"][\"val_macro_f1\"])\n    if val_f1_last is not None and val_f1_last > best_val_f1:\n        best_val_f1 = val_f1_last\n        best_key = wd_str\n\nbest_rec = spr_dict[best_key]\n\n# ---------- compute metrics ----------\ntrain_macro_f1 = final_value(best_rec[\"metrics\"][\"train_macro_f1\"])\nvalidation_macro_f1 = final_value(best_rec[\"metrics\"][\"val_macro_f1\"])\ntrain_loss = final_value(best_rec[\"losses\"][\"train\"])\nvalidation_loss = final_value(best_rec[\"losses\"][\"val\"])\n\n# recompute test macro-F1 from stored predictions / labels\ntest_preds = best_rec[\"predictions\"]\ntest_labels = best_rec[\"ground_truth\"]\ntest_macro_f1 = f1_score(test_labels, test_preds, average=\"macro\")\n\n# ---------- reporting ----------\nprint(\"Dataset: SPR_BENCH\")\nprint(f\"Best weight_decay: {best_key}\")\nprint(f\"train macro F1 score: {train_macro_f1:.4f}\")\nprint(f\"validation macro F1 score: {validation_macro_f1:.4f}\")\nprint(f\"test macro F1 score: {test_macro_f1:.4f}\")\nprint(f\"train loss: {train_loss:.6f}\")\nprint(f\"validation loss: {validation_loss:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- display ----------\nfor dropout_rate, record in experiment_data.get(\"dropout_rate\", {}).items():\n    print(f\"\\n=== Results for dropout rate: {dropout_rate} ===\")\n\n    # Training metrics (final epoch)\n    print(\"Training dataset\")\n    final_train_f1 = record[\"metrics\"][\"train_macro_f1\"][-1]\n    final_train_loss = record[\"losses\"][\"train\"][-1]\n    print(f\"Training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Training loss: {final_train_loss:.4f}\")\n\n    # Validation metrics (final epoch)\n    print(\"Validation dataset\")\n    final_val_f1 = record[\"metrics\"][\"val_macro_f1\"][-1]\n    final_val_loss = record[\"losses\"][\"val\"][-1]\n    print(f\"Validation macro F1 score: {final_val_f1:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\")\n\n    # Test metrics\n    print(\"Test dataset\")\n    test_f1 = record[\"test_macro_f1\"]\n    print(f\"Test macro F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- display ----------\nfor dropout_rate, record in experiment_data.get(\"dropout_rate\", {}).items():\n    print(f\"\\n=== Results for dropout rate: {dropout_rate} ===\")\n\n    # Training metrics (final epoch)\n    print(\"Training dataset\")\n    final_train_f1 = record[\"metrics\"][\"train_macro_f1\"][-1]\n    final_train_loss = record[\"losses\"][\"train\"][-1]\n    print(f\"Training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Training loss: {final_train_loss:.4f}\")\n\n    # Validation metrics (final epoch)\n    print(\"Validation dataset\")\n    final_val_f1 = record[\"metrics\"][\"val_macro_f1\"][-1]\n    final_val_loss = record[\"losses\"][\"val\"][-1]\n    print(f\"Validation macro F1 score: {final_val_f1:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\")\n\n    # Test metrics\n    print(\"Test dataset\")\n    test_f1 = record[\"test_macro_f1\"]\n    print(f\"Test macro F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- display ----------\nfor dropout_rate, record in experiment_data.get(\"dropout_rate\", {}).items():\n    print(f\"\\n=== Results for dropout rate: {dropout_rate} ===\")\n\n    # Training metrics (final epoch)\n    print(\"Training dataset\")\n    final_train_f1 = record[\"metrics\"][\"train_macro_f1\"][-1]\n    final_train_loss = record[\"losses\"][\"train\"][-1]\n    print(f\"Training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Training loss: {final_train_loss:.4f}\")\n\n    # Validation metrics (final epoch)\n    print(\"Validation dataset\")\n    final_val_f1 = record[\"metrics\"][\"val_macro_f1\"][-1]\n    final_val_loss = record[\"losses\"][\"val\"][-1]\n    print(f\"Validation macro F1 score: {final_val_f1:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\")\n\n    # Test metrics\n    print(\"Test dataset\")\n    test_f1 = record[\"test_macro_f1\"]\n    print(f\"Test macro F1 score: {test_f1:.4f}\")\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'Best training macro F1 score: 0.6831', '\\n',\n'Best validation macro F1 score: 0.6815', '\\n', 'Best training loss: 0.6196',\n'\\n', 'Best validation loss: 0.6396', '\\n', 'Test macro F1 score: 0.6629', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training macro F1 score: 0.6840', '\\n',\n'Best validation macro F1 score: 0.6815', '\\n', 'Lowest training loss: 0.6103',\n'\\n', 'Lowest validation loss: 0.6396', '\\n', 'Test macro F1 score: 0.6659',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['Dataset: SPR_BENCH_bs32', '\\n', 'Final train macro F1 score: 0.6748', '\\n',\n'Best validation macro F1 score: 0.6833', '\\n', 'Final train loss: 0.6180',\n'\\n', 'Best validation loss: 0.6375', '\\n', 'Test macro F1 score: 0.6849\\n',\n'\\n', 'Dataset: SPR_BENCH_bs64', '\\n', 'Final train macro F1 score: 0.6833',\n'\\n', 'Best validation macro F1 score: 0.6788', '\\n', 'Final train loss:\n0.6192', '\\n', 'Best validation loss: 0.6364', '\\n', 'Test macro F1 score:\n0.6862\\n', '\\n', 'Dataset: SPR_BENCH_bs256', '\\n', 'Final train macro F1 score:\n0.6776', '\\n', 'Best validation macro F1 score: 0.6809', '\\n', 'Final train\nloss: 0.6190', '\\n', 'Best validation loss: 0.6381', '\\n', 'Test macro F1 score:\n0.6842\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nEmbedding dimension = 32', '\\n', '  Train dataset', '\\n', '    Training\nmacro F1 score: 0.6820', '\\n', '    Training loss: 0.6175', '\\n', '  Validation\ndataset', '\\n', '    Validation macro F1 score: 0.6720', '\\n', '    Validation\nloss: 0.6436', '\\n', '\\nEmbedding dimension = 64', '\\n', '  Train dataset',\n'\\n', '    Training macro F1 score: 0.6794', '\\n', '    Training loss: 0.6151',\n'\\n', '  Validation dataset', '\\n', '    Validation macro F1 score: 0.6793',\n'\\n', '    Validation loss: 0.6398', '\\n', '\\nEmbedding dimension = 128', '\\n',\n'  Train dataset', '\\n', '    Training macro F1 score: 0.6825', '\\n', '\nTraining loss: 0.6172', '\\n', '  Validation dataset', '\\n', '    Validation\nmacro F1 score: 0.6680', '\\n', '    Validation loss: 0.6501', '\\n', '\\nEmbedding\ndimension = 256', '\\n', '  Train dataset', '\\n', '    Training macro F1 score:\n0.6804', '\\n', '    Training loss: 0.6153', '\\n', '  Validation dataset', '\\n',\n'    Validation macro F1 score: 0.6754', '\\n', '    Validation loss: 0.6451',\n'\\n', '  Test dataset', '\\n', '    Test macro F1 score: 0.6890', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best learning rate: 0.0005', '\\n', 'training macro F1\nscore: 0.6805', '\\n', 'validation macro F1 score: 0.6833', '\\n', 'training BCE\nloss: 0.6184', '\\n', 'validation BCE loss: 0.6391', '\\n', 'test macro F1 score:\n0.6878', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: hidden_size=64', '\\n', 'Final train macro F1 score: 0.6804', '\\n',\n'Final validation macro F1 score: 0.6834', '\\n', 'Final train loss: 0.6193',\n'\\n', 'Final validation loss: 0.6387', '\\n', '\\n', 'Dataset: hidden_size=128',\n'\\n', 'Final train macro F1 score: 0.6845', '\\n', 'Final validation macro F1\nscore: 0.6773', '\\n', 'Final train loss: 0.6168', '\\n', 'Final validation loss:\n0.6377', '\\n', '\\n', 'Dataset: hidden_size=256', '\\n', 'Final train macro F1\nscore: 0.6866', '\\n', 'Final validation macro F1 score: 0.6735', '\\n', 'Final\ntrain loss: 0.6172', '\\n', 'Final validation loss: 0.6472', '\\n', '\\n',\n'Dataset: hidden_size=512', '\\n', 'Final train macro F1 score: 0.6836', '\\n',\n'Final validation macro F1 score: 0.6849', '\\n', 'Final train loss: 0.6149',\n'\\n', 'Final validation loss: 0.6442', '\\n', '\\n', 'Dataset: best_model\n(hidden_size=512)', '\\n', 'Test macro F1 score: 0.6868', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\n=== Results for dropout rate: 0.0 ===', '\\n', 'Training dataset', '\\n',\n'Training macro F1 score: 0.6772', '\\n', 'Training loss: 0.6196', '\\n',\n'Validation dataset', '\\n', 'Validation macro F1 score: 0.6498', '\\n',\n'Validation loss: 0.6487', '\\n', 'Test dataset', '\\n', 'Test macro F1 score:\n0.6629', '\\n', '\\n=== Results for dropout rate: 0.1 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6788', '\\n', 'Training loss:\n0.6196', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6768',\n'\\n', 'Validation loss: 0.6417', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6824', '\\n', '\\n=== Results for dropout rate: 0.3 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6810', '\\n', 'Training loss:\n0.6213', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6775',\n'\\n', 'Validation loss: 0.6385', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6883', '\\n', '\\n=== Results for dropout rate: 0.5 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6801', '\\n', 'Training loss:\n0.6223', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6748',\n'\\n', 'Validation loss: 0.6408', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6864', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best weight_decay: 1e-05', '\\n', 'train macro F1\nscore: 0.6821', '\\n', 'validation macro F1 score: 0.6838', '\\n', 'test macro F1\nscore: 0.6787', '\\n', 'train loss: 0.619674', '\\n', 'validation loss: 0.644718',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== Results for dropout rate: 0.0 ===', '\\n', 'Training dataset', '\\n',\n'Training macro F1 score: 0.6837', '\\n', 'Training loss: 0.6155', '\\n',\n'Validation dataset', '\\n', 'Validation macro F1 score: 0.6797', '\\n',\n'Validation loss: 0.6420', '\\n', 'Test dataset', '\\n', 'Test macro F1 score:\n0.6874', '\\n', '\\n=== Results for dropout rate: 0.1 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6818', '\\n', 'Training loss:\n0.6179', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6783',\n'\\n', 'Validation loss: 0.6456', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6810', '\\n', '\\n=== Results for dropout rate: 0.3 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6802', '\\n', 'Training loss:\n0.6205', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6836',\n'\\n', 'Validation loss: 0.6400', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6842', '\\n', '\\n=== Results for dropout rate: 0.5 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6786', '\\n', 'Training loss:\n0.6210', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6680',\n'\\n', 'Validation loss: 0.6431', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6869', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\n=== Results for dropout rate: 0.0 ===', '\\n', 'Training dataset', '\\n',\n'Training macro F1 score: 0.6786', '\\n', 'Training loss: 0.6205', '\\n',\n'Validation dataset', '\\n', 'Validation macro F1 score: 0.6791', '\\n',\n'Validation loss: 0.6373', '\\n', 'Test dataset', '\\n', 'Test macro F1 score:\n0.6858', '\\n', '\\n=== Results for dropout rate: 0.1 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6843', '\\n', 'Training loss:\n0.6182', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6794',\n'\\n', 'Validation loss: 0.6395', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6882', '\\n', '\\n=== Results for dropout rate: 0.3 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6789', '\\n', 'Training loss:\n0.6185', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6787',\n'\\n', 'Validation loss: 0.6386', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6824', '\\n', '\\n=== Results for dropout rate: 0.5 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6814', '\\n', 'Training loss:\n0.6202', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6833',\n'\\n', 'Validation loss: 0.6420', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6899', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\n=== Results for dropout rate: 0.0 ===', '\\n', 'Training dataset', '\\n',\n'Training macro F1 score: 0.6852', '\\n', 'Training loss: 0.6157', '\\n',\n'Validation dataset', '\\n', 'Validation macro F1 score: 0.6697', '\\n',\n'Validation loss: 0.6424', '\\n', 'Test dataset', '\\n', 'Test macro F1 score:\n0.6845', '\\n', '\\n=== Results for dropout rate: 0.1 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6770', '\\n', 'Training loss:\n0.6165', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6680',\n'\\n', 'Validation loss: 0.6452', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6759', '\\n', '\\n=== Results for dropout rate: 0.3 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6849', '\\n', 'Training loss:\n0.6154', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6766',\n'\\n', 'Validation loss: 0.6463', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6830', '\\n', '\\n=== Results for dropout rate: 0.5 ===', '\\n', 'Training\ndataset', '\\n', 'Training macro F1 score: 0.6746', '\\n', 'Training loss:\n0.6229', '\\n', 'Validation dataset', '\\n', 'Validation macro F1 score: 0.6834',\n'\\n', 'Validation loss: 0.6393', '\\n', 'Test dataset', '\\n', 'Test macro F1\nscore: 0.6872', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
