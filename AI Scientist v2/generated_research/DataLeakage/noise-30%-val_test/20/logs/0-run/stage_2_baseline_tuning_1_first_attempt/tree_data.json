{"edges": [[0, 4], [0, 1], [0, 7], [0, 2], [0, 8], [0, 5], [0, 3], [0, 6], [3, 10], [3, 9], [3, 11], [3, 12]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.14285714285714285, 0.5], [0.2857142857142857, 0.5], [0.42857142857142855, 0.5], [0.5714285714285714, 0.5], [0.7142857142857143, 0.5], [0.8571428571428571, 0.5], [1.0, 0.5], [0.07142857142857142, 1.0], [0.21428571428571427, 1.0], [0.35714285714285715, 1.0], [0.5, 1.0]], "plan": ["We start with a straightforward character-level baseline: build a vocabulary\nfrom every symbol that appears in the SPR_BENCH training split, encode each\nsequence as an integer array, pad to a fixed length, and feed it to a compact\nGRU classifier (Embedding\u2192GRU\u2192Linear\u2192Softmax).  The model is trained with cross-\nentropy for a handful of epochs, using Adam and a modest learning-rate; after\nevery epoch we compute validation loss and the requested Macro-F1 on the dev\nsplit.  All tensors and the model are moved to GPU when available, respecting\nthe mandatory device-handling instructions.  Training/validation losses and\nMacro-F1 scores are stored in the experiment_data dictionary and finally saved\nas a NumPy file inside ./working.  If the benchmark folder is missing, the\nscript automatically fabricates a tiny synthetic dataset so it can still run\nend-to-end.  This gives us a reliable, fully reproducible baseline that is\nsimple yet expressive enough to capture many positional and frequency cues in\nthe symbol sequences.  Later work can extend this by adding attention, larger\nmodels, or rule-induction modules, but this establishes a solid reference point.", "Hyperparam tuning name: num_epochs. The solution trains several models with\ndifferent maximum\u2010epoch budgets (5, 10, 15, 20) while using early-stopping\n(patience = 3) on dev-set macro-F1.   For every budget we keep per-epoch\nlosses/metrics, final predictions, and ground truth, store them in a nested\nexperiment_data dictionary under the hyper-parameter key \u201cnum_epochs\u201d, and\nfinally save everything to experiment_data.npy.  No architectural changes are\nintroduced; we only allow the optimiser to run longer and stop when dev\nperformance plateaus.", "Hyperparam tuning name: learning_rate. We sweep four Adam learning-rates (3e-4,\n5e-4, 1e-3, 3e-3).   For each value we (re)initialise the model, train five\nepochs exactly as the baseline, evaluate after every epoch on the dev set, and\nstore losses, macro-F1 scores, final predictions and labels.   Results are\ncollected in the mandatory experiment_data structure under the top-level key\n\"learning_rate\", then saved to \u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: batch_size. We will sweep over four candidate batch\nsizes (32, 64, 128, 256).   For each value we rebuild the dataloaders, train the\nsame GRU for five epochs, record per-epoch train/validation loss and macro-F1,\nand keep the final-epoch predictions/labels.   All results are stored in a\nhierarchical experiment_data dictionary under the top-level key \"batch_size\",\nthen saved to experiment_data.npy for later plotting/analysis.", "Hyperparam tuning name: dropout_rate. We extend the original script by (1)\nadding a `nn.Dropout` layer after the GRU, (2) making the dropout rate\nconfigurable, and (3) looping over four candidate values (0 / 0.2 / 0.3 / 0.5).\nFor every rate we re-initialize the model, run the same 5-epoch routine, log\nlosses and macro-F1 for train/validation, keep the final predictions/labels, and\nstore everything in `experiment_data['dropout_rate']['SPR_BENCH'][<rate>]`.\nAfter all runs the whole dictionary is saved to `experiment_data.npy`, fully\nsatisfying the required structure.", "Hyperparam tuning name: hidden_size. We loop over a list of hidden sizes (64 /\n128 / 256 / 512), train an identical Char-GRU model for a few epochs on each\nsize, evaluate after every epoch, and store all losses, macro-F1 scores, final\npredictions and ground-truth labels in an experiment_data dictionary structured\nfor \u201chidden_size\u201d hyper-parameter tuning. Finally we save the whole dictionary\nto experiment_data.npy for later plotting or analysis. Everything (data loading,\nmodel, training, evaluation, saving) is contained in a single executable script.", "Hyperparam tuning name: embedding_dim. The solution loops over embedding\ndimensions [32, 128, 256], trains a fresh CharGRU for each value, records epoch-\nwise losses/F1, and stores the results in experiment_data under keys \u201cdim_32\u201d,\n\u201cdim_128\u201d, and \u201cdim_256\u201d. All metrics, losses, predictions, and ground-truth\nvectors are finally saved to experiment_data.npy.", "Hyperparam tuning name: weight_decay. The solution sweeps five weight-decay\nvalues (0, 1e-5, 1e-4, 1e-3, 1e-2).   For each value a fresh CharGRU model is\ntrained for 5 epochs and evaluated on the dev set; training/validation losses\nand validation macro-F1 are logged, together with the final-epoch predictions\nand labels.   Results are stored in a single dict whose top-level keys identify\nthe specific weight-decay run (e.g. \"weight_decay_0.001\").   After all runs the\ndict is saved as \u201cexperiment_data.npy\u201d, ready for later analysis and plotting.", "Hyperparam tuning name: num_gru_layers. We extend the baseline by letting\nCharGRU accept a variable num_layers, then train three separate models (1, 2 and\n3 GRU layers) with identical hyper-parameters otherwise. For each variant we log\nepoch-wise train/validation loss and macro-F1, plus final\u2010epoch\npredictions/labels, and store everything in a nested experiment_data dictionary\nunder the key 'num_gru_layers'. Results are saved to working/experiment_data.npy\nso they can be compared afterwards.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def to_hf(rows):\n        return DatasetDict(\n            {\"train\": load_dataset(\"json\", data_files={\"train\": [rows]}, split=\"train\")}\n        )[\"train\"]\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = max(len(ex[\"sequence\"]) for ex in spr[\"train\"])\nmax_len = min(max_len, 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------------------------------------------\n# Model\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        logits = self.fc(h.squeeze(0))\n        return logits\n\n\nmodel = CharGRU(vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# -------------------------------------------------\n# tracking dict\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# -------------------------------------------------\n# training loop\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    # train\n    model.train()\n    total_loss, total_items = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        total_items += batch[\"labels\"].size(0)\n    train_loss = total_loss / total_items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # validation\n    model.eval()\n    val_loss, val_items = 0.0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            val_items += batch[\"labels\"].size(0)\n            preds = logits.argmax(dim=1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            all_preds.extend(list(preds))\n            all_labels.extend(list(labels))\n    val_loss /= val_items\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(macro_f1)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, Macro-F1 = {macro_f1:.4f}\")\n\n# Store predictions and ground truth from final epoch\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# num_epochs hyper-parameter tuning \u2013 single-file script\nimport os, pathlib, random, string, numpy as np, torch, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------- misc ----------------------------\nseed = 13\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nprint(\"Running on\", device)\n\n\n# -------------------- load / synth dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real SPR_BENCH not found, creating synthetic data.\")\n\n    def synth(n):\n        rows = []\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:10], k=random.randint(5, 15))\n            )\n            rows.append(\n                {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n            )\n        return rows\n\n    def to_ds(rows):\n        return load_dataset(\"json\", data_files={\"train\": [rows]}, split=\"train\")\n\n    spr = DatasetDict()\n    spr[\"train\"] = to_ds(synth(2000))\n    spr[\"dev\"] = to_ds(synth(400))\n    spr[\"test\"] = to_ds(synth(400))\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------- vocab + encode ---------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        vocab.setdefault(ch, len(vocab))\nvocab_size = len(vocab)\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\ndef encode(seq):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hfds):\n        self.hfds = hfds\n\n    def __len__(self):\n        return len(self.hfds)\n\n    def __getitem__(self, idx):\n        ex = self.hfds[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(ex[\"sequence\"]), dtype=torch.long),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    return {\n        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n        \"labels\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\ntrain_ds, dev_ds = SPRTorchDataset(spr[\"train\"]), SPRTorchDataset(spr[\"dev\"])\ntrain_loader = lambda bs: DataLoader(\n    train_ds, batch_size=bs, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------------- model ---------------------------\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb=64, hid=128, cls=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=0)\n        self.gru = nn.GRU(emb, hid, batch_first=True)\n        self.fc = nn.Linear(hid, cls)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return self.fc(h.squeeze(0))\n\n\n# ------------------- training procedure -------------------\ndef train_for(max_epochs, patience=3):\n    model = CharGRU(vocab_size).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n\n    tr_losses, val_losses, tr_f1s, val_f1s = [], [], [], []\n    best_f1, best_pred, best_label = -1, None, None\n    no_up = 0\n    for ep in range(1, max_epochs + 1):\n        # --- train\n        model.train()\n        tot_loss = tot_items = 0\n        for batch in train_loader(128):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optim.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = crit(logits, batch[\"labels\"])\n            loss.backward()\n            optim.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n        tr_losses.append(tot_loss / tot_items)\n\n        # --- eval\n        model.eval()\n        v_loss = v_items = 0\n        preds, labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                log = model(batch[\"input_ids\"])\n                loss = crit(log, batch[\"labels\"])\n                v_loss += loss.item() * batch[\"labels\"].size(0)\n                v_items += batch[\"labels\"].size(0)\n                preds.extend(log.argmax(1).cpu().tolist())\n                labels.extend(batch[\"labels\"].cpu().tolist())\n        val_losses.append(v_loss / v_items)\n        macro = f1_score(labels, preds, average=\"macro\")\n        val_f1s.append(macro)\n\n        # collect train f1 quickly\n        tr_preds, tr_labels = [], []\n        for batch in train_loader(512):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.no_grad():\n                tr_logits = model(batch[\"input_ids\"])\n            tr_preds.extend(tr_logits.argmax(1).cpu().tolist())\n            tr_labels.extend(batch[\"labels\"].cpu().tolist())\n        tr_f1s.append(f1_score(tr_labels, tr_preds, average=\"macro\"))\n\n        print(\n            f\"[{max_epochs}-ep run] epoch {ep}: val_loss={val_losses[-1]:.4f}, val_F1={macro:.4f}\"\n        )\n\n        # early stop\n        if macro > best_f1:\n            best_f1, best_pred, best_label = macro, preds.copy(), labels.copy()\n            no_up = 0\n        else:\n            no_up += 1\n        if no_up >= patience:\n            print(\"Early stopping.\")\n            break\n    return {\n        \"metrics\": {\"train\": tr_f1s, \"val\": val_f1s},\n        \"losses\": {\"train\": tr_losses, \"val\": val_losses},\n        \"predictions\": best_pred,\n        \"ground_truth\": best_label,\n    }\n\n\n# ------------------- hyper-param search --------------------\nexperiment_data = {\"num_epochs\": {\"SPR_BENCH\": {}}}\nfor ep_budget in [5, 10, 15, 20]:\n    result = train_for(ep_budget, patience=3)\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][f\"epochs_{ep_budget}\"] = result\n\n# ----------------------- save ------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------\n# experiment data container\nexperiment_data = {\n    \"learning_rate\": {  # tuning type\n        \"SPR_BENCH\": {}  # dataset name \u2013 per-lr sub-dicts will be inserted\n    }\n}\n\n# ---------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------\n# helper to load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# ---------------------------------------------\n# load dataset (fall back to synthetic)\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        spr[split] = load_dataset(\n            \"json\", data_files={split: synth_split(n)}, split=\"train\"\n        )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# ---------------------------------------------\n# Torch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# ---------------------------------------------\nlrs_to_try = [3e-4, 5e-4, 1e-3, 3e-3]\nepochs = 5\n\nfor lr in lrs_to_try:\n    print(f\"\\n=== Training with learning rate {lr} ===\")\n    # set random seeds for reproducibility\n    torch.manual_seed(0)\n    np.random.seed(0)\n    random.seed(0)\n\n    # (re)initialise model & optimiser\n    model = CharGRU(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # create entry in experiment_data\n    run_store = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][str(lr)] = run_store\n\n    for epoch in range(1, epochs + 1):\n        # -------- training ---------\n        model.train()\n        total_loss, total_items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            n = batch[\"labels\"].size(0)\n            total_loss += loss.item() * n\n            total_items += n\n        train_loss = total_loss / total_items\n        run_store[\"losses\"][\"train\"].append(train_loss)\n\n        # -------- validation --------\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                n = batch[\"labels\"].size(0)\n                val_loss += loss.item() * n\n                val_items += n\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds.tolist())\n                all_labels.extend(labels.tolist())\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        run_store[\"losses\"][\"val\"].append(val_loss)\n        run_store[\"metrics\"][\"val\"].append(macro_f1)\n        print(\n            f\"LR {lr} | Epoch {epoch}: val_loss={val_loss:.4f}, Macro-F1={macro_f1:.4f}\"\n        )\n\n    # store final epoch predictions\n    run_store[\"predictions\"] = all_preds\n    run_store[\"ground_truth\"] = all_labels\n\n# ---------------------------------------------\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\n# -------------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\n# hyperparameter sweep over batch sizes\nbatch_sizes = [32, 64, 128, 256]\nepochs = 5\nexperiment_data = {\"batch_size\": {}}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size = {bs} ===\")\n    # dataloaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate)\n    dev_loader = DataLoader(dev_ds, batch_size=512, shuffle=False, collate_fn=collate)\n    # model, loss, optim\n    model = CharGRU(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # logs\n    logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tot_loss, tot_items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n        train_loss = tot_loss / tot_items\n        # validation\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds.tolist())\n                all_labels.extend(labels.tolist())\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        # record\n        logs[\"losses\"][\"train\"].append(train_loss)\n        logs[\"losses\"][\"val\"].append(val_loss)\n        logs[\"metrics\"][\"train\"].append(None)  # not computing train F1 to save time\n        logs[\"metrics\"][\"val\"].append(macro_f1)\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | Macro-F1={macro_f1:.4f}\"\n        )\n    # store final epoch preds/labels\n    logs[\"predictions\"] = all_preds\n    logs[\"ground_truth\"] = all_labels\n    # save under experiment_data\n    if \"SPR_BENCH\" not in experiment_data[\"batch_size\"]:\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"] = {}\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][f\"bs_{bs}\"] = logs\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------------------------------------------\n# Model with tunable dropout\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2, dropout=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        h = self.drop(h.squeeze(0))\n        return self.fc(h)\n\n\n# -------------------------------------------------\n# experiment data container\nexperiment_data = {\"dropout_rate\": {\"SPR_BENCH\": {}}}\n\ndropout_values = [0.0, 0.2, 0.3, 0.5]\nepochs = 5\n\nfor d_rate in dropout_values:\n    key = str(d_rate)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    model = CharGRU(vocab_size, dropout=d_rate).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, epochs + 1):\n        # -------- training --------\n        model.train()\n        tot_loss, tot_items = 0.0, 0\n        train_preds, train_labels = [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n            train_preds.extend(logits.argmax(1).detach().cpu().numpy())\n            train_labels.extend(batch[\"labels\"].cpu().numpy())\n        train_loss = tot_loss / tot_items\n        train_f1 = f1_score(train_labels, train_preds, average=\"macro\")\n\n        # -------- validation --------\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                all_preds.extend(logits.argmax(1).cpu().numpy())\n                all_labels.extend(batch[\"labels\"].cpu().numpy())\n        val_loss /= val_items\n        val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n\n        # record\n        rec = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][key]\n        rec[\"losses\"][\"train\"].append(train_loss)\n        rec[\"losses\"][\"val\"].append(val_loss)\n        rec[\"metrics\"][\"train\"].append(train_f1)\n        rec[\"metrics\"][\"val\"].append(val_f1)\n\n        print(\n            f\"[dropout={d_rate}] Epoch {epoch}: train_loss {train_loss:.4f} val_loss {val_loss:.4f} val_F1 {val_f1:.4f}\"\n        )\n\n    # store final predictions\n    rec[\"predictions\"] = all_preds\n    rec[\"ground_truth\"] = all_labels\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, numpy as np, torch, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# experiment dict\nexperiment_data = {\n    \"hidden_size\": {\"SPR_BENCH\": {}}  # hyperparam_tuning_type_1  # dataset_name_1\n}\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    return {\n        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n        \"labels\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\nhidden_sizes = [64, 128, 256, 512]\nepochs = 5\n\nfor hs in hidden_sizes:\n    print(f\"\\n=== Training with hidden_size = {hs} ===\")\n    # create entry in experiment_data\n    experiment_data[\"hidden_size\"][\"SPR_BENCH\"][hs] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    model = CharGRU(vocab_size, hidden=hs).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, epochs + 1):\n        # ------------------ train ------------------\n        model.train()\n        total_loss, total_items = 0.0, 0\n        all_preds_train, all_labels_train = [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            total_items += batch[\"labels\"].size(0)\n            all_preds_train.extend(logits.argmax(1).cpu().numpy())\n            all_labels_train.extend(batch[\"labels\"].cpu().numpy())\n        train_loss = total_loss / total_items\n        train_f1 = f1_score(all_labels_train, all_preds_train, average=\"macro\")\n\n        # ------------------ validate ------------------\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds_val, all_labels_val = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                all_preds_val.extend(logits.argmax(1).cpu().numpy())\n                all_labels_val.extend(batch[\"labels\"].cpu().numpy())\n        val_loss /= val_items\n        val_f1 = f1_score(all_labels_val, all_preds_val, average=\"macro\")\n\n        # store\n        exp_entry = experiment_data[\"hidden_size\"][\"SPR_BENCH\"][hs]\n        exp_entry[\"losses\"][\"train\"].append(train_loss)\n        exp_entry[\"losses\"][\"val\"].append(val_loss)\n        exp_entry[\"metrics\"][\"train\"].append(train_f1)\n        exp_entry[\"metrics\"][\"val\"].append(val_f1)\n\n        print(\n            f\"Epoch {epoch}/{epochs} | TrainLoss {train_loss:.4f} F1 {train_f1:.3f} \"\n            f\"| ValLoss {val_loss:.4f} F1 {val_f1:.3f}\"\n        )\n\n    # store final predictions / ground_truth\n    exp_entry[\"predictions\"] = all_preds_val\n    exp_entry[\"ground_truth\"] = all_labels_val\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------------------------------------------\n# Model\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\n# experiment tracking dict\nexperiment_data = {\"embedding_dim\": {}}\n\n# -------------------------------------------------\n# hyperparameter tuning loop\nfor emb_dim in [32, 128, 256]:\n    print(f\"\\n===== Running experiment with emb_dim={emb_dim} =====\")\n    model = CharGRU(vocab_size, emb_dim=emb_dim).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    run_key = f\"dim_{emb_dim}\"\n    experiment_data[\"embedding_dim\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    epochs = 5\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        total_loss, total_items, all_preds, all_labels = 0.0, 0, [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            total_items += batch[\"labels\"].size(0)\n            preds = logits.argmax(1).detach().cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n        train_loss = total_loss / total_items\n        train_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        exp_run = experiment_data[\"embedding_dim\"][run_key]\n        exp_run[\"losses\"][\"train\"].append(train_loss)\n        exp_run[\"metrics\"][\"train\"].append(train_f1)\n\n        # ---- validation ----\n        model.eval()\n        val_loss, val_items, vpreds, vlabels = 0.0, 0, [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                vpreds.extend(logits.argmax(1).cpu().numpy())\n                vlabels.extend(batch[\"labels\"].cpu().numpy())\n        val_loss /= val_items\n        val_f1 = f1_score(vlabels, vpreds, average=\"macro\")\n        exp_run[\"losses\"][\"val\"].append(val_loss)\n        exp_run[\"metrics\"][\"val\"].append(val_f1)\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_F1={train_f1:.4f} | \"\n            f\"val_loss={val_loss:.4f}, val_F1={val_f1:.4f}\"\n        )\n\n    # store final epoch predictions / ground truth\n    exp_run[\"predictions\"] = vpreds\n    exp_run[\"ground_truth\"] = vlabels\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\nweight_decay_values = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\nexperiment_data = {}\nepochs = 5\ncriterion = nn.CrossEntropyLoss()\n\nfor wd in weight_decay_values:\n    key = f\"weight_decay_{wd}\"\n    experiment_data[key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    model = CharGRU(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    print(f\"\\n--- Training with weight_decay={wd} ---\")\n    for epoch in range(1, epochs + 1):\n        # training\n        model.train()\n        total_loss, items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            items += batch[\"labels\"].size(0)\n        train_loss = total_loss / items\n        experiment_data[key][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        # validation\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds)\n                all_labels.extend(labels)\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        experiment_data[key][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[key][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(macro_f1)\n        print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, Macro-F1={macro_f1:.4f}\")\n    # store final epoch preds/labels\n    experiment_data[key][\"SPR_BENCH\"][\"predictions\"] = all_preds\n    experiment_data[key][\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n    # free memory\n    del model, optimizer\n    torch.cuda.empty_cache()\n\n# -------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# folders / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------------------------------------------------------\n# reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# ------------------------------------------------------------------\n# load or synthesize SPR_BENCH\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    for sp in (\"train\", \"dev\", \"test\"):\n        dd[sp] = _load(f\"{sp}.csv\")\n    return dd\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:10], k=random.randint(5, 15))\n            )\n            label = int(seq.count(\"A\") % 2 == 0)\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    def hf_from(rows):\n        return load_dataset(\"json\", data_files={\"data\": [rows]}, split=\"train\")\n\n    spr = DatasetDict()\n    spr[\"train\"] = hf_from(synth_split(2000))\n    spr[\"dev\"] = hf_from(synth_split(400))\n    spr[\"test\"] = hf_from(synth_split(400))\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------------\n# vocab + encoding helpers\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    ids.extend([0] * (max_len - len(ids)))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# ------------------------------------------------------------------\n# pytorch datasets\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    return {\n        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n        \"labels\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------------------------------------------------------\n# model that supports variable layer depth\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2, num_layers=1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        logits = self.fc(h[-1])\n        return logits\n\n\n# ------------------------------------------------------------------\n# training routine\ndef train_model(num_layers, epochs=5, lr=1e-3):\n    model = CharGRU(vocab_size, num_layers=num_layers).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    metrics = {\"train\": [], \"val\": []}\n    losses = {\"train\": [], \"val\": []}\n    final_preds, final_labels = None, None\n    for ep in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        tot_loss, items = 0, 0\n        tr_preds, tr_labels = [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            items += batch[\"labels\"].size(0)\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_labels.extend(batch[\"labels\"].cpu().numpy())\n        train_loss = tot_loss / items\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n        losses[\"train\"].append(train_loss)\n        metrics[\"train\"].append(train_f1)\n        # ---- validation ----\n        model.eval()\n        tot_loss, items = 0, 0\n        v_preds, v_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                tot_loss += loss.item() * batch[\"labels\"].size(0)\n                items += batch[\"labels\"].size(0)\n                v_preds.extend(logits.argmax(1).cpu().numpy())\n                v_labels.extend(batch[\"labels\"].cpu().numpy())\n        val_loss = tot_loss / items\n        val_f1 = f1_score(v_labels, v_preds, average=\"macro\")\n        losses[\"val\"].append(val_loss)\n        metrics[\"val\"].append(val_f1)\n        print(\n            f\"[layers={num_layers}] Ep {ep}: train F1 {train_f1:.3f} | val F1 {val_f1:.3f}\"\n        )\n        final_preds, final_labels = v_preds, v_labels\n    return {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"predictions\": final_preds,\n        \"ground_truth\": final_labels,\n    }\n\n\n# ------------------------------------------------------------------\n# hyper-parameter sweep: 1,2,3 GRU layers\nexperiment_data = {\"num_gru_layers\": {\"SPR_BENCH\": {}}}\nfor nl in (1, 2, 3):\n    set_seed(42 + nl)  # different but reproducible\n    result = train_model(num_layers=nl, epochs=5)\n    experiment_data[\"num_gru_layers\"][\"SPR_BENCH\"][f\"layers_{nl}\"] = result\n\n# ------------------------------------------------------------------\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All results saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\n# -------------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\n# hyperparameter sweep over batch sizes\nbatch_sizes = [32, 64, 128, 256]\nepochs = 5\nexperiment_data = {\"batch_size\": {}}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size = {bs} ===\")\n    # dataloaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate)\n    dev_loader = DataLoader(dev_ds, batch_size=512, shuffle=False, collate_fn=collate)\n    # model, loss, optim\n    model = CharGRU(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # logs\n    logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tot_loss, tot_items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n        train_loss = tot_loss / tot_items\n        # validation\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds.tolist())\n                all_labels.extend(labels.tolist())\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        # record\n        logs[\"losses\"][\"train\"].append(train_loss)\n        logs[\"losses\"][\"val\"].append(val_loss)\n        logs[\"metrics\"][\"train\"].append(None)  # not computing train F1 to save time\n        logs[\"metrics\"][\"val\"].append(macro_f1)\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | Macro-F1={macro_f1:.4f}\"\n        )\n    # store final epoch preds/labels\n    logs[\"predictions\"] = all_preds\n    logs[\"ground_truth\"] = all_labels\n    # save under experiment_data\n    if \"SPR_BENCH\" not in experiment_data[\"batch_size\"]:\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"] = {}\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][f\"bs_{bs}\"] = logs\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\n# -------------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\n# hyperparameter sweep over batch sizes\nbatch_sizes = [32, 64, 128, 256]\nepochs = 5\nexperiment_data = {\"batch_size\": {}}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size = {bs} ===\")\n    # dataloaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate)\n    dev_loader = DataLoader(dev_ds, batch_size=512, shuffle=False, collate_fn=collate)\n    # model, loss, optim\n    model = CharGRU(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # logs\n    logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tot_loss, tot_items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n        train_loss = tot_loss / tot_items\n        # validation\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds.tolist())\n                all_labels.extend(labels.tolist())\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        # record\n        logs[\"losses\"][\"train\"].append(train_loss)\n        logs[\"losses\"][\"val\"].append(val_loss)\n        logs[\"metrics\"][\"train\"].append(None)  # not computing train F1 to save time\n        logs[\"metrics\"][\"val\"].append(macro_f1)\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | Macro-F1={macro_f1:.4f}\"\n        )\n    # store final epoch preds/labels\n    logs[\"predictions\"] = all_preds\n    logs[\"ground_truth\"] = all_labels\n    # save under experiment_data\n    if \"SPR_BENCH\" not in experiment_data[\"batch_size\"]:\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"] = {}\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][f\"bs_{bs}\"] = logs\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# helper to load SPR benchmark (copied from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------------------------------------\n# attempt to load dataset, otherwise create synthetic\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_data = DATA_PATH.exists()\nif have_data:\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found, generating synthetic dataset.\")\n\n    def synth_split(n):\n        rows = []\n        for i in range(n):\n            seq_len = random.randint(5, 15)\n            seq = \"\".join(random.choices(string.ascii_uppercase[:10], k=seq_len))\n            label = int(seq.count(\"A\") % 2 == 0)  # simple parity rule\n            rows.append({\"id\": i, \"sequence\": seq, \"label\": label})\n        return rows\n\n    spr = DatasetDict()\n    spr[\"train\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(2000)}, split=\"train\"\n    )\n    spr[\"dev\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\n    spr[\"test\"] = load_dataset(\n        \"json\", data_files={\"train\": synth_split(400)}, split=\"train\"\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------\n# build vocabulary\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq, max_len):\n    ids = [vocab.get(ch, 1) for ch in seq][:max_len]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    return ids\n\n\nmax_len = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 120)\n\n\n# -------------------------------------------------\n# PyTorch Dataset wrapper\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        ex = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(\n                encode(ex[\"sequence\"], max_len), dtype=torch.long\n            ),\n            \"label\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\n# -------------------------------------------------\n# Model definition\nclass CharGRU(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hidden=128, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return self.fc(h.squeeze(0))\n\n\n# -------------------------------------------------\n# hyperparameter sweep over batch sizes\nbatch_sizes = [32, 64, 128, 256]\nepochs = 5\nexperiment_data = {\"batch_size\": {}}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size = {bs} ===\")\n    # dataloaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate)\n    dev_loader = DataLoader(dev_ds, batch_size=512, shuffle=False, collate_fn=collate)\n    # model, loss, optim\n    model = CharGRU(vocab_size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # logs\n    logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tot_loss, tot_items = 0.0, 0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            tot_items += batch[\"labels\"].size(0)\n        train_loss = tot_loss / tot_items\n        # validation\n        model.eval()\n        val_loss, val_items = 0.0, 0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                val_items += batch[\"labels\"].size(0)\n                preds = logits.argmax(dim=1).cpu().numpy()\n                labels = batch[\"labels\"].cpu().numpy()\n                all_preds.extend(preds.tolist())\n                all_labels.extend(labels.tolist())\n        val_loss /= val_items\n        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        # record\n        logs[\"losses\"][\"train\"].append(train_loss)\n        logs[\"losses\"][\"val\"].append(val_loss)\n        logs[\"metrics\"][\"train\"].append(None)  # not computing train F1 to save time\n        logs[\"metrics\"][\"val\"].append(macro_f1)\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | Macro-F1={macro_f1:.4f}\"\n        )\n    # store final epoch preds/labels\n    logs[\"predictions\"] = all_preds\n    logs[\"ground_truth\"] = all_labels\n    # save under experiment_data\n    if \"SPR_BENCH\" not in experiment_data[\"batch_size\"]:\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"] = {}\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][f\"bs_{bs}\"] = logs\n\n# -------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 134607.55\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 82286.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 98508.71\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '11', '\\n', 'Epoch 1: validation_loss = 0.6941, Macro-F1 = 0.5721',\n'\\n', 'Epoch 2: validation_loss = 0.6855, Macro-F1 = 0.5653', '\\n', 'Epoch 3:\nvalidation_loss = 0.6338, Macro-F1 = 0.6737', '\\n', 'Epoch 4: validation_loss =\n0.6342, Macro-F1 = 0.6759', '\\n', 'Epoch 5: validation_loss = 0.6336, Macro-F1 =\n0.6855', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-1/working/experiment_data.npy', '\\n', 'Execution time: 4\nseconds seconds (time limit is 30 minutes).']", "['Running on', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples [00:00,\n? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 129718.07\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 91259.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 140809.88\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', '[5-ep\nrun] epoch 1: val_loss=0.7022, val_F1=0.4471', '\\n', '[5-ep run] epoch 2:\nval_loss=0.6913, val_F1=0.5760', '\\n', '[5-ep run] epoch 3: val_loss=0.6505,\nval_F1=0.6613', '\\n', '[5-ep run] epoch 4: val_loss=0.6338, val_F1=0.6812',\n'\\n', '[5-ep run] epoch 5: val_loss=0.6591, val_F1=0.6186', '\\n', '[10-ep run]\nepoch 1: val_loss=0.6942, val_F1=0.5726', '\\n', '[10-ep run] epoch 2:\nval_loss=0.6898, val_F1=0.5912', '\\n', '[10-ep run] epoch 3: val_loss=0.6621,\nval_F1=0.6418', '\\n', '[10-ep run] epoch 4: val_loss=0.7274, val_F1=0.4979',\n'\\n', '[10-ep run] epoch 5: val_loss=0.6438, val_F1=0.6573', '\\n', '[10-ep run]\nepoch 6: val_loss=0.6312, val_F1=0.6679', '\\n', '[10-ep run] epoch 7:\nval_loss=0.6339, val_F1=0.6700', '\\n', '[10-ep run] epoch 8: val_loss=0.6417,\nval_F1=0.6558', '\\n', '[10-ep run] epoch 9: val_loss=0.6358, val_F1=0.6819',\n'\\n', '[10-ep run] epoch 10: val_loss=0.6337, val_F1=0.6640', '\\n', '[15-ep run]\nepoch 1: val_loss=0.6967, val_F1=0.4682', '\\n', '[15-ep run] epoch 2:\nval_loss=0.6940, val_F1=0.5620', '\\n', '[15-ep run] epoch 3: val_loss=0.6795,\nval_F1=0.6349', '\\n', '[15-ep run] epoch 4: val_loss=0.6385, val_F1=0.6750',\n'\\n', '[15-ep run] epoch 5: val_loss=0.7040, val_F1=0.4708', '\\n', '[15-ep run]\nepoch 6: val_loss=0.7248, val_F1=0.4986', '\\n', '[15-ep run] epoch 7:\nval_loss=0.6774, val_F1=0.6045', '\\n', 'Early stopping.', '\\n', '[20-ep run]\nepoch 1: val_loss=0.6907, val_F1=0.5082', '\\n', '[20-ep run] epoch 2:\nval_loss=0.6856, val_F1=0.6116', '\\n', '[20-ep run] epoch 3: val_loss=0.6525,\nval_F1=0.6354', '\\n', '[20-ep run] epoch 4: val_loss=0.6424, val_F1=0.6663',\n'\\n', '[20-ep run] epoch 5: val_loss=0.6364, val_F1=0.6599', '\\n', '[20-ep run]\nepoch 6: val_loss=0.6734, val_F1=0.6310', '\\n', '[20-ep run] epoch 7:\nval_loss=0.6602, val_F1=0.6491', '\\n', 'Early stopping.', '\\n', 'Saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 123609.10\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 94786.53\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 79378.94\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '11', '\\n', '\\n=== Training with learning rate 0.0003 ===', '\\n',\n'LR 0.0003 | Epoch 1: val_loss=0.6910, Macro-F1=0.5376', '\\n', 'LR 0.0003 |\nEpoch 2: val_loss=0.6890, Macro-F1=0.5383', '\\n', 'LR 0.0003 | Epoch 3:\nval_loss=0.6906, Macro-F1=0.5698', '\\n', 'LR 0.0003 | Epoch 4: val_loss=0.6874,\nMacro-F1=0.5590', '\\n', 'LR 0.0003 | Epoch 5: val_loss=0.6876, Macro-F1=0.5722',\n'\\n', '\\n=== Training with learning rate 0.0005 ===', '\\n', 'LR 0.0005 | Epoch\n1: val_loss=0.6903, Macro-F1=0.5125', '\\n', 'LR 0.0005 | Epoch 2:\nval_loss=0.6897, Macro-F1=0.5649', '\\n', 'LR 0.0005 | Epoch 3: val_loss=0.6919,\nMacro-F1=0.5760', '\\n', 'LR 0.0005 | Epoch 4: val_loss=0.6808, Macro-F1=0.6043',\n'\\n', 'LR 0.0005 | Epoch 5: val_loss=0.6727, Macro-F1=0.6446', '\\n', '\\n===\nTraining with learning rate 0.001 ===', '\\n', 'LR 0.001 | Epoch 1:\nval_loss=0.6915, Macro-F1=0.5226', '\\n', 'LR 0.001 | Epoch 2: val_loss=0.6912,\nMacro-F1=0.5559', '\\n', 'LR 0.001 | Epoch 3: val_loss=0.6707, Macro-F1=0.6092',\n'\\n', 'LR 0.001 | Epoch 4: val_loss=0.6492, Macro-F1=0.6563', '\\n', 'LR 0.001 |\nEpoch 5: val_loss=0.6480, Macro-F1=0.6659', '\\n', '\\n=== Training with learning\nrate 0.003 ===', '\\n', 'LR 0.003 | Epoch 1: val_loss=0.6971, Macro-F1=0.4771',\n'\\n', 'LR 0.003 | Epoch 2: val_loss=0.6856, Macro-F1=0.5619', '\\n', 'LR 0.003 |\nEpoch 3: val_loss=0.7118, Macro-F1=0.3351', '\\n', 'LR 0.003 | Epoch 4:\nval_loss=0.6659, Macro-F1=0.6320', '\\n', 'LR 0.003 | Epoch 5: val_loss=0.6429,\nMacro-F1=0.6668', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Sci\nentist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 96517.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 90982.73\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 100054.96\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '11', '\\n', '\\n=== Training with batch_size = 32 ===', '\\n', 'Epoch\n1/5 | train_loss=0.6826 | val_loss=0.6755 | Macro-F1=0.5956', '\\n', 'Epoch 2/5 |\ntrain_loss=0.6699 | val_loss=0.6886 | Macro-F1=0.6047', '\\n', 'Epoch 3/5 |\ntrain_loss=0.6648 | val_loss=0.6528 | Macro-F1=0.6454', '\\n', 'Epoch 4/5 |\ntrain_loss=0.6418 | val_loss=0.6389 | Macro-F1=0.6820', '\\n', 'Epoch 5/5 |\ntrain_loss=0.6295 | val_loss=0.6436 | Macro-F1=0.6840', '\\n', '\\n=== Training\nwith batch_size = 64 ===', '\\n', 'Epoch 1/5 | train_loss=0.6881 |\nval_loss=0.7011 | Macro-F1=0.5601', '\\n', 'Epoch 2/5 | train_loss=0.6736 |\nval_loss=0.6640 | Macro-F1=0.6480', '\\n', 'Epoch 3/5 | train_loss=0.6827 |\nval_loss=0.7561 | Macro-F1=0.3316', '\\n', 'Epoch 4/5 | train_loss=0.6928 |\nval_loss=0.6888 | Macro-F1=0.4890', '\\n', 'Epoch 5/5 | train_loss=0.6773 |\nval_loss=0.6889 | Macro-F1=0.5567', '\\n', '\\n=== Training with batch_size = 128\n===', '\\n', 'Epoch 1/5 | train_loss=0.6917 | val_loss=0.6933 | Macro-F1=0.5319',\n'\\n', 'Epoch 2/5 | train_loss=0.6772 | val_loss=0.6981 | Macro-F1=0.5792', '\\n',\n'Epoch 3/5 | train_loss=0.6684 | val_loss=0.6770 | Macro-F1=0.6299', '\\n',\n'Epoch 4/5 | train_loss=0.6465 | val_loss=0.6398 | Macro-F1=0.6839', '\\n',\n'Epoch 5/5 | train_loss=0.6535 | val_loss=0.6497 | Macro-F1=0.6596', '\\n',\n'\\n=== Training with batch_size = 256 ===', '\\n', 'Epoch 1/5 | train_loss=0.6921\n| val_loss=0.6898 | Macro-F1=0.5580', '\\n', 'Epoch 2/5 | train_loss=0.6791 |\nval_loss=0.6903 | Macro-F1=0.5668', '\\n', 'Epoch 3/5 | train_loss=0.6696 |\nval_loss=0.6873 | Macro-F1=0.5737', '\\n', 'Epoch 4/5 | train_loss=0.6599 |\nval_loss=0.6668 | Macro-F1=0.6400', '\\n', 'Epoch 5/5 | train_loss=0.6384 |\nval_loss=0.6420 | Macro-F1=0.6790', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 141689.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 146041.23\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 251487.23\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '11', '\\n', '[dropout=0.0] Epoch 1: train_loss 0.6844 val_loss\n0.6941 val_F1 0.5721', '\\n', '[dropout=0.0] Epoch 2: train_loss 0.6713 val_loss\n0.6855 val_F1 0.5653', '\\n', '[dropout=0.0] Epoch 3: train_loss 0.6518 val_loss\n0.6338 val_F1 0.6737', '\\n', '[dropout=0.0] Epoch 4: train_loss 0.6267 val_loss\n0.6342 val_F1 0.6759', '\\n', '[dropout=0.0] Epoch 5: train_loss 0.6217 val_loss\n0.6336 val_F1 0.6855', '\\n', '[dropout=0.2] Epoch 1: train_loss 0.6876 val_loss\n0.6979 val_F1 0.5490', '\\n', '[dropout=0.2] Epoch 2: train_loss 0.6757 val_loss\n0.6875 val_F1 0.5864', '\\n', '[dropout=0.2] Epoch 3: train_loss 0.6631 val_loss\n0.6628 val_F1 0.6544', '\\n', '[dropout=0.2] Epoch 4: train_loss 0.6507 val_loss\n0.6495 val_F1 0.6364', '\\n', '[dropout=0.2] Epoch 5: train_loss 0.6243 val_loss\n0.6415 val_F1 0.6645', '\\n', '[dropout=0.3] Epoch 1: train_loss 0.6925 val_loss\n0.6920 val_F1 0.5434', '\\n', '[dropout=0.3] Epoch 2: train_loss 0.6804 val_loss\n0.6979 val_F1 0.5518', '\\n', '[dropout=0.3] Epoch 3: train_loss 0.6698 val_loss\n0.6817 val_F1 0.6239', '\\n', '[dropout=0.3] Epoch 4: train_loss 0.6529 val_loss\n0.6348 val_F1 0.6920', '\\n', '[dropout=0.3] Epoch 5: train_loss 0.6455 val_loss\n0.6656 val_F1 0.6031', '\\n', '[dropout=0.5] Epoch 1: train_loss 0.6944 val_loss\n0.6880 val_F1 0.5445', '\\n', '[dropout=0.5] Epoch 2: train_loss 0.6802 val_loss\n0.6929 val_F1 0.5706', '\\n', '[dropout=0.5] Epoch 3: train_loss 0.6628 val_loss\n0.6591 val_F1 0.6416', '\\n', '[dropout=0.5] Epoch 4: train_loss 0.6790 val_loss\n0.7042 val_F1 0.3351', '\\n', '[dropout=0.5] Epoch 5: train_loss 0.7005 val_loss\n0.6949 val_F1 0.4457', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab size:', ' ', '11', '\\n', '\\n=== Training with hidden_size =\n64 ===', '\\n', 'Epoch 1/5 | TrainLoss 0.6896 F1 0.522 | ValLoss 0.6958 F1\n0.465', '\\n', 'Epoch 2/5 | TrainLoss 0.6772 F1 0.566 | ValLoss 0.7004 F1 0.496',\n'\\n', 'Epoch 3/5 | TrainLoss 0.6732 F1 0.577 | ValLoss 0.6885 F1 0.606', '\\n',\n'Epoch 4/5 | TrainLoss 0.6616 F1 0.601 | ValLoss 0.6738 F1 0.632', '\\n', 'Epoch\n5/5 | TrainLoss 0.6386 F1 0.653 | ValLoss 0.6407 F1 0.671', '\\n', '\\n===\nTraining with hidden_size = 128 ===', '\\n', 'Epoch 1/5 | TrainLoss 0.6972 F1\n0.495 | ValLoss 0.6915 F1 0.539', '\\n', 'Epoch 2/5 | TrainLoss 0.6772 F1 0.562 |\nValLoss 0.6886 F1 0.570', '\\n', 'Epoch 3/5 | TrainLoss 0.6615 F1 0.624 | ValLoss\n0.6688 F1 0.622', '\\n', 'Epoch 4/5 | TrainLoss 0.6557 F1 0.632 | ValLoss 0.6650\nF1 0.619', '\\n', 'Epoch 5/5 | TrainLoss 0.6405 F1 0.650 | ValLoss 0.6430 F1\n0.657', '\\n', '\\n=== Training with hidden_size = 256 ===', '\\n', 'Epoch 1/5 |\nTrainLoss 0.6895 F1 0.540 | ValLoss 0.6925 F1 0.547', '\\n', 'Epoch 2/5 |\nTrainLoss 0.6596 F1 0.621 | ValLoss 0.6594 F1 0.632', '\\n', 'Epoch 3/5 |\nTrainLoss 0.6563 F1 0.614 | ValLoss 0.6616 F1 0.607', '\\n', 'Epoch 4/5 |\nTrainLoss 0.6367 F1 0.666 | ValLoss 0.6580 F1 0.658', '\\n', 'Epoch 5/5 |\nTrainLoss 0.6391 F1 0.654 | ValLoss 0.6463 F1 0.650', '\\n', '\\n=== Training with\nhidden_size = 512 ===', '\\n', 'Epoch 1/5 | TrainLoss 0.6931 F1 0.543 | ValLoss\n0.6917 F1 0.577', '\\n', 'Epoch 2/5 | TrainLoss 0.6744 F1 0.599 | ValLoss 0.6918\nF1 0.497', '\\n', 'Epoch 3/5 | TrainLoss 0.6855 F1 0.564 | ValLoss 0.6799 F1\n0.574', '\\n', 'Epoch 4/5 | TrainLoss 0.6625 F1 0.613 | ValLoss 0.6712 F1 0.629',\n'\\n', 'Epoch 5/5 | TrainLoss 0.6446 F1 0.648 | ValLoss 0.7629 F1 0.387', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '11', '\\n', '\\n===== Running experiment with emb_dim=32\n=====', '\\n', 'Epoch 1: train_loss=0.6916, train_F1=0.5296 | val_loss=0.6914,\nval_F1=0.4727', '\\n', 'Epoch 2: train_loss=0.6797, train_F1=0.5506 |\nval_loss=0.6909, val_F1=0.5619', '\\n', 'Epoch 3: train_loss=0.6784,\ntrain_F1=0.6178 | val_loss=0.6478, val_F1=0.6734', '\\n', 'Epoch 4:\ntrain_loss=0.6642, train_F1=0.5660 | val_loss=0.6803, val_F1=0.5743', '\\n',\n'Epoch 5: train_loss=0.6718, train_F1=0.5949 | val_loss=0.6779, val_F1=0.5802',\n'\\n', '\\n===== Running experiment with emb_dim=128 =====', '\\n', 'Epoch 1:\ntrain_loss=0.6979, train_F1=0.5440 | val_loss=0.7020, val_F1=0.4677', '\\n',\n'Epoch 2: train_loss=0.6845, train_F1=0.5412 | val_loss=0.6946, val_F1=0.5338',\n'\\n', 'Epoch 3: train_loss=0.6736, train_F1=0.5771 | val_loss=0.6820,\nval_F1=0.5970', '\\n', 'Epoch 4: train_loss=0.6546, train_F1=0.6136 |\nval_loss=0.6460, val_F1=0.6597', '\\n', 'Epoch 5: train_loss=0.6393,\ntrain_F1=0.6424 | val_loss=0.6698, val_F1=0.5687', '\\n', '\\n===== Running\nexperiment with emb_dim=256 =====', '\\n', 'Epoch 1: train_loss=0.6906,\ntrain_F1=0.5488 | val_loss=0.7097, val_F1=0.5030', '\\n', 'Epoch 2:\ntrain_loss=0.6826, train_F1=0.5744 | val_loss=0.6898, val_F1=0.5693', '\\n',\n'Epoch 3: train_loss=0.6752, train_F1=0.5906 | val_loss=0.6729, val_F1=0.6219',\n'\\n', 'Epoch 4: train_loss=0.6524, train_F1=0.6276 | val_loss=0.6587,\nval_F1=0.6305', '\\n', 'Epoch 5: train_loss=0.6338, train_F1=0.6614 |\nval_loss=0.6523, val_F1=0.6317', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '11', '\\n', '\\n--- Training with weight_decay=0.0 ---',\n'\\n', 'Epoch 1: val_loss=0.6941, Macro-F1=0.5721', '\\n', 'Epoch 2:\nval_loss=0.6855, Macro-F1=0.5653', '\\n', 'Epoch 3: val_loss=0.6338,\nMacro-F1=0.6737', '\\n', 'Epoch 4: val_loss=0.6342, Macro-F1=0.6759', '\\n',\n'Epoch 5: val_loss=0.6336, Macro-F1=0.6855', '\\n', '\\n--- Training with\nweight_decay=1e-05 ---', '\\n', 'Epoch 1: val_loss=0.6992, Macro-F1=0.5438',\n'\\n', 'Epoch 2: val_loss=0.6853, Macro-F1=0.6116', '\\n', 'Epoch 3:\nval_loss=0.6617, Macro-F1=0.6215', '\\n', 'Epoch 4: val_loss=0.6580,\nMacro-F1=0.6084', '\\n', 'Epoch 5: val_loss=0.6306, Macro-F1=0.6797', '\\n',\n'\\n--- Training with weight_decay=0.0001 ---', '\\n', 'Epoch 1: val_loss=0.6932,\nMacro-F1=0.5342', '\\n', 'Epoch 2: val_loss=0.6980, Macro-F1=0.5792', '\\n',\n'Epoch 3: val_loss=0.6777, Macro-F1=0.6220', '\\n', 'Epoch 4: val_loss=0.6417,\nMacro-F1=0.6797', '\\n', 'Epoch 5: val_loss=0.6655, Macro-F1=0.6417', '\\n',\n'\\n--- Training with weight_decay=0.001 ---', '\\n', 'Epoch 1: val_loss=0.6889,\nMacro-F1=0.5342', '\\n', 'Epoch 2: val_loss=0.6938, Macro-F1=0.5413', '\\n',\n'Epoch 3: val_loss=0.6683, Macro-F1=0.6177', '\\n', 'Epoch 4: val_loss=0.6699,\nMacro-F1=0.5671', '\\n', 'Epoch 5: val_loss=0.6698, Macro-F1=0.6238', '\\n',\n'\\n--- Training with weight_decay=0.01 ---', '\\n', 'Epoch 1: val_loss=0.6940,\nMacro-F1=0.5081', '\\n', 'Epoch 2: val_loss=0.6910, Macro-F1=0.5414', '\\n',\n'Epoch 3: val_loss=0.6923, Macro-F1=0.5426', '\\n', 'Epoch 4: val_loss=0.6919,\nMacro-F1=0.5480', '\\n', 'Epoch 5: val_loss=0.6911, Macro-F1=0.5414', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab size:', ' ', '11', '\\n', '[layers=1] Ep 1: train F1 0.529 |\nval F1 0.526', '\\n', '[layers=1] Ep 2: train F1 0.567 | val F1 0.509', '\\n',\n'[layers=1] Ep 3: train F1 0.588 | val F1 0.596', '\\n', '[layers=1] Ep 4: train\nF1 0.654 | val F1 0.676', '\\n', '[layers=1] Ep 5: train F1 0.671 | val F1\n0.594', '\\n', '[layers=2] Ep 1: train F1 0.546 | val F1 0.555', '\\n',\n'[layers=2] Ep 2: train F1 0.597 | val F1 0.593', '\\n', '[layers=2] Ep 3: train\nF1 0.610 | val F1 0.636', '\\n', '[layers=2] Ep 4: train F1 0.640 | val F1\n0.657', '\\n', '[layers=2] Ep 5: train F1 0.655 | val F1 0.574', '\\n',\n'[layers=3] Ep 1: train F1 0.558 | val F1 0.596', '\\n', '[layers=3] Ep 2: train\nF1 0.607 | val F1 0.602', '\\n', '[layers=3] Ep 3: train F1 0.626 | val F1\n0.638', '\\n', '[layers=3] Ep 4: train F1 0.622 | val F1 0.509', '\\n',\n'[layers=3] Ep 5: train F1 0.604 | val F1 0.632', '\\n', 'All results saved to',\n' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '11', '\\n', '\\n=== Training with batch_size = 32 ===', '\\n',\n'Epoch 1/5 | train_loss=0.6923 | val_loss=0.6915 | Macro-F1=0.5478', '\\n',\n'Epoch 2/5 | train_loss=0.6811 | val_loss=0.6868 | Macro-F1=0.5404', '\\n',\n'Epoch 3/5 | train_loss=0.6657 | val_loss=0.6509 | Macro-F1=0.6513', '\\n',\n'Epoch 4/5 | train_loss=0.6326 | val_loss=0.6369 | Macro-F1=0.6637', '\\n',\n'Epoch 5/5 | train_loss=0.6227 | val_loss=0.6505 | Macro-F1=0.6618', '\\n',\n'\\n=== Training with batch_size = 64 ===', '\\n', 'Epoch 1/5 | train_loss=0.6846\n| val_loss=0.6901 | Macro-F1=0.5460', '\\n', 'Epoch 2/5 | train_loss=0.6814 |\nval_loss=0.6798 | Macro-F1=0.5842', '\\n', 'Epoch 3/5 | train_loss=0.6459 |\nval_loss=0.6311 | Macro-F1=0.6814', '\\n', 'Epoch 4/5 | train_loss=0.6588 |\nval_loss=0.6835 | Macro-F1=0.5270', '\\n', 'Epoch 5/5 | train_loss=0.6623 |\nval_loss=0.6702 | Macro-F1=0.6317', '\\n', '\\n=== Training with batch_size = 128\n===', '\\n', 'Epoch 1/5 | train_loss=0.6851 | val_loss=0.6992 | Macro-F1=0.5490',\n'\\n', 'Epoch 2/5 | train_loss=0.6751 | val_loss=0.6770 | Macro-F1=0.5800', '\\n',\n'Epoch 3/5 | train_loss=0.6553 | val_loss=0.6431 | Macro-F1=0.6799', '\\n',\n'Epoch 4/5 | train_loss=0.6250 | val_loss=0.6313 | Macro-F1=0.6718', '\\n',\n'Epoch 5/5 | train_loss=0.6207 | val_loss=0.6338 | Macro-F1=0.6773', '\\n',\n'\\n=== Training with batch_size = 256 ===', '\\n', 'Epoch 1/5 | train_loss=0.6909\n| val_loss=0.6942 | Macro-F1=0.5376', '\\n', 'Epoch 2/5 | train_loss=0.6809 |\nval_loss=0.6938 | Macro-F1=0.5620', '\\n', 'Epoch 3/5 | train_loss=0.6741 |\nval_loss=0.6896 | Macro-F1=0.5615', '\\n', 'Epoch 4/5 | train_loss=0.6671 |\nval_loss=0.6796 | Macro-F1=0.6089', '\\n', 'Epoch 5/5 | train_loss=0.6494 |\nval_loss=0.6553 | Macro-F1=0.6659', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '11', '\\n', '\\n=== Training with batch_size = 32 ===', '\\n',\n'Epoch 1/5 | train_loss=0.6858 | val_loss=0.6921 | Macro-F1=0.5313', '\\n',\n'Epoch 2/5 | train_loss=0.6760 | val_loss=0.6748 | Macro-F1=0.6337', '\\n',\n'Epoch 3/5 | train_loss=0.6504 | val_loss=0.6300 | Macro-F1=0.6699', '\\n',\n'Epoch 4/5 | train_loss=0.6325 | val_loss=0.6627 | Macro-F1=0.5844', '\\n',\n'Epoch 5/5 | train_loss=0.6336 | val_loss=0.6298 | Macro-F1=0.6739', '\\n',\n'\\n=== Training with batch_size = 64 ===', '\\n', 'Epoch 1/5 | train_loss=0.6907\n| val_loss=0.6951 | Macro-F1=0.4798', '\\n', 'Epoch 2/5 | train_loss=0.6725 |\nval_loss=0.6631 | Macro-F1=0.6360', '\\n', 'Epoch 3/5 | train_loss=0.6542 |\nval_loss=0.6824 | Macro-F1=0.5608', '\\n', 'Epoch 4/5 | train_loss=0.6652 |\nval_loss=0.6809 | Macro-F1=0.6240', '\\n', 'Epoch 5/5 | train_loss=0.6559 |\nval_loss=0.6621 | Macro-F1=0.6536', '\\n', '\\n=== Training with batch_size = 128\n===', '\\n', 'Epoch 1/5 | train_loss=0.6859 | val_loss=0.6887 | Macro-F1=0.5539',\n'\\n', 'Epoch 2/5 | train_loss=0.6732 | val_loss=0.6796 | Macro-F1=0.5829', '\\n',\n'Epoch 3/5 | train_loss=0.6543 | val_loss=0.6352 | Macro-F1=0.6678', '\\n',\n'Epoch 4/5 | train_loss=0.6523 | val_loss=0.6541 | Macro-F1=0.6376', '\\n',\n'Epoch 5/5 | train_loss=0.6377 | val_loss=0.6254 | Macro-F1=0.6840', '\\n',\n'\\n=== Training with batch_size = 256 ===', '\\n', 'Epoch 1/5 | train_loss=0.6880\n| val_loss=0.6928 | Macro-F1=0.5136', '\\n', 'Epoch 2/5 | train_loss=0.6779 |\nval_loss=0.6960 | Macro-F1=0.5443', '\\n', 'Epoch 3/5 | train_loss=0.6709 |\nval_loss=0.6903 | Macro-F1=0.5802', '\\n', 'Epoch 4/5 | train_loss=0.6613 |\nval_loss=0.6751 | Macro-F1=0.6257', '\\n', 'Epoch 5/5 | train_loss=0.6463 |\nval_loss=0.6464 | Macro-F1=0.6650', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '11', '\\n', '\\n=== Training with batch_size = 32 ===', '\\n',\n'Epoch 1/5 | train_loss=0.6942 | val_loss=0.6896 | Macro-F1=0.5173', '\\n',\n'Epoch 2/5 | train_loss=0.6805 | val_loss=0.6790 | Macro-F1=0.5325', '\\n',\n'Epoch 3/5 | train_loss=0.6525 | val_loss=0.6414 | Macro-F1=0.6736', '\\n',\n'Epoch 4/5 | train_loss=0.6337 | val_loss=0.6392 | Macro-F1=0.6649', '\\n',\n'Epoch 5/5 | train_loss=0.6261 | val_loss=0.6382 | Macro-F1=0.6840', '\\n',\n'\\n=== Training with batch_size = 64 ===', '\\n', 'Epoch 1/5 | train_loss=0.6972\n| val_loss=0.6940 | Macro-F1=0.5448', '\\n', 'Epoch 2/5 | train_loss=0.6789 |\nval_loss=0.6763 | Macro-F1=0.5942', '\\n', 'Epoch 3/5 | train_loss=0.6507 |\nval_loss=0.6425 | Macro-F1=0.6600', '\\n', 'Epoch 4/5 | train_loss=0.6340 |\nval_loss=0.6442 | Macro-F1=0.6559', '\\n', 'Epoch 5/5 | train_loss=0.6425 |\nval_loss=0.6804 | Macro-F1=0.5401', '\\n', '\\n=== Training with batch_size = 128\n===', '\\n', 'Epoch 1/5 | train_loss=0.6927 | val_loss=0.6883 | Macro-F1=0.4958',\n'\\n', 'Epoch 2/5 | train_loss=0.6764 | val_loss=0.6881 | Macro-F1=0.5584', '\\n',\n'Epoch 3/5 | train_loss=0.6558 | val_loss=0.6786 | Macro-F1=0.6282', '\\n',\n'Epoch 4/5 | train_loss=0.6532 | val_loss=0.6469 | Macro-F1=0.6605', '\\n',\n'Epoch 5/5 | train_loss=0.6293 | val_loss=0.6245 | Macro-F1=0.6880', '\\n',\n'\\n=== Training with batch_size = 256 ===', '\\n', 'Epoch 1/5 | train_loss=0.6872\n| val_loss=0.6900 | Macro-F1=0.5475', '\\n', 'Epoch 2/5 | train_loss=0.6805 |\nval_loss=0.6959 | Macro-F1=0.5487', '\\n', 'Epoch 3/5 | train_loss=0.6754 |\nval_loss=0.6898 | Macro-F1=0.5333', '\\n', 'Epoch 4/5 | train_loss=0.6718 |\nval_loss=0.6841 | Macro-F1=0.6209', '\\n', 'Epoch 5/5 | train_loss=0.6638 |\nval_loss=0.6731 | Macro-F1=0.5897', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["", "The script executed successfully without any errors or bugs. The training\nprocess iterated over different epoch budgets (5, 10, 15, 20) and utilized a\npatience mechanism for early stopping. The validation F1 scores and losses were\nlogged for each epoch, and the results were saved to a file. The implementation\nappears robust and efficient.", "", "", "", "", "The training script executed successfully without any errors or bugs. The model\ntrained on the provided dataset with three different embedding dimensions (32,\n128, 256). Training and validation losses, as well as F1 scores, were reported\nfor each epoch. The experiment data was saved successfully. The code appears to\nbe functioning as intended and adheres to the goals of the current stage of\nhyperparameter tuning.", "The training script executed successfully without any errors or bugs. The model\nwas trained with different weight decay values, and the results were logged for\neach configuration. The script also saved the experiment data to a file. The\nresults show that the Macro-F1 score varied across different weight decay\nvalues, with the best performance achieved at weight_decay=0.0. The experiment\nwas completed within the time limit, indicating efficient execution.", "The execution of the training script completed successfully without any errors\nor bugs. The model trained with 1, 2, and 3 GRU layers, and the F1 scores for\nboth training and validation were logged for each epoch. The results were saved\ncorrectly. There is no indication of any issues in the provided output.", "The training script executed successfully without any errors or bugs. The model\nwas trained with varying batch sizes (32, 64, 128, 256) for 5 epochs each. The\ntraining and validation losses, as well as the Macro-F1 scores, were logged. The\nresults showed improvement in Macro-F1 scores over epochs for most batch sizes,\nindicating the model's learning capability. Experiment data was saved\nsuccessfully, and the execution time was well within the limit.", "The execution of the training script was successful. The script performed a\nhyperparameter sweep over batch sizes and evaluated the model's performance\nusing training loss, validation loss, and Macro-F1 score. The results were saved\nto a file for further analysis. No errors or bugs were detected in the output\nlog.", "The code executed successfully, and the training script completed without any\nissues. The model was trained with varying batch sizes, and the results were\nlogged appropriately. The macro F1 scores showed improvement across epochs for\ncertain batch sizes, indicating that the model is learning. No bugs or errors\nwere identified in the execution.", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error rate during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6217, "best_value": 0.6217}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error rate on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6336, "best_value": 0.6336}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Evaluates the macro-averaged F1 score on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6855, "best_value": 0.6855}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score for the training dataset, indicating the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6568, "best_value": 0.686}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score for the validation dataset, indicating the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6663, "best_value": 0.6819}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value for the training dataset, representing the error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6518, "best_value": 0.6243}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset, representing the error during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6602, "best_value": 0.6337}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final training loss value achieved during the training process.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6352, "best_value": 0.6352}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final validation loss value achieved during the training process.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6429, "best_value": 0.6429}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The best macro F1 score achieved on the validation set during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6668, "best_value": 0.6668}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error in predictions during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6295, "best_value": 0.6295}, {"dataset_name": "SPR_BENCH", "final_value": 0.6773, "best_value": 0.6773}, {"dataset_name": "SPR_BENCH", "final_value": 0.6535, "best_value": 0.6535}, {"dataset_name": "SPR_BENCH", "final_value": 0.6384, "best_value": 0.6384}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error in predictions on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6436, "best_value": 0.6389}, {"dataset_name": "SPR_BENCH", "final_value": 0.6889, "best_value": 0.664}, {"dataset_name": "SPR_BENCH", "final_value": 0.6497, "best_value": 0.6398}, {"dataset_name": "SPR_BENCH", "final_value": 0.642, "best_value": 0.642}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Evaluates the balance between precision and recall on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}, {"dataset_name": "SPR_BENCH", "final_value": 0.5567, "best_value": 0.648}, {"dataset_name": "SPR_BENCH", "final_value": 0.6596, "best_value": 0.6839}, {"dataset_name": "SPR_BENCH", "final_value": 0.679, "best_value": 0.679}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score achieved during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.511, "best_value": 0.6844}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score achieved during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4457, "best_value": 0.6855}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7005, "best_value": 0.6217}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6949, "best_value": 0.6336}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score on the training dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.648, "best_value": 0.6536}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score on the validation dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3872, "best_value": 0.6707}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset, measuring the error in predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6446, "best_value": 0.6386}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset, measuring the error in predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7629, "best_value": 0.6407}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score during training, higher is better.", "data": [{"dataset_name": "dim_32", "final_value": 0.6178, "best_value": 0.6178}, {"dataset_name": "dim_128", "final_value": 0.6424, "best_value": 0.6424}, {"dataset_name": "dim_256", "final_value": 0.6614, "best_value": 0.6614}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score during validation, higher is better.", "data": [{"dataset_name": "dim_32", "final_value": 0.6734, "best_value": 0.6734}, {"dataset_name": "dim_128", "final_value": 0.6597, "best_value": 0.6597}, {"dataset_name": "dim_256", "final_value": 0.6317, "best_value": 0.6317}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training, lower is better.", "data": [{"dataset_name": "dim_32", "final_value": 0.6718, "best_value": 0.6718}, {"dataset_name": "dim_128", "final_value": 0.6393, "best_value": 0.6393}, {"dataset_name": "dim_256", "final_value": 0.6338, "best_value": 0.6338}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation, lower is better.", "data": [{"dataset_name": "dim_32", "final_value": 0.6779, "best_value": 0.6779}, {"dataset_name": "dim_128", "final_value": 0.6698, "best_value": 0.6698}, {"dataset_name": "dim_256", "final_value": 0.6523, "best_value": 0.6523}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures how well the model is fitting the training data. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6217, "best_value": 0.6217}, {"dataset_name": "SPR_BENCH", "final_value": 0.6273, "best_value": 0.6273}, {"dataset_name": "SPR_BENCH", "final_value": 0.6627, "best_value": 0.6627}, {"dataset_name": "SPR_BENCH", "final_value": 0.6438, "best_value": 0.6438}, {"dataset_name": "SPR_BENCH", "final_value": 0.678, "best_value": 0.678}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model is generalizing to unseen data. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6336, "best_value": 0.6336}, {"dataset_name": "SPR_BENCH", "final_value": 0.6306, "best_value": 0.6306}, {"dataset_name": "SPR_BENCH", "final_value": 0.6655, "best_value": 0.6655}, {"dataset_name": "SPR_BENCH", "final_value": 0.6698, "best_value": 0.6698}, {"dataset_name": "SPR_BENCH", "final_value": 0.6911, "best_value": 0.6911}]}, {"metric_name": "validation Macro-F1 score", "lower_is_better": false, "description": "A measure of the model's classification performance, considering precision and recall equally. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6855, "best_value": 0.6855}, {"dataset_name": "SPR_BENCH", "final_value": 0.6797, "best_value": 0.6797}, {"dataset_name": "SPR_BENCH", "final_value": 0.6417, "best_value": 0.6417}, {"dataset_name": "SPR_BENCH", "final_value": 0.6238, "best_value": 0.6238}, {"dataset_name": "SPR_BENCH", "final_value": 0.5414, "best_value": 0.5414}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "The F1 score achieved on the training set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6715, "best_value": 0.6715}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score achieved on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6324, "best_value": 0.6324}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss achieved on the training set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6342, "best_value": 0.6342}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss achieved on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6594, "best_value": 0.6594}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6227, "best_value": 0.6227}, {"dataset_name": "SPR_BENCH", "final_value": 0.6623, "best_value": 0.6623}, {"dataset_name": "SPR_BENCH", "final_value": 0.6207, "best_value": 0.6207}, {"dataset_name": "SPR_BENCH", "final_value": 0.6494, "best_value": 0.6494}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6505, "best_value": 0.6369}, {"dataset_name": "SPR_BENCH", "final_value": 0.6702, "best_value": 0.6311}, {"dataset_name": "SPR_BENCH", "final_value": 0.6338, "best_value": 0.6313}, {"dataset_name": "SPR_BENCH", "final_value": 0.6553, "best_value": 0.6553}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The F1 score for the validation set, taking into account class imbalance. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6618, "best_value": 0.6637}, {"dataset_name": "SPR_BENCH", "final_value": 0.6317, "best_value": 0.6814}, {"dataset_name": "SPR_BENCH", "final_value": 0.6773, "best_value": 0.6799}, {"dataset_name": "SPR_BENCH", "final_value": 0.6659, "best_value": 0.6659}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6336, "best_value": 0.6336}, {"dataset_name": "SPR_BENCH", "final_value": 0.6559, "best_value": 0.6559}, {"dataset_name": "SPR_BENCH", "final_value": 0.6377, "best_value": 0.6377}, {"dataset_name": "SPR_BENCH", "final_value": 0.6463, "best_value": 0.6463}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6298, "best_value": 0.6298}, {"dataset_name": "SPR_BENCH", "final_value": 0.6621, "best_value": 0.6621}, {"dataset_name": "SPR_BENCH", "final_value": 0.6254, "best_value": 0.6254}, {"dataset_name": "SPR_BENCH", "final_value": 0.6464, "best_value": 0.6464}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6739, "best_value": 0.6739}, {"dataset_name": "SPR_BENCH", "final_value": 0.6536, "best_value": 0.6536}, {"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}, {"dataset_name": "SPR_BENCH", "final_value": 0.665, "best_value": 0.665}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error on the training dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6261, "best_value": 0.6261}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6382, "best_value": 0.6382}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Measures the F1 score (macro averaged) on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, true, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_conf_matrix_epochs_10.png"], ["../../logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_train_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_val_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_val_f1_curves.png", "../../logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_confusion_matrix_lr_0.003.png"], ["../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_confusion_matrix_bs_32.png"], ["../../logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_F1_curves.png", "../../logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_final_F1_bar.png", "../../logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_confusion_matrix_dropout_0.0.png"], ["../../logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_F1_curves.png", "../../logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_final_val_F1_bar.png", "../../logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_confusion_matrix_h64.png"], ["../../logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_f1_curves.png", "../../logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_val_f1_curves.png", "../../logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_confusion_matrix_weight_decay_0.0.png"], ["../../logs/0-run/experiment_results/experiment_24b2379ae04e4e38b14ef924328edbf7_proc_3442581/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_24b2379ae04e4e38b14ef924328edbf7_proc_3442581/SPR_BENCH_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_confusion_matrix_bs_128.png"], ["../../logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_confusion_matrix_bs_128.png"], ["../../logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_confusion_matrix_bs_128.png"], ["../../logs/0-run/experiment_results/seed_aggregation_47fd777c3fe24d84b19f14c4ebff633d/SPR_BENCH_aggregated_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_47fd777c3fe24d84b19f14c4ebff633d/SPR_BENCH_aggregated_macro_f1_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_47fd777c3fe24d84b19f14c4ebff633d/SPR_BENCH_aggregated_final_f1_bar.png"]], "plot_paths": [["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_conf_matrix_epochs_10.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_train_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_val_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_val_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_confusion_matrix_lr_0.003.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_confusion_matrix_bs_32.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_F1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_final_F1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_confusion_matrix_dropout_0.0.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_F1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_final_val_F1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_confusion_matrix_h64.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_final_val_f1_bar.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_val_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_confusion_matrix_weight_decay_0.0.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_24b2379ae04e4e38b14ef924328edbf7_proc_3442581/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_24b2379ae04e4e38b14ef924328edbf7_proc_3442581/SPR_BENCH_loss_curves.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_confusion_matrix_bs_128.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_confusion_matrix_bs_128.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_confusion_matrix_bs_128.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_47fd777c3fe24d84b19f14c4ebff633d/SPR_BENCH_aggregated_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_47fd777c3fe24d84b19f14c4ebff633d/SPR_BENCH_aggregated_macro_f1_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_47fd777c3fe24d84b19f14c4ebff633d/SPR_BENCH_aggregated_final_f1_bar.png"]], "plot_analyses": [[{"analysis": "The loss curves indicate a consistent decrease in both training and validation loss over the epochs, suggesting that the model is learning effectively. By epoch 3, the validation loss stabilizes, implying that the model is not overfitting to the training data. The gap between training and validation loss remains minimal, which is a positive indication of generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_loss_curves.png"}, {"analysis": "The macro-F1 score on the validation set shows a significant improvement from epoch 2 onward, reaching a peak around epoch 5. This indicates that the model's ability to classify sequences governed by symbolic poly-factor rules is improving as training progresses. The steady rise in macro-F1 score also suggests that the model is learning to balance performance across classes effectively.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The confusion matrix shows that the model performs well in classifying both classes, with a higher true positive count for one class compared to the other. However, there is still a noticeable number of false positives and false negatives, indicating room for improvement in the model's precision and recall. The imbalance in misclassifications could be addressed by refining the training process or using techniques to handle class imbalance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_35070aabd828495c9f49ac1dbb2f59ad_proc_3440934/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that as the number of epochs increases, there is a general trend of loss reduction, particularly for the training set. However, the validation loss exhibits some fluctuations, suggesting potential overfitting at higher epochs. For example, the validation loss for 10 epochs shows significant instability, spiking upward after an initial decrease. This suggests that while the model learns effectively during the early epochs, it struggles to generalize well to unseen data as training progresses.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show a steady improvement in performance for both training and validation sets up to around 5 epochs, after which the validation scores begin to fluctuate. This fluctuation is particularly pronounced for models trained for 10 epochs, where the validation Macro-F1 score drops significantly before recovering. This could indicate that the model is sensitive to hyperparameter settings and may require further tuning to stabilize the validation performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix for the model trained for 10 epochs reveals that there are 174 true positives and 167 true negatives, suggesting a reasonably balanced performance. However, there are also 74 false positives and 85 false negatives, indicating that the model has room for improvement in both precision and recall. This highlights the need for further optimization, potentially through adjustments to the learning rate or regularization techniques.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_28b96bba06114be7a22f15ea51e8e8c4_proc_3442578/SPR_BENCH_conf_matrix_epochs_10.png"}], [{"analysis": "This plot shows the training loss across different learning rates (LRs) over five epochs. The learning rate of 0.001 achieves the fastest and most consistent reduction in training loss, indicating effective learning. In contrast, the learning rate of 0.003 exhibits instability, as seen in the sudden spike in loss at epoch 3, suggesting potential overfitting or difficulty in convergence at this rate. Lower learning rates, such as 0.0003 and 0.0005, show slower convergence, which might indicate underfitting or insufficient gradient updates.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_train_loss_curves.png"}, {"analysis": "This plot illustrates the validation loss across different learning rates over epochs. Similar to the training loss, the learning rate of 0.001 demonstrates the most stable and significant reduction in validation loss, suggesting good generalization. The learning rate of 0.003, while initially reducing loss, shows instability with a sharp drop and spike, indicating overfitting or erratic behavior. Lower learning rates (0.0003 and 0.0005) fail to achieve significant improvements in validation loss, which may indicate underfitting or slower convergence.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_val_loss_curves.png"}, {"analysis": "This plot tracks the validation Macro-F1 score over epochs for different learning rates. The learning rate of 0.001 shows the most consistent and rapid improvement in Macro-F1, achieving the highest score by epoch 5. The learning rate of 0.003, despite a significant spike in performance at later epochs, exhibits erratic behavior earlier, which could indicate instability. Lower learning rates (0.0003 and 0.0005) show slower and more gradual improvements, with 0.0005 performing better than 0.0003 but still not reaching the performance of higher learning rates.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_val_f1_curves.png"}, {"analysis": "This bar plot summarizes the final Macro-F1 scores achieved by different learning rates. The learning rates of 0.001 and 0.003 achieve the highest scores, with 0.003 slightly outperforming 0.001. However, given the instability observed in earlier plots for 0.003, the learning rate of 0.001 might be a more reliable choice. Lower learning rates (0.0003 and 0.0005) achieve lower final scores, reflecting their slower convergence and potentially suboptimal learning dynamics.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_final_f1_bar.png"}, {"analysis": "The confusion matrix for the best-performing learning rate (0.003) shows the model's classification performance. While the model correctly classifies a significant number of samples in both classes, there is noticeable misclassification, particularly in the first class, where 96 samples are incorrectly predicted. This suggests that while the model performs well overall, there is room for improvement in reducing false negatives and false positives, potentially through better regularization or hyperparameter tuning.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f7f02b206043408db9c3a6ff0fd2befd_proc_3442579/SPR_BENCH_confusion_matrix_lr_0.003.png"}], [{"analysis": "This plot compares training and validation loss across different batch sizes (32, 64, 128, 256) as a function of epochs. Smaller batch sizes (e.g., 32) show a steady decrease in both training and validation loss, indicating stable convergence. However, larger batch sizes (e.g., 64) exhibit erratic behavior in validation loss, with a sharp spike at epoch 3, suggesting overfitting or instability in gradient updates. Batch sizes 128 and 256 maintain a more consistent decline in both losses, but their validation loss remains higher compared to batch size 32, implying suboptimal generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot focuses on the macro-F1 score on the validation set for different batch sizes over epochs. Batch size 32 consistently delivers a high macro-F1 score, peaking at around 0.68, indicating robust performance. Batch size 64 shows significant instability, with a drop in macro-F1 score at epoch 3, aligning with the loss spike observed earlier. Batch sizes 128 and 256 show gradual improvement, eventually converging to competitive macro-F1 scores, but they do not outperform batch size 32.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "This bar chart summarizes the final macro-F1 scores for different batch sizes. Batch sizes 32 and 256 achieve the highest final macro-F1 scores (0.68), while batch size 64 performs the worst (0.56). Batch size 128 achieves a decent score of 0.66, but overall, smaller and larger batch sizes (32 and 256) seem to be more effective for this task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_final_f1_bar.png"}, {"analysis": "This confusion matrix evaluates the classification performance for batch size 32. The model correctly predicts 172 true positives and 170 true negatives, but it also misclassifies 76 false positives and 82 false negatives. This indicates a balanced performance with room for improvement in reducing misclassifications, particularly false negatives.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_e9901d40fea04cdb8730bd3a5d6ea854_proc_3442580/SPR_BENCH_confusion_matrix_bs_32.png"}], [{"analysis": "This plot illustrates the training and validation loss trends across different epochs for various dropout rates (d=0.0, 0.2, 0.3, 0.5). A key observation is that lower dropout rates (e.g., d=0.0, 0.2) exhibit smoother and more consistent reductions in both training and validation loss. However, higher dropout rates (e.g., d=0.5) show erratic behavior, particularly in the validation loss, which increases sharply after initial improvement. This suggests that higher dropout rates might be introducing excessive regularization, hindering the model's ability to generalize effectively.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the macro-F1 score trends for training and validation across epochs for different dropout rates. Lower dropout rates (e.g., d=0.0, 0.2) exhibit a steady improvement in F1 scores for both training and validation, with minimal divergence between the two, indicating good generalization. Conversely, higher dropout rates (e.g., d=0.5) result in a sharp decline in validation F1 scores after an initial increase, reflecting over-regularization and poor generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_F1_curves.png"}, {"analysis": "This bar chart summarizes the final validation macro-F1 scores for different dropout rates. The results clearly indicate that lower dropout rates (d=0.0 and d=0.2) achieve the highest performance, with a gradual decline as the dropout rate increases. The lowest F1 score is observed at d=0.5, reinforcing the observation that excessive dropout negatively impacts model performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_final_F1_bar.png"}, {"analysis": "The confusion matrix for dropout=0.0 shows the distribution of correct and incorrect predictions. True positives (181) and true negatives (162) significantly outnumber false positives (86) and false negatives (71), indicating good classification performance. However, there is still room for improvement in reducing the false prediction counts to enhance overall accuracy.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_74897b01fd0d44879ae25e5fc05a7bef_proc_3442581/SPR_BENCH_confusion_matrix_dropout_0.0.png"}], [{"analysis": "The loss curves demonstrate the model's training and validation loss for various hidden sizes (64, 128, 256, and 512). It is evident that the models with smaller hidden sizes (64 and 128) achieve better convergence in both training and validation loss compared to larger hidden sizes (256 and 512). Notably, the validation loss for hidden size 512 exhibits instability, suggesting overfitting or difficulty in generalizing to unseen data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_loss_curves.png"}, {"analysis": "The F1 curves show the macro F1 scores for training and validation across epochs for different hidden sizes. Smaller hidden sizes (64 and 128) achieve consistently better validation F1 scores, indicating better performance in capturing the true positive rates across all classes. The hidden size 512 shows a steep drop in validation F1 after epoch 4, further supporting the overfitting hypothesis.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_F1_curves.png"}, {"analysis": "The bar plot highlights the final validation macro F1 scores for each hidden size. Hidden sizes 64, 128, and 256 perform comparably, with scores around 0.65, while the hidden size 512 significantly underperforms with a final F1 score below 0.5. This reinforces the observation that larger hidden sizes are less effective for this task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_final_val_F1_bar.png"}, {"analysis": "The confusion matrix for the best-performing hidden size (64) provides insight into the model's classification performance. The true positive rates for both classes are reasonably high, with some misclassifications (96 false positives and 68 false negatives). This suggests that while the model is effective overall, there is room for improvement in reducing misclassification errors.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_16131e72b3b644ee9fd59a50aad6bfbf_proc_3442580/SPR_BENCH_confusion_matrix_h64.png"}], [{"analysis": "The first plot shows the training and validation cross-entropy loss trends across different embedding dimensions (32, 128, 256). For all dimensions, the training loss decreases steadily, indicating that the model is learning effectively. However, the validation loss behavior varies: while dim_32 and dim_128 show a slight improvement initially, dim_256 exhibits a more consistent decline in validation loss. This suggests that dim_256 might generalize better to unseen data, although overfitting tendencies are observed at later epochs for dim_32 and dim_128 as their validation loss increases after a point.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_loss_curves.png"}, {"analysis": "The second plot provides insights into the macro-F1 scores for training and validation sets across different embedding dimensions. While dim_32 and dim_128 show a peak in validation macro-F1 around epoch 2-3 before dropping, dim_256 maintains a more stable and improving trend. This indicates that larger embedding dimensions might be more effective for capturing the complexities of the SPR task. However, the fluctuations in validation F1 for dim_32 and dim_128 suggest potential overfitting or instability in learning.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_f1_curves.png"}, {"analysis": "The third plot compares the final validation macro-F1 scores across embedding dimensions. Dim_256 achieves the highest score, followed by dim_128 and dim_32. This reinforces the observation that larger embedding dimensions are better suited for the SPR task, likely due to their ability to represent more complex patterns and relationships in the data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_c0c70ae2684d40939a5ffd5369930cf1_proc_3442578/SPR_final_val_f1_bar.png"}], [{"analysis": "The loss curves show the training and validation loss across epochs for different weight decay values. The training loss decreases steadily for all weight decay values, with weight_decay=0.0 showing the most rapid decline. However, the validation loss exhibits varied behavior. For weight_decay=0.01, validation loss remains relatively high and flat, indicating potential overfitting. On the other hand, weight_decay=0.0 and weight_decay=1e-05 show the most prominent decrease in validation loss, suggesting these values might generalize better.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Macro-F1 plot shows the performance of the models on the validation set across epochs. Weight_decay=0.0 and weight_decay=1e-05 consistently achieve the highest Macro-F1 scores, with weight_decay=0.0 peaking earlier and maintaining stability. Weight_decay=0.01 performs poorly, showing no significant improvement in Macro-F1 over epochs, indicating it might be too high for this task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_val_f1_curves.png"}, {"analysis": "The bar chart provides a summary of the final Macro-F1 scores for each weight decay value. Weight_decay=0.0 and weight_decay=1e-05 achieve the highest final scores, confirming their superior performance compared to other configurations. Weight_decay=0.01 has the lowest final score, further supporting its ineffectiveness in this setup.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_final_f1_bar.png"}, {"analysis": "The confusion matrix indicates the performance of the best model (weight_decay=0.0) on the test set. The model achieves good accuracy, with relatively balanced predictions across both classes. However, there is a notable number of false positives and false negatives, suggesting room for improvement in fine-tuning the model or addressing class imbalance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b408e0adfe2447268f33eaccd0bb7dda_proc_3442579/SPR_BENCH_confusion_matrix_weight_decay_0.0.png"}], [{"analysis": "The F1 Score vs. Epoch plot reveals the performance trends of GRU models with varying layers on the SPR_BENCH dataset. For layers_1, there is a consistent improvement in F1 score for both training and validation sets, though the validation F1 score plateaus after epoch 3. This indicates that the model with one layer stabilizes its performance early. For layers_2, there is a significant increase in F1 score for both training and validation sets up to epoch 4, suggesting better generalization and learning capabilities. However, the drop in validation F1 score at epoch 5 may indicate overfitting. Layers_3 shows a less consistent trend, with fluctuations in both training and validation F1 scores, pointing to potential instability in training with three layers. Overall, layers_2 seems to strike a balance between performance and stability.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_24b2379ae04e4e38b14ef924328edbf7_proc_3442581/SPR_BENCH_f1_curves.png"}, {"analysis": "The Cross-Entropy Loss vs. Epoch plot provides insights into the loss trends for different GRU layer configurations. For layers_1, both training and validation losses decrease steadily, aligning with the F1 score trends and suggesting effective learning. For layers_2, the losses also decrease, but the validation loss begins to diverge slightly from the training loss after epoch 3, indicating the onset of overfitting. Layers_3 shows erratic loss behavior, with the validation loss increasing significantly after epoch 3, further supporting the observation of instability in training with three layers. This plot underscores that while deeper models may have more capacity, they are harder to train effectively and may not generalize well without careful regularization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_24b2379ae04e4e38b14ef924328edbf7_proc_3442581/SPR_BENCH_loss_curves.png"}], [{"analysis": "This plot shows the training and validation loss trends for different batch sizes over 5 epochs. Smaller batch sizes (32 and 64) demonstrate more consistent convergence, with both training and validation losses decreasing steadily. Batch size 128 achieves the lowest validation loss, indicating better generalization. However, batch size 256 shows higher validation loss, suggesting overfitting or insufficient updates per epoch for this size. The plot highlights the trade-off between batch size and model performance, with medium batch sizes (128) performing optimally.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot displays the validation Macro-F1 scores for different batch sizes across 5 epochs. Batch size 128 achieves the highest peak F1 score, indicating superior classification performance. Batch size 256 shows a steady improvement, while batch size 64 exhibits a sharp decline after epoch 3, suggesting instability. Batch size 32 shows consistent improvement but does not surpass batch size 128. This suggests that batch size 128 provides the best balance of stability and performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "This bar chart compares the final Macro-F1 scores for different batch sizes. Batch size 128 achieves the highest final score (0.68), followed closely by batch size 256 (0.67). Batch size 32 also performs well (0.66), while batch size 64 achieves the lowest score (0.63). This reinforces the observation that medium to large batch sizes (128 and 256) are more effective for this task, but extremely large batch sizes (256) may not generalize as well as 128.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_final_f1_bar.png"}, {"analysis": "This confusion matrix for batch size 128 provides insight into the model's classification performance. The true positive and true negative counts are relatively high, indicating good predictive accuracy. However, there are noticeable false positives and false negatives, suggesting room for improvement in handling borderline or ambiguous cases. Batch size 128 demonstrates strong overall performance, but further tuning or architectural adjustments might reduce misclassification.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/SPR_BENCH_confusion_matrix_bs_128.png"}], [{"analysis": "The plot shows the training and validation loss for different batch sizes over 5 epochs. Smaller batch sizes (e.g., 32 and 64) exhibit faster convergence during training, while larger batch sizes (e.g., 128 and 256) converge more slowly. Validation losses for smaller batch sizes diverge more significantly from training losses, indicating potential overfitting. Larger batch sizes show more stable validation loss trends, suggesting better generalization. However, the final convergence is comparable across batch sizes, with batch size 128 showing a good balance between convergence speed and generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_loss_curves.png"}, {"analysis": "The plot illustrates the progression of validation Macro-F1 scores over 5 epochs for different batch sizes. Smaller batch sizes (32 and 64) achieve higher Macro-F1 scores earlier in training but show fluctuations, indicating instability. Larger batch sizes (128 and 256) demonstrate more stable and consistent improvements in Macro-F1. Batch size 128 achieves the highest Macro-F1 score at epoch 3, suggesting it provides the best trade-off between stability and performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "The bar chart compares the final Macro-F1 scores across different batch sizes. Batch size 128 achieves the highest final Macro-F1 score (0.68), slightly outperforming other batch sizes. Batch sizes 32 and 256 follow closely with scores of 0.67, while batch size 64 lags slightly at 0.65. This suggests that batch size 128 is optimal for this task, balancing performance and generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_final_f1_bar.png"}, {"analysis": "The confusion matrix for batch size 128 shows the distribution of true and predicted labels. True positives (172) and true negatives (170) are relatively high, indicating good classification performance. However, there are still a notable number of false positives (76) and false negatives (82), suggesting room for improvement in model precision and recall. Optimizing hyperparameters or incorporating additional regularization techniques could help reduce these errors.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/SPR_BENCH_confusion_matrix_bs_128.png"}], [{"analysis": "This plot demonstrates the training and validation cross-entropy loss trends for different batch sizes over epochs. Smaller batch sizes (e.g., 32 and 64) show a more consistent decrease in both training and validation loss, indicating stable learning. Larger batch sizes (e.g., 256) exhibit slower convergence and higher validation loss, suggesting potential overfitting or insufficient generalization for large batch sizes.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot highlights the macro-F1 score on the validation set across epochs for different batch sizes. Smaller batch sizes (e.g., 32 and 128) achieve higher macro-F1 scores, indicating better generalization. Larger batch sizes (e.g., 256) show lower and less stable macro-F1 scores, suggesting that they may not capture the nuances of the data as effectively.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "This bar chart shows the final macro-F1 scores for different batch sizes. The results indicate that batch sizes of 32 and 128 achieve the highest macro-F1 scores, with 128 slightly outperforming 32. Batch sizes of 64 and 256 perform worse, suggesting that intermediate and very large batch sizes may not be optimal for this task.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_final_f1_bar.png"}, {"analysis": "The confusion matrix for batch size 128 shows a relatively balanced classification performance, with true positives and true negatives being significantly higher than false positives and false negatives. This indicates that a batch size of 128 achieves a good balance between precision and recall, making it a strong candidate for the optimal batch size.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/SPR_BENCH_confusion_matrix_bs_128.png"}], []], "vlm_feedback_summary": ["The plots collectively demonstrate that the model is learning effectively and\ngeneralizing well to the validation set. The loss curves indicate consistent\nimprovement without overfitting, the macro-F1 score reflects balanced\nclassification performance, and the confusion matrix highlights areas for\nfurther optimization in precision and recall.", "The experimental results show that while the models exhibit promising trends in\nloss reduction and Macro-F1 improvement, there are notable issues with\nvalidation stability and generalization. The confusion matrix further highlights\nareas for improvement in classification precision and recall. Hyperparameter\ntuning, particularly around regularization and learning rate, may be necessary\nto enhance model performance on the SPR_BENCH benchmark.", "The analysis identifies that the learning rate of 0.001 provides the most stable\nand effective performance across training and validation metrics, while 0.003\nachieves slightly higher final scores but with instability. Lower learning rates\nunderperform due to slower convergence. The confusion matrix highlights areas\nfor improvement in reducing misclassification, particularly in one class.", "The results indicate that batch size 32 is the most stable and effective for\nachieving low loss and high macro-F1 scores. Larger batch sizes (e.g., 256) also\nperform well but show slightly less stability in validation metrics. Batch size\n64 exhibits instability and poor performance, making it less suitable for this\ntask. The confusion matrix for batch size 32 reveals balanced performance but\nhighlights the need to reduce false negatives for further improvement.", "The analysis highlights that lower dropout rates (d=0.0, 0.2) result in better\nmodel generalization and performance, as evident from the loss trends, F1 score\nimprovements, and final validation metrics. Higher dropout rates (d=0.5)\nintroduce excessive regularization, leading to erratic performance and poor\ngeneralization. The confusion matrix for d=0.0 confirms good but improvable\nclassification accuracy.", "The plots collectively indicate that smaller hidden sizes (64 and 128) are\nbetter suited for the SPR_BENCH task, achieving better convergence,\ngeneralization, and classification performance. Larger hidden sizes,\nparticularly 512, exhibit overfitting and poor performance. Further tuning of\nhyperparameters like learning rate or regularization techniques may improve\noutcomes.", "The analysis highlights that embedding dimension significantly impacts\nperformance. Dim_256 demonstrates the best generalization, achieving the highest\nvalidation macro-F1 score. Dim_32 and dim_128 show signs of overfitting or\ninstability, as evidenced by fluctuations in validation loss and F1 scores. The\nresults suggest that larger embedding dimensions are more effective for handling\nthe complexities of Symbolic PolyRule Reasoning.", "The analysis highlights weight_decay=0.0 and weight_decay=1e-05 as the most\neffective configurations, showing superior performance in terms of loss\nreduction and Macro-F1 scores. The confusion matrix reveals balanced predictions\nbut some misclassifications, indicating potential areas for further\noptimization.", "The analysis indicates that a two-layer GRU model (layers_2) achieves the best\nbalance between training and validation performance, as evidenced by its F1\nscore and loss trends. However, overfitting becomes apparent after epoch 4.\nModels with one layer (layers_1) show stable but modest performance, while\nthree-layer models (layers_3) exhibit training instability and poor\ngeneralization. Further hyperparameter tuning or regularization may be needed to\noptimize deeper models.", "The analysis highlights that batch size 128 consistently outperforms other batch\nsizes, achieving the best validation loss, highest Macro-F1 score, and strong\nconfusion matrix results. Smaller batch sizes show stable performance but do not\nreach the same peak metrics, while larger batch sizes (256) demonstrate slight\noverfitting or instability. Further experiments could focus on fine-tuning\nhyperparameters around batch size 128 to enhance performance.", "The analysis highlights that batch size 128 offers the best trade-off between\nperformance and generalization. Smaller batch sizes converge faster but exhibit\ninstability and overfitting. Larger batch sizes generalize better but converge\nmore slowly. The confusion matrix suggests good classification performance but\nindicates areas for further optimization to improve precision and recall.", "The analysis highlights that smaller batch sizes (32 and 128) generally yield\nbetter performance in terms of both loss reduction and macro-F1 scores. Larger\nbatch sizes (256) show slower convergence and poorer generalization. Batch size\n128 appears to strike a good balance, as indicated by its high macro-F1 score\nand balanced confusion matrix. These insights suggest that tuning batch sizes is\ncrucial for optimizing performance in Symbolic PolyRule Reasoning tasks.", "[]"], "exec_time": [4.457351207733154, 15.035954475402832, 6.675984859466553, 6.715019464492798, 8.839093923568726, 10.148487329483032, 5.432686805725098, 7.20948338508606, 7.648454189300537, 11.093656301498413, 7.658114671707153, 6.836511850357056, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['dim_256']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data.get(\"SPR_BENCH\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\nlosses = data.get(\"losses\", {})\nmetrics = data.get(\"metrics\", {})\npreds = np.array(data.get(\"predictions\", []))\ngts = np.array(data.get(\"ground_truth\", []))\n\nepochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    if len(losses.get(\"train\", [])):\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n    if len(losses.get(\"val\", [])):\n        plt.plot(epochs, losses[\"val\"], label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot 2: Validation Macro-F1\ntry:\n    val_f1 = metrics.get(\"val\", [])\n    if len(val_f1):\n        plt.figure()\n        plt.plot(epochs, val_f1, marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\n            \"SPR_BENCH Validation Macro-F1\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# Plot 3: Confusion Matrix\ntry:\n    if preds.size and gts.size:\n        num_classes = int(max(preds.max(), gts.max())) + 1\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\n            \"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"black\" if cm[i, j] < cm.max() / 2 else \"white\",\n                    fontsize=8,\n                )\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# Print final metric for quick reference\nif len(metrics.get(\"val\", [])):\n    print(f\"Final validation Macro-F1: {metrics['val'][-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data ---------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndset = \"SPR_BENCH\"\nbudgets = experiment_data.get(\"num_epochs\", {}).get(dset, {})\nbest_setting, best_val_f1 = None, -1\n\n# -------------------- summary print -----------------------\nprint(\"\\nBest val-F1 per epoch budget:\")\nfor tag, res in budgets.items():\n    val_f1 = max(res[\"metrics\"][\"val\"])\n    print(f\"{tag:12s} : {val_f1:.4f}\")\n    if val_f1 > best_val_f1:\n        best_val_f1, best_setting = val_f1, tag\n\n# -------------------- loss curves -------------------------\ntry:\n    plt.figure()\n    for tag, res in budgets.items():\n        epochs = np.arange(1, len(res[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, res[\"losses\"][\"train\"], label=f\"train_{tag}\")\n        plt.plot(epochs, res[\"losses\"][\"val\"], label=f\"val_{tag}\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Loss Curves across Epoch Budgets (SPR_BENCH)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# -------------------- F1 curves ---------------------------\ntry:\n    plt.figure()\n    for tag, res in budgets.items():\n        epochs = np.arange(1, len(res[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, res[\"metrics\"][\"train\"], label=f\"train_{tag}\")\n        plt.plot(epochs, res[\"metrics\"][\"val\"], label=f\"val_{tag}\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Macro-F1 Curves across Epoch Budgets (SPR_BENCH)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# -------------------- confusion matrix --------------------\ntry:\n    if best_setting:\n        preds = np.array(budgets[best_setting][\"predictions\"])\n        gts = np.array(budgets[best_setting][\"ground_truth\"])\n        n_cls = max(preds.max(), gts.max()) + 1\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"Confusion Matrix \u2013 {best_setting} (SPR_BENCH)\")\n        for i in range(n_cls):\n            for j in range(n_cls):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    fontsize=8,\n                )\n        fname = os.path.join(working_dir, f\"SPR_BENCH_conf_matrix_{best_setting}.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    lr_runs = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\n    lrs = sorted(lr_runs.keys(), key=float)\n    epochs = len(next(iter(lr_runs.values()))[\"losses\"][\"train\"])  # assume same length\n\n    # utility to get colors\n    cmap = plt.cm.get_cmap(\"tab10\", len(lrs))\n\n    # 1) Training loss curves\n    try:\n        plt.figure()\n        for idx, lr in enumerate(lrs):\n            plt.plot(\n                range(1, epochs + 1),\n                lr_runs[lr][\"losses\"][\"train\"],\n                label=f\"LR {lr}\",\n                color=cmap(idx),\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Train Loss\")\n        plt.title(\"SPR_BENCH: Training Loss vs. Epoch\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train loss plot: {e}\")\n        plt.close()\n\n    # 2) Validation loss curves\n    try:\n        plt.figure()\n        for idx, lr in enumerate(lrs):\n            plt.plot(\n                range(1, epochs + 1),\n                lr_runs[lr][\"losses\"][\"val\"],\n                label=f\"LR {lr}\",\n                color=cmap(idx),\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Loss\")\n        plt.title(\"SPR_BENCH: Validation Loss vs. Epoch\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val loss plot: {e}\")\n        plt.close()\n\n    # 3) Validation Macro-F1 curves\n    try:\n        plt.figure()\n        for idx, lr in enumerate(lrs):\n            plt.plot(\n                range(1, epochs + 1),\n                lr_runs[lr][\"metrics\"][\"val\"],\n                label=f\"LR {lr}\",\n                color=cmap(idx),\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Validation Macro-F1 vs. Epoch\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # 4) Final Macro-F1 bar chart\n    try:\n        final_f1 = [lr_runs[lr][\"metrics\"][\"val\"][-1] for lr in lrs]\n        plt.figure()\n        plt.bar(range(len(lrs)), final_f1, tick_label=lrs)\n        plt.ylabel(\"Final Macro-F1\")\n        plt.title(\"SPR_BENCH: Final Macro-F1 per Learning Rate\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final F1 bar plot: {e}\")\n        plt.close()\n\n    # 5) Confusion matrix for best LR\n    try:\n        best_idx = int(np.argmax(final_f1))\n        best_lr = lrs[best_idx]\n        gt = lr_runs[best_lr][\"ground_truth\"]\n        preds = lr_runs[best_lr][\"predictions\"]\n        cm = confusion_matrix(gt, preds)\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"SPR_BENCH: Confusion Matrix (Best LR {best_lr})\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        fname = os.path.join(\n            working_dir, f\"SPR_BENCH_confusion_matrix_lr_{best_lr}.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # print evaluation metric\n    print(f\"Best LR: {best_lr} | Final Macro-F1: {final_f1[best_idx]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nbatch_sizes = sorted(int(k.split(\"_\")[-1]) for k in exp.keys())\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            logs[\"losses\"][\"train\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, logs[\"losses\"][\"val\"], color=c, linestyle=\"--\", label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"metrics\"][\"val\"]) + 1)\n        plt.plot(epochs, logs[\"metrics\"][\"val\"], color=c, label=f\"val F1 bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Final-epoch F1 bar plot\ntry:\n    final_f1 = [exp[f\"bs_{bs}\"][\"metrics\"][\"val\"][-1] for bs in batch_sizes]\n    plt.figure()\n    plt.bar([str(bs) for bs in batch_sizes], final_f1, color=colors[: len(batch_sizes)])\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Final Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Macro-F1 vs Batch Size\")\n    for x, y in zip(batch_sizes, final_f1):\n        plt.text(str(x), y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best batch size\ntry:\n    best_idx = int(np.argmax(final_f1))\n    best_bs = batch_sizes[best_idx]\n    logs = exp[f\"bs_{best_bs}\"]\n    preds = np.array(logs[\"predictions\"])\n    gts = np.array(logs[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (bs={best_bs})\")\n    plt.colorbar()\n    plt.savefig(\n        os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_bs_{best_bs}.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# gather basic info\ndropouts = list(exp.keys())\nepochs = len(next(iter(exp.values()))[\"losses\"][\"train\"]) if exp else 0\nepoch_idx = np.arange(1, epochs + 1)\n\n# helper: pick best dropout based on final val F1\nbest_dropout, best_f1 = None, -1.0\nfinal_f1s = {}\nfor d in dropouts:\n    val_f1 = exp[d][\"metrics\"][\"val\"][-1]\n    final_f1s[d] = val_f1\n    if val_f1 > best_f1:\n        best_f1, best_dropout = val_f1, d\n\nprint(\"Final validation F1 per dropout:\")\nfor d, f1 in final_f1s.items():\n    print(f\"  dropout={d}: {f1:.4f}\")\nprint(f\"Best dropout according to val F1: {best_dropout} ({best_f1:.4f})\")\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(7, 5))\n    for d in dropouts:\n        plt.plot(epoch_idx, exp[d][\"losses\"][\"train\"], label=f\"train d={d}\")\n        plt.plot(epoch_idx, exp[d][\"losses\"][\"val\"], label=f\"val d={d}\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) F1 curves\ntry:\n    plt.figure(figsize=(7, 5))\n    for d in dropouts:\n        plt.plot(epoch_idx, exp[d][\"metrics\"][\"train\"], label=f\"train d={d}\")\n        plt.plot(\n            epoch_idx, exp[d][\"metrics\"][\"val\"], label=f\"val d={d}\", linestyle=\"--\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_F1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Final F1 bar plot\ntry:\n    plt.figure(figsize=(6, 4))\n    plt.bar(\n        range(len(dropouts)),\n        [final_f1s[d] for d in dropouts],\n        tick_label=dropouts,\n        color=\"skyblue\",\n    )\n    plt.ylabel(\"Final Validation Macro-F1\")\n    plt.title(\"SPR_BENCH: Final F1 vs Dropout Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_F1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final F1 bar: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best model\ntry:\n    gt = np.array(exp[best_dropout][\"ground_truth\"])\n    prd = np.array(exp[best_dropout][\"predictions\"])\n    num_classes = len(np.unique(gt))\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for g, p in zip(gt, prd):\n        cm[g, p] += 1\n\n    plt.figure(figsize=(5, 4))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (dropout={best_dropout})\")\n    for i in range(num_classes):\n        for j in range(num_classes):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_dropout_{best_dropout}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ load experiment data ------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_runs = experiment_data.get(\"hidden_size\", {}).get(\"SPR_BENCH\", {})\nhidden_sizes = sorted(spr_runs.keys())\n\n\n# quick helpers\ndef to_np(seq):\n    return np.array(seq)\n\n\n# ------------------ figure 1: loss curves ------------------\ntry:\n    plt.figure(figsize=(6, 4))\n    for hs in hidden_sizes:\n        epochs = np.arange(1, len(spr_runs[hs][\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            to_np(spr_runs[hs][\"losses\"][\"train\"]),\n            label=f\"train_h{hs}\",\n            linestyle=\"-\",\n        )\n        plt.plot(\n            epochs,\n            to_np(spr_runs[hs][\"losses\"][\"val\"]),\n            label=f\"val_h{hs}\",\n            linestyle=\"--\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ------------------ figure 2: F1 curves ------------------\ntry:\n    plt.figure(figsize=(6, 4))\n    for hs in hidden_sizes:\n        epochs = np.arange(1, len(spr_runs[hs][\"metrics\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            to_np(spr_runs[hs][\"metrics\"][\"train\"]),\n            label=f\"train_h{hs}\",\n            linestyle=\"-\",\n        )\n        plt.plot(\n            epochs,\n            to_np(spr_runs[hs][\"metrics\"][\"val\"]),\n            label=f\"val_h{hs}\",\n            linestyle=\"--\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH F1 Curves\\nLeft: Train, Right: Validation\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_F1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# identify best run (highest final val F1)\nbest_hs, best_f1 = None, -1.0\nfor hs in hidden_sizes:\n    f1 = spr_runs[hs][\"metrics\"][\"val\"][-1]\n    if f1 > best_f1:\n        best_f1, best_hs = f1, hs\nprint(f\"Best hidden size: {best_hs} | Val F1: {best_f1:.3f}\")\n\n# ------------------ figure 3: final F1 bar chart ------------------\ntry:\n    final_f1s = [spr_runs[hs][\"metrics\"][\"val\"][-1] for hs in hidden_sizes]\n    plt.figure(figsize=(6, 4))\n    plt.bar([str(hs) for hs in hidden_sizes], final_f1s, color=\"skyblue\")\n    plt.xlabel(\"Hidden Size\")\n    plt.ylabel(\"Final Val Macro F1\")\n    plt.title(\"SPR_BENCH Final Validation F1 by Hidden Size\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_F1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------ figure 4: confusion matrix for best model ------------------\ntry:\n    preds = np.array(spr_runs[best_hs][\"predictions\"])\n    gts = np.array(spr_runs[best_hs][\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, g in zip(preds, gts):\n        cm[g, p] += 1\n\n    plt.figure(figsize=(4, 4))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(f\"SPR_BENCH Confusion Matrix\\nBest Hidden Size={best_hs}\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_h{best_hs}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"embedding_dim\", {})\nif not runs:\n    print(\"No embedding_dim data found in experiment_data.npy\")\n    exit()\n\n# collect final-epoch validation F1 for console print and bar plot\nfinal_val_f1 = {k: v[\"metrics\"][\"val\"][-1] for k, v in runs.items()}\nprint(\"Final validation F1 scores:\")\nfor k, f1 in final_val_f1.items():\n    print(f\"  {k}: {f1:.4f}\")\n\n# --------------------------------------------------------------------\n# 1) Loss curves\ntry:\n    plt.figure()\n    for k, v in runs.items():\n        plt.plot(v[\"losses\"][\"train\"], label=f\"{k}_train\")\n        plt.plot(v[\"losses\"][\"val\"], linestyle=\"--\", label=f\"{k}_val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR Synthetic Dataset\\nTraining vs Validation Loss (all embedding dims)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------\n# 2) F1 curves\ntry:\n    plt.figure()\n    for k, v in runs.items():\n        plt.plot(v[\"metrics\"][\"train\"], label=f\"{k}_train\")\n        plt.plot(v[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"{k}_val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\n        \"SPR Synthetic Dataset\\nTraining vs Validation Macro-F1 (all embedding dims)\"\n    )\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------\n# 3) Final F1 bar chart\ntry:\n    plt.figure()\n    keys = list(final_val_f1.keys())\n    values = [final_val_f1[k] for k in keys]\n    plt.bar(keys, values, color=\"skyblue\")\n    plt.ylabel(\"Validation Macro-F1 (final epoch)\")\n    plt.title(\"SPR Synthetic Dataset\\nFinal Validation F1 by Embedding Dimension\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_final_val_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset_name = \"SPR_BENCH\"\nkeys = sorted(experiment_data.keys())\nepochs = len(next(iter(experiment_data.values()))[dataset_name][\"losses\"][\"train\"])\nepoch_axis = np.arange(1, epochs + 1)\n\n# gather results\nfinal_f1 = {}\nfor k in keys:\n    f1_vals = experiment_data[k][dataset_name][\"metrics\"][\"val\"]\n    if len(f1_vals):\n        final_f1[k] = f1_vals[-1]\n\nbest_key = max(final_f1, key=final_f1.get)\n\n# -------------------------------------------------\n# 1) Training & Validation Loss plot\ntry:\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n    for k in keys:\n        train_losses = experiment_data[k][dataset_name][\"losses\"][\"train\"]\n        val_losses = experiment_data[k][dataset_name][\"losses\"][\"val\"]\n        ax[0].plot(epoch_axis, train_losses, label=k)\n        ax[1].plot(epoch_axis, val_losses, label=k)\n    ax[0].set_ylabel(\"Train Loss\")\n    ax[1].set_ylabel(\"Val Loss\")\n    ax[1].set_xlabel(\"Epoch\")\n    ax[0].set_title(f\"{dataset_name} - Loss Curves (Top: Train, Bottom: Val)\")\n    ax[0].legend(fontsize=7)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, f\"{dataset_name}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# -------------------------------------------------\n# 2) Validation Macro-F1 curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for k in keys:\n        f1_vals = experiment_data[k][dataset_name][\"metrics\"][\"val\"]\n        plt.plot(epoch_axis, f1_vals, label=k)\n    plt.title(f\"{dataset_name} - Validation Macro-F1 over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=7)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, f\"{dataset_name}_val_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 curve plot: {e}\")\n    plt.close()\n\n# -------------------------------------------------\n# 3) Final Macro-F1 bar chart\ntry:\n    plt.figure(figsize=(8, 5))\n    plt.bar(list(final_f1.keys()), list(final_f1.values()))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Final Macro-F1\")\n    plt.title(f\"{dataset_name} - Final Epoch Macro-F1 per Weight Decay\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, f\"{dataset_name}_final_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# -------------------------------------------------\n# 4) Confusion matrix for best config\ntry:\n    preds = experiment_data[best_key][dataset_name][\"predictions\"]\n    gts = experiment_data[best_key][dataset_name][\"ground_truth\"]\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(4, 4))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(\n        f\"{dataset_name} - Confusion Matrix (Best: {best_key})\\nLeft: Ground Truth, Right: Predicted\"\n    )\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    plt.tight_layout()\n    fname = os.path.join(working_dir, f\"{dataset_name}_confusion_matrix_{best_key}.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# -------------------------------------------------\n# Print final metrics\nprint(\"Final Macro-F1 per weight_decay:\")\nfor k, v in final_f1.items():\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# paths / loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    sweep = experiment_data[\"num_gru_layers\"][\"SPR_BENCH\"]\n    epochs = list(range(1, len(next(iter(sweep.values()))[\"metrics\"][\"train\"]) + 1))\n\n    # ------------------------------ 1st plot: F1 curves\n    try:\n        plt.figure()\n        for k, res in sweep.items():\n            plt.plot(epochs, res[\"metrics\"][\"train\"], marker=\"o\", label=f\"{k} train\")\n            plt.plot(epochs, res[\"metrics\"][\"val\"], marker=\"x\", label=f\"{k} val\")\n        plt.title(\"SPR_BENCH: F1 Score vs Epoch\\nGRU Layer Sweep\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"F1 Score\")\n        plt.grid(True, linestyle=\"--\", alpha=0.4)\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ------------------------------ 2nd plot: Loss curves\n    try:\n        plt.figure()\n        for k, res in sweep.items():\n            plt.plot(epochs, res[\"losses\"][\"train\"], marker=\"o\", label=f\"{k} train\")\n            plt.plot(epochs, res[\"losses\"][\"val\"], marker=\"x\", label=f\"{k} val\")\n        plt.title(\"SPR_BENCH: Cross-Entropy Loss vs Epoch\\nGRU Layer Sweep\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.grid(True, linestyle=\"--\", alpha=0.4)\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------ print final metrics\n    print(\"\\nFinal Validation F1 Scores:\")\n    for k, res in sweep.items():\n        final_f1 = res[\"metrics\"][\"val\"][-1]\n        print(f\"  {k}: {final_f1:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nbatch_sizes = sorted(int(k.split(\"_\")[-1]) for k in exp.keys())\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            logs[\"losses\"][\"train\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, logs[\"losses\"][\"val\"], color=c, linestyle=\"--\", label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"metrics\"][\"val\"]) + 1)\n        plt.plot(epochs, logs[\"metrics\"][\"val\"], color=c, label=f\"val F1 bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Final-epoch F1 bar plot\ntry:\n    final_f1 = [exp[f\"bs_{bs}\"][\"metrics\"][\"val\"][-1] for bs in batch_sizes]\n    plt.figure()\n    plt.bar([str(bs) for bs in batch_sizes], final_f1, color=colors[: len(batch_sizes)])\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Final Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Macro-F1 vs Batch Size\")\n    for x, y in zip(batch_sizes, final_f1):\n        plt.text(str(x), y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best batch size\ntry:\n    best_idx = int(np.argmax(final_f1))\n    best_bs = batch_sizes[best_idx]\n    logs = exp[f\"bs_{best_bs}\"]\n    preds = np.array(logs[\"predictions\"])\n    gts = np.array(logs[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (bs={best_bs})\")\n    plt.colorbar()\n    plt.savefig(\n        os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_bs_{best_bs}.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nbatch_sizes = sorted(int(k.split(\"_\")[-1]) for k in exp.keys())\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            logs[\"losses\"][\"train\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, logs[\"losses\"][\"val\"], color=c, linestyle=\"--\", label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"metrics\"][\"val\"]) + 1)\n        plt.plot(epochs, logs[\"metrics\"][\"val\"], color=c, label=f\"val F1 bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Final-epoch F1 bar plot\ntry:\n    final_f1 = [exp[f\"bs_{bs}\"][\"metrics\"][\"val\"][-1] for bs in batch_sizes]\n    plt.figure()\n    plt.bar([str(bs) for bs in batch_sizes], final_f1, color=colors[: len(batch_sizes)])\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Final Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Macro-F1 vs Batch Size\")\n    for x, y in zip(batch_sizes, final_f1):\n        plt.text(str(x), y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best batch size\ntry:\n    best_idx = int(np.argmax(final_f1))\n    best_bs = batch_sizes[best_idx]\n    logs = exp[f\"bs_{best_bs}\"]\n    preds = np.array(logs[\"predictions\"])\n    gts = np.array(logs[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (bs={best_bs})\")\n    plt.colorbar()\n    plt.savefig(\n        os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_bs_{best_bs}.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nbatch_sizes = sorted(int(k.split(\"_\")[-1]) for k in exp.keys())\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            logs[\"losses\"][\"train\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, logs[\"losses\"][\"val\"], color=c, linestyle=\"--\", label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for c, bs in zip(colors, batch_sizes):\n        logs = exp[f\"bs_{bs}\"]\n        epochs = np.arange(1, len(logs[\"metrics\"][\"val\"]) + 1)\n        plt.plot(epochs, logs[\"metrics\"][\"val\"], color=c, label=f\"val F1 bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Validation Macro-F1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Final-epoch F1 bar plot\ntry:\n    final_f1 = [exp[f\"bs_{bs}\"][\"metrics\"][\"val\"][-1] for bs in batch_sizes]\n    plt.figure()\n    plt.bar([str(bs) for bs in batch_sizes], final_f1, color=colors[: len(batch_sizes)])\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Final Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Macro-F1 vs Batch Size\")\n    for x, y in zip(batch_sizes, final_f1):\n        plt.text(str(x), y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best batch size\ntry:\n    best_idx = int(np.argmax(final_f1))\n    best_bs = batch_sizes[best_idx]\n    logs = exp[f\"bs_{best_bs}\"]\n    preds = np.array(logs[\"predictions\"])\n    gts = np.array(logs[\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (bs={best_bs})\")\n    plt.colorbar()\n    plt.savefig(\n        os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_bs_{best_bs}.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# Basic set-up\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Paths supplied by the task\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8796629ee0ee4696b8354597c67d165a_proc_3442580/experiment_data.npy\",\n    \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_09faf3d23c114b388a04589277926081_proc_3442578/experiment_data.npy\",\n    \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_05cccbfe149f4503a4ec07948432e6fe_proc_3442579/experiment_data.npy\",\n]\n\n# ------------------------------------------------------------------\n# Load every experiment file\n# ------------------------------------------------------------------\nall_experiment_data = []\nroot = os.getenv(\"AI_SCIENTIST_ROOT\", \".\")  # fall back to current dir\nfor p in experiment_data_path_list:\n    try:\n        obj = np.load(os.path.join(root, p), allow_pickle=True).item()\n        all_experiment_data.append(obj)\n    except Exception as e:\n        print(f\"Error loading experiment data from {p}: {e}\")\n\nif len(all_experiment_data) == 0:\n    print(\"No experiment data found \u2013 nothing to plot.\")\n    quit()\n\n# ------------------------------------------------------------------\n# Aggregate across runs (dataset = SPR_BENCH, selection = 'batch_size')\n# ------------------------------------------------------------------\nruns = []\nfor exp in all_experiment_data:\n    try:\n        runs.append(exp[\"batch_size\"][\"SPR_BENCH\"])\n    except Exception as e:\n        print(f\"Run missing SPR_BENCH: {e}\")\n\nif len(runs) == 0:\n    print(\"No SPR_BENCH data present.\")\n    quit()\n\n# Determine the batch-sizes that are present in every run\nbs_sets = [set(int(k.split(\"_\")[-1]) for k in r.keys()) for r in runs]\ncommon_bs = sorted(set.intersection(*bs_sets))\nif len(common_bs) == 0:\n    print(\"No common batch sizes across runs.\")\n    quit()\n\nn_runs = len(runs)\n\n# Helper containers\nagg = {}\nfor bs in common_bs:\n    agg[bs] = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n\n# Collect data per batch size\nfor r in runs:\n    for bs in common_bs:\n        logs = r[f\"bs_{bs}\"]\n        agg[bs][\"train_loss\"].append(np.array(logs[\"losses\"][\"train\"]))\n        agg[bs][\"val_loss\"].append(np.array(logs[\"losses\"][\"val\"]))\n        agg[bs][\"val_f1\"].append(np.array(logs[\"metrics\"][\"val\"]))\n\n# Convert lists to stacked numpy arrays (trim to minimum length)\nfor bs in common_bs:\n    for key in [\"train_loss\", \"val_loss\", \"val_f1\"]:\n        # Find shortest epoch length for this metric across runs\n        min_len = min(arr.shape[0] for arr in agg[bs][key])\n        trimmed = [arr[:min_len] for arr in agg[bs][key]]\n        agg[bs][key] = np.stack(trimmed, axis=0)  # shape = (runs, epochs)\n\n# Colors for plotting\ncolor_cycle = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n\n# ------------------------------------------------------------------\n# 1) Mean \u00b1 SE Training / Validation Loss\n# ------------------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, bs in enumerate(common_bs):\n        c = color_cycle[idx % len(color_cycle)]\n        epochs = np.arange(1, agg[bs][\"train_loss\"].shape[1] + 1)\n\n        # Train\n        mean_train = agg[bs][\"train_loss\"].mean(axis=0)\n        se_train = agg[bs][\"train_loss\"].std(axis=0, ddof=1) / np.sqrt(n_runs)\n        plt.plot(epochs, mean_train, color=c, linestyle=\"-\", label=f\"train bs={bs}\")\n        plt.fill_between(\n            epochs, mean_train - se_train, mean_train + se_train, color=c, alpha=0.2\n        )\n\n        # Validation\n        mean_val = agg[bs][\"val_loss\"].mean(axis=0)\n        se_val = agg[bs][\"val_loss\"].std(axis=0, ddof=1) / np.sqrt(n_runs)\n        plt.plot(epochs, mean_val, color=c, linestyle=\"--\", label=f\"val bs={bs}\")\n        plt.fill_between(\n            epochs, mean_val - se_val, mean_val + se_val, color=c, alpha=0.2\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"SPR_BENCH: Mean \u00b1 SE Training vs Validation Loss (n={n_runs})\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_aggregated_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Mean \u00b1 SE Validation Macro-F1\n# ------------------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, bs in enumerate(common_bs):\n        c = color_cycle[idx % len(color_cycle)]\n        epochs = np.arange(1, agg[bs][\"val_f1\"].shape[1] + 1)\n        mean_f1 = agg[bs][\"val_f1\"].mean(axis=0)\n        se_f1 = agg[bs][\"val_f1\"].std(axis=0, ddof=1) / np.sqrt(n_runs)\n        plt.plot(epochs, mean_f1, color=c, label=f\"val F1 bs={bs}\")\n        plt.fill_between(epochs, mean_f1 - se_f1, mean_f1 + se_f1, color=c, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(f\"SPR_BENCH: Mean \u00b1 SE Validation Macro-F1 (n={n_runs})\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_aggregated_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated F1 curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Final-epoch Macro-F1 Bar Plot with SE\n# ------------------------------------------------------------------\ntry:\n    plt.figure()\n    means = []\n    ses = []\n    for bs in common_bs:\n        final_vals = agg[bs][\"val_f1\"][:, -1]  # take last epoch\u2019s F1 for every run\n        means.append(final_vals.mean())\n        ses.append(final_vals.std(ddof=1) / np.sqrt(n_runs))\n    x_pos = np.arange(len(common_bs))\n    plt.bar(\n        x_pos,\n        means,\n        yerr=ses,\n        capsize=5,\n        color=[color_cycle[i % len(color_cycle)] for i in range(len(common_bs))],\n    )\n    plt.xticks(x_pos, [str(bs) for bs in common_bs])\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Final-Epoch Macro-F1\")\n    plt.title(f\"SPR_BENCH: Final-Epoch Macro-F1 (Mean \u00b1 SE, n={n_runs})\")\n    for x, m in zip(x_pos, means):\n        plt.text(x, m + 0.005, f\"{m:.2f}\", ha=\"center\", va=\"bottom\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_aggregated_final_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated final F1 bar plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We will first load experiment_data.npy from the working directory and extract the stored items for the \u201cSPR_BENCH\u201d dataset: the train/val loss arrays logged during the five epochs, the validation macro-F1 scores, and the final predictions/ground-truth vectors.  Using only these arrays, we generate three standard plots: (1) a line plot showing training and validation loss versus epoch, (2) a line plot of validation macro-F1 versus epoch, and (3) a confusion-matrix heat-map of the final predictions against the ground-truth labels.  Each plot is wrapped in its own try-except block so that a failure in one does not prevent the others from being created, and every figure is saved under a descriptive name (including dataset and plot type) inside working_dir, then immediately closed.  We respect the \u201c\u22645 figures\u201d rule, include clear titles/subtitles for every plot, and never attempt to visualize data that are absent in the file.  After plotting, we print the final validation macro-F1 to the console for a quick textual summary.  The code begins with the required imports, defines working_dir, and remains concise while following all style and safety constraints.  All figures are saved in PNG format, ensuring they can be easily inspected later.  Finally, we close every figure to avoid memory leaks and keep the workspace clean.", "We first load the saved experiment_data.npy, which contains per-epoch training/validation losses and F1 scores for four epoch budgets. We then prepare three figures (well within the \u201c\u22645\u201d rule): (1) a single plot that overlays the training and validation loss curves for every epoch budget; (2) a similar plot for training and validation macro-F1 curves; (3) a confusion matrix for the best-performing setting, identified by the highest validation F1. Each figure is wrapped in an independent try-except block, saved to working_dir with descriptive names, given a clear title and closed afterward. Arrays of different lengths (because of early stopping) are handled safely by plotting only their available points. After plotting, the script prints a concise table of the best validation F1 obtained for every epoch budget to give an immediate numerical summary. All requirements on imports, path handling, figure titles, saving, closing and not fabricating data are followed. The whole workflow is contained in a short, self-contained code snippet.", "We will load experiment_data.npy, extract the per-learning-rate logs for the SPR_BENCH dataset, and generate up to five clear figures: (1) training loss vs. epoch, (2) validation loss vs. epoch, (3) validation Macro-F1 vs. epoch, (4) final Macro-F1 bar chart across learning rates, and (5) a confusion matrix for the LR achieving the best final Macro-F1. Each figure is built in its own try-except block, saved to working_dir with descriptive names, closed afterward, and titled appropriately. We overlay all learning-rate runs on the same axes for the first three plots to keep the figure count low. Basic matplotlib is used without extra styling. After plotting, we print the best LR and its final Macro-F1 so that an evaluation metric appears in stdout. The code respects the developer guidelines and never fabricates data, relying solely on the contents of experiment_data.npy.", "We will load the saved numpy dictionary, locate the \u201cSPR_BENCH\u201d results, and iterate over the four batch-size sub-experiments. For every experiment we extract the per-epoch training/validation losses and the per-epoch validation macro-F1 scores. Plot 1 overlays the training and validation loss curves for all batch sizes, giving each setting a distinct colour/style. Plot 2 overlays the validation macro-F1 curves. Plot 3 is a bar chart of the final-epoch macro-F1 versus batch size. We then pick the batch size that achieved the best final macro-F1, compute its 2\u00d72 confusion matrix from the stored final-epoch predictions and ground-truth labels, and visualise it as a heat-map (Plot 4). Each figure is wrapped in its own try-except block, saved to the working directory with descriptive names, and explicitly closed. No fabricated data are introduced and we create at most four figures, all derived from experiment_data.npy.", "We first load experiment_data.npy, extract the \u201cdropout_rate / SPR_BENCH\u201d section and iterate over the four dropout settings. For every key we collect per-epoch training/validation losses and macro-F1 scores. Four figures are generated inside separate try-except blocks: (1) combined loss curves for all dropout values, (2) combined F1 curves, (3) a bar chart comparing the final validation F1 of each dropout rate, and (4) a confusion-matrix heat-map for the model whose final validation F1 is highest. Each figure is titled clearly, saved inside the working directory with descriptive names and then closed. We respect the 5-figure limit, use only data in the .npy file, and employ basic matplotlib; every figure is always closed even on error. After plotting we also print each dropout\u2019s final validation F1 and identify which dropout performed best so that the reader can match this with the plots.", "We will load the saved experiment_data.npy, extract all runs conducted on the SPR_BENCH dataset, and gather per-epoch train/val losses and F1 scores for each hidden size. Using these series we will generate at most four figures: (1) train-vs-val loss curves over epochs for every hidden size, (2) corresponding F1 curves, (3) a bar chart showing the final validation F1 achieved by each hidden size, and (4) a confusion-matrix heat-map for the model with the best final validation F1. Each figure will be produced inside its own try-except block, saved with descriptive filenames to working_dir, and the figure will be closed in all cases to avoid handle leakage. To stay self-contained we implement the confusion matrix with basic NumPy counts instead of external libraries. Finally, the script prints the best validation F1 so users can verify the selection logic.", "We first load experiment_data.npy, read the \u201cembedding_dim\u201d section, and iterate over its three runs (dim_32/128/256).  For each run we extract the epoch-wise train/val losses and macro-F1 scores that were already stored during the experiment.  We then build three standard visualizations: (1) a single loss-curve figure where every run contributes a solid line for training loss and a dashed line for validation loss; (2) an analogous figure for macro-F1 curves; (3) a bar chart summarising the final-epoch validation F1 for easy comparison of the three embedding sizes.  Each figure is wrapped in its own try-except block, saved under a descriptive name in the working directory, given an informative title, and the figure is always closed.  To satisfy the \u201cat most five figures\u201d rule we stop after these three plots, which completely cover the available metrics.  After plotting we also print a small table of the final validation F1 scores so the user can see the numbers directly in the console.  The code starts with the required imports, ensures the working directory exists, and never tries to generate or guess any data not present in the .npy file.  It should run in a few seconds and produce three PNG files ready for inspection.", "We load experiment_data.npy, gather per-epoch train/val losses and validation macro-F1 for every weight-decay run on the SPR_BENCH dataset, and compute the best configuration by final macro-F1. Four figures are created: (1) two-panel plot showing training (top) and validation (bottom) loss curves for all weight-decays; (2) validation macro-F1 vs. epoch for all weight-decays; (3) bar chart of final-epoch macro-F1 per weight-decay; (4) confusion-matrix heat-map for the best weight-decay with a subtitle clarifying axes. Each figure is built inside an independent try-except block, saved to working_dir with descriptive names, and closed afterward. We also print a small table of final macro-F1 scores so the numeric results accompany the visuals. Only data present in experiment_data.npy are used\u2014no synthetic values are invented. At most four figures are produced, satisfying the five-figure limit, and the script begins with the required imports and directory setup.", "We first load experiment_data.npy from the working directory and locate the SPR_BENCH results for the three GRU-layer settings.  We extract per-epoch training/validation F1 scores and losses, store them in simple lists, and compute each model\u2019s final validation F1 so we can print a concise comparison table.  Plotting is wrapped in separate try-except blocks to guard against missing keys or corrupted arrays, and every figure is closed whether the block succeeds or fails.  The first figure shows F1 curves for all layer depths (train and val) while the second shows the corresponding loss curves, both titled with the dataset name and experiment description.  We limit ourselves to these two plots to satisfy the \u201c\u22645 figures\u201d rule and save each PNG into working_dir with descriptive filenames.  Basic matplotlib is used with default styles and explicit axis labels, legends, and grid lines for clarity.  After plotting, the script prints the final validation F1 for 1-, 2-, and 3-layer GRUs so users can quickly see which depth performed best.  The code is concise, self-contained, and respects all directory, naming, and cleanup conventions.  No synthetic values are fabricated; everything comes directly from experiment_data.npy.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "batch_size", "dropout_rate", "hidden_size", "embedding_dim", "weight_decay", "num_gru_layers", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will directly load the stored numpy dictionary from the working\ndirectory, iterate over every dataset entry (e.g., \u201cSPR_BENCH\u201d), and gather the\nrecorded lists for training loss, validation loss, and validation macro-F1.  For\neach list it will select the \u201cbest\u201d value\u2014minimum for losses and maximum for\nF1\u2014then print them with explicit names so the output is self-explanatory.  All\nlogic is placed at the top level so the file executes immediately when run.", "The script will locate the working directory, load the saved NumPy file, and\nconvert it back to a regular Python dict (using allow_pickle=True).   For every\ndataset stored under the num-epochs sweep, the code will loop through each\n\u201cepochs_X\u201d run, extract the last training-loss, last validation-loss, last\ntraining-F1, and the best (maximum) validation-F1.   It then prints the dataset\nname once, followed by the hyper-parameter setting and the four clearly-named\nmetrics.   All code lives at global scope so the file executes immediately when\nrun.", "The code loads the saved experiment_data.npy file from the working directory,\ntraverses its nested dictionary structure (tuning type \u2192 dataset \u2192 individual\nruns), and extracts the last recorded training loss, the last recorded\nvalidation loss, and the best (maximum) validation macro-F1 score for every\nlearning-rate run. For every dataset it prints the dataset name first, then\nprints the learning rate and the three clearly-labelled metric values. The\nscript executes immediately on run and conforms to the requested structure (no\nmain-guard, no plots).", "The script will load the saved NumPy dictionary, traverse the nested structure\nto reach each batch-size configuration for the single dataset (SPR_BENCH), and\nthen print the final and best values for training loss, validation loss, and\nvalidation macro-F1. All printing is done with explicit metric names so the\noutput is clear and unambiguous.", "The script will load the saved NumPy file from the working directory, traverse\nthe nested dictionary to reach each dropout-rate run for the SPR_BENCH dataset,\ngrab the last (i.e., final-epoch) entry for every stored series\n(training/validation F1 score and loss), and then print them with fully\nspecified metric names. All logic is executed at the top level so the file runs\nimmediately when invoked.", "The script will load the stored NumPy dictionary, walk through the nested\nstructure (tuning-type \u2192 dataset \u2192 hyper-parameter value), and, for every\ndataset, print the last-epoch values of each recorded metric (train F1 score,\nvalidation F1 score, train loss, validation loss). Metrics are clearly labelled\nand grouped by the dataset name and the corresponding hidden size used in the\nexperiment.", "The script will load the saved numpy dictionary from the working directory,\niterate over each embedding-dimension experiment (treated here as individual\n\u201cdatasets\u201d), and print clearly-named metrics. For every run it reports the best\ntraining F1 score, best validation F1 score, final training loss, and final\nvalidation loss. Nothing is plotted, and the code executes immediately upon\nrunning.", "The code will load the NumPy file, loop over every experiment (each weight-decay\nsetting), then for each dataset it will pull the last recorded entry in every\nstored list (training loss, validation loss, validation macro-F1) and print them\nwith explicit, descriptive labels that satisfy the formatting rules.", "Below is a concise plan followed by a complete script.   The code locates the\nworking directory, loads experiment_data.npy, iterates through the stored\nresults, and prints the final (last-epoch) values for train F1 score, validation\nF1 score, train loss, and validation loss.   Each dataset name is printed first,\nfollowed by clearly-labeled metrics that include the GRU-layer configuration so\nit\u2019s easy to read.", "The script will load the saved NumPy dictionary, traverse the nested structure\nto reach each batch-size configuration for the single dataset (SPR_BENCH), and\nthen print the final and best values for training loss, validation loss, and\nvalidation macro-F1. All printing is done with explicit metric names so the\noutput is clear and unambiguous.", "The script will load the saved NumPy dictionary, traverse the nested structure\nto reach each batch-size configuration for the single dataset (SPR_BENCH), and\nthen print the final and best values for training loss, validation loss, and\nvalidation macro-F1. All printing is done with explicit metric names so the\noutput is clear and unambiguous.", "The script will load the saved NumPy dictionary, traverse the nested structure\nto reach each batch-size configuration for the single dataset (SPR_BENCH), and\nthen print the final and best values for training loss, validation loss, and\nvalidation macro-F1. All printing is done with explicit metric names so the\noutput is clear and unambiguous.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# Helper to pick best values\ndef best_loss(loss_list):\n    return min(loss_list) if loss_list else None\n\n\ndef best_metric(metric_list):\n    return max(metric_list) if metric_list else None\n\n\n# -------------------------------------------------\n# Iterate over datasets and print results\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # Losses\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"  training loss: {best_loss(train_losses):.4f}\")\n    if val_losses:\n        print(f\"  validation loss: {best_loss(val_losses):.4f}\")\n\n    # Validation metrics (macro-F1 in this experiment)\n    val_f1s = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_f1s:\n        print(f\"  validation macro F1 score: {best_metric(val_f1s):.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------\n# 0. Locate and load the experiment_data.npy file\n# -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------\n# 1. Traverse the data structure and collect desired metrics\n# -----------------------------------------------------------\nfor dataset_name, runs in experiment_data.get(\"num_epochs\", {}).items():\n    # Print dataset header\n    print(f\"Dataset: {dataset_name}\")\n\n    for run_name, result in runs.items():\n        # Metrics and losses are stored as lists (one entry per epoch)\n        train_f1_history = result[\"metrics\"][\"train\"]\n        val_f1_history = result[\"metrics\"][\"val\"]\n        train_loss_history = result[\"losses\"][\"train\"]\n        val_loss_history = result[\"losses\"][\"val\"]\n\n        # Final / best values as requested\n        training_f1_score = train_f1_history[-1]  # final epoch\n        validation_f1_score = max(val_f1_history)  # best epoch\n        training_loss = train_loss_history[-1]  # final epoch\n        validation_loss = val_loss_history[-1]  # final epoch\n\n        # -------------------------------------------------------\n        # 2. Print metrics with explicit names\n        # -------------------------------------------------------\n        print(f\"  Run: {run_name}\")\n        print(f\"    training F1 score: {training_f1_score:.4f}\")\n        print(f\"    validation F1 score: {validation_f1_score:.4f}\")\n        print(f\"    training loss: {training_loss:.4f}\")\n        print(f\"    validation loss: {validation_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate through the structure and report metrics\nfor tuning_type, dataset_dict in experiment_data.items():\n    for dataset_name, runs in dataset_dict.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        for run_id, run_content in runs.items():\n            losses = run_content.get(\"losses\", {})\n            metrics = run_content.get(\"metrics\", {})\n\n            # Retrieve required values\n            final_train_loss = losses.get(\"train\", [None])[-1]\n            final_val_loss = losses.get(\"val\", [None])[-1]\n            val_f1_history = metrics.get(\"val\", [])\n            best_val_f1 = max(val_f1_history) if val_f1_history else None\n\n            # Print metrics for this run (learning rate)\n            print(f\"  Learning rate: {run_id}\")\n            if final_train_loss is not None:\n                print(f\"    Training loss (final): {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"    Validation loss (final): {final_val_loss:.4f}\")\n            if best_val_f1 is not None:\n                print(f\"    Validation macro F1 score (best): {best_val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# 1. Iterate through datasets and configurations\nfor dataset_name, cfgs in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for cfg_name, logs in cfgs.items():\n        train_losses = logs[\"losses\"][\"train\"]\n        val_losses = logs[\"losses\"][\"val\"]\n        val_f1s = logs[\"metrics\"][\"val\"]\n\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        final_val_f1 = val_f1s[-1]\n\n        best_val_loss = min(val_losses)\n        best_val_f1 = max(val_f1s)\n\n        print(f\"  Configuration: {cfg_name}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    best validation loss: {best_val_loss:.4f}\")\n        print(f\"    final validation macro F1 score: {final_val_f1:.4f}\")\n        print(f\"    best validation macro F1 score: {best_val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# pretty-print helper\ndef fmt(num, digits=4):\n    return f\"{num:.{digits}f}\"\n\n\n# ------------------------------------------------------------------\n# iterate over datasets and hyper-parameter settings\nfor dataset_name, runs in experiment_data.get(\"dropout_rate\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    for dropout_key, data in runs.items():\n        print(f\"  Dropout rate {dropout_key}:\")\n        # take final epoch value for each series\n        train_f1 = data[\"metrics\"][\"train\"][-1]\n        val_f1 = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n\n        print(f\"    training F1 score: {fmt(train_f1)}\")\n        print(f\"    validation F1 score: {fmt(val_f1)}\")\n        print(f\"    training loss: {fmt(train_loss)}\")\n        print(f\"    validation loss: {fmt(val_loss)}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# iterate over the stored experiments\nfor tuning_type, datasets in experiment_data.items():  # e.g. \"hidden_size\"\n    for dataset_name, hp_dict in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\n=== Dataset: {dataset_name} ===\")\n        for hp_value, results in hp_dict.items():  # e.g. 64, 128, ...\n            # fetch the final values (last epoch) for each metric\n            final_train_f1 = results[\"metrics\"][\"train\"][-1]\n            final_val_f1 = results[\"metrics\"][\"val\"][-1]\n            final_train_loss = results[\"losses\"][\"train\"][-1]\n            final_val_loss = results[\"losses\"][\"val\"][-1]\n\n            print(f\"\\nHyper-parameter ({tuning_type}) value: {hp_value}\")\n            print(f\"train F1 score: {final_train_f1:.4f}\")\n            print(f\"validation F1 score: {final_val_f1:.4f}\")\n            print(f\"train loss: {final_train_loss:.4f}\")\n            print(f\"validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate through each experiment run and print requested metrics\nfor run_key, run_data in experiment_data.get(\"embedding_dim\", {}).items():\n    # Metric extraction\n    train_f1_best = max(run_data[\"metrics\"][\"train\"])  # highest train F1\n    val_f1_best = max(run_data[\"metrics\"][\"val\"])  # highest val  F1\n    train_loss_final = run_data[\"losses\"][\"train\"][-1]  # loss after last epoch\n    val_loss_final = run_data[\"losses\"][\"val\"][-1]  # loss after last epoch\n\n    # Output\n    print(f\"Dataset: {run_key}\")\n    print(f\"training F1 score: {train_f1_best:.4f}\")\n    print(f\"validation F1 score: {val_f1_best:.4f}\")\n    print(f\"training loss: {train_loss_final:.4f}\")\n    print(f\"validation loss: {val_loss_final:.4f}\")\n    print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Establish working directory and load file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# 2\u20135. Extract and print metrics\nfor exp_name, exp_content in experiment_data.items():\n    for dataset_name, ds_content in exp_content.items():\n        print(f\"\\nDataset: {dataset_name}  (experiment: {exp_name})\")\n\n        # Losses ------------------------------------------------------------\n        losses = ds_content.get(\"losses\", {})\n        train_losses = losses.get(\"train\", [])\n        val_losses = losses.get(\"val\", [])\n\n        if train_losses:\n            print(f\"final training loss: {train_losses[-1]:.4f}\")\n        if val_losses:\n            print(f\"final validation loss: {val_losses[-1]:.4f}\")\n\n        # Metrics -----------------------------------------------------------\n        metrics = ds_content.get(\"metrics\", {})\n        train_metrics = metrics.get(\"train\", [])\n        val_metrics = metrics.get(\"val\", [])\n\n        if train_metrics:\n            print(f\"final training Macro-F1 score: {train_metrics[-1]:.4f}\")\n        if val_metrics:\n            print(f\"final validation Macro-F1 score: {val_metrics[-1]:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the results file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# iterate and print metrics\nfor sweep_name, datasets in experiment_data.items():  # e.g. 'num_gru_layers'\n    for dataset_name, configs in datasets.items():  # e.g. 'SPR_BENCH'\n        print(f\"\\nDataset: {dataset_name}\")\n        for config_name, result in configs.items():  # e.g. 'layers_1'\n            metrics = result[\"metrics\"]\n            losses = result[\"losses\"]\n\n            final_train_f1 = metrics[\"train\"][-1]\n            final_val_f1 = metrics[\"val\"][-1]\n            final_train_loss = losses[\"train\"][-1]\n            final_val_loss = losses[\"val\"][-1]\n\n            print(f\"  Configuration: {config_name}\")\n            print(f\"    final train F1 score: {final_train_f1:.4f}\")\n            print(f\"    final validation F1 score: {final_val_f1:.4f}\")\n            print(f\"    final train loss: {final_train_loss:.4f}\")\n            print(f\"    final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# 1. Iterate through datasets and configurations\nfor dataset_name, cfgs in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for cfg_name, logs in cfgs.items():\n        train_losses = logs[\"losses\"][\"train\"]\n        val_losses = logs[\"losses\"][\"val\"]\n        val_f1s = logs[\"metrics\"][\"val\"]\n\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        final_val_f1 = val_f1s[-1]\n\n        best_val_loss = min(val_losses)\n        best_val_f1 = max(val_f1s)\n\n        print(f\"  Configuration: {cfg_name}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    best validation loss: {best_val_loss:.4f}\")\n        print(f\"    final validation macro F1 score: {final_val_f1:.4f}\")\n        print(f\"    best validation macro F1 score: {best_val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# 1. Iterate through datasets and configurations\nfor dataset_name, cfgs in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for cfg_name, logs in cfgs.items():\n        train_losses = logs[\"losses\"][\"train\"]\n        val_losses = logs[\"losses\"][\"val\"]\n        val_f1s = logs[\"metrics\"][\"val\"]\n\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        final_val_f1 = val_f1s[-1]\n\n        best_val_loss = min(val_losses)\n        best_val_f1 = max(val_f1s)\n\n        print(f\"  Configuration: {cfg_name}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    best validation loss: {best_val_loss:.4f}\")\n        print(f\"    final validation macro F1 score: {final_val_f1:.4f}\")\n        print(f\"    best validation macro F1 score: {best_val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# 1. Iterate through datasets and configurations\nfor dataset_name, cfgs in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for cfg_name, logs in cfgs.items():\n        train_losses = logs[\"losses\"][\"train\"]\n        val_losses = logs[\"losses\"][\"val\"]\n        val_f1s = logs[\"metrics\"][\"val\"]\n\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        final_val_f1 = val_f1s[-1]\n\n        best_val_loss = min(val_losses)\n        best_val_f1 = max(val_f1s)\n\n        print(f\"  Configuration: {cfg_name}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    best validation loss: {best_val_loss:.4f}\")\n        print(f\"    final validation macro F1 score: {final_val_f1:.4f}\")\n        print(f\"    best validation macro F1 score: {best_val_f1:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH:', '\\n', '  training loss: 0.6217', '\\n', '  validation loss:\n0.6336', '\\n', '  validation macro F1 score: 0.6855', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Run: epochs_5', '\\n', '    training F1 score:\n0.6391', '\\n', '    validation F1 score: 0.6812', '\\n', '    training loss:\n0.6421', '\\n', '    validation loss: 0.6591', '\\n', '  Run: epochs_10', '\\n', '\ntraining F1 score: 0.6860', '\\n', '    validation F1 score: 0.6819', '\\n', '\ntraining loss: 0.6243', '\\n', '    validation loss: 0.6337', '\\n', '  Run:\nepochs_15', '\\n', '    training F1 score: 0.6193', '\\n', '    validation F1\nscore: 0.6750', '\\n', '    training loss: 0.6747', '\\n', '    validation loss:\n0.6774', '\\n', '  Run: epochs_20', '\\n', '    training F1 score: 0.6568', '\\n',\n'    validation F1 score: 0.6663', '\\n', '    training loss: 0.6518', '\\n', '\nvalidation loss: 0.6602', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Learning rate: 0.0003', '\\n', '    Training\nloss (final): 0.6698', '\\n', '    Validation loss (final): 0.6876', '\\n', '\nValidation macro F1 score (best): 0.5722', '\\n', '  Learning rate: 0.0005',\n'\\n', '    Training loss (final): 0.6552', '\\n', '    Validation loss (final):\n0.6727', '\\n', '    Validation macro F1 score (best): 0.6446', '\\n', '  Learning\nrate: 0.001', '\\n', '    Training loss (final): 0.6303', '\\n', '    Validation\nloss (final): 0.6480', '\\n', '    Validation macro F1 score (best): 0.6659',\n'\\n', '  Learning rate: 0.003', '\\n', '    Training loss (final): 0.6352', '\\n',\n'    Validation loss (final): 0.6429', '\\n', '    Validation macro F1 score\n(best): 0.6668', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: bs_32', '\\n', '    final\ntraining loss: 0.6295', '\\n', '    final validation loss: 0.6436', '\\n', '\nbest validation loss: 0.6389', '\\n', '    final validation macro F1 score:\n0.6840', '\\n', '    best validation macro F1 score: 0.6840', '\\n', '\nConfiguration: bs_64', '\\n', '    final training loss: 0.6773', '\\n', '    final\nvalidation loss: 0.6889', '\\n', '    best validation loss: 0.6640', '\\n', '\nfinal validation macro F1 score: 0.5567', '\\n', '    best validation macro F1\nscore: 0.6480', '\\n', '  Configuration: bs_128', '\\n', '    final training loss:\n0.6535', '\\n', '    final validation loss: 0.6497', '\\n', '    best validation\nloss: 0.6398', '\\n', '    final validation macro F1 score: 0.6596', '\\n', '\nbest validation macro F1 score: 0.6839', '\\n', '  Configuration: bs_256', '\\n',\n'    final training loss: 0.6384', '\\n', '    final validation loss: 0.6420',\n'\\n', '    best validation loss: 0.6420', '\\n', '    final validation macro F1\nscore: 0.6790', '\\n', '    best validation macro F1 score: 0.6790', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Dropout rate 0.0:', '\\n', '    training F1\nscore: 0.6825', '\\n', '    validation F1 score: 0.6855', '\\n', '    training\nloss: 0.6217', '\\n', '    validation loss: 0.6336', '\\n', '  Dropout rate 0.2:',\n'\\n', '    training F1 score: 0.6844', '\\n', '    validation F1 score: 0.6645',\n'\\n', '    training loss: 0.6243', '\\n', '    validation loss: 0.6415', '\\n', '\nDropout rate 0.3:', '\\n', '    training F1 score: 0.6404', '\\n', '    validation\nF1 score: 0.6031', '\\n', '    training loss: 0.6455', '\\n', '    validation\nloss: 0.6656', '\\n', '  Dropout rate 0.5:', '\\n', '    training F1 score:\n0.5110', '\\n', '    validation F1 score: 0.4457', '\\n', '    training loss:\n0.7005', '\\n', '    validation loss: 0.6949', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\n=== Dataset: SPR_BENCH ===', '\\n', '\\nHyper-parameter (hidden_size) value:\n64', '\\n', 'train F1 score: 0.6525', '\\n', 'validation F1 score: 0.6707', '\\n',\n'train loss: 0.6386', '\\n', 'validation loss: 0.6407', '\\n', '\\nHyper-parameter\n(hidden_size) value: 128', '\\n', 'train F1 score: 0.6505', '\\n', 'validation F1\nscore: 0.6565', '\\n', 'train loss: 0.6405', '\\n', 'validation loss: 0.6430',\n'\\n', '\\nHyper-parameter (hidden_size) value: 256', '\\n', 'train F1 score:\n0.6536', '\\n', 'validation F1 score: 0.6502', '\\n', 'train loss: 0.6391', '\\n',\n'validation loss: 0.6463', '\\n', '\\nHyper-parameter (hidden_size) value: 512',\n'\\n', 'train F1 score: 0.6480', '\\n', 'validation F1 score: 0.3872', '\\n',\n'train loss: 0.6446', '\\n', 'validation loss: 0.7629', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: dim_32', '\\n', 'training F1 score: 0.6178', '\\n', 'validation F1\nscore: 0.6734', '\\n', 'training loss: 0.6718', '\\n', 'validation loss: 0.6779',\n'\\n', '\\n', 'Dataset: dim_128', '\\n', 'training F1 score: 0.6424', '\\n',\n'validation F1 score: 0.6597', '\\n', 'training loss: 0.6393', '\\n', 'validation\nloss: 0.6698', '\\n', '\\n', 'Dataset: dim_256', '\\n', 'training F1 score:\n0.6614', '\\n', 'validation F1 score: 0.6317', '\\n', 'training loss: 0.6338',\n'\\n', 'validation loss: 0.6523', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  (experiment: weight_decay_0.0)', '\\n', 'final training\nloss: 0.6217', '\\n', 'final validation loss: 0.6336', '\\n', 'final validation\nMacro-F1 score: 0.6855', '\\n', '\\nDataset: SPR_BENCH  (experiment:\nweight_decay_1e-05)', '\\n', 'final training loss: 0.6273', '\\n', 'final\nvalidation loss: 0.6306', '\\n', 'final validation Macro-F1 score: 0.6797', '\\n',\n'\\nDataset: SPR_BENCH  (experiment: weight_decay_0.0001)', '\\n', 'final training\nloss: 0.6627', '\\n', 'final validation loss: 0.6655', '\\n', 'final validation\nMacro-F1 score: 0.6417', '\\n', '\\nDataset: SPR_BENCH  (experiment:\nweight_decay_0.001)', '\\n', 'final training loss: 0.6438', '\\n', 'final\nvalidation loss: 0.6698', '\\n', 'final validation Macro-F1 score: 0.6238', '\\n',\n'\\nDataset: SPR_BENCH  (experiment: weight_decay_0.01)', '\\n', 'final training\nloss: 0.6780', '\\n', 'final validation loss: 0.6911', '\\n', 'final validation\nMacro-F1 score: 0.5414', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: layers_1', '\\n', '    final\ntrain F1 score: 0.6715', '\\n', '    final validation F1 score: 0.5945', '\\n', '\nfinal train loss: 0.6342', '\\n', '    final validation loss: 0.6838', '\\n', '\nConfiguration: layers_2', '\\n', '    final train F1 score: 0.6554', '\\n', '\nfinal validation F1 score: 0.5736', '\\n', '    final train loss: 0.6537', '\\n',\n'    final validation loss: 0.6811', '\\n', '  Configuration: layers_3', '\\n', '\nfinal train F1 score: 0.6045', '\\n', '    final validation F1 score: 0.6324',\n'\\n', '    final train loss: 0.6675', '\\n', '    final validation loss: 0.6594',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: bs_32', '\\n', '    final\ntraining loss: 0.6227', '\\n', '    final validation loss: 0.6505', '\\n', '\nbest validation loss: 0.6369', '\\n', '    final validation macro F1 score:\n0.6618', '\\n', '    best validation macro F1 score: 0.6637', '\\n', '\nConfiguration: bs_64', '\\n', '    final training loss: 0.6623', '\\n', '    final\nvalidation loss: 0.6702', '\\n', '    best validation loss: 0.6311', '\\n', '\nfinal validation macro F1 score: 0.6317', '\\n', '    best validation macro F1\nscore: 0.6814', '\\n', '  Configuration: bs_128', '\\n', '    final training loss:\n0.6207', '\\n', '    final validation loss: 0.6338', '\\n', '    best validation\nloss: 0.6313', '\\n', '    final validation macro F1 score: 0.6773', '\\n', '\nbest validation macro F1 score: 0.6799', '\\n', '  Configuration: bs_256', '\\n',\n'    final training loss: 0.6494', '\\n', '    final validation loss: 0.6553',\n'\\n', '    best validation loss: 0.6553', '\\n', '    final validation macro F1\nscore: 0.6659', '\\n', '    best validation macro F1 score: 0.6659', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: bs_32', '\\n', '    final\ntraining loss: 0.6336', '\\n', '    final validation loss: 0.6298', '\\n', '\nbest validation loss: 0.6298', '\\n', '    final validation macro F1 score:\n0.6739', '\\n', '    best validation macro F1 score: 0.6739', '\\n', '\nConfiguration: bs_64', '\\n', '    final training loss: 0.6559', '\\n', '    final\nvalidation loss: 0.6621', '\\n', '    best validation loss: 0.6621', '\\n', '\nfinal validation macro F1 score: 0.6536', '\\n', '    best validation macro F1\nscore: 0.6536', '\\n', '  Configuration: bs_128', '\\n', '    final training loss:\n0.6377', '\\n', '    final validation loss: 0.6254', '\\n', '    best validation\nloss: 0.6254', '\\n', '    final validation macro F1 score: 0.6840', '\\n', '\nbest validation macro F1 score: 0.6840', '\\n', '  Configuration: bs_256', '\\n',\n'    final training loss: 0.6463', '\\n', '    final validation loss: 0.6464',\n'\\n', '    best validation loss: 0.6464', '\\n', '    final validation macro F1\nscore: 0.6650', '\\n', '    best validation macro F1 score: 0.6650', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: bs_32', '\\n', '    final\ntraining loss: 0.6261', '\\n', '    final validation loss: 0.6382', '\\n', '\nbest validation loss: 0.6382', '\\n', '    final validation macro F1 score:\n0.6840', '\\n', '    best validation macro F1 score: 0.6840', '\\n', '\nConfiguration: bs_64', '\\n', '    final training loss: 0.6425', '\\n', '    final\nvalidation loss: 0.6804', '\\n', '    best validation loss: 0.6425', '\\n', '\nfinal validation macro F1 score: 0.5401', '\\n', '    best validation macro F1\nscore: 0.6600', '\\n', '  Configuration: bs_128', '\\n', '    final training loss:\n0.6293', '\\n', '    final validation loss: 0.6245', '\\n', '    best validation\nloss: 0.6245', '\\n', '    final validation macro F1 score: 0.6880', '\\n', '\nbest validation macro F1 score: 0.6880', '\\n', '  Configuration: bs_256', '\\n',\n'    final training loss: 0.6638', '\\n', '    final validation loss: 0.6731',\n'\\n', '    best validation loss: 0.6731', '\\n', '    final validation macro F1\nscore: 0.5897', '\\n', '    best validation macro F1 score: 0.6209', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}