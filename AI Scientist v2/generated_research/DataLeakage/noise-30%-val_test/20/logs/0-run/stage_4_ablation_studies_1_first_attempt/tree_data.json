{"edges": [[0, 2], [0, 4], [0, 1], [0, 5], [0, 6], [0, 8], [0, 7], [0, 3], [6, 12], [6, 11], [6, 9], [6, 10]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.14285714285714285, 0.5], [0.2857142857142857, 0.5], [0.42857142857142855, 0.5], [0.5714285714285714, 0.5], [0.7142857142857143, 0.5], [0.8571428571428571, 0.5], [1.0, 0.5], [0.5, 1.0], [0.6428571428571429, 1.0], [0.7857142857142857, 1.0], [0.9285714285714286, 1.0]], "plan": ["We enhance the previous baseline with three ideas: (1) a deeper Transformer (4\nlayers, larger hidden size) plus learnable relative-positional bias to better\ncapture long-range poly-factor rules; (2) label-smoothing Cross-Entropy to\nimprove robustness on ambiguous rules; (3) curriculum training by initially\ndown-weighting examples with high estimated complexity (unique-token count) and\nthen annealing weights toward 1.0 \u2013 this steadies early learning yet emphasises\ndifficult rules later, boosting final Complexity-Weighted Accuracy. We keep the\ndata pipeline, ensure full device handling, track CWA and Macro-F1 every epoch,\nand stop early when validation loss hasn\u2019t improved for 3 epochs. The script\nstores all metrics/predictions under ./working and prints the final scores.", "Ablation name: No Curriculum Complexity Weighting. We reuse the baseline\ndata/model pipeline but drop the curriculum mechanism: every sample always has\nweight = 1 during training, so the loss is simply averaged. All logging, early-\nstopping, saving and evaluation remain identical to ensure a clean comparison.", "Ablation name: Remove Learned Positional Embedding. We reproduce the baseline\ntraining pipeline but instantiate a variant of the CharTransformer in which the\nadditive learned positional embedding (`self.pos`) is completely removed. All\nremaining components (token embedding, encoder layers, curriculum weighting,\noptimizer schedule, saving logic, etc.) are kept identical, ensuring that any\nperformance change is attributable solely to the lack of absolute position\ninformation. Results are stored under the ablation key `\"no_learned_pos_emb\"`\nand written to `experiment_data.npy` for later comparison.", "Ablation name: Replace CLS Token with Mean-Pooling Read-out. The CLS token is\nremoved entirely from the vocabulary and inputs.   All sequences are encoded as\nordinary character-IDs (with padding) and the model\u2019s sequence representation is\nobtained by mean-pooling the final hidden states over all non-padding positions.\nThe pooled vector is LayerNorm\u2019ed and passed to the same linear classifier. The\nrest of the pipeline (data loading, training loop, metric collection, saving)\nstays unchanged.", "Ablation name: No Label Smoothing. We keep every component of the baseline\nfixed, but set label_smoothing = 0.0 in the CrossEntropyLoss to isolate its\ninfluence.  All logging, curriculum weighting, optimiser, scheduler, early-\nstopping logic, etc. remain unchanged.  Results are stored under the ablation\nkey \"no_label_smoothing\" in the mandated experiment_data dictionary and saved to\nworking/experiment_data.npy.  The script is fully self-contained and will\nsynthesise a small toy task if the real SPR-BENCH files are absent.", "Ablation name: Replace AdamW with SGD + Momentum Optimizer. We duplicate the\nbaseline pipeline but switch the optimizer from AdamW to vanilla SGD with\nmomentum 0.9 (lr = 0.1, identical weight-decay). Everything else\u2014model,\nscheduler, curriculum, batch-size, early-stopping, clipping\u2014remains untouched so\ndifferences come solely from removing Adam\u2019s adaptive updates. All plottable\nvalues are stored under the ablation key 'SGD_momentum' and saved to\n\u201cexperiment_data.npy\u201d.", "Ablation name: Remove Final LayerNorm. We ablate the final LayerNorm by\nreplacing it with `nn.Identity`, leaving all other architecture and training\nhyper-parameters unchanged. This lets us isolate the contribution of the last\nnormalization stage while keeping the encoder\u2019s pre-norm layers intact. Metrics,\nlosses and predictions are logged in the required `experiment_data` structure\nunder the key `remove_final_layernorm` and finally saved to\n`experiment_data.npy`.", "Ablation name: Remove Gradient Clipping. We rerun the original training script\nbut simply omit the gradient-clipping step (`torch.nn.utils.clip_grad_norm_`).\nAll other hyper-parameters, curriculum weighting, label smoothing, optimiser and\nscheduler remain untouched so that any change in stability or accuracy is\nattributable solely to removing clipping.  Results, losses and predictions are\nstored under the ablation key `'no_grad_clip'` and saved as\n`experiment_data.npy`.", "Ablation name: Replace Transformer Encoder with Bi-LSTM Backbone. We keep every\ncomponent of the baseline unchanged except for the encoder: the multi-layer\nTransformer is replaced with a single-layer bidirectional LSTM whose two\ndirectional hidden states are concatenated to match the original model size.\nEmbedding and learned positional parameters are kept identical, and\ntraining/optimisation, curriculum weighting, evaluation and data-saving logic\nare reused verbatim. This isolates the effect of self-attention depth versus a\nsimpler recurrent backbone on SPR_BENCH performance.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):  # simple parity rule on \u201cA\u201d\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\n# estimated complexity = number of unique tokens (proxy if not provided)\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):  # simple T5-style bias\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = torch.arange(klen)[None] - torch.arange(qlen)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)  # [qlen,klen,heads]\n\n    def forward(self, qlen, klen):\n        return self.forward__(qlen, klen).permute(2, 0, 1)  # [heads,qlen,klen]\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encs = []\n        for _ in range(layers):\n            encs.append(\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n            )\n        self.enc = nn.ModuleList(encs)\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS token\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# curriculum weight schedule\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))  # linearly to 1 by half epochs\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"weights\": [],\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        # down-weight complex examples early\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(\n        f\"Epoch {epoch+1}: validation_loss = {vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\"\n    )\n    # early stopping\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):  # proxy complexity\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = (\n            torch.arange(klen, device=self.rel.weight.device)[None]\n            - torch.arange(qlen, device=self.rel.weight.device)[:, None]\n        )\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx).permute(2, 0, 1)  # [heads,qlen,klen]\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.rpb = RelPosBias(nhead, MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# ---------------- training params -------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n# ---------------- experiment logging ----------\nexperiment_data = {\n    \"no_curriculum_weighting\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\nexp = experiment_data[\"no_curriculum_weighting\"][\"SPR_BENCH\"]\n\n# ---------------- training loop ---------------\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0, 0\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"]).mean()  # NO curriculum weighting\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    exp[\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    exp[\"losses\"][\"val\"].append(vloss)\n    exp[\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexp[\"predictions\"] = preds\nexp[\"ground_truth\"] = labels\nexp[\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# container for all logged values (ablation key)\nexperiment_data = {\n    \"no_learned_pos_emb\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model (no learned pos) ----------------\nclass RelPosBias(nn.Module):  # defined but unused here\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = torch.arange(klen)[None] - torch.arange(qlen)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)\n\n    def forward(self, qlen, klen):\n        return self.forward__(qlen, klen).permute(2, 0, 1)\n\n\nclass CharTransformerNoPos(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        encs = [\n            nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n            for _ in range(layers)\n        ]\n        self.enc = nn.ModuleList(encs)\n        self.rpb = RelPosBias(\n            nhead, max_dist=MAX_LEN\n        )  # kept for fairness, not injected\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x)  # NO positional embedding added\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS token\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):  # simple linear ramp-up\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformerNoPos(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    # ----- training -----\n    model.train()\n    tot_loss, items = 0.0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ----- validation -----\n    model.eval()\n    vloss, vitems = 0.0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\n# store last predictions etc.\nexperiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# mean_pool_no_cls_ablation.py\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):  # simple parity rule on \u201cA\u201d\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}  # NO <cls> token\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(\n    max(len(ex[\"sequence\"]) for ex in spr[\"train\"]), 127\n)  # 128 would include old <cls>\n\n\ndef enc(seq):\n    ids = [vocab.get(c, 1) for c in seq][:MAX_LEN]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\n# estimated complexity = number of unique tokens (proxy if not provided)\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds = SPRTorch(spr[\"train\"]), SPRTorch(spr[\"dev\"])\ntest_ds = SPRTorch(spr[\"test\"])\n\n\n# ---------------- model -----------------------\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0  # padding mask\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)  # [B, L, d]\n        # mean-pool over non-pad positions\n        lengths = (~mask).sum(1).clamp(min=1).unsqueeze(-1)  # [B,1]\n        pooled = h.masked_fill(mask.unsqueeze(-1), 0.0).sum(1) / lengths\n        return self.fc(pooled)  # [B, num_cls]\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\n# curriculum weight schedule\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ---------------\nexperiment_data = {\n    \"mean_pooling_no_cls\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    # ---- training ----\n    model.train()\n    tot_loss, items = 0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"mean_pooling_no_cls\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    exp_entry = experiment_data[\"mean_pooling_no_cls\"][\"SPR_BENCH\"]\n    exp_entry[\"losses\"][\"val\"].append(vloss)\n    exp_entry[\"metrics\"][\"val\"].append({\"macro_f1\": mf1, \"cwa\": cw})\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\n# store last predictions/labels/weights\nexp_entry[\"predictions\"] = preds\nexp_entry[\"ground_truth\"] = labels\nexp_entry[\"weights\"] = weights\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- paths / device --------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ---------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    out = DatasetDict()\n    out[\"train\"], out[\"dev\"], out[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return out\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing \u2013 building tiny synthetic set\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# --------------- vocab / encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):  # proxy complexity\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, ds):\n        self.d = ds\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model ----------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, q, k):\n        ctx = torch.arange(k)[None] - torch.arange(q)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)  # [q,k,h]\n\n    def forward(self, q, k):\n        return self.forward__(q, k).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, vocab, d=128, heads=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, d, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d))\n        self.encs = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d, heads, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.norm = nn.LayerNorm(d)\n        self.fc = nn.Linear(d, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.encs:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])  # CLS\n\n\n# ---------------- utils ----------------------\ndef cwa(pred, lab, w):\n    return ((pred == lab).astype(float) * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# --------------- training setup --------------\nbatch_size, epochs = 32, 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.0)  # \u2190 ablation: no smoothing\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"no_label_smoothing\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor ep in range(epochs):\n    model.train()\n    running_loss, n_items = 0.0, 0\n    cur_w = curriculum(ep, epochs)\n    for bd in train_loader:\n        bd = {k: v.to(device) for k, v in bd.items()}\n        opt.zero_grad()\n        logits = model(bd[\"input_ids\"])\n        loss = criterion(logits, bd[\"labels\"])\n        loss = (loss * torch.where(bd[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        running_loss += loss.item() * bd[\"labels\"].size(0)\n        n_items += bd[\"labels\"].size(0)\n    train_loss = running_loss / n_items\n    experiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ----- validation -----\n    model.eval()\n    v_loss, v_count = 0.0, 0\n    preds, labs, wts = [], [], []\n    with torch.no_grad():\n        for bd in dev_loader:\n            bd = {k: v.to(device) for k, v in bd.items()}\n            out = model(bd[\"input_ids\"])\n            loss = criterion(out, bd[\"labels\"])\n            v_loss += loss.item() * bd[\"labels\"].size(0)\n            v_count += bd[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = bd[\"labels\"].cpu().numpy()\n            w = bd[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labs.extend(l)\n            wts.extend(w)\n    v_loss /= v_count\n    mf1 = f1_score(labs, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labs), np.array(wts))\n    experiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {ep+1}: val_loss={v_loss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    # early stopping\n    if v_loss < best_val - 1e-4:\n        best_val, bad_epochs = v_loss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"][\"ground_truth\"] = labs\nexperiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"][\"weights\"] = wts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# -------------------------------------------------------------\n# Ablation : Replace AdamW with SGD + Momentum\n# -------------------------------------------------------------\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- experiment bookkeeping ---------------------\nexperiment_data = {\n    \"SGD_momentum\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- basic set-up -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading -------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ---------------------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model --------------------------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, ql, kl):\n        ctx = torch.arange(kl)[None] - torch.arange(ql)[:, None]\n        ctx = ctx.clamp(-self.max_dist, self.max_dist) + self.max_dist\n        return self.rel(ctx)\n\n    def forward(self, ql, kl):\n        return self.forward__(ql, kl).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.rpb = RelPosBias(nhead, MAX_LEN)\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])\n\n\n# ---------------- utils --------------------------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ------------------------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n\n# --- Ablation: SGD + momentum (replacing AdamW) --------------\nopt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0.0, 0\n    cur_w = curriculum(epoch, epochs)\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"SGD_momentum\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ------ validation ---------------------------------------\n    model.eval()\n    vloss, vitems = 0.0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"SGD_momentum\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SGD_momentum\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"SGD_momentum\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SGD_momentum\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"SGD_momentum\"][\"SPR_BENCH\"][\"weights\"] = weights\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# remove_final_layernorm_ablation.py\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = (torch.arange(klen)[None] - torch.arange(qlen)[:, None]).clamp(\n            -self.max_dist, self.max_dist\n        ) + self.max_dist\n        return self.rel(ctx).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.Identity()  # Ablation: removed final LayerNorm\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"remove_final_layernorm\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0.0, 0\n    cur_w = curriculum(epoch, epochs)\n    for bd in train_loader:\n        bd = {k: v.to(device) for k, v in bd.items()}\n        opt.zero_grad()\n        logits = model(bd[\"input_ids\"])\n        loss = criterion(logits, bd[\"labels\"])\n        loss = (loss * torch.where(bd[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * bd[\"labels\"].size(0)\n        items += bd[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0.0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for bd in dev_loader:\n            bd = {k: v.to(device) for k, v in bd.items()}\n            out = model(bd[\"input_ids\"])\n            loss = criterion(out, bd[\"labels\"])\n            vloss += loss.item() * bd[\"labels\"].size(0)\n            vitems += bd[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = bd[\"labels\"].cpu().numpy()\n            w = bd[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        vloss\n    )\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- paths / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using\", device)\n\n\n# ---------- data ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing -> synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- vocab / encoding ----------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    return ids + [0] * (MAX_LEN - len(ids))\n\n\ndef complexity(ex):  # proxy\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, d):\n        self.d = d\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, i):\n        ex = self.d[i]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------- model ----------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.m = max_dist\n\n    def forward(self, q, k):\n        ctx = torch.arange(k)[None] - torch.arange(q)[:, None]\n        ctx = ctx.clamp(-self.m, self.m) + self.m\n        return self.rel(ctx).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d=128, h=8, layers=4, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d, h, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.norm = nn.LayerNorm(d)\n        self.fc = nn.Linear(d, 2)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        return self.fc(self.norm(h)[:, 0])\n\n\ndef cwa(pred, lab, w):\n    return ((pred == lab).astype(float) * w).sum() / w.sum()\n\n\ndef curriculum(ep, total):\n    return min(1.0, (ep + 1) / (total / 2))\n\n\n# ---------- training ----------\nbatch, epochs = 32, 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"no_grad_clip\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor ep in range(epochs):\n    model.train()\n    tot_loss = items = 0\n    cur_w = curriculum(ep, epochs)\n    for bd in train_loader:\n        bd = {k: v.to(device) for k, v in bd.items()}\n        opt.zero_grad()\n        logits = model(bd[\"input_ids\"])\n        loss = criterion(logits, bd[\"labels\"])\n        loss = (loss * torch.where(bd[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        # ---- NO GRADIENT CLIPPING HERE ----\n        opt.step()\n        tot_loss += loss.item() * bd[\"labels\"].size(0)\n        items += bd[\"labels\"].size(0)\n    tr_loss = tot_loss / items\n    experiment_data[\"no_grad_clip\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n\n    # validation\n    model.eval()\n    vloss = vitems = 0\n    preds = []\n    labs = []\n    ws = []\n    with torch.no_grad():\n        for bd in dev_loader:\n            bd = {k: v.to(device) for k, v in bd.items()}\n            out = model(bd[\"input_ids\"])\n            loss = criterion(out, bd[\"labels\"])\n            vloss += loss.item() * bd[\"labels\"].size(0)\n            vitems += bd[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = bd[\"labels\"].cpu().numpy()\n            w = bd[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labs.extend(l)\n            ws.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labs, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labs), np.array(ws))\n    experiment_data[\"no_grad_clip\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"no_grad_clip\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Ep {ep+1}: val_loss={vloss:.4f} Macro-F1={mf1:.3f} CWA={cw:.3f}\")\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"no_grad_clip\"][\"SPR_BENCH\"][\"predictions\"] = np.array(preds)\nexperiment_data[\"no_grad_clip\"][\"SPR_BENCH\"][\"ground_truth\"] = np.array(labs)\nexperiment_data[\"no_grad_clip\"][\"SPR_BENCH\"][\"weights\"] = np.array(ws)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# bi_lstm_ablation.py\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load SPR_BENCH or synth ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- vocab ----------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------- Bi-LSTM backbone ----------\nclass CharBiLSTM(nn.Module):\n    def __init__(self, v, d_model=128, num_cls=2, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        hidden = d_model // 2  # bidirectional => hidden*2 = d_model\n        self.lstm = nn.LSTM(\n            d_model,\n            hidden,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout,\n        )\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        lens = (~mask).sum(1).clamp(min=1).cpu()\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        packed = nn.utils.rnn.pack_padded_sequence(\n            h, lens, batch_first=True, enforce_sorted=False\n        )\n        _, (h_n, _) = self.lstm(packed)  # h_n: (2,batch,hidden)\n        h_cat = torch.cat([h_n[0], h_n[1]], dim=-1)  # (batch, d_model)\n        h_cat = self.norm(h_cat)\n        return self.fc(h_cat)\n\n\n# ---------- utils ----------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------- training settings ----------\nbatch = 32\nepochs = 12\nmodel = CharBiLSTM(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"bi_lstm_backbone\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    cur_w = curriculum(epoch, epochs)\n    tot_loss, items = 0, 0\n    for batch_d in train_loader:\n        batch_d = {k: v.to(device) for k, v in batch_d.items()}\n        opt.zero_grad()\n        logits = model(batch_d[\"input_ids\"])\n        loss = criterion(logits, batch_d[\"labels\"])\n        loss = (loss * torch.where(batch_d[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * batch_d[\"labels\"].size(0)\n        items += batch_d[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"bi_lstm_backbone\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for batch_d in dev_loader:\n            batch_d = {k: v.to(device) for k, v in batch_d.items()}\n            out = model(batch_d[\"input_ids\"])\n            loss = criterion(out, batch_d[\"labels\"])\n            vloss += loss.item() * batch_d[\"labels\"].size(0)\n            vitems += batch_d[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = batch_d[\"labels\"].cpu().numpy()\n            w = batch_d[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"bi_lstm_backbone\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"bi_lstm_backbone\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n    if vloss < best_val - 1e-4:\n        best_val = vloss\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"bi_lstm_backbone\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"bi_lstm_backbone\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"bi_lstm_backbone\"][\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# remove_final_layernorm_ablation.py\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = (torch.arange(klen)[None] - torch.arange(qlen)[:, None]).clamp(\n            -self.max_dist, self.max_dist\n        ) + self.max_dist\n        return self.rel(ctx).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.Identity()  # Ablation: removed final LayerNorm\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"remove_final_layernorm\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0.0, 0\n    cur_w = curriculum(epoch, epochs)\n    for bd in train_loader:\n        bd = {k: v.to(device) for k, v in bd.items()}\n        opt.zero_grad()\n        logits = model(bd[\"input_ids\"])\n        loss = criterion(logits, bd[\"labels\"])\n        loss = (loss * torch.where(bd[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * bd[\"labels\"].size(0)\n        items += bd[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0.0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for bd in dev_loader:\n            bd = {k: v.to(device) for k, v in bd.items()}\n            out = model(bd[\"input_ids\"])\n            loss = criterion(out, bd[\"labels\"])\n            vloss += loss.item() * bd[\"labels\"].size(0)\n            vitems += bd[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = bd[\"labels\"].cpu().numpy()\n            w = bd[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        vloss\n    )\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# remove_final_layernorm_ablation.py\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = (torch.arange(klen)[None] - torch.arange(qlen)[:, None]).clamp(\n            -self.max_dist, self.max_dist\n        ) + self.max_dist\n        return self.rel(ctx).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.Identity()  # Ablation: removed final LayerNorm\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"remove_final_layernorm\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0.0, 0\n    cur_w = curriculum(epoch, epochs)\n    for bd in train_loader:\n        bd = {k: v.to(device) for k, v in bd.items()}\n        opt.zero_grad()\n        logits = model(bd[\"input_ids\"])\n        loss = criterion(logits, bd[\"labels\"])\n        loss = (loss * torch.where(bd[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * bd[\"labels\"].size(0)\n        items += bd[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0.0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for bd in dev_loader:\n            bd = {k: v.to(device) for k, v in bd.items()}\n            out = model(bd[\"input_ids\"])\n            loss = criterion(out, bd[\"labels\"])\n            vloss += loss.item() * bd[\"labels\"].size(0)\n            vitems += bd[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = bd[\"labels\"].cpu().numpy()\n            w = bd[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        vloss\n    )\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# remove_final_layernorm_ablation.py\nimport os, pathlib, math, time, random, string, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH missing: synthesising toy data\")\n\n    def synth(n):\n        for i in range(n):\n            seq = \"\".join(\n                random.choices(string.ascii_uppercase[:12], k=random.randint(5, 15))\n            )\n            yield {\"id\": i, \"sequence\": seq, \"label\": int(seq.count(\"A\") % 2 == 0)}\n\n    spr = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(4000))}, split=\"train\"\n            ),\n            \"dev\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n            \"test\": load_dataset(\n                \"json\", data_files={\"train\": list(synth(800))}, split=\"train\"\n            ),\n        }\n    )\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab + encoding ------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1, \"<cls>\": 2}\nfor ex in spr[\"train\"]:\n    for ch in ex[\"sequence\"]:\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\nvsize = len(vocab)\nMAX_LEN = min(max(len(ex[\"sequence\"]) for ex in spr[\"train\"]) + 1, 128)\n\n\ndef enc(seq):\n    ids = [vocab[\"<cls>\"]] + [vocab.get(c, 1) for c in seq][: MAX_LEN - 1]\n    ids += [0] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef complexity(ex):\n    return float(len(set(ex[\"sequence\"])))\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf):\n        self.d = hf\n\n    def __len__(self):\n        return len(self.d)\n\n    def __getitem__(self, idx):\n        ex = self.d[idx]\n        return {\n            \"input_ids\": torch.tensor(enc(ex[\"sequence\"]), dtype=torch.long),\n            \"labels\": torch.tensor(int(ex[\"label\"]), dtype=torch.long),\n            \"weights\": torch.tensor(\n                float(ex.get(\"complexity\", complexity(ex))), dtype=torch.float\n            ),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorch(spr[\"train\"]),\n    SPRTorch(spr[\"dev\"]),\n    SPRTorch(spr[\"test\"]),\n)\n\n\n# ---------------- model -----------------------\nclass RelPosBias(nn.Module):\n    def __init__(self, heads, max_dist=128):\n        super().__init__()\n        self.rel = nn.Embedding(2 * max_dist, heads)\n        self.max_dist = max_dist\n\n    def forward__(self, qlen, klen):\n        ctx = (torch.arange(klen)[None] - torch.arange(qlen)[:, None]).clamp(\n            -self.max_dist, self.max_dist\n        ) + self.max_dist\n        return self.rel(ctx).permute(2, 0, 1)\n\n\nclass CharTransformer(nn.Module):\n    def __init__(self, v, d_model=128, nhead=8, layers=4, num_cls=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(v, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        self.enc = nn.ModuleList(\n            [\n                nn.TransformerEncoderLayer(d_model, nhead, ff, 0.1, batch_first=True)\n                for _ in range(layers)\n            ]\n        )\n        self.rpb = RelPosBias(nhead, max_dist=MAX_LEN)\n        self.norm = nn.Identity()  # Ablation: removed final LayerNorm\n        self.fc = nn.Linear(d_model, num_cls)\n\n    def forward(self, x):\n        mask = x == 0\n        h = self.emb(x) + self.pos[:, : x.size(1)]\n        for layer in self.enc:\n            h = layer(h, src_key_padding_mask=mask)\n        h = self.norm(h)\n        return self.fc(h[:, 0])\n\n\n# ---------------- utils -----------------------\ndef cwa(pred, lab, w):\n    correct = (pred == lab).astype(float)\n    return (correct * w).sum() / w.sum()\n\n\ndef curriculum(epoch, total):\n    return min(1.0, (epoch + 1) / (total / 2))\n\n\n# ---------------- training loop ---------------\nbatch = 32\nepochs = 12\nmodel = CharTransformer(vsize).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\nopt = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-2)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\ntrain_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nexperiment_data = {\n    \"remove_final_layernorm\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"weights\": [],\n        }\n    }\n}\n\nbest_val, bad_epochs = 1e9, 0\nfor epoch in range(epochs):\n    model.train()\n    tot_loss, items = 0.0, 0\n    cur_w = curriculum(epoch, epochs)\n    for bd in train_loader:\n        bd = {k: v.to(device) for k, v in bd.items()}\n        opt.zero_grad()\n        logits = model(bd[\"input_ids\"])\n        loss = criterion(logits, bd[\"labels\"])\n        loss = (loss * torch.where(bd[\"weights\"] > 5, cur_w, 1.0)).mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        tot_loss += loss.item() * bd[\"labels\"].size(0)\n        items += bd[\"labels\"].size(0)\n    train_loss = tot_loss / items\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n\n    # ---- validation ----\n    model.eval()\n    vloss, vitems = 0.0, 0\n    preds, labels, weights = [], [], []\n    with torch.no_grad():\n        for bd in dev_loader:\n            bd = {k: v.to(device) for k, v in bd.items()}\n            out = model(bd[\"input_ids\"])\n            loss = criterion(out, bd[\"labels\"])\n            vloss += loss.item() * bd[\"labels\"].size(0)\n            vitems += bd[\"labels\"].size(0)\n            p = out.argmax(1).cpu().numpy()\n            l = bd[\"labels\"].cpu().numpy()\n            w = bd[\"weights\"].cpu().numpy()\n            preds.extend(p)\n            labels.extend(l)\n            weights.extend(w)\n    vloss /= vitems\n    mf1 = f1_score(labels, preds, average=\"macro\")\n    cw = cwa(np.array(preds), np.array(labels), np.array(weights))\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        vloss\n    )\n    experiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"macro_f1\": mf1, \"cwa\": cw}\n    )\n    print(f\"Epoch {epoch+1}: val_loss={vloss:.4f} | Macro-F1={mf1:.3f} | CWA={cw:.3f}\")\n\n    if vloss < best_val - 1e-4:\n        best_val, bad_epochs = vloss, 0\n    else:\n        bad_epochs += 1\n    if bad_epochs >= 3:\n        print(\"Early stopping.\")\n        break\n    sched.step()\n\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"ground_truth\"] = labels\nexperiment_data[\"remove_final_layernorm\"][\"SPR_BENCH\"][\"weights\"] = weights\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: validation_loss = 0.6511 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Epoch\n2: validation_loss = 0.6362 | Macro-F1=0.672 | CWA=0.672', '\\n', 'Epoch 3:\nvalidation_loss = 0.6652 | Macro-F1=0.637 | CWA=0.642', '\\n', 'Epoch 4:\nvalidation_loss = 0.6425 | Macro-F1=0.663 | CWA=0.664', '\\n', 'Epoch 5:\nvalidation_loss = 0.6335 | Macro-F1=0.684 | CWA=0.684', '\\n', 'Epoch 6:\nvalidation_loss = 0.6323 | Macro-F1=0.684 | CWA=0.684', '\\n', 'Epoch 7:\nvalidation_loss = 0.6342 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 8:\nvalidation_loss = 0.6399 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Epoch 9:\nvalidation_loss = 0.6272 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Epoch 10:\nvalidation_loss = 0.6279 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Epoch 11:\nvalidation_loss = 0.6310 | Macro-F1=0.694 | CWA=0.694', '\\n', 'Epoch 12:\nvalidation_loss = 0.6266 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Saved metrics\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Execution time: 20 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 159579.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 137095.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 234961.85\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1:\nval_loss=0.6521 | Macro-F1=0.694 | CWA=0.694', '\\n', 'Epoch 2: val_loss=0.6369 |\nMacro-F1=0.688 | CWA=0.688', '\\n', 'Epoch 3: val_loss=0.6440 | Macro-F1=0.659 |\nCWA=0.660', '\\n', 'Epoch 4: val_loss=0.6381 | Macro-F1=0.665 | CWA=0.666', '\\n',\n'Epoch 5: val_loss=0.6568 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Early\nstopping.', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-16/working/experiment_data.npy', '\\n', 'Execution time:\n6 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 115888.76\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 155759.95\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 246419.36\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1:\nval_loss=0.6444 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 2: val_loss=0.6357 |\nMacro-F1=0.676 | CWA=0.676', '\\n', 'Epoch 3: val_loss=0.6652 | Macro-F1=0.641 |\nCWA=0.646', '\\n', 'Epoch 4: val_loss=0.6362 | Macro-F1=0.671 | CWA=0.672', '\\n',\n'Epoch 5: val_loss=0.6365 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Early\nstopping.', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-17/working/experiment_data.npy', '\\n', 'Execution time:\n6 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 129935.07\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 87210.55\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 159436.80\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1:\nval_loss=0.6421 | Macro-F1=0.665 | CWA=0.666', '\\n', 'Epoch 2: val_loss=0.6458 |\nMacro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 3: val_loss=0.6619 | Macro-F1=0.659 |\nCWA=0.662', '\\n', 'Epoch 4: val_loss=0.6431 | Macro-F1=0.686 | CWA=0.686', '\\n',\n'Early stopping.', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-18/working/experiment_data.npy', '\\n', 'Execution time:\n5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 144218.41\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 149189.16\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 215523.56\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch 1:\nval_loss=0.6361 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 2: val_loss=0.6769 |\nMacro-F1=0.655 | CWA=0.658', '\\n', 'Epoch 3: val_loss=0.6413 | Macro-F1=0.680 |\nCWA=0.680', '\\n', 'Epoch 4: val_loss=0.6295 | Macro-F1=0.676 | CWA=0.676', '\\n',\n'Epoch 5: val_loss=0.6386 | Macro-F1=0.688 | CWA=0.688', '\\n', 'Epoch 6:\nval_loss=0.6438 | Macro-F1=0.673 | CWA=0.674', '\\n', 'Epoch 7: val_loss=0.6245 |\nMacro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 8: val_loss=0.6260 | Macro-F1=0.690 |\nCWA=0.690', '\\n', 'Epoch 9: val_loss=0.6293 | Macro-F1=0.696 | CWA=0.696', '\\n',\n'Epoch 10: val_loss=0.6227 | Macro-F1=0.692 | CWA=0.692', '\\n', 'Epoch 11:\nval_loss=0.6220 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 12: val_loss=0.6218\n| Macro-F1=0.692 | CWA=0.692', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n19/working/experiment_data.npy', '\\n', 'Execution time: 17 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: val_loss=0.6890 | Macro-F1=0.335 | CWA=0.504', '\\n', 'Epoch 2:\nval_loss=0.6986 | Macro-F1=0.332 | CWA=0.496', '\\n', 'Epoch 3: val_loss=0.6933 |\nMacro-F1=0.332 | CWA=0.496', '\\n', 'Epoch 4: val_loss=0.6960 | Macro-F1=0.335 |\nCWA=0.504', '\\n', 'Early stopping.', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n16/working/experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: val_loss=0.6419 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 2:\nval_loss=0.6350 | Macro-F1=0.680 | CWA=0.680', '\\n', 'Epoch 3: val_loss=0.6601 |\nMacro-F1=0.642 | CWA=0.646', '\\n', 'Epoch 4: val_loss=0.6335 | Macro-F1=0.680 |\nCWA=0.680', '\\n', 'Epoch 5: val_loss=0.6340 | Macro-F1=0.684 | CWA=0.684', '\\n',\n'Epoch 6: val_loss=0.6383 | Macro-F1=0.671 | CWA=0.672', '\\n', 'Epoch 7:\nval_loss=0.6313 | Macro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 8: val_loss=0.6309 |\nMacro-F1=0.696 | CWA=0.696', '\\n', 'Epoch 9: val_loss=0.6265 | Macro-F1=0.696 |\nCWA=0.696', '\\n', 'Epoch 10: val_loss=0.6288 | Macro-F1=0.696 | CWA=0.696',\n'\\n', 'Epoch 11: val_loss=0.6307 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Epoch 12:\nval_loss=0.6264 | Macro-F1=0.696 | CWA=0.696', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n18/working/experiment_data.npy', '\\n', 'Execution time: 12 seconds seconds (time\nlimit is 30 minutes).']", "['Using', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Ep 1: val_loss=0.6430 Macro-F1=0.686 CWA=0.686', '\\n', 'Ep 2: val_loss=0.6613\nMacro-F1=0.662 CWA=0.664', '\\n', 'Ep 3: val_loss=0.6429 Macro-F1=0.686\nCWA=0.686', '\\n', 'Ep 4: val_loss=0.6346 Macro-F1=0.680 CWA=0.680', '\\n', 'Ep 5:\nval_loss=0.6434 Macro-F1=0.670 CWA=0.670', '\\n', 'Ep 6: val_loss=0.6490\nMacro-F1=0.684 CWA=0.684', '\\n', 'Ep 7: val_loss=0.6288 Macro-F1=0.690\nCWA=0.690', '\\n', 'Ep 8: val_loss=0.6287 Macro-F1=0.690 CWA=0.690', '\\n', 'Ep 9:\nval_loss=0.6330 Macro-F1=0.696 CWA=0.696', '\\n', 'Ep 10: val_loss=0.6269\nMacro-F1=0.690 CWA=0.690', '\\n', 'Ep 11: val_loss=0.6270 Macro-F1=0.692\nCWA=0.692', '\\n', 'Ep 12: val_loss=0.6268 Macro-F1=0.696 CWA=0.696', '\\n',\n'Saved to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n17/working/experiment_data.npy', '\\n', 'Execution time: 14 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Epoch 1: val_loss=0.6452 | Macro-F1=0.680 | CWA=0.682', '\\n',\n'Epoch 2: val_loss=0.6480 | Macro-F1=0.664 | CWA=0.664', '\\n', 'Epoch 3:\nval_loss=0.6586 | Macro-F1=0.645 | CWA=0.646', '\\n', 'Epoch 4: val_loss=0.6488 |\nMacro-F1=0.672 | CWA=0.672', '\\n', 'Early stopping.', '\\n', 'Saved metrics to',\n' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n19/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: val_loss=0.6448 | Macro-F1=0.664 | CWA=0.666', '\\n', 'Epoch 2:\nval_loss=0.6866 | Macro-F1=0.495 | CWA=0.546', '\\n', 'Epoch 3: val_loss=0.6572 |\nMacro-F1=0.658 | CWA=0.660', '\\n', 'Epoch 4: val_loss=0.6394 | Macro-F1=0.667 |\nCWA=0.668', '\\n', 'Epoch 5: val_loss=0.6410 | Macro-F1=0.674 | CWA=0.674', '\\n',\n'Epoch 6: val_loss=0.6315 | Macro-F1=0.684 | CWA=0.684', '\\n', 'Epoch 7:\nval_loss=0.6509 | Macro-F1=0.675 | CWA=0.676', '\\n', 'Epoch 8: val_loss=0.6387 |\nMacro-F1=0.686 | CWA=0.686', '\\n', 'Epoch 9: val_loss=0.6350 | Macro-F1=0.694 |\nCWA=0.694', '\\n', 'Early stopping.', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n16/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: val_loss=0.6399 | Macro-F1=0.669 | CWA=0.670', '\\n', 'Epoch 2:\nval_loss=0.6751 | Macro-F1=0.648 | CWA=0.654', '\\n', 'Epoch 3: val_loss=0.6725 |\nMacro-F1=0.599 | CWA=0.614', '\\n', 'Epoch 4: val_loss=0.6357 | Macro-F1=0.672 |\nCWA=0.672', '\\n', 'Epoch 5: val_loss=0.6583 | Macro-F1=0.674 | CWA=0.674', '\\n',\n'Epoch 6: val_loss=0.6340 | Macro-F1=0.692 | CWA=0.692', '\\n', 'Epoch 7:\nval_loss=0.6350 | Macro-F1=0.682 | CWA=0.682', '\\n', 'Epoch 8: val_loss=0.6298 |\nMacro-F1=0.692 | CWA=0.692', '\\n', 'Epoch 9: val_loss=0.6311 | Macro-F1=0.690 |\nCWA=0.690', '\\n', 'Epoch 10: val_loss=0.6296 | Macro-F1=0.690 | CWA=0.690',\n'\\n', 'Epoch 11: val_loss=0.6305 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 12:\nval_loss=0.6292 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n18/working/experiment_data.npy', '\\n', 'Execution time: 12 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Epoch 1: val_loss=0.6704 | Macro-F1=0.623 | CWA=0.634', '\\n', 'Epoch 2:\nval_loss=0.6374 | Macro-F1=0.670 | CWA=0.670', '\\n', 'Epoch 3: val_loss=0.6406 |\nMacro-F1=0.688 | CWA=0.688', '\\n', 'Epoch 4: val_loss=0.6385 | Macro-F1=0.663 |\nCWA=0.664', '\\n', 'Epoch 5: val_loss=0.6318 | Macro-F1=0.690 | CWA=0.690', '\\n',\n'Epoch 6: val_loss=0.6336 | Macro-F1=0.690 | CWA=0.690', '\\n', 'Epoch 7:\nval_loss=0.6356 | Macro-F1=0.678 | CWA=0.678', '\\n', 'Epoch 8: val_loss=0.6375 |\nMacro-F1=0.686 | CWA=0.686', '\\n', 'Early stopping.', '\\n', 'Saved metrics to',\n' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_22-28-\n20_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n17/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["", "The execution of the training script was successful without any errors or bugs.\nThe training process ran as expected, and the model trained and validated on the\nsynthetic dataset. Early stopping was triggered after 5 epochs due to the\nvalidation loss not improving for 3 consecutive epochs. The experiment data,\nincluding metrics and losses, was saved successfully. While the results are not\nhighly impressive, this could be due to the synthetic dataset being overly\nsimplistic or not representative of the actual SPR_BENCH benchmark. No\nmodifications are needed to the script at this point.", "The execution of the training script completed successfully without any bugs.\nThe training and validation processes were conducted as expected, with metrics\nsuch as validation loss, Macro-F1, and CWA being logged. The script also\nimplemented early stopping after three consecutive validation epochs without\nimprovement, which is a valid approach. Metrics and predictions were saved\ncorrectly for further analysis.", "", "", "The execution output indicates that the model training did not progress\neffectively. The validation loss and metrics (Macro-F1 and CWA) did not improve\nsignificantly over the epochs, and early stopping was triggered after just four\nepochs. This suggests that the ablation study using SGD + Momentum for\noptimization might not be suitable for this task. Possible fixes include tuning\nthe hyperparameters of SGD (e.g., learning rate, momentum) or reverting to AdamW\noptimization to compare performance. Additionally, analyzing the model's\narchitecture and loss function for compatibility with the dataset could help\nidentify potential issues.", "", "", "The training script executed successfully without any errors or bugs. The model\nutilized a Bi-LSTM architecture and trained on the SPR_BENCH dataset. The\ntraining process included curriculum learning and early stopping. Early stopping\nwas triggered after three consecutive epochs without improvement in validation\nloss. The final validation loss, Macro-F1 score, and CWA were reported for each\nepoch. Metrics and predictions were saved successfully. The execution time was\nwell within the limit.", "The execution of the training script completed successfully without any bugs.\nThe model was trained on the SPR_BENCH dataset, and the validation loss,\nMacro-F1, and CWA metrics were reported for each epoch. Early stopping was\ntriggered after 9 epochs due to no significant improvement in validation loss.\nAll metrics and predictions were saved successfully. The model appears to\nfunction as expected, and no issues were encountered during execution.", "", "The execution of the training script completed successfully without any bugs.\nThe model trained on the SPR_BENCH dataset, and the validation loss, macro F1\nscore, and CWA metrics were logged for each epoch. Early stopping was applied\nafter 8 epochs due to no significant improvement in validation loss. Experiment\ndata was saved successfully. The results show reasonable performance and the\nscript performed as expected.", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.61965, "best_value": 0.61965}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.626568, "best_value": 0.626568}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695981, "best_value": 0.695981}]}, {"metric_name": "validation curriculum-weighted accuracy", "lower_is_better": false, "description": "The curriculum-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Represents the loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.635918, "best_value": 0.635918}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Represents the loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.636898, "best_value": 0.636898}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score for validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695981, "best_value": 0.695981}]}, {"metric_name": "validation CWA score", "lower_is_better": false, "description": "CWA score for validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training; lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5332, "best_value": 0.5332}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation set; lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6357, "best_value": 0.6357}]}, {"metric_name": "validation macro-F1", "lower_is_better": false, "description": "Macro-averaged F1 score for the validation set; higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.688, "best_value": 0.688}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "CWA metric for the validation set; higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.688, "best_value": 0.688}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset during the final iteration.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4244226856231689, "best_value": 0.4244226856231689}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset. Lower values indicate better model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6420555820465088, "best_value": 0.6420555820465088}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score calculated on the validation dataset. Higher values indicate better model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6858982310268527, "best_value": 0.6858982310268527}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "CWA (Class Weighted Accuracy) calculated on the validation dataset. Higher values indicate better model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.686, "best_value": 0.686}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6114, "best_value": 0.6114}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6218, "best_value": 0.6218}]}, {"metric_name": "validation macro_f1", "lower_is_better": false, "description": "The macro-averaged F1 score during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "The cwa metric during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.464353, "best_value": 0.464353}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.689036, "best_value": 0.689036}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.335106, "best_value": 0.335106}]}, {"metric_name": "validation class-weighted accuracy", "lower_is_better": false, "description": "The class-weighted accuracy during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.504, "best_value": 0.504}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.622285, "best_value": 0.622285}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.626422, "best_value": 0.626422}]}, {"metric_name": "validation macro_f1", "lower_is_better": false, "description": "Final validation macro F1 score of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695981, "best_value": 0.695981}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "Final validation class-weighted accuracy of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss observed.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.620159, "best_value": 0.620159}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Best validation loss observed during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.626774, "best_value": 0.626774}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Best macro F1 score observed during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695981, "best_value": 0.695981}]}, {"metric_name": "validation class-weighted accuracy", "lower_is_better": false, "description": "Best class-weighted accuracy observed during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.420365, "best_value": 0.420365}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.645231, "best_value": 0.645231}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.680053, "best_value": 0.680053}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correctly Weighted Accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.682, "best_value": 0.682}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.623832, "best_value": 0.623832}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation set, used to evaluate the model's performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.634971, "best_value": 0.634971}]}, {"metric_name": "validation macro_f1", "lower_is_better": false, "description": "The macro F1 score on the validation set, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.693989, "best_value": 0.693989}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "The CWA metric on the validation set, indicating some specific performance measure.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.694, "best_value": 0.694}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.621343, "best_value": 0.621343}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.629241, "best_value": 0.629241}]}, {"metric_name": "validation macro_f1", "lower_is_better": false, "description": "The macro F1 score on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.689939, "best_value": 0.689939}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "The class-wise accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.69, "best_value": 0.69}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.628429, "best_value": 0.628429}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.637515, "best_value": 0.637515}]}, {"metric_name": "validation macro_f1", "lower_is_better": false, "description": "Macro-averaged F1 score calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.685898, "best_value": 0.685898}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "Classification Weighted Accuracy (CWA) calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.686, "best_value": 0.686}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, true, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_macro_f1.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_cwa.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_weight_hist.png"], ["../../logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_CWA_curve.png", "../../logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_weight_histogram.png", "../../logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_weight_correct_scatter.png", "../../logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_macro_f1_curve.png", "../../logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_cwa_curve.png", "../../logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_metrics_curves.png", "../../logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_gt_vs_pred.png"], [], ["../../logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_loss_curves_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_val_metrics_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["../../logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_loss_curves_no_grad_clip.png", "../../logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_metric_curves_no_grad_clip.png", "../../logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_conf_matrix_no_grad_clip.png", "../../logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_weight_hist_no_grad_clip.png", "../../logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_weight_vs_correct_no_grad_clip.png"], ["../../logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_macro_f1.png", "../../logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_cwa.png", "../../logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_weight_histogram.png"], ["../../logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_loss_curves_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_val_metrics_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["../../logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_loss_curves_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_val_metrics_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["../../logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_loss_curves_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_val_metrics_remove_final_layernorm.png", "../../logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["../../logs/0-run/experiment_results/seed_aggregation_7c0344fa19a6458ca8f0a9660d055830/SPR_BENCH_loss_curves_aggregate.png", "../../logs/0-run/experiment_results/seed_aggregation_7c0344fa19a6458ca8f0a9660d055830/SPR_BENCH_val_metrics_aggregate.png"]], "plot_paths": [["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_macro_f1.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_cwa.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_weight_hist.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_CWA_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_metric_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_weight_histogram.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_weight_correct_scatter.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_macro_f1_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_cwa_curve.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_label_distribution.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_metrics_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_gt_vs_pred.png"], [], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_loss_curves_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_val_metrics_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_loss_curves_no_grad_clip.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_metric_curves_no_grad_clip.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_conf_matrix_no_grad_clip.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_weight_hist_no_grad_clip.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_weight_vs_correct_no_grad_clip.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_loss_curves.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_macro_f1.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_cwa.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_confusion_matrix.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_weight_histogram.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_loss_curves_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_val_metrics_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_loss_curves_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_val_metrics_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_loss_curves_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_val_metrics_remove_final_layernorm.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"], ["experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_7c0344fa19a6458ca8f0a9660d055830/SPR_BENCH_loss_curves_aggregate.png", "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_7c0344fa19a6458ca8f0a9660d055830/SPR_BENCH_val_metrics_aggregate.png"]], "plot_analyses": [[{"analysis": "This plot illustrates the cross-entropy loss for both the training and validation datasets over 12 epochs. The training loss decreases sharply at the beginning, indicating effective learning, but then stabilizes and aligns with the validation loss around epoch 6. This alignment suggests that the model is not overfitting. However, the relatively high and stable validation loss (~0.6) indicates that the model struggles to generalize well to unseen data, possibly due to the complexity of the symbolic rules in SPR_BENCH.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the macro-F1 score on the validation set over 12 epochs. The macro-F1 score initially drops significantly, indicating early instability in the model's predictions. After epoch 4, the score improves steadily, surpassing 0.69 by the final epochs. This reflects that the model eventually learns to balance precision and recall across classes effectively, although the initial instability may suggest the need for better initialization or early training strategies.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_macro_f1.png"}, {"analysis": "This plot displays the complexity-weighted accuracy (CWA) on the validation set. Similar to the macro-F1 trend, the CWA drops sharply early on, reaching a minimum around epoch 4, but then recovers and stabilizes above 0.69. This suggests that the model increasingly adapts to the complexity of the rules, but the early drop highlights potential difficulties in learning from complex examples during initial training.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_cwa.png"}, {"analysis": "The confusion matrix for the final epoch reveals a balance between true positives and true negatives, with 172 and 176 correctly classified examples, respectively. However, there are 76 false positives and 76 false negatives, indicating that while the model performs reasonably well, there is room for improvement in distinguishing between classes. This could involve refining the model architecture or incorporating additional features to better capture the nuances of the symbolic rules.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_confusion_matrix.png"}, {"analysis": "This plot presents the distribution of example weights, which appears uniform. This uniformity suggests that the model is treating all examples equally without biasing towards specific samples. While this is generally desirable, it may also indicate that the model is not leveraging the potential importance of more challenging examples, which could be an area for future investigation.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_ae43f6b6b7cd4aa384820f4d0632a952_proc_3445459/SPR_BENCH_weight_hist.png"}], [{"analysis": "The plot shows the training and validation loss over five epochs. The training loss decreases consistently, indicating that the model is learning from the training data. However, the validation loss initially decreases but starts to increase after epoch 4, suggesting potential overfitting. This pattern indicates that while the model is learning well on the training data, it may not generalize effectively to unseen data beyond a certain point. Regularization techniques or early stopping could help mitigate this issue.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_loss_curve.png"}, {"analysis": "The plot displays the validation Macro-F1 score across epochs. There is a decline in the Macro-F1 score from epoch 1 to epoch 3, followed by a sharp improvement in epochs 4 and 5. This suggests that the model initially struggles to generalize well but then improves its ability to balance precision and recall across classes. The sharp rise in the later epochs could indicate that the model is starting to capture the underlying patterns in the data more effectively.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_macroF1_curve.png"}, {"analysis": "This plot illustrates the validation complexity-weighted accuracy (CWA) over epochs. The trend mirrors the pattern seen in the Macro-F1 score, with an initial decline followed by a sharp increase in the later epochs. The CWA metric emphasizes the model\u2019s performance on more complex cases, and the improvement in the later epochs suggests that the model is starting to handle complex symbolic rules better as training progresses.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_CWA_curve.png"}, {"analysis": "The confusion matrix provides a visual summary of the model's predictions compared to the ground truth. The diagonal dominance indicates that the model is correctly classifying most samples. However, there are some off-diagonal elements, which suggest misclassifications. The intensity of the misclassified samples could be analyzed further to identify specific areas where the model struggles, such as certain classes or types of rules.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_7522ce3363a74682b67e5e13d5753ce6_proc_3448830/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over epochs, indicating that the model is learning from the training data. However, the validation loss remains relatively flat and higher than the training loss, suggesting potential overfitting or a lack of generalization to the validation data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation metrics, Macro-F1 and CWA, show minimal variation across epochs, hovering around the 0.6-0.7 range. This indicates that the model's performance on the validation set is relatively stable but not improving significantly, which could imply a bottleneck in the model's learning capacity or a need for hyperparameter tuning.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_metric_curves.png"}, {"analysis": "The distribution of instance weights shows a single concentrated value at 9. This lack of diversity in weights might indicate that the dataset does not incorporate instance-level weighting or that all instances are treated equally, which could limit the model's ability to prioritize certain data points.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_weight_histogram.png"}, {"analysis": "The weight vs. prediction correctness plot shows only one incorrect prediction at the uniform instance weight of 9. This suggests that instance weight does not play a significant role in prediction correctness under the current setup, possibly due to uniform weighting across data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_weight_correct_scatter.png"}, {"analysis": "The confusion matrix reveals a moderate imbalance in predictions. The model correctly predicts 168 and 176 instances for the two classes, but the misclassification rates (80 and 76) are relatively high, indicating room for improvement in model accuracy and class discrimination.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_244c7a2a7ebe42da8513b46b0651b2f4_proc_3448831/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate a disparity between training and validation performance. The training loss decreases steadily over epochs, showing that the model learns from the training data. However, the validation loss remains relatively flat and high, suggesting overfitting. The model may be memorizing the training data rather than generalizing to unseen data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_loss_curve.png"}, {"analysis": "The Macro-F1 scores exhibit instability across epochs, with drastic fluctuations. This suggests that the model's performance on different classes is inconsistent and may require better regularization or more balanced training techniques to stabilize the performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_macro_f1_curve.png"}, {"analysis": "The Complexity-Weighted Accuracy (CWA) displays a similar pattern to the Macro-F1 score, with significant oscillations. This indicates that the model struggles to consistently handle sequences with varying rule complexities, highlighting potential weaknesses in its ability to generalize across different levels of difficulty.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_cwa_curve.png"}, {"analysis": "The label distribution comparison shows a close match between the ground truth and predictions, indicating that the model captures the overall label distribution well. However, this does not guarantee that individual predictions are accurate, as it only reflects aggregate statistics.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_94bc854fb02044c1ad7c303cb4c163b7_proc_3448832/SPR_BENCH_label_distribution.png"}], [{"analysis": "The loss curve shows a rapid increase in training loss over the initial epochs, followed by stabilization after epoch 6. The validation loss mirrors the training loss trend but remains consistently higher, indicating potential overfitting or a mismatch between the training and validation data distributions. The lack of label smoothing may contribute to this instability, as the model might be struggling with hard labels.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_loss_curves.png"}, {"analysis": "The validation metrics plot demonstrates fluctuations in both the Macro-F1 and CWA scores, with a general upward trend over epochs. This suggests that the model is gradually improving in its ability to generalize, though the oscillations indicate instability in the learning process. The scores plateau around epoch 10, hinting at a potential convergence of the model's performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_metrics_curves.png"}, {"analysis": "The scatter plot comparing ground truth and predictions reveals a binary classification setup. The points are clustered around the diagonal, indicating that the model is making correct predictions for the most part. However, the sparse distribution suggests that the model may not be capturing the full complexity of the symbolic rules, possibly due to limitations in the architecture or training process.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9b425d3406a64611922e8ff523c148cf_proc_3448833/SPR_BENCH_no_label_smoothing_gt_vs_pred.png"}], [], [{"analysis": "The loss curves show a steady decrease in training loss over epochs, indicating that the model is learning from the data. However, the validation loss remains relatively flat and does not decrease alongside the training loss, suggesting potential overfitting. This indicates that while the model is fitting well to the training data, it is not generalizing effectively to unseen data. Further regularization techniques or model adjustments may be necessary to improve generalization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_loss_curves_remove_final_layernorm.png"}, {"analysis": "The validation metrics plot shows the Macro-F1 and CWA metrics over epochs. Both metrics exhibit some variability, with a significant drop around epoch 3 followed by recovery. This fluctuation might indicate instability in the model's performance during early training. However, the metrics stabilize and improve after epoch 6, suggesting that the model eventually learns to classify sequences more effectively. Fine-tuning the learning rate or employing early stopping could help mitigate the initial instability.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_val_metrics_remove_final_layernorm.png"}, {"analysis": "The confusion matrix for the final development predictions highlights a balanced performance across both classes, with 172 true negatives, 176 true positives, and 76 false positives/negatives each. This suggests that the model is not biased towards any particular class but still has room for improvement in reducing misclassifications. Techniques such as class-specific loss weighting or additional data augmentation could help improve accuracy.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_1a50f28d416f45438721fa65546c3096_proc_3448832/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"}], [{"analysis": "The loss curves for training and validation show a clear overfitting trend. The training loss decreases steadily throughout the epochs, whereas the validation loss remains relatively flat after an initial increase and does not improve over time. This suggests that the model is memorizing the training data rather than generalizing well to unseen data. The lack of gradient clipping could be contributing to this behavior, as it might allow the model's weights to grow excessively, leading to overfitting.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_loss_curves_no_grad_clip.png"}, {"analysis": "The validation metrics, including Macro-F1 and CWA, remain relatively stable over the epochs, without significant improvement. This stability, combined with the earlier observation of overfitting in the loss curves, suggests that the model is not learning to generalize well to the validation set. The flat trends in both metrics indicate that model adjustments or changes in training strategy might be required to improve performance.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_metric_curves_no_grad_clip.png"}, {"analysis": "The confusion matrix shows a balanced distribution of correct and incorrect predictions across the two classes. While this indicates that the model is not biased toward one class, the relatively high number of misclassifications (76 for both classes) highlights a need for improvement in model accuracy. This could be addressed by exploring more advanced architectures or incorporating techniques such as data augmentation or regularization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_conf_matrix_no_grad_clip.png"}, {"analysis": "The weight distribution plot shows a single, sharp peak at a specific weight value. This uniformity suggests that the model's weights are not being updated effectively or are converging to a narrow range. This could indicate issues with the learning rate, optimizer settings, or the absence of gradient clipping, which might be causing numerical instabilities.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_weight_hist_no_grad_clip.png"}, {"analysis": "The correctness vs. weight plot shows two distinct points, one for correct and one for incorrect predictions, both associated with the same weight value. This further emphasizes the uniform weight distribution observed earlier and suggests that the model's capacity to differentiate between correct and incorrect predictions is limited. This limitation could stem from insufficient model complexity or inadequate training dynamics.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_933a0d27b5eb4c729e7ea159626d4ad9_proc_3448831/SPR_BENCH_weight_vs_correct_no_grad_clip.png"}], [{"analysis": "This plot shows the cross-entropy loss for both training and validation over epochs. The training loss decreases steadily, indicating that the model is learning from the training data. However, the validation loss remains relatively flat and significantly higher than the training loss, suggesting potential overfitting. This indicates that the model may not generalize well to unseen data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_loss_curves.png"}, {"analysis": "This plot illustrates the Macro-F1 score on the validation set across epochs. The score initially decreases, reaching its lowest point at epoch 3, and then improves slightly by epoch 4. This fluctuation suggests that the model's performance on class balance is inconsistent, which could be attributed to overfitting or challenges in handling the complex rules in the dataset.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_macro_f1.png"}, {"analysis": "This plot shows the class-weighted accuracy (CWA) for the validation set across epochs. Similar to the Macro-F1 score, the CWA decreases until epoch 3 and then improves by epoch 4. This trend reinforces the observation of inconsistent generalization performance, possibly due to the complexity of the symbolic rules or insufficient regularization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_cwa.png"}, {"analysis": "The confusion matrix provides a breakdown of the model's predictions versus the true labels. The model demonstrates a moderate level of accuracy, but there is a noticeable number of misclassifications in both classes. This suggests that while the model has learned some patterns, it struggles to fully capture the intricacies of the symbolic rules.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_confusion_matrix.png"}, {"analysis": "This plot shows the distribution of example weights by correctness. The majority of predictions are correct, as indicated by the larger blue bar. However, there is a non-negligible proportion of incorrect predictions (orange bar), which aligns with the observations from the confusion matrix. This highlights areas where the model could be improved, such as better handling of edge cases or improving rule representation.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_8a9877485fd84ffc9b4c856e4d6f2356_proc_3448833/spr_bench_bi_lstm_backbone_weight_histogram.png"}], [{"analysis": "The training loss decreases steadily across epochs, indicating that the model is learning from the data. However, the validation loss initially decreases but starts to plateau and slightly increase after epoch 6, suggesting potential overfitting. This indicates that the model is starting to memorize the training data rather than generalizing well to unseen data. Adjustments such as regularization or early stopping might be needed.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_loss_curves_remove_final_layernorm.png"}, {"analysis": "The Macro-F1 score and CWA metric both show an improvement over epochs, with the Macro-F1 score stabilizing around 0.70 and the CWA metric closely tracking it. This indicates that the model's classification performance is improving and stabilizing. However, the drop in performance at epoch 2 might suggest an issue with the learning rate or initialization that stabilizes in later epochs. The alignment of the two metrics suggests consistency in model evaluation.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_val_metrics_remove_final_layernorm.png"}, {"analysis": "The confusion matrix shows that the model has a balanced performance between the two classes, with slightly more errors in predicting class 0 as compared to class 1. The numbers (172 true negatives, 175 true positives, 76 false positives, and 77 false negatives) indicate that the model is not heavily biased towards one class. However, the false positive and false negative rates suggest there is room for improvement in fine-tuning the decision threshold or addressing class imbalance, if present.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"}], [{"analysis": "The loss curves indicate that the training loss decreases steadily over the epochs, showing good convergence of the model. However, the validation loss remains relatively stable after an initial increase, suggesting potential overfitting or the model's inability to generalize well to unseen data. This could point to the need for regularization techniques or more diverse training data.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_loss_curves_remove_final_layernorm.png"}, {"analysis": "The validation metrics show that both Macro-F1 and CWA (Class-Wise Accuracy) improve significantly after an initial drop, stabilizing towards the later epochs. The close alignment of the two metrics indicates consistent performance across classes. However, the initial drop suggests that the model struggles at the beginning of training, possibly due to the complexity of the task or the model's initialization.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_val_metrics_remove_final_layernorm.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well, with true positive and true negative counts outnumbering the false positives and false negatives. However, the false negative count (76) is slightly higher than the false positive count (79), indicating that the model may be biased towards predicting negative classes. This imbalance could be addressed by adjusting the class weights or using a more balanced training dataset.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"}], [{"analysis": "The training loss decreases steadily over epochs, suggesting that the model is learning effectively during training. However, the validation loss remains relatively flat and does not decrease significantly, which may indicate potential overfitting or insufficient model generalization. The convergence of training and validation loss towards the later epochs could imply that the model is stabilizing, but the lack of significant improvement in validation loss warrants further investigation, such as hyperparameter tuning or data augmentation strategies.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_loss_curves_remove_final_layernorm.png"}, {"analysis": "The Macro-F1 and CWA metrics show an overall upward trend with some fluctuations. This indicates that the model's performance is improving over epochs, but the fluctuations suggest some instability in learning. The alignment of the two metrics indicates consistency in performance evaluation. The peak values around certain epochs suggest that the model achieves its best performance intermittently, and careful selection of the optimal epoch for evaluation might be necessary.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_val_metrics_remove_final_layernorm.png"}, {"analysis": "The confusion matrix indicates that the model has a moderate level of accuracy, with a noticeable number of false positives (81) and false negatives (76). The true positive (176) and true negative (167) counts are fairly balanced, which is a positive sign. However, the misclassification rates suggest that there is still room for improvement in the model's ability to distinguish between classes. This could be addressed by refining the model architecture or incorporating additional features to improve predictive accuracy.", "plot_path": "experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/SPR_BENCH_confusion_matrix_remove_final_layernorm.png"}], []], "vlm_feedback_summary": ["The plots reveal key insights into the model's training and evaluation process.\nWhile the model shows improvement in validation metrics over time, early\ninstability and relatively high loss indicate challenges in generalizing to\ncomplex rules. The confusion matrix highlights a balanced but imperfect\nclassification, and the uniform weight distribution suggests no bias but also no\nemphasis on challenging examples. Future work could focus on refining early\ntraining strategies, enhancing model architecture, and investigating the impact\nof weighting challenging samples.", "The plots reveal a mixed performance trend: the model shows learning progress\nbut struggles with generalization initially. Validation loss increases after\nepoch 4, indicating overfitting. The Macro-F1 and complexity-weighted accuracy\nmetrics show an initial decline but later improve significantly, suggesting the\nmodel gradually adapts to complex rules. The confusion matrix highlights areas\nof misclassification, which could guide further model refinement.", "The plots reveal issues with model generalization, stability in validation\nmetrics without improvement, and potential limitations in dataset diversity or\nmodel architecture. Further experimentation with hyperparameters, model\ncomplexity, and data augmentation might be necessary to address these\nchallenges.", "The plots reveal issues with overfitting, inconsistent performance across\nclasses and complexities, and potential generalization challenges. While the\nmodel captures the overall label distribution, its effectiveness on individual\npredictions remains uncertain. Improvements in regularization, data\nrepresentation, and model architecture may be needed to address these\nchallenges.", "The results indicate that while the model shows signs of learning, there are\nchallenges with overfitting, instability in performance metrics, and potential\nlimitations in capturing the complexity of the symbolic rules. Further tuning\nand architectural improvements may be necessary to enhance robustness and\ngeneralization.", "[]", "The plots reveal that while the model is learning effectively during training,\nits generalization to validation data is limited, as evidenced by the flat\nvalidation loss curve. The performance metrics indicate some instability during\nearly training, but eventual stabilization suggests potential for improvement\nwith fine-tuning. The confusion matrix shows balanced but imperfect\nclassification, highlighting areas for further optimization.", "The plots reveal significant issues with overfitting, limited generalization,\nand ineffective weight updates. The lack of improvement in validation metrics\nand the uniform weight distribution highlight potential problems with the\ntraining strategy and model architecture. Addressing these challenges through\ntechniques such as regularization, gradient clipping, or architectural changes\ncould improve performance.", "The provided plots reveal that the model faces challenges with generalization\nand consistency in performance metrics. While the training loss decreases,\nvalidation loss remains high, indicating overfitting. The fluctuations in\nvalidation Macro-F1 and CWA scores further suggest difficulties in handling the\ncomplexity of the task. The confusion matrix and weight distribution plot\nhighlight areas of improvement, such as reducing misclassifications and better\naddressing symbolic rule intricacies.", "The plots collectively indicate that the model is learning effectively but faces\npotential overfitting and some misclassification issues. Regularization, early\nstopping, and threshold tuning could improve performance. Metrics are\nconsistent, but further ablation studies are recommended to refine the model's\nrobustness.", "The provided plots reveal that the model is converging during training but faces\nchallenges with generalization and initial learning. While the validation\nmetrics stabilize, the loss curves and confusion matrix suggest potential\noverfitting and class imbalance issues. Addressing these could enhance the\nmodel's performance and robustness.", "The results indicate that while the model demonstrates learning and achieves\nmoderate accuracy, there are clear signs of potential overfitting, instability\nin performance metrics, and room for improvement in classification accuracy.\nFurther experimentation and optimization are recommended to enhance model\nrobustness and generalization.", "[]"], "exec_time": [20.778529167175293, 6.411267280578613, 6.60211968421936, 5.585347414016724, 17.701423168182373, 8.5613272190094, 12.704315900802612, 14.00307583808899, 9.634409427642822, 15.849936723709106, 12.689835786819458, 9.536717653274536, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[]"], ["[]"], ["[]"], [], ["\"\""], ["[]"], ["\"\""], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate datasets ----------\nfor dname, logs in experiment_data.items():\n    # basic sanity\n    train_loss = np.array(logs[\"losses\"].get(\"train\", []), dtype=float)\n    val_loss = np.array(logs[\"losses\"].get(\"val\", []), dtype=float)\n    val_metrics = logs[\"metrics\"].get(\"val\", [])\n    macro_f1 = (\n        np.array([m[\"macro_f1\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    cwa = (\n        np.array([m[\"cwa\"] for m in val_metrics], dtype=float)\n        if val_metrics\n        else np.array([])\n    )\n    preds = np.array(logs.get(\"predictions\", []))\n    gts = np.array(logs.get(\"ground_truth\", []))\n    wts = np.array(logs.get(\"weights\", []))\n\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # 1) loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"train\", color=\"tab:blue\")\n        plt.plot(epochs, val_loss, label=\"val\", color=\"tab:orange\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dname}: {e}\")\n        plt.close()\n\n    # 2) macro-F1\n    try:\n        if macro_f1.size:\n            plt.figure()\n            plt.plot(epochs, macro_f1, color=\"tab:green\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Validation Macro-F1\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_macro_f1.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 plot for {dname}: {e}\")\n        plt.close()\n\n    # 3) CWA\n    try:\n        if cwa.size:\n            plt.figure()\n            plt.plot(epochs, cwa, color=\"tab:red\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-Weighted Acc.\")\n            plt.title(f\"{dname}: Validation CWA\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_cwa.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot for {dname}: {e}\")\n        plt.close()\n\n    # 4) confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i in range(num_cls):\n                for j in range(num_cls):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (final epoch)\")\n            plt.colorbar()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # 5) weight histogram\n    try:\n        if wts.size:\n            plt.figure()\n            plt.hist(wts, bins=min(30, len(np.unique(wts))), color=\"tab:purple\")\n            plt.xlabel(\"Example Weight\")\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dname}: Distribution of Weights\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_weight_hist.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating weight histogram for {dname}: {e}\")\n        plt.close()\n\n    # ---- quick metric summary ----\n    if macro_f1.size:\n        print(\n            f\"{dname}: best Macro-F1={macro_f1.max():.3f} | final Macro-F1={macro_f1[-1]:.3f}\"\n        )\n    if cwa.size:\n        print(f\"{dname}: best CWA={cwa.max():.3f} | final CWA={cwa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to safely fetch nested keys\ndef get_path(d, *keys, default=None):\n    for k in keys:\n        if d is None or k not in d:\n            return default\n        d = d[k]\n    return d\n\n\nexp = get_path(experiment_data, \"no_curriculum_weighting\", \"SPR_BENCH\", default={})\n\nloss_train = np.asarray(get_path(exp, \"losses\", \"train\", default=[]))\nloss_val = np.asarray(get_path(exp, \"losses\", \"val\", default=[]))\nval_metrics = get_path(exp, \"metrics\", \"val\", default=[])\n\nmacro_f1 = (\n    np.asarray([m.get(\"macro_f1\") for m in val_metrics])\n    if val_metrics\n    else np.array([])\n)\ncwa_arr = (\n    np.asarray([m.get(\"cwa\") for m in val_metrics]) if val_metrics else np.array([])\n)\n\npreds = np.asarray(get_path(exp, \"predictions\", default=[]))\nlabels = np.asarray(get_path(exp, \"ground_truth\", default=[]))\n\n# ---------- plotting ----------\ntry:\n    if loss_train.size and loss_val.size:\n        plt.figure()\n        epochs = np.arange(1, len(loss_train) + 1)\n        plt.plot(epochs, loss_train, label=\"Train Loss\")\n        plt.plot(epochs, loss_val, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    if macro_f1.size:\n        plt.figure()\n        plt.plot(np.arange(1, len(macro_f1) + 1), macro_f1, marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Validation Macro-F1 over Epochs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 plot: {e}\")\n    plt.close()\n\ntry:\n    if cwa_arr.size:\n        plt.figure()\n        plt.plot(np.arange(1, len(cwa_arr) + 1), cwa_arr, color=\"green\", marker=\"s\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA\")\n        plt.title(\"SPR_BENCH: Validation Complexity-Weighted Accuracy over Epochs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA plot: {e}\")\n    plt.close()\n\ntry:\n    if preds.size and labels.size:\n        cm = confusion_matrix(labels, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print best metrics ----------\nif macro_f1.size:\n    print(f\"Best Macro-F1: {macro_f1.max():.3f}\")\nif cwa_arr.size:\n    print(f\"Best CWA:      {cwa_arr.max():.3f}\")\nprint(f\"Plots saved to {working_dir}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ensure working directory exists\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# quick guard\nif not experiment_data:\n    print(\"No experiment data found; nothing to plot.\")\n    exit()\n\nrun = experiment_data[\"no_learned_pos_emb\"][\"SPR_BENCH\"]\ntrain_loss = run[\"losses\"][\"train\"]\nval_loss = run[\"losses\"][\"val\"]\nmetrics = run[\"metrics\"][\"val\"]  # list of dicts\nmacro_f1 = [m[\"macro_f1\"] for m in metrics]\ncwa = [m[\"cwa\"] for m in metrics]\npreds = np.array(run[\"predictions\"])\nlabels = np.array(run[\"ground_truth\"])\nweights = np.array(run[\"weights\"])\nepochs = np.arange(1, len(train_loss) + 1)\n\n\n# helper for epoch thinning (max 5 markers)\ndef idx_subset(x, n=5):\n    if len(x) <= n:\n        return np.arange(len(x))\n    step = max(1, len(x) // n)\n    return np.arange(0, len(x), step)[:n]\n\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.scatter(\n        epochs[idx_subset(epochs)], np.array(train_loss)[idx_subset(epochs)], c=\"blue\"\n    )\n    plt.scatter(\n        epochs[idx_subset(epochs)], np.array(val_loss)[idx_subset(epochs)], c=\"orange\"\n    )\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# 2) Metric curves\ntry:\n    plt.figure()\n    plt.plot(epochs, macro_f1, label=\"Macro-F1\")\n    plt.plot(epochs, cwa, label=\"CWA\")\n    plt.scatter(\n        epochs[idx_subset(epochs)], np.array(macro_f1)[idx_subset(epochs)], c=\"green\"\n    )\n    plt.scatter(epochs[idx_subset(epochs)], np.array(cwa)[idx_subset(epochs)], c=\"red\")\n    plt.title(\"SPR_BENCH: Validation Metrics over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.ylim(0, 1)\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_metric_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve plot: {e}\")\n    plt.close()\n\n# 3) Histogram of weights\ntry:\n    plt.figure()\n    plt.hist(weights, bins=20, color=\"purple\", alpha=0.7)\n    plt.title(\"SPR_BENCH: Distribution of Instance Weights\")\n    plt.xlabel(\"Weight\")\n    plt.ylabel(\"Count\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_weight_histogram.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating weight histogram: {e}\")\n    plt.close()\n\n# 4) Weight vs Correctness scatter\ntry:\n    correctness = preds == labels\n    plt.figure()\n    plt.scatter(\n        weights[correctness],\n        np.zeros_like(weights[correctness]),\n        c=\"green\",\n        label=\"Correct\",\n        alpha=0.5,\n    )\n    plt.scatter(\n        weights[~correctness],\n        np.zeros_like(weights[~correctness]),\n        c=\"red\",\n        label=\"Incorrect\",\n        alpha=0.5,\n    )\n    plt.yticks([])\n    plt.title(\"SPR_BENCH: Weight vs Prediction Correctness\")\n    plt.xlabel(\"Instance Weight\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_weight_correct_scatter.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating weight-correctness scatter: {e}\")\n    plt.close()\n\n# 5) Confusion matrix\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    cm = confusion_matrix(labels, preds, labels=sorted(set(labels)))\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(\"SPR_BENCH: Confusion Matrix (Validation)\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# -------- print final metrics ---------------\nif macro_f1:\n    print(f\"Final Macro-F1: {macro_f1[-1]:.3f}\")\n    print(f\"Final CWA: {cwa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper: safely fetch nested dicts\ndef get(d, *keys, default=None):\n    for k in keys:\n        d = d.get(k, {})\n    return d if d else default\n\n\nrun_key, ds_key = \"mean_pooling_no_cls\", \"SPR_BENCH\"\nloss_train = get(experiment_data, run_key, ds_key, \"losses\", \"train\", default=[])\nloss_val = get(experiment_data, run_key, ds_key, \"losses\", \"val\", default=[])\nmetrics_val = get(experiment_data, run_key, ds_key, \"metrics\", \"val\", default=[])\npreds = np.array(get(experiment_data, run_key, ds_key, \"predictions\", default=[]))\ngts = np.array(get(experiment_data, run_key, ds_key, \"ground_truth\", default=[]))\nepochs = np.arange(1, len(loss_val) + 1)\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    if loss_train:\n        plt.plot(epochs, loss_train[: len(epochs)], label=\"Train\")\n    if loss_val:\n        plt.plot(epochs, loss_val, label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curve\ntry:\n    plt.figure()\n    macro_f1 = [m.get(\"macro_f1\") for m in metrics_val if m]\n    if macro_f1:\n        plt.plot(epochs, macro_f1, marker=\"o\")\n        plt.title(\"SPR_BENCH Macro-F1 over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macro_f1_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 plot: {e}\")\n    plt.close()\n\n# 3) CWA curve\ntry:\n    plt.figure()\n    cwa_vals = [m.get(\"cwa\") for m in metrics_val if m]\n    if cwa_vals:\n        plt.plot(epochs, cwa_vals, marker=\"o\", color=\"green\")\n        plt.title(\"SPR_BENCH Complexity-Weighted Accuracy (CWA)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_cwa_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA plot: {e}\")\n    plt.close()\n\n# 4) Ground truth vs prediction label distribution\ntry:\n    if preds.size and gts.size:\n        labels = sorted(set(np.concatenate([gts, preds])))\n        gt_counts = [np.sum(gts == lbl) for lbl in labels]\n        pr_counts = [np.sum(preds == lbl) for lbl in labels]\n\n        fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n        axes[0].bar(labels, gt_counts, color=\"steelblue\")\n        axes[0].set_title(\"Ground Truth\")\n        axes[1].bar(labels, pr_counts, color=\"darkorange\")\n        axes[1].set_title(\"Predictions\")\n        for ax in axes:\n            ax.set_xlabel(\"Label\")\n            ax.set_ylabel(\"Count\")\n        fig.suptitle(\n            \"SPR_BENCH Label Distribution\\nLeft: Ground Truth, Right: Generated Samples\",\n            fontsize=12,\n        )\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_label_distribution.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating label distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nsaved = []\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor exp_name, datasets in experiment_data.items():\n    for ds_name, content in datasets.items():\n        losses = content[\"losses\"]\n        metrics = content[\"metrics\"]\n        preds = np.array(content.get(\"predictions\", []))\n        gts = np.array(content.get(\"ground_truth\", []))\n        # ---- 1: loss curves ----\n        try:\n            plt.figure()\n            epochs = range(1, len(losses[\"train\"]) + 1)\n            plt.plot(epochs, losses[\"train\"], label=\"Train\")\n            plt.plot(epochs, losses[\"val\"], label=\"Validation\")\n            plt.title(f\"{ds_name} \u2013 Loss Curves ({exp_name})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fn = os.path.join(working_dir, f\"{ds_name}_{exp_name}_loss_curves.png\")\n            plt.savefig(fn)\n            saved.append(fn)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot: {e}\")\n            plt.close()\n        # ---- 2: metric curves ----\n        try:\n            plt.figure()\n            macro_f1 = [m[\"macro_f1\"] for m in metrics[\"val\"]]\n            cwa = [m[\"cwa\"] for m in metrics[\"val\"]]\n            plt.plot(epochs, macro_f1, label=\"Macro-F1\")\n            plt.plot(epochs, cwa, label=\"CWA\")\n            plt.title(f\"{ds_name} \u2013 Validation Metrics ({exp_name})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.legend()\n            fn = os.path.join(working_dir, f\"{ds_name}_{exp_name}_metrics_curves.png\")\n            plt.savefig(fn)\n            saved.append(fn)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating metric plot: {e}\")\n            plt.close()\n        # ---- 3: prediction scatter ----\n        try:\n            if preds.size and gts.size:\n                idx = np.linspace(0, len(preds) - 1, num=min(200, len(preds))).astype(\n                    int\n                )\n                plt.figure()\n                plt.scatter(gts[idx], preds[idx], alpha=0.6, s=10)\n                plt.title(\n                    f\"Ground Truth vs Predictions \u2013 {ds_name} ({exp_name})\\nLeft: Ground Truth, Right: Generated Samples\"\n                )\n                plt.xlabel(\"Ground Truth\")\n                plt.ylabel(\"Predictions\")\n                fn = os.path.join(working_dir, f\"{ds_name}_{exp_name}_gt_vs_pred.png\")\n                plt.savefig(fn)\n                saved.append(fn)\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating scatter plot: {e}\")\n            plt.close()\n        # ---- print last-epoch metrics ----\n        if metrics[\"val\"]:\n            last = metrics[\"val\"][-1]\n            print(\n                f\"{exp_name}/{ds_name} \u2013 final Macro-F1: {last['macro_f1']:.3f}, CWA: {last['cwa']:.3f}\"\n            )\n\nprint(\"Saved figures:\")\nfor s in saved:\n    print(\"  \", s)\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nrun_key = \"remove_final_layernorm\"\ndset_key = \"SPR_BENCH\"\nrun = experiment_data.get(run_key, {}).get(dset_key, {})\n\n# ---------- Plot 1: Train/Val loss ----------\ntry:\n    losses = run.get(\"losses\", {})\n    train_loss = losses.get(\"train\", [])\n    val_loss = losses.get(\"val\", [])\n    epochs = range(1, min(len(train_loss), len(val_loss)) + 1)\n\n    plt.figure()\n    plt.plot(epochs, train_loss[: len(epochs)], label=\"Train Loss\")\n    plt.plot(epochs, val_loss[: len(epochs)], label=\"Val Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_loss_curves_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: Val Macro-F1 & CWA ----------\ntry:\n    metrics = run.get(\"metrics\", {}).get(\"val\", [])\n    macro_f1 = [m.get(\"macro_f1\") for m in metrics if m]\n    cwa = [m.get(\"cwa\") for m in metrics if m]\n    epochs = range(1, len(macro_f1) + 1)\n\n    plt.figure()\n    ax1 = plt.gca()\n    ax1.plot(epochs, macro_f1, \"b-\", label=\"Macro-F1\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Macro-F1\", color=\"b\")\n    ax2 = ax1.twinx()\n    ax2.plot(epochs, cwa, \"r--\", label=\"CWA\")\n    ax2.set_ylabel(\"CWA\", color=\"r\")\n    plt.title(\"SPR_BENCH Validation Metrics\\nLeft: Macro-F1, Right: CWA\")\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_val_metrics_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating metrics plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: Confusion Matrix ----------\ntry:\n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n    preds = np.array(run.get(\"predictions\", []))\n    gts = np.array(run.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        disp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n        disp.plot(cmap=\"Blues\", ax=plt.gca(), colorbar=False)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nFinal Dev Predictions\")\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_confusion_matrix_remove_final_layernorm.png\"\n        )\n        plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor exp_name, exp_dict in experiment_data.items():\n    for dset, res in exp_dict.items():\n        losses_tr = res.get(\"losses\", {}).get(\"train\", [])\n        losses_val = res.get(\"losses\", {}).get(\"val\", [])\n        metrics_val = res.get(\"metrics\", {}).get(\"val\", [])\n        preds = res.get(\"predictions\", np.array([]))\n        gts = res.get(\"ground_truth\", np.array([]))\n        ws = res.get(\"weights\", np.array([]))\n        epochs = range(1, len(losses_tr) + 1)\n\n        # 1) Loss curves\n        try:\n            plt.figure()\n            plt.plot(epochs, losses_tr, label=\"Train\")\n            plt.plot(epochs, losses_val, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dset} Loss Curves ({exp_name})\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_loss_curves_{exp_name}.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating loss plot: {e}\")\n            plt.close()\n\n        # 2) Metric curves\n        try:\n            plt.figure()\n            mf1 = [m.get(\"macro_f1\", np.nan) for m in metrics_val]\n            cwa = [m.get(\"cwa\", np.nan) for m in metrics_val]\n            plt.plot(epochs, mf1, label=\"Macro-F1\")\n            plt.plot(epochs, cwa, label=\"CWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.ylim(0, 1)\n            plt.title(f\"{dset} Validation Metrics ({exp_name})\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_metric_curves_{exp_name}.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating metric plot: {e}\")\n            plt.close()\n\n        # 3) Confusion matrix\n        try:\n            if preds.size and gts.size:\n                cm = confusion_matrix(gts, preds, labels=sorted(np.unique(gts)))\n                cm_norm = cm / cm.sum(axis=1, keepdims=True)\n                plt.figure()\n                im = plt.imshow(cm_norm, cmap=\"Blues\")\n                plt.colorbar(im, fraction=0.046, pad=0.04)\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"True\")\n                plt.title(f\"{dset} Confusion Matrix ({exp_name})\")\n                for i in range(cm.shape[0]):\n                    for j in range(cm.shape[1]):\n                        plt.text(\n                            j, i, f\"{cm[i,j]}\", ha=\"center\", va=\"center\", color=\"black\"\n                        )\n                fname = os.path.join(working_dir, f\"{dset}_conf_matrix_{exp_name}.png\")\n                plt.savefig(fname)\n                plt.close()\n                print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n\n        # 4) Weight distribution\n        try:\n            if ws.size:\n                plt.figure()\n                plt.hist(ws, bins=30, color=\"gray\")\n                plt.xlabel(\"Example Weight\")\n                plt.ylabel(\"Count\")\n                plt.title(f\"{dset} Weight Distribution ({exp_name})\")\n                fname = os.path.join(working_dir, f\"{dset}_weight_hist_{exp_name}.png\")\n                plt.savefig(fname)\n                plt.close()\n                print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating weight hist: {e}\")\n            plt.close()\n\n        # 5) Correctness vs weight scatter\n        try:\n            if ws.size and preds.size:\n                correct = (preds == gts).astype(int)\n                plt.figure()\n                plt.scatter(ws, correct, alpha=0.3, s=10)\n                plt.yticks([0, 1], [\"Wrong\", \"Correct\"])\n                plt.xlabel(\"Weight\")\n                plt.title(f\"{dset} Correctness vs Weight ({exp_name})\")\n                fname = os.path.join(\n                    working_dir, f\"{dset}_weight_vs_correct_{exp_name}.png\"\n                )\n                plt.savefig(fname)\n                plt.close()\n                print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating scatter plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load experiment data -----\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nmdl = \"bi_lstm_backbone\"\ndset = \"SPR_BENCH\"\nrun = experiment_data.get(mdl, {}).get(dset, {})\n\nloss_train = run.get(\"losses\", {}).get(\"train\", [])\nloss_val = run.get(\"losses\", {}).get(\"val\", [])\nmetrics_val = run.get(\"metrics\", {}).get(\"val\", [])\npreds = np.array(run.get(\"predictions\", []))\ngts = np.array(run.get(\"ground_truth\", []))\nwts = np.array(run.get(\"weights\", []))\nepochs = np.arange(1, len(loss_train) + 1)\n\n# 1) Train vs Val loss\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-entropy Loss\")\n    plt.title(f\"{mdl} on {dset} \u2013 Train vs Val Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dset.lower()}_{mdl}_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 2) Macro-F1\ntry:\n    plt.figure()\n    f1_vals = [m[\"macro_f1\"] for m in metrics_val]\n    plt.plot(epochs, f1_vals, marker=\"o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(f\"{mdl} on {dset} \u2013 Validation Macro-F1\")\n    fname = os.path.join(working_dir, f\"{dset.lower()}_{mdl}_macro_f1.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 plot: {e}\")\n    plt.close()\n\n# 3) CWA\ntry:\n    plt.figure()\n    cwa_vals = [m[\"cwa\"] for m in metrics_val]\n    plt.plot(epochs, cwa_vals, marker=\"s\", color=\"green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Class-weighted Accuracy\")\n    plt.title(f\"{mdl} on {dset} \u2013 Validation CWA\")\n    fname = os.path.join(working_dir, f\"{dset.lower()}_{mdl}_cwa.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA plot: {e}\")\n    plt.close()\n\n# 4) Confusion Matrix (final epoch)\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{mdl} on {dset} \u2013 Confusion Matrix (Final)\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"red\")\n        fname = os.path.join(working_dir, f\"{dset.lower()}_{mdl}_confusion_matrix.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# 5) Weight histogram vs correctness\ntry:\n    if preds.size and wts.size:\n        correct = preds == gts\n        plt.figure()\n        plt.hist(wts[correct], bins=20, alpha=0.7, label=\"Correct\", color=\"blue\")\n        plt.hist(wts[~correct], bins=20, alpha=0.7, label=\"Incorrect\", color=\"orange\")\n        plt.xlabel(\"Example Weight\")\n        plt.ylabel(\"Count\")\n        plt.title(f\"{mdl} on {dset} \u2013 Weight Distribution by Correctness\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dset.lower()}_{mdl}_weight_histogram.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating weight histogram: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nrun_key = \"remove_final_layernorm\"\ndset_key = \"SPR_BENCH\"\nrun = experiment_data.get(run_key, {}).get(dset_key, {})\n\n# ---------- Plot 1: Train/Val loss ----------\ntry:\n    losses = run.get(\"losses\", {})\n    train_loss = losses.get(\"train\", [])\n    val_loss = losses.get(\"val\", [])\n    epochs = range(1, min(len(train_loss), len(val_loss)) + 1)\n\n    plt.figure()\n    plt.plot(epochs, train_loss[: len(epochs)], label=\"Train Loss\")\n    plt.plot(epochs, val_loss[: len(epochs)], label=\"Val Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_loss_curves_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: Val Macro-F1 & CWA ----------\ntry:\n    metrics = run.get(\"metrics\", {}).get(\"val\", [])\n    macro_f1 = [m.get(\"macro_f1\") for m in metrics if m]\n    cwa = [m.get(\"cwa\") for m in metrics if m]\n    epochs = range(1, len(macro_f1) + 1)\n\n    plt.figure()\n    ax1 = plt.gca()\n    ax1.plot(epochs, macro_f1, \"b-\", label=\"Macro-F1\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Macro-F1\", color=\"b\")\n    ax2 = ax1.twinx()\n    ax2.plot(epochs, cwa, \"r--\", label=\"CWA\")\n    ax2.set_ylabel(\"CWA\", color=\"r\")\n    plt.title(\"SPR_BENCH Validation Metrics\\nLeft: Macro-F1, Right: CWA\")\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_val_metrics_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating metrics plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: Confusion Matrix ----------\ntry:\n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n    preds = np.array(run.get(\"predictions\", []))\n    gts = np.array(run.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        disp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n        disp.plot(cmap=\"Blues\", ax=plt.gca(), colorbar=False)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nFinal Dev Predictions\")\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_confusion_matrix_remove_final_layernorm.png\"\n        )\n        plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nrun_key = \"remove_final_layernorm\"\ndset_key = \"SPR_BENCH\"\nrun = experiment_data.get(run_key, {}).get(dset_key, {})\n\n# ---------- Plot 1: Train/Val loss ----------\ntry:\n    losses = run.get(\"losses\", {})\n    train_loss = losses.get(\"train\", [])\n    val_loss = losses.get(\"val\", [])\n    epochs = range(1, min(len(train_loss), len(val_loss)) + 1)\n\n    plt.figure()\n    plt.plot(epochs, train_loss[: len(epochs)], label=\"Train Loss\")\n    plt.plot(epochs, val_loss[: len(epochs)], label=\"Val Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_loss_curves_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: Val Macro-F1 & CWA ----------\ntry:\n    metrics = run.get(\"metrics\", {}).get(\"val\", [])\n    macro_f1 = [m.get(\"macro_f1\") for m in metrics if m]\n    cwa = [m.get(\"cwa\") for m in metrics if m]\n    epochs = range(1, len(macro_f1) + 1)\n\n    plt.figure()\n    ax1 = plt.gca()\n    ax1.plot(epochs, macro_f1, \"b-\", label=\"Macro-F1\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Macro-F1\", color=\"b\")\n    ax2 = ax1.twinx()\n    ax2.plot(epochs, cwa, \"r--\", label=\"CWA\")\n    ax2.set_ylabel(\"CWA\", color=\"r\")\n    plt.title(\"SPR_BENCH Validation Metrics\\nLeft: Macro-F1, Right: CWA\")\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_val_metrics_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating metrics plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: Confusion Matrix ----------\ntry:\n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n    preds = np.array(run.get(\"predictions\", []))\n    gts = np.array(run.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        disp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n        disp.plot(cmap=\"Blues\", ax=plt.gca(), colorbar=False)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nFinal Dev Predictions\")\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_confusion_matrix_remove_final_layernorm.png\"\n        )\n        plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nrun_key = \"remove_final_layernorm\"\ndset_key = \"SPR_BENCH\"\nrun = experiment_data.get(run_key, {}).get(dset_key, {})\n\n# ---------- Plot 1: Train/Val loss ----------\ntry:\n    losses = run.get(\"losses\", {})\n    train_loss = losses.get(\"train\", [])\n    val_loss = losses.get(\"val\", [])\n    epochs = range(1, min(len(train_loss), len(val_loss)) + 1)\n\n    plt.figure()\n    plt.plot(epochs, train_loss[: len(epochs)], label=\"Train Loss\")\n    plt.plot(epochs, val_loss[: len(epochs)], label=\"Val Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_loss_curves_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: Val Macro-F1 & CWA ----------\ntry:\n    metrics = run.get(\"metrics\", {}).get(\"val\", [])\n    macro_f1 = [m.get(\"macro_f1\") for m in metrics if m]\n    cwa = [m.get(\"cwa\") for m in metrics if m]\n    epochs = range(1, len(macro_f1) + 1)\n\n    plt.figure()\n    ax1 = plt.gca()\n    ax1.plot(epochs, macro_f1, \"b-\", label=\"Macro-F1\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Macro-F1\", color=\"b\")\n    ax2 = ax1.twinx()\n    ax2.plot(epochs, cwa, \"r--\", label=\"CWA\")\n    ax2.set_ylabel(\"CWA\", color=\"r\")\n    plt.title(\"SPR_BENCH Validation Metrics\\nLeft: Macro-F1, Right: CWA\")\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_val_metrics_remove_final_layernorm.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating metrics plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: Confusion Matrix ----------\ntry:\n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n    preds = np.array(run.get(\"predictions\", []))\n    gts = np.array(run.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        disp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n        disp.plot(cmap=\"Blues\", ax=plt.gca(), colorbar=False)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nFinal Dev Predictions\")\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_confusion_matrix_remove_final_layernorm.png\"\n        )\n        plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------- load ALL experiment_data -----------------\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_03dddee9063945c78a9e952454ebd4b8_proc_3448830/experiment_data.npy\",\n        \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_954a5ed87064423f96659d52a86570d1_proc_3448832/experiment_data.npy\",\n        \"experiments/2025-08-17_22-28-20_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_aa44a7465317479e9722d17c3f983963_proc_3448831/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n\n# helper ----------------------------------------------------\ndef pad_to_max(list_of_lists, pad_val=np.nan):\n    max_len = max(len(x) for x in list_of_lists)\n    arr = np.full((len(list_of_lists), max_len), pad_val, dtype=float)\n    for i, l in enumerate(list_of_lists):\n        arr[i, : len(l)] = l\n    return arr\n\n\n# ---------------- iterate over datasets --------------------\n# collect every dataset key that appears anywhere\ndataset_keys = set()\nfor exp in all_experiment_data:\n    for run_key in exp:\n        dataset_keys.update(exp[run_key].keys())\n\nfor dset in dataset_keys:\n    # ---------- aggregate losses -----------\n    try:\n        train_losses_runs, val_losses_runs = [], []\n        for exp in all_experiment_data:\n            for run_key in exp:\n                run = exp[run_key].get(dset, {})\n                losses = run.get(\"losses\", {})\n                if losses:\n                    train_losses_runs.append(list(losses.get(\"train\", [])))\n                    val_losses_runs.append(list(losses.get(\"val\", [])))\n        if train_losses_runs and val_losses_runs:\n            train_arr = pad_to_max(train_losses_runs)\n            val_arr = pad_to_max(val_losses_runs)\n            epochs = np.arange(1, train_arr.shape[1] + 1)\n\n            train_mean = np.nanmean(train_arr, axis=0)\n            val_mean = np.nanmean(val_arr, axis=0)\n            train_se = np.nanstd(train_arr, axis=0, ddof=1) / np.sqrt(\n                np.sum(~np.isnan(train_arr), axis=0)\n            )\n            val_se = np.nanstd(val_arr, axis=0, ddof=1) / np.sqrt(\n                np.sum(~np.isnan(val_arr), axis=0)\n            )\n\n            plt.figure()\n            plt.plot(epochs, train_mean, label=\"Train Loss (mean)\", color=\"tab:blue\")\n            plt.fill_between(\n                epochs,\n                train_mean - train_se,\n                train_mean + train_se,\n                color=\"tab:blue\",\n                alpha=0.3,\n                label=\"Train \u00b1 SE\",\n            )\n            plt.plot(epochs, val_mean, label=\"Val Loss (mean)\", color=\"tab:orange\")\n            plt.fill_between(\n                epochs,\n                val_mean - val_se,\n                val_mean + val_se,\n                color=\"tab:orange\",\n                alpha=0.3,\n                label=\"Val \u00b1 SE\",\n            )\n            plt.title(\n                f\"{dset} Aggregate Loss Curves\\nMean \u00b11 SE across {train_arr.shape[0]} runs\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_loss_curves_aggregate.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating aggregate loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- aggregate val metrics -----------\n    try:\n        macro_runs, cwa_runs = [], []\n        for exp in all_experiment_data:\n            for run_key in exp:\n                run = exp[run_key].get(dset, {})\n                metrics_seq = run.get(\"metrics\", {}).get(\"val\", [])\n                if metrics_seq:\n                    macro_runs.append([m.get(\"macro_f1\") for m in metrics_seq if m])\n                    cwa_runs.append([m.get(\"cwa\") for m in metrics_seq if m])\n        if macro_runs and cwa_runs:\n            macro_arr = pad_to_max(macro_runs)\n            cwa_arr = pad_to_max(cwa_runs)\n            epochs = np.arange(1, macro_arr.shape[1] + 1)\n\n            macro_mean = np.nanmean(macro_arr, axis=0)\n            cwa_mean = np.nanmean(cwa_arr, axis=0)\n            macro_se = np.nanstd(macro_arr, axis=0, ddof=1) / np.sqrt(\n                np.sum(~np.isnan(macro_arr), axis=0)\n            )\n            cwa_se = np.nanstd(cwa_arr, axis=0, ddof=1) / np.sqrt(\n                np.sum(~np.isnan(cwa_arr), axis=0)\n            )\n\n            plt.figure()\n            ax1 = plt.gca()\n            ax1.plot(epochs, macro_mean, color=\"b\", label=\"Macro-F1 (mean)\")\n            ax1.fill_between(\n                epochs,\n                macro_mean - macro_se,\n                macro_mean + macro_se,\n                color=\"b\",\n                alpha=0.3,\n                label=\"Macro-F1 \u00b1 SE\",\n            )\n            ax1.set_xlabel(\"Epoch\")\n            ax1.set_ylabel(\"Macro-F1\", color=\"b\")\n            ax2 = ax1.twinx()\n            ax2.plot(epochs, cwa_mean, color=\"r\", linestyle=\"--\", label=\"CWA (mean)\")\n            ax2.fill_between(\n                epochs,\n                cwa_mean - cwa_se,\n                cwa_mean + cwa_se,\n                color=\"r\",\n                alpha=0.3,\n                label=\"CWA \u00b1 SE\",\n            )\n            ax2.set_ylabel(\"CWA\", color=\"r\")\n            lines, labels = ax1.get_legend_handles_labels()\n            lines2, labels2 = ax2.get_legend_handles_labels()\n            plt.legend(lines + lines2, labels + labels2, loc=\"best\")\n            plt.title(\n                f\"{dset} Validation Metrics\\nMean \u00b11 SE across {macro_arr.shape[0]} runs\"\n            )\n            fname = os.path.join(working_dir, f\"{dset}_val_metrics_aggregate.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating aggregate val metric plot for {dset}: {e}\")\n        plt.close()\n"], "plot_plan": ["Load experiment_data.npy, loop over each dataset key, and safely extract losses, validation metrics, predictions, ground-truth labels, and optional weights. For every dataset, create up to five standard figures: (1) training vs. validation loss curves; (2) validation macro-F1 over epochs; (3) validation CWA over epochs; (4) a final-epoch confusion matrix; and (5) a histogram of per-example weights (if weight information exists). Each plotting section is wrapped in its own try-except block to guarantee figure closure even on error. Use clear titles that embed the dataset name and concise subtitles when appropriate, and save each figure to working_dir with descriptive filenames like \u201cSPR_BENCH_loss_curves.png\u201d. After plotting, compute and print the best and last macro-F1/CWA values so the script also outputs a quick numerical summary. All plotting relies exclusively on data present in experiment_data.npy; no synthetic values are introduced. The code begins with the required imports, ensures the working directory exists, and remains compact and easily extensible to multiple datasets.", "We first load the stored numpy dictionary, extract the only experiment (\u201cno_curriculum_weighting/SPR_BENCH\u201d), and recover epoch-wise arrays: training losses, validation losses, and the two validation metrics (macro-F1, CWA).  \nFour concise visualisations are generated\u2014one for the loss curves, one for macro-F1, one for CWA, and one confusion-matrix heat-map comparing final predictions with ground-truth.  \nEvery figure is wrapped in its own try-except block, given an informative title and a descriptive filename that embeds the dataset name and plot type, then saved in the predefined working_dir and closed.  \nIf any array is missing or empty the respective plot is skipped gracefully.  \nAfter plotting, we compute the best (maximum) macro-F1 and CWA obtained during validation and print them so the user can quickly assess peak performance.  \nThe script follows the mandated imports header, obeys the five-plot limit, uses vanilla matplotlib, and always closes figures\u2014even on error.  \nFinally, it verifies that the output directory exists before saving and reports where the plots were stored.", "We will load the saved experiment_data.npy object, detect the single run stored under the \u201cno_learned_pos_emb / SPR_BENCH\u201d keys and extract per-epoch losses and metrics plus the final predictions/labels/weights.  Five independent plots are produced (each wrapped in a try-except block): (1) train vs. validation loss over epochs; (2) macro-F1 and CWA over epochs; (3) histogram of token-level weight values; (4) scatter of weight versus correctness to show curriculum influence (correct predictions in one colour, wrong in another); and (5) a confusion-matrix heat-map for the final validation predictions.  Figures are saved into working_dir with descriptive names that include the dataset and plot type, e.g. \u201cSPR_BENCH_loss_curves.png\u201d.  If more than 12 epochs are present we down-sample to at most five tick labels/points, satisfying the \u201c\u22645 similar figures\u201d requirement.  All figures are closed after saving to avoid memory leakage.  After plotting, we also print the last-epoch macro-F1 and CWA so users can see numerical results directly.  The code is concise, uses only numpy/matplotlib, and never fabricates data; every plotted value comes from experiment_data.npy.  Finally, paths are created if missing to guarantee save success.", "We will load the saved experiment_data.npy, extract the single run \u201cmean_pooling_no_cls\u201d on the SPR_BENCH dataset and derive per-epoch arrays for training loss, validation loss, macro-F1 and CWA.  \nFour plots are produced, each wrapped in an independent try-except block: (1) training/validation loss curves, (2) macro-F1 versus epoch, (3) CWA versus epoch, and (4) a two-panel bar chart comparing class frequencies in ground-truth (left) and predictions (right) to satisfy the \u201cLeft: Ground Truth, Right: Generated Samples\u201d subtitle guideline.  \nAll plots follow basic matplotlib conventions, have clear titles with the dataset name, and are saved to the pre-defined working_dir using descriptive filenames (e.g. \u201cSPR_BENCH_loss_curve.png\u201d).  \nFigures are always closed after saving, regardless of success or failure, to prevent memory leakage.  \nEpoch indices are computed directly from the length of the stored loss list so we never invent data.  \nWe ensure that no more than five figures are generated, respecting the plot-count guideline.  \nFinally, the script can be run standalone; it prints a short confirmation message for each saved figure.", "We load experiment_data.npy, loop over experiments and the SPR_BENCH dataset, then (1) draw train/validation loss curves, (2) draw validation Macro-F1 and CWA over epochs, and (3) visualise a confusion-style scatter of ground-truth vs. predictions (sub-sampled to \u2264200 points to keep it readable).  Each figure is wrapped in its own try/except, saved to working_dir with descriptive names, and the figure is always closed.  We create at most these three plots (\u22645 as required).  After plotting we print the last-epoch metrics so the user can quickly inspect performance.  No values are invented; everything comes from experiment_data.npy.  The code is concise, uses only matplotlib, numpy and os, and follows the naming, title and closing conventions.  A list of successfully written files is also printed for convenience.", null, "We will load experiment_data.npy, extract the SPR_BENCH entry for the remove_final_layernorm run, and build three plots: (1) training vs. validation loss over epochs, (2) validation Macro-F1 and CWA over epochs (dual y-axis so both are visible), and (3) a confusion matrix of final predictions vs. ground-truth labels. Each plot is wrapped in its own try-except, saved to working_dir with descriptive filenames, given clear titles/subtitles, and closed afterwards. We respect the \u201c\u22645 figures\u201d rule and rely exclusively on the data actually present in the .npy file. Finally, the script prints confirmation messages so the user can see which figures were produced.", "We load experiment_data.npy, iterate over experiments and datasets, and for each dataset we: (1) draw training vs validation loss curves; (2) plot validation Macro-F1 and CWA over epochs; (3) show a normalized confusion matrix of final predictions; (4) visualise the distribution of example weights; and (5) scatter the per-example weights against a binary correctness indicator to reveal any bias. Each plot is wrapped in its own try-except, saved with descriptive filenames containing the dataset and plot type, and closed afterwards. No values are invented\u2014everything is taken directly from the numpy file. We also respect the \u201cat most five figures\u201d rule by limiting ourselves to these five plots per dataset. Finally, the script prints a short confirmation message for every successfully written figure.", "We first load experiment_data.npy, safely extracting the only available run (\u201cbi_lstm_backbone\u201d on \u201cSPR_BENCH\u201d).  From the nested dict we pull out per-epoch arrays for train/val loss, and lists of dicts that hold validation metrics (macro-F1 and CWA).  With this information we prepare up to five simple figures: (1) a line plot comparing train vs. validation loss across epochs; (2) a line plot of macro-F1 versus epoch; (3) a line plot of CWA versus epoch; (4) a confusion-matrix heat-map built from the final epoch\u2019s predictions and ground-truth; and (5) a histogram of example weights, colored by whether each example was predicted correctly.  Each figure is wrapped in its own try-except so that failures in one do not block the rest, every figure is titled clearly with dataset/model names, saved to working_dir with descriptive names, and then closed.  We also make sure to cap the number of figures at five and to close figures even when errors occur, complying with the provided plotting guidelines.  Finally, the script prints a confirmation line for each successfully written plot so the user knows where to find them.", null, null, null, null], "ablation_name": [null, "No Curriculum Complexity Weighting", "Remove Learned Positional Embedding", "Replace CLS Token with Mean-Pooling Read-out", "No Label Smoothing", "Replace AdamW with SGD + Momentum Optimizer", "Remove Final LayerNorm", "Remove Gradient Clipping", "Replace Transformer Encoder with Bi-LSTM Backbone", null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate the \u201cworking\u201d directory, load the saved NumPy dictionary,\nand iterate over each dataset contained in it.   For every dataset, the code\nretrieves (1) the final training loss, (2) the best (minimum) validation loss,\nand (3) the best values of every validation-set metric recorded (macro-F1 and\ncurriculum-weighted accuracy).   Each datum is printed with a clear, explicit\nlabel so there is no ambiguity about what is being shown.   The whole routine is\nplaced at the global level so it runs immediately when executed.", "The solution loads the saved NumPy dictionary, walks through each experiment and\ndataset, pulls out the recorded per-epoch vectors, derives the final training\nloss as well as the best (minimum or maximum) validation statistics, and prints\nthem with explicit labels so the output is self-explanatory.", "The script will locate the saved experiment_data.npy file inside the working\nsub-directory, load it into memory, and iterate through every ablation setting\nand dataset contained inside.   For each dataset it extracts (i) the final\ntraining loss, (ii) the best (minimum) validation loss, and (iii) the best\n(maximum) validation macro-F1 and CWA scores.   Each dataset name is printed\nfirst, followed by the metric names and the corresponding scalar values, clearly\nlabelled so there is no ambiguity.   All code is at top level so it executes\nimmediately when the file is run.", "The script will read `experiment_data.npy` from the `working` directory, turn it\ninto a Python dictionary, and iterate through every dataset that was logged.\nFor each dataset it prints:   \u2022 the final training loss (last element in the\ntraining-loss list)   \u2022 the best validation loss (minimum over the validation-\nloss list)   \u2022 the best validation macro-F1 score and its corresponding CWA\n(taken from the same epoch that achieved the highest macro-F1).   All output\nhappens immediately when the file is executed; nothing is placed under an `if\n__name__ == \"__main__\":` guard and no plots are created.", "We will load the NumPy file from the \u201cworking\u201d directory, navigate its nested\ndictionary, and for each dataset print (1) the final training loss, (2) the best\n(minimum) validation loss, and (3) the final values of every validation-time\nmetric that was logged (e.g., macro F1 and CWA). All names are printed\nexplicitly so nothing vague like \u201ctrain\u201d or \u201cval\u201d appears by itself. The script\nexecutes immediately when run, without any special entry point or plotting.", "The script will load the saved NumPy dictionary, iterate over every dataset\nunder every experiment, and print: \u2022 the final training loss,   \u2022 the best\n(lowest) validation loss, and   \u2022 the final validation macro-F1 and CWA.   All\nmetrics are clearly labelled and preceded by the dataset name, fulfilling the\nprinting guidelines without generating any plots or needing an entry-point\nguard.", "The script will locate the saved file in the \u201cworking\u201d directory, load it with\nNumPy\u2019s allow_pickle flag, and then iterate over every experiment and dataset\nfound in the nested dict.   For each dataset it will pull the recorded lists of\nlosses and validation metrics, select the last element (i.e., the final value\nafter training), and print them with explicit, descriptive names such as \u201cfinal\ntraining loss\u201d or \u201cfinal validation macro F1.\u201d   The code is kept at global\nscope so that it executes immediately when the file is run, and it avoids any\nplotting or unnecessary boiler-plate.", "The script will locate the saved numpy file in the working directory, load it\ninto a Python dictionary, and then iterate over every dataset contained in it.\nFor each dataset, it will compute and print the final training loss as well as\nthe best (i.e., minimum) validation loss, the best validation macro-F1 score,\nand the best validation class-weighted accuracy (CWA). Each metric is clearly\nlabeled, and printing is preceded by the dataset\u2019s name so the results are\nunambiguous. No plots are created and the code executes immediately when run.", "Below is a small utility that immediately loads the stored NumPy dictionary,\nfinds the final training loss and the best validation scores/losses for each\nrecorded dataset, and prints them with clear, explicit labels. The code follows\nall structural constraints (no `if __name__ == \"__main__\":` guard, no plots,\nruns on import).", "The script will locate the saved file in the \u201cworking\u201d directory, load it with\nNumPy\u2019s allow_pickle flag, and then iterate over every experiment and dataset\nfound in the nested dict.   For each dataset it will pull the recorded lists of\nlosses and validation metrics, select the last element (i.e., the final value\nafter training), and print them with explicit, descriptive names such as \u201cfinal\ntraining loss\u201d or \u201cfinal validation macro F1.\u201d   The code is kept at global\nscope so that it executes immediately when the file is run, and it avoids any\nplotting or unnecessary boiler-plate.", "The script will locate the saved file in the \u201cworking\u201d directory, load it with\nNumPy\u2019s allow_pickle flag, and then iterate over every experiment and dataset\nfound in the nested dict.   For each dataset it will pull the recorded lists of\nlosses and validation metrics, select the last element (i.e., the final value\nafter training), and print them with explicit, descriptive names such as \u201cfinal\ntraining loss\u201d or \u201cfinal validation macro F1.\u201d   The code is kept at global\nscope so that it executes immediately when the file is run, and it avoids any\nplotting or unnecessary boiler-plate.", "The script will locate the saved file in the \u201cworking\u201d directory, load it with\nNumPy\u2019s allow_pickle flag, and then iterate over every experiment and dataset\nfound in the nested dict.   For each dataset it will pull the recorded lists of\nlosses and validation metrics, select the last element (i.e., the final value\nafter training), and print them with explicit, descriptive names such as \u201cfinal\ntraining loss\u201d or \u201cfinal validation macro F1.\u201d   The code is kept at global\nscope so that it executes immediately when the file is run, and it avoids any\nplotting or unnecessary boiler-plate.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate the .npy file and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# helper to print best / final statistics\n# -------------------------------------------------\nfor dataset_name, ds_data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    # ----- metrics -----\n    val_metrics = ds_data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # Find the epoch with the best macro-F1\n        best_macro_f1_idx = int(\n            np.argmax([m.get(\"macro_f1\", -np.inf) for m in val_metrics])\n        )\n        best_metrics = val_metrics[best_macro_f1_idx]\n\n        macro_f1 = best_metrics.get(\"macro_f1\")\n        cwa = best_metrics.get(\"cwa\")\n\n        if macro_f1 is not None:\n            print(f\"best validation macro F1 score: {macro_f1:.6f}\")\n        if cwa is not None:\n            print(f\"best validation curriculum-weighted accuracy: {cwa:.6f}\")\n", "import os\nimport numpy as np\n\n# -------- load data --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------- utility helpers --------\ndef _best(lst, mode=\"min\"):\n    return (min if mode == \"min\" else max)(lst) if lst else None\n\n\n# -------- iterate and report --------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, record in datasets.items():\n        print(dataset_name)  # dataset header\n\n        # losses\n        train_losses = record[\"losses\"].get(\"train\", [])\n        val_losses = record[\"losses\"].get(\"val\", [])\n        if train_losses:\n            print(f\"final training loss: {train_losses[-1]:.6f}\")\n        if val_losses:\n            print(f\"best validation loss: {_best(val_losses, 'min'):.6f}\")\n\n        # validation metrics (macro-F1 and CWA)\n        val_metrics = record.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            macro_f1s = [m[\"macro_f1\"] for m in val_metrics]\n            cwas = [m[\"cwa\"] for m in val_metrics]\n            print(f\"best validation macro F1 score: {_best(macro_f1s, 'max'):.6f}\")\n            print(f\"best validation CWA score: {_best(cwas, 'max'):.6f}\")\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------\n# helper functions\n# -----------------------------------------------------------------------------\ndef best_metric(metric_list, key, mode=\"max\"):\n    \"\"\"\n    Return the best value for a given key inside a list of dictionaries.\n    mode='max' -> choose the maximum value, 'min' -> minimum value.\n    \"\"\"\n    values = [m[key] for m in metric_list if key in m]\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\ndef pretty_print(name, value, precision=4):\n    \"\"\"\n    Nicely format floating-point numbers; leave integers untouched.\n    \"\"\"\n    if value is None:\n        print(f\"    {name}: N/A\")\n    elif isinstance(value, float):\n        print(f\"    {name}: {value:.{precision}f}\")\n    else:\n        print(f\"    {name}: {value}\")\n\n\n# -----------------------------------------------------------------------------\n# data loading\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------\n# metric extraction and printing\n# -----------------------------------------------------------------------------\nfor ablation_key, datasets in experiment_data.items():\n    for dataset_name, data_dict in datasets.items():\n\n        # retrieve arrays / lists\n        train_losses = data_dict.get(\"losses\", {}).get(\"train\", [])\n        val_losses = data_dict.get(\"losses\", {}).get(\"val\", [])\n        val_metrics = data_dict.get(\"metrics\", {}).get(\"val\", [])\n\n        # compute best/final figures\n        final_train_loss = train_losses[-1] if train_losses else None\n        best_val_loss = min(val_losses) if val_losses else None\n        best_macro_f1 = best_metric(val_metrics, \"macro_f1\", mode=\"max\")\n        best_cwa = best_metric(val_metrics, \"cwa\", mode=\"max\")\n\n        # print results\n        print(f\"Dataset: {dataset_name} (setting: {ablation_key})\")\n        pretty_print(\"final training loss\", final_train_loss)\n        pretty_print(\"best validation loss\", best_val_loss)\n        pretty_print(\"best validation macro-F1\", best_macro_f1, precision=3)\n        pretty_print(\"best validation CWA\", best_cwa, precision=3)\n", "import os\nimport numpy as np\n\n# ---------------- load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------- helper --------------------\ndef print_metrics(exp_dict):\n    \"\"\"\n    exp_dict : nested dictionary coming from experiment_data.npy\n    \"\"\"\n    for _, datasets in exp_dict.items():  # model level\n        for dataset_name, content in datasets.items():  # dataset level\n            print(dataset_name)  # Requirement #3\n\n            # ----- losses -----\n            train_losses = content.get(\"losses\", {}).get(\"train\", [])\n            if train_losses:\n                print(\"final training loss:\", train_losses[-1])\n\n            val_losses = content.get(\"losses\", {}).get(\"val\", [])\n            if val_losses:\n                print(\"best validation loss:\", min(val_losses))\n\n            # ----- metrics -----\n            val_metrics = content.get(\"metrics\", {}).get(\"val\", [])\n            if val_metrics:\n                # find epoch with highest macro-F1\n                best_idx = max(\n                    range(len(val_metrics)),\n                    key=lambda i: val_metrics[i].get(\"macro_f1\", -1),\n                )\n                best_entry = val_metrics[best_idx]\n                print(\"best validation macro F1 score:\", best_entry.get(\"macro_f1\"))\n                print(\"corresponding validation CWA:\", best_entry.get(\"cwa\"))\n\n\n# ---------------- run summary ---------------\nprint_metrics(experiment_data)\n", "import os\nimport numpy as np\n\n# -------- load saved experiment data --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------- iterate and report metrics --------\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, dset_info in datasets.items():\n        print(f\"{dataset_name}:\")  # dataset header\n\n        # ----- losses -----\n        train_losses = dset_info.get(\"losses\", {}).get(\"train\", [])\n        if train_losses:\n            print(f\"final training loss: {train_losses[-1]:.4f}\")\n\n        val_losses = dset_info.get(\"losses\", {}).get(\"val\", [])\n        if val_losses:\n            best_val_loss = min(val_losses)\n            print(f\"best validation loss: {best_val_loss:.4f}\")\n\n        # ----- validation metrics -----\n        val_metrics = dset_info.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            final_metrics = val_metrics[-1]\n            for metric_name, value in final_metrics.items():\n                print(f\"final validation {metric_name}: {value:.4f}\")\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# 0. Resolve working directory and file path\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# 1. Load the experiment results (allow_pickle=True needed for arbitrary objects)\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2-5. Iterate, extract, and print required metrics\nfor exp_name, datasets in experiment_data.items():  # e.g. 'SGD_momentum'\n    for dataset_name, data in datasets.items():  # e.g. 'SPR_BENCH'\n        print(dataset_name)  # Dataset heading\n\n        # Losses\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n\n        if train_losses:\n            print(f\"final training loss: {train_losses[-1]:.6f}\")\n        if val_losses:\n            best_val_loss = min(val_losses)\n            print(f\"best validation loss: {best_val_loss:.6f}\")\n\n        # Validation metrics (macro-F1 and CWA stored per epoch)\n        val_metrics = data[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            final_metrics = val_metrics[-1]  # take last epoch\u2019s values\n            macro_f1 = final_metrics.get(\"macro_f1\")\n            cwa = final_metrics.get(\"cwa\")\n            if macro_f1 is not None:\n                print(f\"final validation macro F1 score: {macro_f1:.6f}\")\n            if cwa is not None:\n                print(f\"final validation class-weighted accuracy: {cwa:.6f}\")\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, result_dict in datasets.items():\n        print(f\"{dataset_name}:\")  # dataset header\n\n        # losses\n        train_losses = result_dict.get(\"losses\", {}).get(\"train\", [])\n        val_losses = result_dict.get(\"losses\", {}).get(\"val\", [])\n\n        if train_losses:\n            print(f\"  final training loss: {train_losses[-1]:.6f}\")\n        if val_losses:\n            print(f\"  final validation loss: {val_losses[-1]:.6f}\")\n\n        # validation metrics (macro F1, CWA, etc.)\n        val_metrics = result_dict.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            final_metrics = val_metrics[-1]  # last epoch\u2019s metrics\n            for metric_name, metric_value in final_metrics.items():\n                print(f\"  final validation {metric_name}: {metric_value:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- extract and print ----------\nfor exp_name, exp_blob in experiment_data.items():  # e.g. \"no_grad_clip\"\n    for dataset_name, data in exp_blob.items():  # e.g. \"SPR_BENCH\"\n        print(dataset_name)  # requirement #3\n\n        # ---- training loss ----\n        train_losses = data.get(\"losses\", {}).get(\"train\", [])\n        if train_losses:\n            print(f\"final training loss: {train_losses[-1]:.6f}\")\n\n        # ---- validation loss ----\n        val_losses = data.get(\"losses\", {}).get(\"val\", [])\n        if val_losses:\n            best_val_loss = min(val_losses)\n            print(f\"best validation loss: {best_val_loss:.6f}\")\n\n        # ---- validation metrics ----\n        val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            macro_f1_vals = [m[\"macro_f1\"] for m in val_metrics if \"macro_f1\" in m]\n            cwa_vals = [m[\"cwa\"] for m in val_metrics if \"cwa\" in m]\n\n            if macro_f1_vals:\n                print(f\"best validation macro F1 score: {max(macro_f1_vals):.6f}\")\n            if cwa_vals:\n                print(f\"best validation class-weighted accuracy: {max(cwa_vals):.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- load saved experiment ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\ndef _best_val_index(metric_list, larger_is_better=True):\n    \"\"\"Return index of best value in a list according to the criterion.\"\"\"\n    if not metric_list:  # empty list guard\n        return None\n    arr = np.array(metric_list, dtype=float)  # convert to ndarray\n    return int(arr.argmax() if larger_is_better else arr.argmin())\n\n\n# -------- iterate over models and datasets --------\nfor model_name, model_block in experiment_data.items():\n    for dataset_name, ds_block in model_block.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---- training loss (final) ----\n        train_losses = ds_block.get(\"losses\", {}).get(\"train\", [])\n        if train_losses:\n            print(f\"training loss: {train_losses[-1]:.6f}\")\n\n        # ---- validation loss (best) ----\n        val_losses = ds_block.get(\"losses\", {}).get(\"val\", [])\n        idx_best_val_loss = _best_val_index(val_losses, larger_is_better=False)\n        if idx_best_val_loss is not None:\n            print(f\"validation loss: {val_losses[idx_best_val_loss]:.6f}\")\n\n        # ---- validation metrics (best) ----\n        val_metrics = ds_block.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            # choose the epoch with the best macro-F1\n            macro_f1_values = [m.get(\"macro_f1\") for m in val_metrics]\n            idx_best_f1 = _best_val_index(macro_f1_values, larger_is_better=True)\n\n            best_metrics = val_metrics[idx_best_f1]\n            print(f\"validation macro F1 score: {best_metrics['macro_f1']:.6f}\")\n            # other stored metrics\n            if \"cwa\" in best_metrics:\n                print(f\"validation CWA: {best_metrics['cwa']:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, result_dict in datasets.items():\n        print(f\"{dataset_name}:\")  # dataset header\n\n        # losses\n        train_losses = result_dict.get(\"losses\", {}).get(\"train\", [])\n        val_losses = result_dict.get(\"losses\", {}).get(\"val\", [])\n\n        if train_losses:\n            print(f\"  final training loss: {train_losses[-1]:.6f}\")\n        if val_losses:\n            print(f\"  final validation loss: {val_losses[-1]:.6f}\")\n\n        # validation metrics (macro F1, CWA, etc.)\n        val_metrics = result_dict.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            final_metrics = val_metrics[-1]  # last epoch\u2019s metrics\n            for metric_name, metric_value in final_metrics.items():\n                print(f\"  final validation {metric_name}: {metric_value:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, result_dict in datasets.items():\n        print(f\"{dataset_name}:\")  # dataset header\n\n        # losses\n        train_losses = result_dict.get(\"losses\", {}).get(\"train\", [])\n        val_losses = result_dict.get(\"losses\", {}).get(\"val\", [])\n\n        if train_losses:\n            print(f\"  final training loss: {train_losses[-1]:.6f}\")\n        if val_losses:\n            print(f\"  final validation loss: {val_losses[-1]:.6f}\")\n\n        # validation metrics (macro F1, CWA, etc.)\n        val_metrics = result_dict.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            final_metrics = val_metrics[-1]  # last epoch\u2019s metrics\n            for metric_name, metric_value in final_metrics.items():\n                print(f\"  final validation {metric_name}: {metric_value:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, result_dict in datasets.items():\n        print(f\"{dataset_name}:\")  # dataset header\n\n        # losses\n        train_losses = result_dict.get(\"losses\", {}).get(\"train\", [])\n        val_losses = result_dict.get(\"losses\", {}).get(\"val\", [])\n\n        if train_losses:\n            print(f\"  final training loss: {train_losses[-1]:.6f}\")\n        if val_losses:\n            print(f\"  final validation loss: {val_losses[-1]:.6f}\")\n\n        # validation metrics (macro F1, CWA, etc.)\n        val_metrics = result_dict.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            final_metrics = val_metrics[-1]  # last epoch\u2019s metrics\n            for metric_name, metric_value in final_metrics.items():\n                print(f\"  final validation {metric_name}: {metric_value:.6f}\")\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.619650', '\\n', 'best\nvalidation loss: 0.626568', '\\n', 'best validation macro F1 score: 0.695981',\n'\\n', 'best validation curriculum-weighted accuracy: 0.696000', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.635918', '\\n', 'best validation\nloss: 0.636898', '\\n', 'best validation macro F1 score: 0.695981', '\\n', 'best\nvalidation CWA score: 0.696000', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH (setting: no_learned_pos_emb)', '\\n', '    final training\nloss: 0.5332', '\\n', '    best validation loss: 0.6357', '\\n', '    best\nvalidation macro-F1: 0.688', '\\n', '    best validation CWA: 0.688', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss:', ' ', '0.4244226856231689', '\\n',\n'best validation loss:', ' ', '0.6420555820465088', '\\n', 'best validation macro\nF1 score:', ' ', '0.6858982310268527', '\\n', 'corresponding validation CWA:', '\n', '0.686', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH:', '\\n', 'final training loss: 0.6114', '\\n', 'best validation loss:\n0.6218', '\\n', 'final validation macro_f1: 0.6920', '\\n', 'final validation cwa:\n0.6920', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.464353', '\\n', 'best validation\nloss: 0.689036', '\\n', 'final validation macro F1 score: 0.335106', '\\n', 'final\nvalidation class-weighted accuracy: 0.504000', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  final training loss: 0.622285', '\\n', '  final\nvalidation loss: 0.626422', '\\n', '  final validation macro_f1: 0.695981', '\\n',\n'  final validation cwa: 0.696000', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.620159', '\\n', 'best validation\nloss: 0.626774', '\\n', 'best validation macro F1 score: 0.695981', '\\n', 'best\nvalidation class-weighted accuracy: 0.696000', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'training loss: 0.420365', '\\n', 'validation\nloss: 0.645231', '\\n', 'validation macro F1 score: 0.680053', '\\n', 'validation\nCWA: 0.682000', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH:', '\\n', '  final training loss: 0.623832', '\\n', '  final\nvalidation loss: 0.634971', '\\n', '  final validation macro_f1: 0.693989', '\\n',\n'  final validation cwa: 0.694000', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  final training loss: 0.621343', '\\n', '  final\nvalidation loss: 0.629241', '\\n', '  final validation macro_f1: 0.689939', '\\n',\n'  final validation cwa: 0.690000', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  final training loss: 0.628429', '\\n', '  final\nvalidation loss: 0.637515', '\\n', '  final validation macro_f1: 0.685898', '\\n',\n'  final validation cwa: 0.686000', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}