{"edges": [[0, 1], [0, 4], [0, 2], [0, 7], [0, 3], [1, 8], [1, 5], [1, 6], [6, 11], [6, 9], [6, 10], [6, 12]], "layout": [[0.6363636363636364, 0.0], [0.2727272727272727, 0.33333333333333337], [0.45454545454545453, 0.33333333333333337], [0.6363636363636364, 0.33333333333333337], [0.8181818181818182, 0.33333333333333337], [0.09090909090909091, 0.6666666666666667], [0.2727272727272727, 0.6666666666666667], [1.0, 0.33333333333333337], [0.45454545454545453, 0.6666666666666667], [0.0, 1.0], [0.18181818181818182, 1.0], [0.36363636363636365, 1.0], [0.5454545454545454, 1.0]], "plan": ["We start by building a character\u2013level vocabulary from every symbol that appears\nin the SPR_BENCH train + dev + test splits, then encode each sequence as an\ninteger array, padding with 0 up to the global maximum length. A very small\nneural baseline is used: an Embedding layer (pad-idx = 0) feeding a single-layer\nbidirectional GRU whose final hidden state is passed to a Linear classifier;\nthis keeps the model lightweight yet capable of capturing order information. We\ntrain for only a few epochs with Adam and BCE-with-logits loss, monitoring\nvalidation loss and Matthews Correlation Coefficient (MCC) after each epoch. All\ntensors and the model are moved to GPU when available, complying with the\nmandatory device handling rules. Metrics, losses, predictions and ground-truth\nlabels are logged in the experiment_data dict and saved as a NumPy file under\n./working. After training, we evaluate on the held-out test set, printing MCC as\nthe key headline figure. This provides a fully working end-to-end baseline\u2014from\ndata loading through evaluation\u2014on which future, more sophisticated SPR\nalgorithms can iterate.", "Hyperparam tuning name: epochs. The solution expands the baseline by exploring\nmultiple maximum-epoch values and adding an early-stopping mechanism that halts\ntraining if the validation MCC stops improving. For each epoch-budget (5, 10,\n15, 20, 25, 30) a fresh GRU model is trained, the best checkpoint (w.r.t. MCC)\nis kept, and full metrics/losses are logged in `experiment_data`. Everything is\nstored in a single self-contained script and saved as `experiment_data.npy`.", "Hyperparam tuning name: learning_rate. We sweep a small set of candidate\nlearning-rates (5e-4, 1e-3, 2e-3).   For every value we (1) instantiate a fresh\nGRU baseline model, (2) train it for a few epochs, (3) record train/validation\nloss and MCC per epoch, and (4) evaluate on the test set.   All results are\nstored in a nested experiment_data dict under the key \"learning_rate\" with one\nsub-entry per lr, then serialized to working/experiment_data.npy.   The rest of\nthe pipeline (data loading, vocab building, model, training loop) is unchanged\nfrom the baseline.", "Hyperparam tuning name: batch_size. We keep the original pipeline but wrap it in\na loop that iterates over the candidate mini-batch sizes (64, 128, 256, 512).\nFor every value we (1) rebuild fresh DataLoaders, (2) instantiate a new GRU\nbaseline, (3) train for a small fixed number of epochs, (4) track per-epoch loss\nand MCC on train/dev, and (5) finally evaluate on the test split.   All\nstatistics, predictions and ground-truth labels are stored under\nexperiment_data['batch_size']['SPR_BENCH'][f'bs_{B}'] so that downstream\nplotting code can compare curves across batch sizes.   After the sweep finishes\nwe dump the whole dictionary to experiment_data.npy as required.", "Hyperparam tuning name: weight_decay. We keep the architecture and all training\nsettings unchanged, but loop over the candidate weight-decay coefficients {0,\n1e-5, 5e-5, 1e-4, 5e-4, 1e-3}.   For each value we create a fresh model, train\nit for 5 epochs, store epoch-wise losses/MCCs, and retain the model that\nachieves the highest validation MCC after the last epoch.   Finally we evaluate\nthis best model on the test split, record its predictions and ground-truth, and\nsave every collected array in the required experiment_data structure.", "The bug arose because `encode_sequence` was defined to accept only the sequence\nstring while it was invoked with two arguments (`seq, self.vocab`) inside\n`SPRTorchDataset.__getitem__`.   The fix is to let `encode_sequence` explicitly\naccept a `vocab` parameter and update all calls accordingly.   While touching\nthe code, we keep the model architecture unchanged, add macro-F1 tracking,\ncomply with GPU/CPU handling rules, and ensure metrics and artefacts are saved\nto the `working` directory.", "The crash occurred because `encode_sequence` was written to accept only the raw\nstring but was called with an extra `vocab` argument.  The fix is to extend the\nhelper so it explicitly takes (seq, vocab) and adjust its callers, guaranteeing\nconsistent signatures.  While touching the data pipeline we also clean the\n`__getitem__` logic, add per\u2010epoch macro-F1 tracking (required metric), keep the\nGRU baseline architecture intact, and respect strict GPU/CPU handling rules.\nAll metrics, losses and predictions are stored in the prescribed\n`experiment_data` structure and saved to the `./working` directory.  Early-\nstopping, reproducibility seeds, and a small epoch grid remain for hyper-\nparameter tuning.", "Hyperparam tuning name: dropout_rate. We extend the baseline by inserting a\n`nn.Dropout` layer between the bidirectional-GRU and the final linear layer, and\nsweep dropout probabilities p \u2208 {0.1, 0.3, 0.5}.   For every rate we train for\nfive epochs, log losses/MCC on train & dev splits, and keep the model that\nachieves the best validation MCC.   All plottable data for every run are saved\nin `experiment_data.npy` under the key `dropout_rate`; predictions/ground-truth\nstored only for the best model.", "We simply remove the superfluous vocab argument when encoding each sequence\ninside the PyTorch\u2010Dataset. The helper already closes over the global vocab, so\npassing it again caused a signature mismatch. No other logic changes are\nrequired in the model; we only adjust a few hyper-parameters (lower LR, larger\nbatch, more epochs) for the tuning stage, add macro-F1 tracking, and comply with\nall GPU / saving guidelines.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nimport pathlib\nfrom typing import Dict, List\nfrom datasets import DatasetDict\n\n# ------------------------------------------------------------------\n# 0. GPU / CPU handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ------------------------------------------------------------------\n# 1. Data loading ---------------------------------------------------\n# Assumes SPR_BENCH exists at this location. Change if necessary.\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr)\n\n\n# ------------------------------------------------------------------\n# 2. Build vocabulary & encode -------------------------------------\ndef build_vocab(dsets) -> Dict[str, int]:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(list(s))\n    vocab = {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n    return vocab\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1  # +PAD\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab[ch] for ch in seq]\n\n\n# compute max length\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\nprint(f\"Max sequence length: {max_len}\")\n\n\ndef pad(seq_ids: List[int], max_len: int) -> List[int]:\n    if len(seq_ids) >= max_len:\n        return seq_ids[:max_len]\n    return seq_ids + [0] * (max_len - len(seq_ids))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.labels = hf_split[\"label\"]\n        self.seqs = hf_split[\"sequence\"]\n        self.vocab = vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        seq_ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(seq_ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=batch_size)\ntest_loader = DataLoader(test_ds, batch_size=batch_size)\n\n\n# ------------------------------------------------------------------\n# 3. Model ----------------------------------------------------------\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)  # h: [2, B, H]\n        h = torch.cat([h[0], h[1]], dim=1)  # [B, 2H]\n        out = self.fc(h).squeeze(1)  # [B]\n        return out\n\n\nmodel = GRUBaseline(vocab_size).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n# ------------------------------------------------------------------\n# 4. Experiment data container -------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------------------------------------------------------\n# 5. Training loop --------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    # ---- train ---\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = running_loss / len(train_ds)\n    # Approx train MCC on one pass (optional quick)\n    with torch.no_grad():\n        preds_all, labels_all = [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"])\n            preds_all.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels_all.append(batch[\"labels\"].cpu().numpy())\n        train_mcc = matthews_corrcoef(\n            np.concatenate(labels_all), np.concatenate(preds_all)\n        )\n    # ---- validation ---\n    model.eval()\n    val_loss = 0.0\n    preds_all, labels_all = [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            preds_all.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels_all.append(batch[\"labels\"].cpu().numpy())\n    val_loss /= len(dev_ds)\n    val_mcc = matthews_corrcoef(np.concatenate(labels_all), np.concatenate(preds_all))\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_mcc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_mcc)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_MCC={val_mcc:.4f}\"\n    )\n\n# ------------------------------------------------------------------\n# 6. Final evaluation on test --------------------------------------\nmodel.eval()\npreds_all, labels_all = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        logits = model(batch[\"input_ids\"])\n        preds_all.append((logits.sigmoid() > 0.5).cpu().numpy())\n        labels_all.append(batch[\"labels\"].cpu().numpy())\npreds_all = np.concatenate(preds_all)\nlabels_all = np.concatenate(labels_all)\ntest_mcc = matthews_corrcoef(labels_all, preds_all)\nprint(f\"Test MCC: {test_mcc:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all\n\n# ------------------------------------------------------------------\n# 7. Save experiment data ------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------- Reproducibility -----------------------------#\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ------------------- Experiment container ------------------------#\nexperiment_data = {\n    \"epochs\": {  # hyperparam tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"results\": []  # list of dicts, one per epoch\u2013budget\n        }\n    }\n}\n# ------------------- Device --------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n# ------------------- Data loading --------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------- Vocab / encoding ----------------------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0=PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str) -> list[int]:\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    return seq_ids[:L] + [0] * (L - len(seq_ids)) if len(seq_ids) < L else seq_ids[:L]\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size)\ntest_loader = DataLoader(test_ds, batch_size)\n\n\n# ------------------- Model ---------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ------------------- EarlyStopping -------------------------------#\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=1e-4, mode=\"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, metric):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ------------------- Training routine ----------------------------#\ndef train_for_epochs(max_epochs: int, patience: int = 3):\n    model = GRUBaseline(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    es = EarlyStopping(patience=patience, mode=\"max\")\n    logs = {\"losses\": {\"train\": [], \"val\": []}, \"metrics\": {\"train\": [], \"val\": []}}\n    best_state = None\n    best_mcc = -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        # ---- Train ----\n        model.train()\n        run_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = run_loss / len(train_ds)\n        # quick train mcc\n        model.eval()\n        preds, labels = [], []\n        with torch.no_grad():\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                p = model(batch[\"input_ids\"]).sigmoid().cpu().numpy() > 0.5\n                preds.append(p)\n                labels.append(batch[\"labels\"].cpu().numpy())\n        train_mcc = matthews_corrcoef(np.concatenate(labels), np.concatenate(preds))\n\n        # ---- Validation ----\n        val_loss, preds, labels = 0.0, [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                val_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                    \"labels\"\n                ].size(0)\n                preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n                labels.append(batch[\"labels\"].cpu().numpy())\n        val_loss /= len(dev_ds)\n        val_mcc = matthews_corrcoef(np.concatenate(labels), np.concatenate(preds))\n\n        logs[\"losses\"][\"train\"].append(train_loss)\n        logs[\"losses\"][\"val\"].append(val_loss)\n        logs[\"metrics\"][\"train\"].append(train_mcc)\n        logs[\"metrics\"][\"val\"].append(val_mcc)\n        print(\n            f\"[max={max_epochs}] Epoch {epoch}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_MCC={val_mcc:.4f}\"\n        )\n\n        # save best & early stop\n        if val_mcc > best_mcc:\n            best_mcc = val_mcc\n            best_state = model.state_dict()\n        if es(val_mcc):\n            print(\"Early stopping triggered.\")\n            break\n\n    # ---------------- Test with best checkpoint -------------------#\n    model.load_state_dict(best_state)\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    test_mcc = matthews_corrcoef(labels, preds)\n    print(f\"[max={max_epochs}] Test MCC: {test_mcc:.4f}\")\n\n    result = {\n        \"config\": {\"max_epochs\": max_epochs, \"patience\": patience},\n        \"losses\": logs[\"losses\"],\n        \"metrics\": logs[\"metrics\"],\n        \"test_mcc\": test_mcc,\n        \"predictions\": preds,\n        \"ground_truth\": labels,\n    }\n    return result\n\n\n# ------------------- Hyperparameter sweep ------------------------#\nepoch_budgets = [5, 10, 15, 20, 25, 30]\nfor ep in epoch_budgets:\n    res = train_for_epochs(ep, patience=3)\n    experiment_data[\"epochs\"][\"SPR_BENCH\"][\"results\"].append(res)\n\n# ------------------- Save all ------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom typing import Dict, List\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------------\n# 0. Repro / env ----------------------------------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# 1. Data loading ---------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr)\n\n\n# ------------------------------------------------------------------\n# 2. Vocab + encoding ----------------------------------------------\ndef build_vocab(dsets) -> Dict[str, int]:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\nprint(f\"Vocab size: {vocab_size}  |  Max len: {max_len}\")\n\n\ndef encode_sequence(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: List[int], max_len: int) -> List[int]:\n    return (seq_ids + [0] * max_len)[:max_len]\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.labels = hf_split[\"label\"]\n        self.seqs = hf_split[\"sequence\"]\n        self.vocab = vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=batch_size)\ntest_loader = DataLoader(test_ds, batch_size=batch_size)\n\n\n# ------------------------------------------------------------------\n# 3. Model ----------------------------------------------------------\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)  # h shape: [2,B,H]\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ------------------------------------------------------------------\n# 4. Hyper-param sweep ---------------------------------------------\nlearning_rates = [5e-4, 1e-3, 2e-3]\nepochs = 5\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"learning_rate\": {}}\n\nfor lr in learning_rates:\n    lr_key = f\"lr_{lr:.0e}\"\n    print(f\"\\n=== Training with learning rate {lr} ===\")\n    # prepare containers\n    experiment_data[\"learning_rate\"][lr_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # model, criterion, optimizer\n    model = GRUBaseline(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # -- training epochs ------------------------------------------\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = running_loss / len(train_ds)\n        # train MCC quick pass\n        with torch.no_grad():\n            preds, labels = [], []\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                p = model(batch[\"input_ids\"]).sigmoid().cpu().numpy() > 0.5\n                preds.append(p)\n                labels.append(batch[\"labels\"].cpu().numpy())\n            train_mcc = matthews_corrcoef(np.concatenate(labels), np.concatenate(preds))\n        # validation\n        model.eval()\n        val_loss = 0.0\n        preds, labels = [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n                labels.append(batch[\"labels\"].cpu().numpy())\n        val_loss /= len(dev_ds)\n        val_mcc = matthews_corrcoef(np.concatenate(labels), np.concatenate(preds))\n        # store\n        ed = experiment_data[\"learning_rate\"][lr_key]\n        ed[\"losses\"][\"train\"].append(train_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train\"].append(train_mcc)\n        ed[\"metrics\"][\"val\"].append(val_mcc)\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_MCC={val_mcc:.4f}\"\n        )\n\n    # -- test evaluation ------------------------------------------\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    test_mcc = matthews_corrcoef(labels, preds)\n    print(f\"Test MCC @ {lr}: {test_mcc:.4f}\")\n    ed = experiment_data[\"learning_rate\"][lr_key]\n    ed[\"predictions\"] = preds\n    ed[\"ground_truth\"] = labels\n    # free memory\n    del model\n    torch.cuda.empty_cache()\n\n# ------------------------------------------------------------------\n# 5. Save -----------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import DatasetDict, load_dataset\nfrom typing import Dict, List\n\n# ------------------------------------------------------------------\n# Reproducibility ---------------------------------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# ------------------------------------------------------------------\n# GPU / CPU handling ------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# Data loading ------------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dd[split] = _load(f\"{split}.csv\")\n    return dd\n\n\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr)\n\n\n# ------------------------------------------------------------------\n# Vocab & helpers ---------------------------------------------------\ndef build_vocab(dsets) -> Dict[str, int]:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 = PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: List[int], max_len: int) -> List[int]:\n    if len(seq_ids) >= max_len:\n        return seq_ids[:max_len]\n    return seq_ids + [0] * (max_len - len(seq_ids))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.labels = hf_split[\"label\"]\n        self.seqs = hf_split[\"sequence\"]\n        self.vocab = vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n\n\n# ------------------------------------------------------------------\n# Model -------------------------------------------------------------\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ------------------------------------------------------------------\n# Experiment container ---------------------------------------------\nexperiment_data = {\"batch_size\": {\"SPR_BENCH\": {}}}\n\n# ------------------------------------------------------------------\n# Hyper-parameter sweep --------------------------------------------\nbatch_sizes = [64, 128, 256, 512]\nepochs = 5\n\nfor bs in batch_sizes:\n    print(f\"\\n==== Experiment: batch_size = {bs} ====\")\n    # DataLoaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    dev_loader = DataLoader(dev_ds, batch_size=bs)\n    test_loader = DataLoader(test_ds, batch_size=bs)\n\n    # Model / loss / optimiser\n    model = GRUBaseline(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Containers for this setting\n    exp_key = f\"bs_{bs}\"\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][exp_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    holder = experiment_data[\"batch_size\"][\"SPR_BENCH\"][exp_key]\n\n    # -------- Training loop ----------\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = running_loss / len(train_ds)\n\n        # quick train MCC\n        model.eval()\n        preds, labels = [], []\n        with torch.no_grad():\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                p = (model(batch[\"input_ids\"]).sigmoid() > 0.5).cpu().numpy()\n                preds.append(p)\n                labels.append(batch[\"labels\"].cpu().numpy())\n        train_mcc = matthews_corrcoef(np.concatenate(labels), np.concatenate(preds))\n\n        # validation\n        val_loss, preds, labels = 0.0, [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"labels\"])\n                val_loss += loss.item() * batch[\"labels\"].size(0)\n                preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n                labels.append(batch[\"labels\"].cpu().numpy())\n        val_loss /= len(dev_ds)\n        val_mcc = matthews_corrcoef(np.concatenate(labels), np.concatenate(preds))\n\n        holder[\"losses\"][\"train\"].append(train_loss)\n        holder[\"losses\"][\"val\"].append(val_loss)\n        holder[\"metrics\"][\"train\"].append(train_mcc)\n        holder[\"metrics\"][\"val\"].append(val_mcc)\n\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_MCC {val_mcc:.4f}\"\n        )\n\n    # -------- Final test evaluation ----------\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    test_mcc = matthews_corrcoef(labels, preds)\n    print(f\"Test MCC (bs={bs}): {test_mcc:.4f}\")\n\n    holder[\"predictions\"] = preds\n    holder[\"ground_truth\"] = labels\n    holder[\"test_mcc\"] = test_mcc\n\n# ------------------------------------------------------------------\n# Save experiment data ---------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom typing import Dict, List\nfrom datasets import DatasetDict, load_dataset\n\n# --------------------------- paths / device -------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# --------------------------- load SPR-BENCH -------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr)\n\n\n# --------------------------- vocabulary & encode --------------------------\ndef build_vocab(dsets) -> Dict[str, int]:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 = PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str) -> List[int]:\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: List[int]) -> List[int]:\n    return (\n        seq_ids[:max_len] + [0] * (max_len - len(seq_ids))\n        if len(seq_ids) < max_len\n        else seq_ids[:max_len]\n    )\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.labels = hf_split[\"label\"]\n        self.seqs = hf_split[\"sequence\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = torch.tensor(pad(encode_sequence(self.seqs[idx])), dtype=torch.long)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return {\"input_ids\": ids, \"labels\": lbl}\n\n\ntrain_ds, dev_ds, test_ds = (SPRTorchDataset(spr[x]) for x in [\"train\", \"dev\", \"test\"])\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=batch_size)\ntest_loader = DataLoader(test_ds, batch_size=batch_size)\n\n\n# --------------------------- model ----------------------------------------\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\ncriterion = nn.BCEWithLogitsLoss()\nweight_decays = [0.0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\n\n# --------------------------- experiment data ------------------------------\nexperiment_data = {\"weight_decay_tuning\": {}}\n\n\ndef train_one_model(wd: float, epochs: int = 5):\n    model = GRUBaseline(vocab_size).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    tr_losses, tr_mcc, vl_losses, vl_mcc = [], [], [], []\n    for ep in range(epochs):\n        # training\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        tr_losses.append(running_loss / len(train_ds))\n        # compute train MCC quickly\n        with torch.no_grad():\n            preds, lbls = [], []\n            for b in train_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                preds.append((model(b[\"input_ids\"]).sigmoid() > 0.5).cpu().numpy())\n                lbls.append(b[\"labels\"].cpu().numpy())\n        tr_mcc.append(matthews_corrcoef(np.concatenate(lbls), np.concatenate(preds)))\n        # validation\n        model.eval()\n        val_loss = 0.0\n        preds, lbls = [], []\n        with torch.no_grad():\n            for b in dev_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                logits = model(b[\"input_ids\"])\n                val_loss += criterion(logits, b[\"labels\"]).item() * b[\"labels\"].size(0)\n                preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n                lbls.append(b[\"labels\"].cpu().numpy())\n        vl_losses.append(val_loss / len(dev_ds))\n        vl_mcc.append(matthews_corrcoef(np.concatenate(lbls), np.concatenate(preds)))\n        print(\n            f\"wd={wd}  epoch={ep+1}  train_loss={tr_losses[-1]:.4f}  \"\n            f\"val_loss={vl_losses[-1]:.4f}  val_MCC={vl_mcc[-1]:.4f}\"\n        )\n    return model, {\n        \"metrics\": {\"train\": tr_mcc, \"val\": vl_mcc},\n        \"losses\": {\"train\": tr_losses, \"val\": vl_losses},\n    }\n\n\nbest_val_mcc, best_wd, best_model = -1.0, None, None\nfor wd in weight_decays:\n    model, logs = train_one_model(wd)\n    experiment_data[\"weight_decay_tuning\"][str(wd)] = logs\n    current_val_mcc = logs[\"metrics\"][\"val\"][-1]\n    if current_val_mcc > best_val_mcc:\n        best_val_mcc, best_wd, best_model = current_val_mcc, wd, model\n\nprint(f\"\\nBest weight_decay = {best_wd}  with val_MCC = {best_val_mcc:.4f}\")\n\n# --------------------------- final test eval ------------------------------\nbest_model = best_model.to(device).eval()\npreds_all, labels_all = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = best_model(batch[\"input_ids\"])\n        preds_all.append((logits.sigmoid() > 0.5).cpu().numpy())\n        labels_all.append(batch[\"labels\"].cpu().numpy())\npreds_all = np.concatenate(preds_all)\nlabels_all = np.concatenate(labels_all)\ntest_mcc = matthews_corrcoef(labels_all, preds_all)\nprint(f\"Test MCC with best weight_decay: {test_mcc:.4f}\")\n\n# store test data in the same dict\nexperiment_data[\"weight_decay_tuning\"][\"best_wd\"] = best_wd\nexperiment_data[\"weight_decay_tuning\"][\"predictions\"] = preds_all\nexperiment_data[\"weight_decay_tuning\"][\"ground_truth\"] = labels_all\nexperiment_data[\"weight_decay_tuning\"][\"test_mcc\"] = test_mcc\n\n# --------------------------- save -----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------------#\n#  I/O SET-UP\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\n#  DEVICE\n# ------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------#\n#  REPRODUCIBILITY\n# ------------------------------------------------------------------#\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ------------------------------------------------------------------#\n#  DATA LOADING\n# ------------------------------------------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nassert DATA_PATH.exists(), f\"Dataset path {DATA_PATH} not found.\"\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------------------------------------------------------#\n#  VOCAB & ENCODING (bug-fixed)\n# ------------------------------------------------------------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        chars.update(\"\".join(split[\"sequence\"]))\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str, vocab: dict) -> list[int]:\n    # BUG-FIX: explicit vocab argument to match call site\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    return seq_ids[:L] + [0] * (L - len(seq_ids)) if len(seq_ids) < L else seq_ids[:L]\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size)\n\n\n# ------------------------------------------------------------------#\n#  MODEL (unchanged architecture)\n# ------------------------------------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ------------------------------------------------------------------#\n#  EARLY STOPPING\n# ------------------------------------------------------------------#\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=1e-4, mode=\"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, metric):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ------------------------------------------------------------------#\n#  EXPERIMENT TRACKING STRUCTURE\n# ------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_mcc\": [], \"val_mcc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"test\": {},\n    }\n}\n\n\n# ------------------------------------------------------------------#\n#  TRAINING LOOP\n# ------------------------------------------------------------------#\ndef train_for_epochs(max_epochs, patience=3, lr=1e-3):\n    model = GRUBaseline(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    early_stop = EarlyStopping(patience=patience, mode=\"max\")\n    best_state, best_val_mcc = None, -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        # ------------------ Train -------------------#\n        model.train()\n        thr_loss = 0.0\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            thr_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = thr_loss / len(train_loader.dataset)\n\n        # metrics on train split\n        model.eval()\n        train_preds, train_lbls = [], []\n        with torch.no_grad():\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                probs = torch.sigmoid(model(batch[\"input_ids\"])).cpu().numpy()\n                train_preds.append(probs > 0.5)\n                train_lbls.append(batch[\"labels\"].cpu().numpy())\n        train_preds = np.concatenate(train_preds)\n        train_lbls = np.concatenate(train_lbls)\n        train_mcc = matthews_corrcoef(train_lbls, train_preds)\n        train_f1 = f1_score(train_lbls, train_preds, average=\"macro\")\n\n        # ------------------ Validation --------------#\n        val_loss, val_preds, val_lbls = 0.0, [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                val_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                    \"labels\"\n                ].size(0)\n                probs = torch.sigmoid(logits).cpu().numpy()\n                val_preds.append(probs > 0.5)\n                val_lbls.append(batch[\"labels\"].cpu().numpy())\n        val_loss /= len(dev_loader.dataset)\n        val_preds = np.concatenate(val_preds)\n        val_lbls = np.concatenate(val_lbls)\n        val_mcc = matthews_corrcoef(val_lbls, val_preds)\n        val_f1 = f1_score(val_lbls, val_preds, average=\"macro\")\n\n        # logging\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_mcc\"].append(train_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_mcc\"].append(val_mcc)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(train_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n\n        print(\n            f\"Epoch {epoch}/{max_epochs}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"val_MCC={val_mcc:.4f} val_macroF1={val_f1:.4f}\"\n        )\n\n        # save best\n        if val_mcc > best_val_mcc:\n            best_val_mcc = val_mcc\n            best_state = model.state_dict()\n\n        # early stopping\n        if early_stop(val_mcc):\n            print(\"Early stopping.\")\n            break\n\n    # ------------------ Test ----------------------#\n    model.load_state_dict(best_state)\n    model.eval()\n    test_preds, test_lbls = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            probs = torch.sigmoid(model(batch[\"input_ids\"])).cpu().numpy()\n            test_preds.append(probs > 0.5)\n            test_lbls.append(batch[\"labels\"].cpu().numpy())\n    test_preds = np.concatenate(test_preds)\n    test_lbls = np.concatenate(test_lbls)\n    test_mcc = matthews_corrcoef(test_lbls, test_preds)\n    test_f1 = f1_score(test_lbls, test_preds, average=\"macro\")\n\n    experiment_data[\"SPR_BENCH\"][\"test\"] = {\n        \"mcc\": test_mcc,\n        \"macro_f1\": test_f1,\n        \"predictions\": test_preds,\n        \"ground_truth\": test_lbls,\n    }\n\n    print(f\"Test MCC: {test_mcc:.4f} | Test macro-F1: {test_f1:.4f}\")\n\n\n# ------------------------------------------------------------------#\n#  RUN A SMALL HYPER-PARAM SEARCH (epoch budgets)\n# ------------------------------------------------------------------#\nfor ep in [5, 10, 15]:  # trimmed list to ensure <30min runtime\n    print(f\"\\n=== Training with max_epochs = {ep} ===\")\n    train_for_epochs(max_epochs=ep, patience=3, lr=1e-3)\n\n# ------------------------------------------------------------------#\n#  SAVE EXPERIMENT DATA\n# ------------------------------------------------------------------#\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------#\n# Reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ------------------------------------------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------------------------------------------------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str, vocab: dict) -> list[int]:\n    \"\"\"Convert a string into list of ids (no padding).\"\"\"\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    seq_ids = seq_ids[:L]\n    return seq_ids + [0] * (L - len(seq_ids))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\n# DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size)\n\n\n# ------------------------------------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=1e-4, mode=\"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, metric):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"configs\": [],\n    }\n}\n\n\ndef evaluate(model, loader):\n    model.eval()\n    criterion = nn.BCEWithLogitsLoss()\n    total_loss, preds, labels = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            total_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                \"labels\"\n            ].size(0)\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    loss = total_loss / len(loader.dataset)\n    mcc = matthews_corrcoef(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return loss, mcc, f1, preds, labels\n\n\ndef train_for_epochs(max_epochs=10, lr=1e-3, patience=3):\n    model = GRUBaseline(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n    es = EarlyStopping(patience=patience, mode=\"max\")\n    best_state, best_f1 = None, -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = running_loss / len(train_loader.dataset)\n        # metrics on training set\n        _, train_mcc, train_f1, _, _ = evaluate(model, train_loader)\n\n        # validation\n        val_loss, val_mcc, val_f1, _, _ = evaluate(model, dev_loader)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n        )\n\n        # log\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n        if es(val_f1):\n            print(\"Early stopping.\")\n            break\n\n    # ----------------- Test with best checkpoint ------------------#\n    model.load_state_dict(best_state)\n    test_loss, test_mcc, test_f1, preds, labels = evaluate(model, test_loader)\n    print(f\"Test macro_F1 = {test_f1:.4f}  |  Test MCC = {test_mcc:.4f}\")\n    # store preds\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(labels)\n    experiment_data[\"SPR_BENCH\"][\"configs\"].append(\n        {\"epochs\": max_epochs, \"lr\": lr, \"patience\": patience}\n    )\n\n\n# ------------------------------------------------------------------#\nepoch_grid = [10, 15]  # small grid to respect runtime\nlr_grid = [1e-3, 5e-4]\n\nfor ep in epoch_grid:\n    for lr in lr_grid:\n        print(f\"\\n=== Training {ep} epochs | lr={lr} ===\")\n        train_for_epochs(max_epochs=ep, lr=lr, patience=3)\n\n# save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------- reproducibility -------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ------------------------- data loading ----------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded:\", spr)\n\n\n# ------------------------- vocab / encoding ------------------------\ndef build_vocab(dsets):\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: idx + 1 for idx, ch in enumerate(sorted(chars))}  # 0=PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode(s):\n    return [vocab[ch] for ch in s]\n\n\ndef pad(seq):\n    return (\n        seq[:max_len] + [0] * (max_len - len(seq))\n        if len(seq) < max_len\n        else seq[:max_len]\n    )\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.labels = hf_split[\"label\"]\n        self.seqs = hf_split[\"sequence\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = torch.tensor(pad(encode(self.seqs[idx])), dtype=torch.long)\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return {\"input_ids\": ids, \"labels\": label}\n\n\ntrain_ds, dev_ds, test_ds = (SPRTorchDataset(spr[k]) for k in [\"train\", \"dev\", \"test\"])\nbatch_size = 128\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=batch_size)\ntest_loader = DataLoader(test_ds, batch_size=batch_size)\n\n\n# -------------------------- model ----------------------------------\nclass GRUWithDropout(nn.Module):\n    def __init__(self, vocab_sz, embed_dim=64, hidden_dim=64, dropout_rate=0.3):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)  # h: [2,B,H]\n        h = torch.cat([h[0], h[1]], dim=1)  # [B,2H]\n        h = self.drop(h)\n        return self.fc(h).squeeze(1)\n\n\n# -------------------------- training utils -------------------------\ndef epoch_pass(model, loader, optim=None):\n    train = optim is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append((logits.sigmoid() > 0.5).detach().cpu().numpy())\n        labels.append(batch[\"labels\"].detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    return total_loss / len(loader.dataset), matthews_corrcoef(labels, preds)\n\n\n# ----------------------- hyperparameter loop -----------------------\ndropout_grid = [0.1, 0.3, 0.5]\nepochs = 5\ncriterion = nn.BCEWithLogitsLoss()\n\nexperiment_data = {\"dropout_rate\": {}}\nbest_val_mcc, best_state, best_p = -1.0, None, None\n\nfor p in dropout_grid:\n    key = f\"p={p}\"\n    print(f\"\\n=== Training with dropout {p} ===\")\n    model = GRUWithDropout(vocab_size, dropout_rate=p).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # containers\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_mcc = epoch_pass(model, train_loader, optimizer)\n        val_loss, val_mcc = epoch_pass(model, dev_loader)\n        exp[\"losses\"][\"train\"].append(tr_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"train\"].append(tr_mcc)\n        exp[\"metrics\"][\"val\"].append(val_mcc)\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_MCC={val_mcc:.4f}\"\n        )\n        if val_mcc > best_val_mcc:\n            best_val_mcc = val_mcc\n            best_state = model.state_dict()\n            best_p = p\n    experiment_data[\"dropout_rate\"][key] = exp\n\n# ----------------------- test on best model ------------------------\nprint(f\"\\nBest dropout probability: {best_p} with dev MCC {best_val_mcc:.4f}\")\nbest_model = GRUWithDropout(vocab_size, dropout_rate=best_p).to(device)\nbest_model.load_state_dict(best_state)\nbest_model.eval()\n\npreds, labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = best_model(batch[\"input_ids\"])\n        preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n        labels.append(batch[\"labels\"].cpu().numpy())\npreds = np.concatenate(preds)\nlabels = np.concatenate(labels)\ntest_mcc = matthews_corrcoef(labels, preds)\nprint(f\"Test MCC (best model): {test_mcc:.4f}\")\n\nbest_key = f\"p={best_p}\"\nexperiment_data[\"dropout_rate\"][best_key][\"predictions\"] = preds\nexperiment_data[\"dropout_rate\"][best_key][\"ground_truth\"] = labels\nexperiment_data[\"dropout_rate\"][best_key][\"test_MCC\"] = test_mcc\n\n# ----------------------- save --------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------- I/O & working dir -----------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- Reproducibility ------------------------------ #\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ----------------- Device ---------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------- Load SPR_BENCH -------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ----------------- Build vocab & encoding helpers ---------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        chars.update(\"\".join(split[\"sequence\"]))\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 = PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str) -> list[int]:\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    return seq_ids[:L] + [0] * (L - len(seq_ids)) if len(seq_ids) < L else seq_ids[:L]\n\n\n# ----------------- Torch Dataset --------------------------------#\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, max_len: int):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx]), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\n# ----------------- DataLoaders ----------------------------------#\nbatch_size = 256  # slightly larger batch for tuning\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], max_len), batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], max_len), batch_size)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"], max_len), batch_size)\n\n\n# ----------------- Model ----------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\n# ----------------- Early Stopping -------------------------------#\nclass EarlyStopping:\n    def __init__(self, patience: int = 4, min_delta: float = 1e-4, mode: str = \"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.count = 0\n        self.stop = False\n\n    def __call__(self, metric: float):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.count = 0\n        else:\n            self.count += 1\n            if self.count >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ----------------- Experiment container -------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epoch_budgets\": [],\n    }\n}\n\n\n# ----------------- Train routine --------------------------------#\ndef run_training(max_epochs: int = 30, patience: int = 4, lr: float = 3e-4):\n    model = GRUBaseline(vocab_size).to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    es = EarlyStopping(patience=patience, mode=\"max\")\n    best_state, best_mcc = None, -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        # ----- train --------------------------------------------------#\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        # -- train metrics\n        model.eval()\n        tr_preds, tr_labels = [], []\n        with torch.no_grad():\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                out = torch.sigmoid(model(batch[\"input_ids\"]))\n                tr_preds.append((out > 0.5).cpu().numpy())\n                tr_labels.append(batch[\"labels\"].cpu().numpy())\n        tr_preds = np.concatenate(tr_preds)\n        tr_labels = np.concatenate(tr_labels)\n        train_mcc = matthews_corrcoef(tr_labels, tr_preds)\n        train_f1 = f1_score(tr_labels, tr_preds, average=\"macro\")\n\n        # ----- validation ---------------------------------------------#\n        val_loss, v_preds, v_labels = 0.0, [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                logits = model(batch[\"input_ids\"])\n                val_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                    \"labels\"\n                ].size(0)\n                v_preds.append((torch.sigmoid(logits) > 0.5).cpu().numpy())\n                v_labels.append(batch[\"labels\"].cpu().numpy())\n        val_loss /= len(dev_loader.dataset)\n        v_preds = np.concatenate(v_preds)\n        v_labels = np.concatenate(v_labels)\n        val_mcc = matthews_corrcoef(v_labels, v_preds)\n        val_f1 = f1_score(v_labels, v_preds, average=\"macro\")\n\n        # logging\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n            {\"mcc\": train_mcc, \"macro_f1\": train_f1}\n        )\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n            {\"mcc\": val_mcc, \"macro_f1\": val_f1}\n        )\n        print(\n            f\"Epoch {epoch}/{max_epochs}: \"\n            f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"val_MCC={val_mcc:.4f}, val_macroF1={val_f1:.4f}\"\n        )\n\n        # early stop\n        if val_mcc > best_mcc:\n            best_mcc = val_mcc\n            best_state = model.state_dict()\n        if es(val_mcc):\n            print(\"Early stopping.\")\n            break\n\n    # ---------------- test -------------------------------------------#\n    model.load_state_dict(best_state)\n    model.eval()\n    t_preds, t_labels = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"input_ids\"])\n            t_preds.append((torch.sigmoid(logits) > 0.5).cpu().numpy())\n            t_labels.append(batch[\"labels\"].cpu().numpy())\n    t_preds = np.concatenate(t_preds)\n    t_labels = np.concatenate(t_labels)\n    test_mcc = matthews_corrcoef(t_labels, t_preds)\n    test_f1 = f1_score(t_labels, t_preds, average=\"macro\")\n    print(f\"Test MCC={test_mcc:.4f}, Test macroF1={test_f1:.4f}\")\n\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(t_preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(t_labels)\n    experiment_data[\"SPR_BENCH\"][\"epoch_budgets\"].append(max_epochs)\n\n\nfor ep_budget in [10, 20, 30]:\n    start = time.time()\n    print(f\"\\n=== Training with max_epochs={ep_budget} ===\")\n    run_training(max_epochs=ep_budget, patience=5, lr=3e-4)\n    print(f\"Elapsed: {time.time()-start:.1f}s\")\n\n# ----------------- Save experiment data --------------------------#\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------#\n# Reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ------------------------------------------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------------------------------------------------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str, vocab: dict) -> list[int]:\n    \"\"\"Convert a string into list of ids (no padding).\"\"\"\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    seq_ids = seq_ids[:L]\n    return seq_ids + [0] * (L - len(seq_ids))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\n# DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size)\n\n\n# ------------------------------------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=1e-4, mode=\"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, metric):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"configs\": [],\n    }\n}\n\n\ndef evaluate(model, loader):\n    model.eval()\n    criterion = nn.BCEWithLogitsLoss()\n    total_loss, preds, labels = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            total_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                \"labels\"\n            ].size(0)\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    loss = total_loss / len(loader.dataset)\n    mcc = matthews_corrcoef(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return loss, mcc, f1, preds, labels\n\n\ndef train_for_epochs(max_epochs=10, lr=1e-3, patience=3):\n    model = GRUBaseline(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n    es = EarlyStopping(patience=patience, mode=\"max\")\n    best_state, best_f1 = None, -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = running_loss / len(train_loader.dataset)\n        # metrics on training set\n        _, train_mcc, train_f1, _, _ = evaluate(model, train_loader)\n\n        # validation\n        val_loss, val_mcc, val_f1, _, _ = evaluate(model, dev_loader)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n        )\n\n        # log\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n        if es(val_f1):\n            print(\"Early stopping.\")\n            break\n\n    # ----------------- Test with best checkpoint ------------------#\n    model.load_state_dict(best_state)\n    test_loss, test_mcc, test_f1, preds, labels = evaluate(model, test_loader)\n    print(f\"Test macro_F1 = {test_f1:.4f}  |  Test MCC = {test_mcc:.4f}\")\n    # store preds\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(labels)\n    experiment_data[\"SPR_BENCH\"][\"configs\"].append(\n        {\"epochs\": max_epochs, \"lr\": lr, \"patience\": patience}\n    )\n\n\n# ------------------------------------------------------------------#\nepoch_grid = [10, 15]  # small grid to respect runtime\nlr_grid = [1e-3, 5e-4]\n\nfor ep in epoch_grid:\n    for lr in lr_grid:\n        print(f\"\\n=== Training {ep} epochs | lr={lr} ===\")\n        train_for_epochs(max_epochs=ep, lr=lr, patience=3)\n\n# save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------#\n# Reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ------------------------------------------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------------------------------------------------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str, vocab: dict) -> list[int]:\n    \"\"\"Convert a string into list of ids (no padding).\"\"\"\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    seq_ids = seq_ids[:L]\n    return seq_ids + [0] * (L - len(seq_ids))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\n# DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size)\n\n\n# ------------------------------------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=1e-4, mode=\"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, metric):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"configs\": [],\n    }\n}\n\n\ndef evaluate(model, loader):\n    model.eval()\n    criterion = nn.BCEWithLogitsLoss()\n    total_loss, preds, labels = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            total_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                \"labels\"\n            ].size(0)\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    loss = total_loss / len(loader.dataset)\n    mcc = matthews_corrcoef(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return loss, mcc, f1, preds, labels\n\n\ndef train_for_epochs(max_epochs=10, lr=1e-3, patience=3):\n    model = GRUBaseline(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n    es = EarlyStopping(patience=patience, mode=\"max\")\n    best_state, best_f1 = None, -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = running_loss / len(train_loader.dataset)\n        # metrics on training set\n        _, train_mcc, train_f1, _, _ = evaluate(model, train_loader)\n\n        # validation\n        val_loss, val_mcc, val_f1, _, _ = evaluate(model, dev_loader)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n        )\n\n        # log\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n        if es(val_f1):\n            print(\"Early stopping.\")\n            break\n\n    # ----------------- Test with best checkpoint ------------------#\n    model.load_state_dict(best_state)\n    test_loss, test_mcc, test_f1, preds, labels = evaluate(model, test_loader)\n    print(f\"Test macro_F1 = {test_f1:.4f}  |  Test MCC = {test_mcc:.4f}\")\n    # store preds\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(labels)\n    experiment_data[\"SPR_BENCH\"][\"configs\"].append(\n        {\"epochs\": max_epochs, \"lr\": lr, \"patience\": patience}\n    )\n\n\n# ------------------------------------------------------------------#\nepoch_grid = [10, 15]  # small grid to respect runtime\nlr_grid = [1e-3, 5e-4]\n\nfor ep in epoch_grid:\n    for lr in lr_grid:\n        print(f\"\\n=== Training {ep} epochs | lr={lr} ===\")\n        train_for_epochs(max_epochs=ep, lr=lr, patience=3)\n\n# save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------#\n# Reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ------------------------------------------------------------------#\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------------------------------------------------------#\ndef build_vocab(dsets) -> dict:\n    chars = set()\n    for split in dsets.values():\n        for s in split[\"sequence\"]:\n            chars.update(s)\n    return {ch: i + 1 for i, ch in enumerate(sorted(chars))}  # 0 reserved for PAD\n\n\nvocab = build_vocab(spr)\nvocab_size = len(vocab) + 1\nmax_len = max(max(len(s) for s in split[\"sequence\"]) for split in spr.values())\n\n\ndef encode_sequence(seq: str, vocab: dict) -> list[int]:\n    \"\"\"Convert a string into list of ids (no padding).\"\"\"\n    return [vocab[ch] for ch in seq]\n\n\ndef pad(seq_ids: list[int], L: int) -> list[int]:\n    seq_ids = seq_ids[:L]\n    return seq_ids + [0] * (L - len(seq_ids))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n        }\n\n\n# DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size)\n\n\n# ------------------------------------------------------------------#\nclass GRUBaseline(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, h = self.gru(x)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.fc(h).squeeze(1)\n\n\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=1e-4, mode=\"max\"):\n        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n        self.best = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, metric):\n        if self.best is None:\n            self.best = metric\n            return False\n        improve = (metric - self.best) if self.mode == \"max\" else (self.best - metric)\n        if improve > self.min_delta:\n            self.best = metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"configs\": [],\n    }\n}\n\n\ndef evaluate(model, loader):\n    model.eval()\n    criterion = nn.BCEWithLogitsLoss()\n    total_loss, preds, labels = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            total_loss += criterion(logits, batch[\"labels\"]).item() * batch[\n                \"labels\"\n            ].size(0)\n            preds.append((logits.sigmoid() > 0.5).cpu().numpy())\n            labels.append(batch[\"labels\"].cpu().numpy())\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    loss = total_loss / len(loader.dataset)\n    mcc = matthews_corrcoef(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return loss, mcc, f1, preds, labels\n\n\ndef train_for_epochs(max_epochs=10, lr=1e-3, patience=3):\n    model = GRUBaseline(vocab_size).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n    es = EarlyStopping(patience=patience, mode=\"max\")\n    best_state, best_f1 = None, -1.0\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"labels\"].size(0)\n        train_loss = running_loss / len(train_loader.dataset)\n        # metrics on training set\n        _, train_mcc, train_f1, _, _ = evaluate(model, train_loader)\n\n        # validation\n        val_loss, val_mcc, val_f1, _, _ = evaluate(model, dev_loader)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n        )\n\n        # log\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n        if es(val_f1):\n            print(\"Early stopping.\")\n            break\n\n    # ----------------- Test with best checkpoint ------------------#\n    model.load_state_dict(best_state)\n    test_loss, test_mcc, test_f1, preds, labels = evaluate(model, test_loader)\n    print(f\"Test macro_F1 = {test_f1:.4f}  |  Test MCC = {test_mcc:.4f}\")\n    # store preds\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].append(preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(labels)\n    experiment_data[\"SPR_BENCH\"][\"configs\"].append(\n        {\"epochs\": max_epochs, \"lr\": lr, \"patience\": patience}\n    )\n\n\n# ------------------------------------------------------------------#\nepoch_grid = [10, 15]  # small grid to respect runtime\nlr_grid = [1e-3, 5e-4]\n\nfor ep in epoch_grid:\n    for lr in lr_grid:\n        print(f\"\\n=== Training {ep} epochs | lr={lr} ===\")\n        train_for_epochs(max_epochs=ep, lr=lr, patience=3)\n\n# save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 143877.06\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 147209.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 228609.80\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"DatasetDict({\\n    train:\nDataset({\\n        features: ['id', 'sequence', 'label'],\\n        num_rows:\n2000\\n    })\\n    dev: Dataset({\\n        features: ['id', 'sequence',\n'label'],\\n        num_rows: 500\\n    })\\n    test: Dataset({\\n        features:\n['id', 'sequence', 'label'],\\n        num_rows: 1000\\n    })\\n})\", '\\n', 'Vocab\nsize: 10', '\\n', 'Max sequence length: 95', '\\n', 'Epoch 1: train_loss=0.6705\nval_loss=0.6550  val_MCC=0.2842', '\\n', 'Epoch 2: train_loss=0.6343\nval_loss=0.6422  val_MCC=0.3645', '\\n', 'Epoch 3: train_loss=0.6239\nval_loss=0.6409  val_MCC=0.3648', '\\n', 'Epoch 4: train_loss=0.6217\nval_loss=0.6362  val_MCC=0.3527', '\\n', 'Epoch 5: train_loss=0.6178\nval_loss=0.6400  val_MCC=0.3609', '\\n', 'Test MCC: 0.3792', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_18-47-59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n1/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n151676.27 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 117480.93\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 203330.62\nexamples/s]', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\",\nline 225, in <module>\\n    res = train_for_epochs(ep, patience=3)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 145, in\ntrain_for_epochs\\n    for batch in train_loader:\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 701, in __next__\\n    data =\nself._next_data()\\n           ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 757, in _next_data\\n    data =\nself._dataset_fetcher.fetch(index)  # may raise StopIteration\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n~~~~~~~~~~~~^^^^^\\n  File \"runfile.py\", line 77, in __getitem__\\n    ids =\npad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: encode_sequence() takes\n1 positional argument but 2 were given\\n', 'Execution time: 2 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 177027.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 158646.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 180268.36\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"DatasetDict({\\n    train:\nDataset({\\n        features: ['id', 'sequence', 'label'],\\n        num_rows:\n2000\\n    })\\n    dev: Dataset({\\n        features: ['id', 'sequence',\n'label'],\\n        num_rows: 500\\n    })\\n    test: Dataset({\\n        features:\n['id', 'sequence', 'label'],\\n        num_rows: 1000\\n    })\\n})\", '\\n', 'Vocab\nsize: 10  |  Max len: 95', '\\n', '\\n=== Training with learning rate 0.0005 ===',\n'\\n', 'Epoch 1: train_loss=0.6742  val_loss=0.6621  val_MCC=0.2962', '\\n',\n'Epoch 2: train_loss=0.6437  val_loss=0.6455  val_MCC=0.3527', '\\n', 'Epoch 3:\ntrain_loss=0.6302  val_loss=0.6394  val_MCC=0.3733', '\\n', 'Epoch 4:\ntrain_loss=0.6260  val_loss=0.6367  val_MCC=0.3731', '\\n', 'Epoch 5:\ntrain_loss=0.6223  val_loss=0.6356  val_MCC=0.3656', '\\n', 'Test MCC @ 0.0005:\n0.3715', '\\n', '\\n=== Training with learning rate 0.001 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6547  val_loss=0.6452  val_MCC=0.3643', '\\n', 'Epoch 2:\ntrain_loss=0.6292  val_loss=0.6394  val_MCC=0.3733', '\\n', 'Epoch 3:\ntrain_loss=0.6248  val_loss=0.6343  val_MCC=0.3617', '\\n', 'Epoch 4:\ntrain_loss=0.6209  val_loss=0.6387  val_MCC=0.3646', '\\n', 'Epoch 5:\ntrain_loss=0.6177  val_loss=0.6383  val_MCC=0.3568', '\\n', 'Test MCC @ 0.001:\n0.3751', '\\n', '\\n=== Training with learning rate 0.002 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6612  val_loss=0.6521  val_MCC=0.3511', '\\n', 'Epoch 2:\ntrain_loss=0.6280  val_loss=0.6338  val_MCC=0.3614', '\\n', 'Epoch 3:\ntrain_loss=0.6226  val_loss=0.6373  val_MCC=0.3646', '\\n', 'Epoch 4:\ntrain_loss=0.6199  val_loss=0.6367  val_MCC=0.3646', '\\n', 'Epoch 5:\ntrain_loss=0.6150  val_loss=0.6388  val_MCC=0.3570', '\\n', 'Test MCC @ 0.002:\n0.3712', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 4\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 148644.58\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 97315.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 174740.82\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"DatasetDict({\\n    train:\nDataset({\\n        features: ['id', 'sequence', 'label'],\\n        num_rows:\n2000\\n    })\\n    dev: Dataset({\\n        features: ['id', 'sequence',\n'label'],\\n        num_rows: 500\\n    })\\n    test: Dataset({\\n        features:\n['id', 'sequence', 'label'],\\n        num_rows: 1000\\n    })\\n})\", '\\n', '\\n====\nExperiment: batch_size = 64 ====', '\\n', 'Epoch 1/5 | train_loss 0.6496 |\nval_loss 0.6420 | val_MCC 0.3697', '\\n', 'Epoch 2/5 | train_loss 0.6293 |\nval_loss 0.6413 | val_MCC 0.3439', '\\n', 'Epoch 3/5 | train_loss 0.6218 |\nval_loss 0.6368 | val_MCC 0.3536', '\\n', 'Epoch 4/5 | train_loss 0.6189 |\nval_loss 0.6450 | val_MCC 0.3662', '\\n', 'Epoch 5/5 | train_loss 0.6189 |\nval_loss 0.6389 | val_MCC 0.3572', '\\n', 'Test MCC (bs=64): 0.3772', '\\n',\n'\\n==== Experiment: batch_size = 128 ====', '\\n', 'Epoch 1/5 | train_loss 0.6547\n| val_loss 0.6452 | val_MCC 0.3643', '\\n', 'Epoch 2/5 | train_loss 0.6292 |\nval_loss 0.6394 | val_MCC 0.3733', '\\n', 'Epoch 3/5 | train_loss 0.6248 |\nval_loss 0.6343 | val_MCC 0.3617', '\\n', 'Epoch 4/5 | train_loss 0.6209 |\nval_loss 0.6387 | val_MCC 0.3646', '\\n', 'Epoch 5/5 | train_loss 0.6177 |\nval_loss 0.6383 | val_MCC 0.3568', '\\n', 'Test MCC (bs=128): 0.3751', '\\n',\n'\\n==== Experiment: batch_size = 256 ====', '\\n', 'Epoch 1/5 | train_loss 0.6745\n| val_loss 0.6547 | val_MCC 0.3124', '\\n', 'Epoch 2/5 | train_loss 0.6379 |\nval_loss 0.6459 | val_MCC 0.3417', '\\n', 'Epoch 3/5 | train_loss 0.6296 |\nval_loss 0.6421 | val_MCC 0.3484', '\\n', 'Epoch 4/5 | train_loss 0.6262 |\nval_loss 0.6370 | val_MCC 0.3607', '\\n', 'Epoch 5/5 | train_loss 0.6225 |\nval_loss 0.6352 | val_MCC 0.3658', '\\n', 'Test MCC (bs=256): 0.3693', '\\n',\n'\\n==== Experiment: batch_size = 512 ====', '\\n', 'Epoch 1/5 | train_loss 0.6903\n| val_loss 0.6730 | val_MCC 0.2540', '\\n', 'Epoch 2/5 | train_loss 0.6612 |\nval_loss 0.6582 | val_MCC 0.2643', '\\n', 'Epoch 3/5 | train_loss 0.6453 |\nval_loss 0.6514 | val_MCC 0.2719', '\\n', 'Epoch 4/5 | train_loss 0.6349 |\nval_loss 0.6472 | val_MCC 0.3290', '\\n', 'Epoch 5/5 | train_loss 0.6309 |\nval_loss 0.6439 | val_MCC 0.3604', '\\n', 'Test MCC (bs=512): 0.3711', '\\n',\n'Saved all results to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n95716.66 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 74203.95\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 92607.89\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"DatasetDict({\\n    train:\nDataset({\\n        features: ['id', 'sequence', 'label'],\\n        num_rows:\n2000\\n    })\\n    dev: Dataset({\\n        features: ['id', 'sequence',\n'label'],\\n        num_rows: 500\\n    })\\n    test: Dataset({\\n        features:\n['id', 'sequence', 'label'],\\n        num_rows: 1000\\n    })\\n})\", '\\n', 'wd=0.0\nepoch=1  train_loss=0.6705  val_loss=0.6550  val_MCC=0.2842', '\\n', 'wd=0.0\nepoch=2  train_loss=0.6343  val_loss=0.6422  val_MCC=0.3645', '\\n', 'wd=0.0\nepoch=3  train_loss=0.6239  val_loss=0.6409  val_MCC=0.3648', '\\n', 'wd=0.0\nepoch=4  train_loss=0.6217  val_loss=0.6362  val_MCC=0.3527', '\\n', 'wd=0.0\nepoch=5  train_loss=0.6178  val_loss=0.6400  val_MCC=0.3609', '\\n', 'wd=1e-05\nepoch=1  train_loss=0.6635  val_loss=0.6466  val_MCC=0.3001', '\\n', 'wd=1e-05\nepoch=2  train_loss=0.6288  val_loss=0.6382  val_MCC=0.3736', '\\n', 'wd=1e-05\nepoch=3  train_loss=0.6264  val_loss=0.6371  val_MCC=0.3662', '\\n', 'wd=1e-05\nepoch=4  train_loss=0.6213  val_loss=0.6390  val_MCC=0.3561', '\\n', 'wd=1e-05\nepoch=5  train_loss=0.6197  val_loss=0.6405  val_MCC=0.3646', '\\n', 'wd=5e-05\nepoch=1  train_loss=0.6674  val_loss=0.6483  val_MCC=0.3424', '\\n', 'wd=5e-05\nepoch=2  train_loss=0.6297  val_loss=0.6423  val_MCC=0.3605', '\\n', 'wd=5e-05\nepoch=3  train_loss=0.6245  val_loss=0.6387  val_MCC=0.3653', '\\n', 'wd=5e-05\nepoch=4  train_loss=0.6210  val_loss=0.6385  val_MCC=0.3731', '\\n', 'wd=5e-05\nepoch=5  train_loss=0.6178  val_loss=0.6368  val_MCC=0.3611', '\\n', 'wd=0.0001\nepoch=1  train_loss=0.6610  val_loss=0.6383  val_MCC=0.3630', '\\n', 'wd=0.0001\nepoch=2  train_loss=0.6327  val_loss=0.6424  val_MCC=0.3360', '\\n', 'wd=0.0001\nepoch=3  train_loss=0.6265  val_loss=0.6352  val_MCC=0.3662', '\\n', 'wd=0.0001\nepoch=4  train_loss=0.6217  val_loss=0.6345  val_MCC=0.3653', '\\n', 'wd=0.0001\nepoch=5  train_loss=0.6188  val_loss=0.6377  val_MCC=0.3607', '\\n', 'wd=0.0005\nepoch=1  train_loss=0.6591  val_loss=0.6491  val_MCC=0.3360', '\\n', 'wd=0.0005\nepoch=2  train_loss=0.6308  val_loss=0.6401  val_MCC=0.3686', '\\n', 'wd=0.0005\nepoch=3  train_loss=0.6249  val_loss=0.6341  val_MCC=0.3653', '\\n', 'wd=0.0005\nepoch=4  train_loss=0.6234  val_loss=0.6461  val_MCC=0.3281', '\\n', 'wd=0.0005\nepoch=5  train_loss=0.6216  val_loss=0.6373  val_MCC=0.3607', '\\n', 'wd=0.001\nepoch=1  train_loss=0.6617  val_loss=0.6450  val_MCC=0.3160', '\\n', 'wd=0.001\nepoch=2  train_loss=0.6321  val_loss=0.6385  val_MCC=0.3620', '\\n', 'wd=0.001\nepoch=3  train_loss=0.6278  val_loss=0.6368  val_MCC=0.3617', '\\n', 'wd=0.001\nepoch=4  train_loss=0.6234  val_loss=0.6368  val_MCC=0.3697', '\\n', 'wd=0.001\nepoch=5  train_loss=0.6211  val_loss=0.6399  val_MCC=0.3653', '\\n', '\\nBest\nweight_decay = 0.001  with val_MCC = 0.3653', '\\n', 'Test MCC with best\nweight_decay: 0.3712', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training with max_epochs = 5 ===', '\\n',\n'Epoch 1/5: train_loss=0.6613 val_loss=0.6489 val_MCC=0.3286\nval_macroF1=0.6632', '\\n', 'Epoch 2/5: train_loss=0.6312 val_loss=0.6396\nval_MCC=0.3692 val_macroF1=0.6830', '\\n', 'Epoch 3/5: train_loss=0.6257\nval_loss=0.6388 val_MCC=0.3584 val_macroF1=0.6762', '\\n', 'Epoch 4/5:\ntrain_loss=0.6259 val_loss=0.6358 val_MCC=0.3648 val_macroF1=0.6812', '\\n',\n'Epoch 5/5: train_loss=0.6184 val_loss=0.6363 val_MCC=0.3653\nval_macroF1=0.6809', '\\n', 'Early stopping.', '\\n', 'Test MCC: 0.3732 | Test\nmacro-F1: 0.6858', '\\n', '\\n=== Training with max_epochs = 10 ===', '\\n', 'Epoch\n1/10: train_loss=0.6547 val_loss=0.6452 val_MCC=0.3643 val_macroF1=0.6815',\n'\\n', 'Epoch 2/10: train_loss=0.6292 val_loss=0.6394 val_MCC=0.3733\nval_macroF1=0.6849', '\\n', 'Epoch 3/10: train_loss=0.6248 val_loss=0.6343\nval_MCC=0.3617 val_macroF1=0.6787', '\\n', 'Epoch 4/10: train_loss=0.6209\nval_loss=0.6387 val_MCC=0.3646 val_macroF1=0.6813', '\\n', 'Epoch 5/10:\ntrain_loss=0.6177 val_loss=0.6383 val_MCC=0.3568 val_macroF1=0.6772', '\\n',\n'Early stopping.', '\\n', 'Test MCC: 0.3751 | Test macro-F1: 0.6870', '\\n',\n'\\n=== Training with max_epochs = 15 ===', '\\n', 'Epoch 1/15: train_loss=0.6665\nval_loss=0.6496 val_MCC=0.3300 val_macroF1=0.6622', '\\n', 'Epoch 2/15:\ntrain_loss=0.6319 val_loss=0.6408 val_MCC=0.3564 val_macroF1=0.6774', '\\n',\n'Epoch 3/15: train_loss=0.6258 val_loss=0.6345 val_MCC=0.3648\nval_macroF1=0.6812', '\\n', 'Epoch 4/15: train_loss=0.6223 val_loss=0.6350\nval_MCC=0.3694 val_macroF1=0.6829', '\\n', 'Epoch 5/15: train_loss=0.6186\nval_loss=0.6348 val_MCC=0.3656 val_macroF1=0.6808', '\\n', 'Epoch 6/15:\ntrain_loss=0.6162 val_loss=0.6428 val_MCC=0.3439 val_macroF1=0.6720', '\\n',\n'Epoch 7/15: train_loss=0.6176 val_loss=0.6421 val_MCC=0.3529\nval_macroF1=0.6751', '\\n', 'Early stopping.', '\\n', 'Test MCC: 0.3772 | Test\nmacro-F1: 0.6878', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Sc\nientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training 10 epochs | lr=0.001 ===', '\\n',\n'Epoch 1: validation_loss = 0.6489, val_macro_f1 = 0.6632', '\\n', 'Epoch 2:\nvalidation_loss = 0.6396, val_macro_f1 = 0.6830', '\\n', 'Epoch 3:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6762', '\\n', 'Epoch 4:\nvalidation_loss = 0.6358, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6363, val_macro_f1 = 0.6809', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6858  |  Test MCC = 0.3732', '\\n', '\\n=== Training 10 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6544, val_macro_f1 =\n0.6582', '\\n', 'Epoch 2: validation_loss = 0.6435, val_macro_f1 = 0.6833', '\\n',\n'Epoch 3: validation_loss = 0.6379, val_macro_f1 = 0.6829', '\\n', 'Epoch 4:\nvalidation_loss = 0.6370, val_macro_f1 = 0.6830', '\\n', 'Epoch 5:\nvalidation_loss = 0.6356, val_macro_f1 = 0.6808', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6854  |  Test MCC = 0.3734', '\\n', '\\n=== Training 15 epochs\n| lr=0.001 ===', '\\n', 'Epoch 1: validation_loss = 0.6496, val_macro_f1 =\n0.6622', '\\n', 'Epoch 2: validation_loss = 0.6408, val_macro_f1 = 0.6774', '\\n',\n'Epoch 3: validation_loss = 0.6345, val_macro_f1 = 0.6812', '\\n', 'Epoch 4:\nvalidation_loss = 0.6350, val_macro_f1 = 0.6829', '\\n', 'Epoch 5:\nvalidation_loss = 0.6348, val_macro_f1 = 0.6808', '\\n', 'Epoch 6:\nvalidation_loss = 0.6428, val_macro_f1 = 0.6720', '\\n', 'Epoch 7:\nvalidation_loss = 0.6421, val_macro_f1 = 0.6751', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6878  |  Test MCC = 0.3772', '\\n', '\\n=== Training 15 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6542, val_macro_f1 =\n0.6794', '\\n', 'Epoch 2: validation_loss = 0.6394, val_macro_f1 = 0.6830', '\\n',\n'Epoch 3: validation_loss = 0.6366, val_macro_f1 = 0.6831', '\\n', 'Epoch 4:\nvalidation_loss = 0.6361, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6381, val_macro_f1 = 0.6872', '\\n', 'Epoch 6:\nvalidation_loss = 0.6375, val_macro_f1 = 0.6811', '\\n', 'Epoch 7:\nvalidation_loss = 0.6377, val_macro_f1 = 0.6793', '\\n', 'Epoch 8:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6834', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6901  |  Test MCC = 0.3811', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded:', ' ', \"DatasetDict({\\n    train:\nDataset({\\n        features: ['id', 'sequence', 'label'],\\n        num_rows:\n2000\\n    })\\n    dev: Dataset({\\n        features: ['id', 'sequence',\n'label'],\\n        num_rows: 500\\n    })\\n    test: Dataset({\\n        features:\n['id', 'sequence', 'label'],\\n        num_rows: 1000\\n    })\\n})\", '\\n', '\\n===\nTraining with dropout 0.1 ===', '\\n', 'Epoch 1: train_loss=0.6617\nval_loss=0.6489 val_MCC=0.3286', '\\n', 'Epoch 2: train_loss=0.6295\nval_loss=0.6503 val_MCC=0.3129', '\\n', 'Epoch 3: train_loss=0.6257\nval_loss=0.6374 val_MCC=0.3588', '\\n', 'Epoch 4: train_loss=0.6231\nval_loss=0.6372 val_MCC=0.3689', '\\n', 'Epoch 5: train_loss=0.6202\nval_loss=0.6371 val_MCC=0.3609', '\\n', '\\n=== Training with dropout 0.3 ===',\n'\\n', 'Epoch 1: train_loss=0.6551 val_loss=0.6434 val_MCC=0.3581', '\\n', 'Epoch\n2: train_loss=0.6326 val_loss=0.6376 val_MCC=0.3575', '\\n', 'Epoch 3:\ntrain_loss=0.6259 val_loss=0.6379 val_MCC=0.3653', '\\n', 'Epoch 4:\ntrain_loss=0.6218 val_loss=0.6343 val_MCC=0.3662', '\\n', 'Epoch 5:\ntrain_loss=0.6257 val_loss=0.6344 val_MCC=0.3614', '\\n', '\\n=== Training with\ndropout 0.5 ===', '\\n', 'Epoch 1: train_loss=0.6646 val_loss=0.6577\nval_MCC=0.2593', '\\n', 'Epoch 2: train_loss=0.6427 val_loss=0.6412\nval_MCC=0.3445', '\\n', 'Epoch 3: train_loss=0.6315 val_loss=0.6400\nval_MCC=0.3686', '\\n', 'Epoch 4: train_loss=0.6279 val_loss=0.6371\nval_MCC=0.3617', '\\n', 'Epoch 5: train_loss=0.6279 val_loss=0.6406\nval_MCC=0.3479', '\\n', '\\nBest dropout probability: 0.1 with dev MCC 0.3689',\n'\\n', 'Test MCC (best model): 0.3671', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training with max_epochs=10 ===', '\\n',\n'Epoch 1/10: train_loss=0.6869, val_loss=0.6791, val_MCC=0.2560,\nval_macroF1=0.6276', '\\n', 'Epoch 2/10: train_loss=0.6717, val_loss=0.6690,\nval_MCC=0.2875, val_macroF1=0.6336', '\\n', 'Epoch 3/10: train_loss=0.6592,\nval_loss=0.6609, val_MCC=0.3055, val_macroF1=0.6504', '\\n', 'Epoch 4/10:\ntrain_loss=0.6490, val_loss=0.6545, val_MCC=0.3000, val_macroF1=0.6497', '\\n',\n'Epoch 5/10: train_loss=0.6404, val_loss=0.6487, val_MCC=0.3488,\nval_macroF1=0.6732', '\\n', 'Epoch 6/10: train_loss=0.6344, val_loss=0.6445,\nval_MCC=0.3527, val_macroF1=0.6753', '\\n', 'Epoch 7/10: train_loss=0.6292,\nval_loss=0.6414, val_MCC=0.3527, val_macroF1=0.6753', '\\n', 'Epoch 8/10:\ntrain_loss=0.6266, val_loss=0.6395, val_MCC=0.3564, val_macroF1=0.6774', '\\n',\n'Epoch 9/10: train_loss=0.6245, val_loss=0.6382, val_MCC=0.3727,\nval_macroF1=0.6853', '\\n', 'Epoch 10/10: train_loss=0.6236, val_loss=0.6374,\nval_MCC=0.3766, val_macroF1=0.6874', '\\n', 'Test MCC=0.3611, Test\nmacroF1=0.6799', '\\n', 'Elapsed: 3.2s', '\\n', '\\n=== Training with max_epochs=20\n===', '\\n', 'Epoch 1/20: train_loss=0.6959, val_loss=0.6863, val_MCC=0.1405,\nval_macroF1=0.5567', '\\n', 'Epoch 2/20: train_loss=0.6774, val_loss=0.6754,\nval_MCC=0.2444, val_macroF1=0.6148', '\\n', 'Epoch 3/20: train_loss=0.6644,\nval_loss=0.6674, val_MCC=0.2410, val_macroF1=0.6182', '\\n', 'Epoch 4/20:\ntrain_loss=0.6536, val_loss=0.6607, val_MCC=0.2566, val_macroF1=0.6268', '\\n',\n'Epoch 5/20: train_loss=0.6464, val_loss=0.6555, val_MCC=0.2640,\nval_macroF1=0.6316', '\\n', 'Epoch 6/20: train_loss=0.6393, val_loss=0.6510,\nval_MCC=0.2972, val_macroF1=0.6466', '\\n', 'Epoch 7/20: train_loss=0.6348,\nval_loss=0.6473, val_MCC=0.3294, val_macroF1=0.6626', '\\n', 'Epoch 8/20:\ntrain_loss=0.6308, val_loss=0.6449, val_MCC=0.3366, val_macroF1=0.6672', '\\n',\n'Epoch 9/20: train_loss=0.6282, val_loss=0.6423, val_MCC=0.3522,\nval_macroF1=0.6756', '\\n', 'Epoch 10/20: train_loss=0.6259, val_loss=0.6403,\nval_MCC=0.3604, val_macroF1=0.6795', '\\n', 'Epoch 11/20: train_loss=0.6244,\nval_loss=0.6383, val_MCC=0.3609, val_macroF1=0.6791', '\\n', 'Epoch 12/20:\ntrain_loss=0.6230, val_loss=0.6381, val_MCC=0.3564, val_macroF1=0.6774', '\\n',\n'Epoch 13/20: train_loss=0.6221, val_loss=0.6377, val_MCC=0.3605,\nval_macroF1=0.6794', '\\n', 'Epoch 14/20: train_loss=0.6205, val_loss=0.6367,\nval_MCC=0.3729, val_macroF1=0.6852', '\\n', 'Epoch 15/20: train_loss=0.6197,\nval_loss=0.6362, val_MCC=0.3729, val_macroF1=0.6852', '\\n', 'Epoch 16/20:\ntrain_loss=0.6189, val_loss=0.6363, val_MCC=0.3770, val_macroF1=0.6872', '\\n',\n'Epoch 17/20: train_loss=0.6184, val_loss=0.6372, val_MCC=0.3645,\nval_macroF1=0.6814', '\\n', 'Epoch 18/20: train_loss=0.6174, val_loss=0.6373,\nval_MCC=0.3729, val_macroF1=0.6852', '\\n', 'Epoch 19/20: train_loss=0.6167,\nval_loss=0.6368, val_MCC=0.3729, val_macroF1=0.6852', '\\n', 'Epoch 20/20:\ntrain_loss=0.6159, val_loss=0.6367, val_MCC=0.3727, val_macroF1=0.6853', '\\n',\n'Test MCC=0.3731, Test macroF1=0.6861', '\\n', 'Elapsed: 3.3s', '\\n', '\\n===\nTraining with max_epochs=30 ===', '\\n', 'Epoch 1/30: train_loss=0.6819,\nval_loss=0.6770, val_MCC=0.2449, val_macroF1=0.6087', '\\n', 'Epoch 2/30:\ntrain_loss=0.6646, val_loss=0.6640, val_MCC=0.2908, val_macroF1=0.6362', '\\n',\n'Epoch 3/30: train_loss=0.6521, val_loss=0.6548, val_MCC=0.3345,\nval_macroF1=0.6640', '\\n', 'Epoch 4/30: train_loss=0.6420, val_loss=0.6488,\nval_MCC=0.3566, val_macroF1=0.6773', '\\n', 'Epoch 5/30: train_loss=0.6349,\nval_loss=0.6435, val_MCC=0.3607, val_macroF1=0.6793', '\\n', 'Epoch 6/30:\ntrain_loss=0.6305, val_loss=0.6403, val_MCC=0.3611, val_macroF1=0.6790', '\\n',\n'Epoch 7/30: train_loss=0.6267, val_loss=0.6395, val_MCC=0.3645,\nval_macroF1=0.6814', '\\n', 'Epoch 8/30: train_loss=0.6248, val_loss=0.6378,\nval_MCC=0.3687, val_macroF1=0.6833', '\\n', 'Epoch 9/30: train_loss=0.6224,\nval_loss=0.6377, val_MCC=0.3687, val_macroF1=0.6833', '\\n', 'Epoch 10/30:\ntrain_loss=0.6214, val_loss=0.6371, val_MCC=0.3609, val_macroF1=0.6791', '\\n',\n'Epoch 11/30: train_loss=0.6204, val_loss=0.6374, val_MCC=0.3650,\nval_macroF1=0.6811', '\\n', 'Epoch 12/30: train_loss=0.6195, val_loss=0.6369,\nval_MCC=0.3650, val_macroF1=0.6811', '\\n', 'Epoch 13/30: train_loss=0.6191,\nval_loss=0.6380, val_MCC=0.3648, val_macroF1=0.6812', '\\n', 'Early stopping.',\n'\\n', 'Test MCC=0.3772, Test macroF1=0.6879', '\\n', 'Elapsed: 2.7s', '\\n',\n'Saved all results to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training 10 epochs | lr=0.001 ===', '\\n',\n'Epoch 1: validation_loss = 0.6489, val_macro_f1 = 0.6632', '\\n', 'Epoch 2:\nvalidation_loss = 0.6396, val_macro_f1 = 0.6830', '\\n', 'Epoch 3:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6762', '\\n', 'Epoch 4:\nvalidation_loss = 0.6358, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6363, val_macro_f1 = 0.6809', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6858  |  Test MCC = 0.3732', '\\n', '\\n=== Training 10 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6544, val_macro_f1 =\n0.6582', '\\n', 'Epoch 2: validation_loss = 0.6435, val_macro_f1 = 0.6833', '\\n',\n'Epoch 3: validation_loss = 0.6379, val_macro_f1 = 0.6829', '\\n', 'Epoch 4:\nvalidation_loss = 0.6370, val_macro_f1 = 0.6830', '\\n', 'Epoch 5:\nvalidation_loss = 0.6356, val_macro_f1 = 0.6808', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6854  |  Test MCC = 0.3734', '\\n', '\\n=== Training 15 epochs\n| lr=0.001 ===', '\\n', 'Epoch 1: validation_loss = 0.6496, val_macro_f1 =\n0.6622', '\\n', 'Epoch 2: validation_loss = 0.6408, val_macro_f1 = 0.6774', '\\n',\n'Epoch 3: validation_loss = 0.6345, val_macro_f1 = 0.6812', '\\n', 'Epoch 4:\nvalidation_loss = 0.6350, val_macro_f1 = 0.6829', '\\n', 'Epoch 5:\nvalidation_loss = 0.6348, val_macro_f1 = 0.6808', '\\n', 'Epoch 6:\nvalidation_loss = 0.6428, val_macro_f1 = 0.6720', '\\n', 'Epoch 7:\nvalidation_loss = 0.6421, val_macro_f1 = 0.6751', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6878  |  Test MCC = 0.3772', '\\n', '\\n=== Training 15 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6542, val_macro_f1 =\n0.6794', '\\n', 'Epoch 2: validation_loss = 0.6394, val_macro_f1 = 0.6830', '\\n',\n'Epoch 3: validation_loss = 0.6366, val_macro_f1 = 0.6831', '\\n', 'Epoch 4:\nvalidation_loss = 0.6361, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6381, val_macro_f1 = 0.6872', '\\n', 'Epoch 6:\nvalidation_loss = 0.6375, val_macro_f1 = 0.6811', '\\n', 'Epoch 7:\nvalidation_loss = 0.6377, val_macro_f1 = 0.6793', '\\n', 'Epoch 8:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6834', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6901  |  Test MCC = 0.3811', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training 10 epochs | lr=0.001 ===', '\\n',\n'Epoch 1: validation_loss = 0.6489, val_macro_f1 = 0.6632', '\\n', 'Epoch 2:\nvalidation_loss = 0.6396, val_macro_f1 = 0.6830', '\\n', 'Epoch 3:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6762', '\\n', 'Epoch 4:\nvalidation_loss = 0.6358, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6363, val_macro_f1 = 0.6809', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6858  |  Test MCC = 0.3732', '\\n', '\\n=== Training 10 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6544, val_macro_f1 =\n0.6582', '\\n', 'Epoch 2: validation_loss = 0.6435, val_macro_f1 = 0.6833', '\\n',\n'Epoch 3: validation_loss = 0.6379, val_macro_f1 = 0.6829', '\\n', 'Epoch 4:\nvalidation_loss = 0.6370, val_macro_f1 = 0.6830', '\\n', 'Epoch 5:\nvalidation_loss = 0.6356, val_macro_f1 = 0.6808', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6854  |  Test MCC = 0.3734', '\\n', '\\n=== Training 15 epochs\n| lr=0.001 ===', '\\n', 'Epoch 1: validation_loss = 0.6496, val_macro_f1 =\n0.6622', '\\n', 'Epoch 2: validation_loss = 0.6408, val_macro_f1 = 0.6774', '\\n',\n'Epoch 3: validation_loss = 0.6345, val_macro_f1 = 0.6812', '\\n', 'Epoch 4:\nvalidation_loss = 0.6350, val_macro_f1 = 0.6829', '\\n', 'Epoch 5:\nvalidation_loss = 0.6348, val_macro_f1 = 0.6808', '\\n', 'Epoch 6:\nvalidation_loss = 0.6428, val_macro_f1 = 0.6720', '\\n', 'Epoch 7:\nvalidation_loss = 0.6421, val_macro_f1 = 0.6751', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6878  |  Test MCC = 0.3772', '\\n', '\\n=== Training 15 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6542, val_macro_f1 =\n0.6794', '\\n', 'Epoch 2: validation_loss = 0.6394, val_macro_f1 = 0.6830', '\\n',\n'Epoch 3: validation_loss = 0.6366, val_macro_f1 = 0.6831', '\\n', 'Epoch 4:\nvalidation_loss = 0.6361, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6381, val_macro_f1 = 0.6872', '\\n', 'Epoch 6:\nvalidation_loss = 0.6375, val_macro_f1 = 0.6811', '\\n', 'Epoch 7:\nvalidation_loss = 0.6377, val_macro_f1 = 0.6793', '\\n', 'Epoch 8:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6834', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6901  |  Test MCC = 0.3811', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training 10 epochs | lr=0.001 ===', '\\n',\n'Epoch 1: validation_loss = 0.6489, val_macro_f1 = 0.6632', '\\n', 'Epoch 2:\nvalidation_loss = 0.6396, val_macro_f1 = 0.6830', '\\n', 'Epoch 3:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6762', '\\n', 'Epoch 4:\nvalidation_loss = 0.6358, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6363, val_macro_f1 = 0.6809', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6858  |  Test MCC = 0.3732', '\\n', '\\n=== Training 10 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6544, val_macro_f1 =\n0.6582', '\\n', 'Epoch 2: validation_loss = 0.6435, val_macro_f1 = 0.6833', '\\n',\n'Epoch 3: validation_loss = 0.6379, val_macro_f1 = 0.6829', '\\n', 'Epoch 4:\nvalidation_loss = 0.6370, val_macro_f1 = 0.6830', '\\n', 'Epoch 5:\nvalidation_loss = 0.6356, val_macro_f1 = 0.6808', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6854  |  Test MCC = 0.3734', '\\n', '\\n=== Training 15 epochs\n| lr=0.001 ===', '\\n', 'Epoch 1: validation_loss = 0.6496, val_macro_f1 =\n0.6622', '\\n', 'Epoch 2: validation_loss = 0.6408, val_macro_f1 = 0.6774', '\\n',\n'Epoch 3: validation_loss = 0.6345, val_macro_f1 = 0.6812', '\\n', 'Epoch 4:\nvalidation_loss = 0.6350, val_macro_f1 = 0.6829', '\\n', 'Epoch 5:\nvalidation_loss = 0.6348, val_macro_f1 = 0.6808', '\\n', 'Epoch 6:\nvalidation_loss = 0.6428, val_macro_f1 = 0.6720', '\\n', 'Epoch 7:\nvalidation_loss = 0.6421, val_macro_f1 = 0.6751', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6878  |  Test MCC = 0.3772', '\\n', '\\n=== Training 15 epochs\n| lr=0.0005 ===', '\\n', 'Epoch 1: validation_loss = 0.6542, val_macro_f1 =\n0.6794', '\\n', 'Epoch 2: validation_loss = 0.6394, val_macro_f1 = 0.6830', '\\n',\n'Epoch 3: validation_loss = 0.6366, val_macro_f1 = 0.6831', '\\n', 'Epoch 4:\nvalidation_loss = 0.6361, val_macro_f1 = 0.6812', '\\n', 'Epoch 5:\nvalidation_loss = 0.6381, val_macro_f1 = 0.6872', '\\n', 'Epoch 6:\nvalidation_loss = 0.6375, val_macro_f1 = 0.6811', '\\n', 'Epoch 7:\nvalidation_loss = 0.6377, val_macro_f1 = 0.6793', '\\n', 'Epoch 8:\nvalidation_loss = 0.6388, val_macro_f1 = 0.6834', '\\n', 'Early stopping.', '\\n',\n'Test macro_F1 = 0.6901  |  Test MCC = 0.3811', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_18-47-\n59_symblic_polyrule_reasoning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["The execution output shows that the code ran successfully without any errors or\nbugs. The training process completed for 5 epochs, and validation as well as\ntest MCC metrics were computed successfully. The final test MCC achieved is\n0.3792, and the experiment data was saved correctly to the specified location.\nNo issues were observed in the implementation or execution.", "The execution failed due to a TypeError in the `encode_sequence` function. The\nerror message indicates that `encode_sequence` was called with two arguments,\nbut it is defined to accept only one argument. This is because the function\nsignature of `encode_sequence` does not include a parameter for `vocab`, which\nis being passed as a second argument in the `__getitem__` method of the\n`SPRTorchDataset` class.  ### Proposed Fix: Modify the `encode_sequence`\nfunction to accept an additional parameter for `vocab`. Update its definition as\nfollows: ```python def encode_sequence(seq: str, vocab: dict) -> list[int]:\nreturn [vocab[ch] for ch in seq] ``` This change ensures that the function can\nhandle the `vocab` parameter correctly, resolving the TypeError.", "The training script executed successfully without any errors or bugs. The script\nloaded the data properly, trained the GRU model on three different learning\nrates, and evaluated performance using the Matthews Correlation Coefficient\n(MCC). Results were saved to a file. However, the MCC values across all learning\nrates are relatively low, indicating room for improvement in model performance\nor dataset representation.", "The training script executed successfully without any errors or bugs. The model\nwas trained and evaluated with different batch sizes (64, 128, 256, 512), and\nthe results were logged for each setting. The validation and test MCC metrics\nwere computed and showed reasonable performance trends. The results were saved\nto a file for further analysis. Overall, the script appears to function as\nintended.", "The training script executed successfully without any errors or bugs. The\nexperiment evaluated different weight decay values to tune the hyperparameters\nof the GRU-based classification model. The best validation MCC (Matthews\nCorrelation Coefficient) achieved was 0.3653 with a weight decay of 0.001, and\nthe test MCC was 0.3712. The experiment data was saved correctly, and the\nexecution completed within the time limit.", "The training script executed successfully without any errors or bugs. The model\nwas trained with multiple configurations of maximum epochs (5, 10, and 15) with\nearly stopping enabled. The results improved slightly across configurations,\nwith the best Test MCC achieved being 0.3772 and the Test macro-F1 being 0.6878.\nThe experiment data was successfully saved, and the runtime was well within the\ntime limit of 30 minutes. No issues were observed during execution.", "The execution of the training script was successful. The model was trained using\na GRU-based baseline on the SPR_BENCH datasets with varying hyperparameters\n(learning rate and number of epochs). Early stopping was implemented to prevent\noverfitting. The results showed consistent improvements in validation and test\nmetrics (macro F1 and MCC) with the best test macro F1 score reaching 0.6901 and\nMCC at 0.3811. The experiment data was saved successfully, and no bugs were\nidentified in the process.", "The training script executed successfully without any errors or bugs. The model\nwas trained with different dropout rates (0.1, 0.3, 0.5) and evaluated using the\nMatthews Correlation Coefficient (MCC) metric. The best dropout rate was\nidentified as 0.1 with a validation MCC of 0.3689, and the corresponding test\nMCC was 0.3671. The experiment data was saved successfully for further analysis.\nNo issues were found in the execution output.", "The training script executed successfully without any errors or bugs. The model\nwas trained on the SPR_BENCH dataset with varying maximum epochs (10, 20, 30) to\nevaluate its performance. Metrics such as Matthews Correlation Coefficient (MCC)\nand Macro F1-score were used for evaluation, and early stopping was implemented\nto prevent overfitting. The results were saved successfully. The execution time\nwas well within the limit, and the output indicates a stable and functional\nimplementation.", "", "The execution of the training script completed successfully without any bugs.\nThe model was trained with different combinations of epochs and learning rates,\nand early stopping was applied effectively. The results showed consistent\nimprovements in validation and test metrics, with the best test macro F1 score\nbeing 0.6901 and MCC being 0.3811. Experiment data was saved successfully for\nfurther analysis.", "", ""], "exc_type": [null, "TypeError", null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, {"args": ["encode_sequence() takes 1 positional argument but 2 were given"]}, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 225, "<module>", "res = train_for_epochs(ep, patience=3)"], ["runfile.py", 145, "train_for_epochs", "for batch in train_loader:"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 701, "__next__", "data = self._next_data()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 757, "_next_data", "data = self._dataset_fetcher.fetch(index)  # may raise StopIteration"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "fetch", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "<listcomp>", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["runfile.py", 77, "__getitem__", "ids = pad(encode_sequence(self.seqs[idx], self.vocab), self.max_len)"]], null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.617833, "best_value": 0.617833}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.636186, "best_value": 0.636186}]}, {"metric_name": "training Matthews correlation coefficient", "lower_is_better": false, "description": "Measures the quality of binary classifications during training. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.369434, "best_value": 0.369434}]}, {"metric_name": "validation Matthews correlation coefficient", "lower_is_better": false, "description": "Measures the quality of binary classifications during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.364823, "best_value": 0.364823}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "Measures the quality of binary classifications during testing. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.37918, "best_value": 0.37918}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset during the final epoch or iteration.", "data": [{"dataset_name": "lr_5e-04", "final_value": 0.6223, "best_value": 0.6223}, {"dataset_name": "lr_1e-03", "final_value": 0.6177, "best_value": 0.6177}, {"dataset_name": "lr_2e-03", "final_value": 0.615, "best_value": 0.615}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset during the final epoch or iteration.", "data": [{"dataset_name": "lr_5e-04", "final_value": 0.6356, "best_value": 0.6356}, {"dataset_name": "lr_1e-03", "final_value": 0.6383, "best_value": 0.6383}, {"dataset_name": "lr_2e-03", "final_value": 0.6388, "best_value": 0.6388}]}, {"metric_name": "training MCC", "lower_is_better": false, "description": "Matthew's Correlation Coefficient calculated on the training dataset during the final epoch or iteration.", "data": [{"dataset_name": "lr_5e-04", "final_value": 0.3641, "best_value": 0.3641}, {"dataset_name": "lr_1e-03", "final_value": 0.3716, "best_value": 0.3716}, {"dataset_name": "lr_2e-03", "final_value": 0.3729, "best_value": 0.3729}]}, {"metric_name": "validation MCC", "lower_is_better": false, "description": "Matthew's Correlation Coefficient calculated on the validation dataset during the final epoch or iteration.", "data": [{"dataset_name": "lr_5e-04", "final_value": 0.3733, "best_value": 0.3733}, {"dataset_name": "lr_1e-03", "final_value": 0.3733, "best_value": 0.3733}, {"dataset_name": "lr_2e-03", "final_value": 0.3646, "best_value": 0.3646}]}, {"metric_name": "testing MCC", "lower_is_better": false, "description": "Matthew's Correlation Coefficient calculated on the testing dataset during the final epoch or iteration.", "data": [{"dataset_name": "lr_5e-04", "final_value": 0.3715, "best_value": 0.3715}, {"dataset_name": "lr_1e-03", "final_value": 0.3751, "best_value": 0.3751}, {"dataset_name": "lr_2e-03", "final_value": 0.3712, "best_value": 0.3712}]}]}, {"metric_names": [{"metric_name": "Matthew's correlation coefficient", "lower_is_better": false, "description": "A measure of the quality of binary classifications.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3772, "best_value": 0.3772}]}, {"metric_name": "loss", "lower_is_better": true, "description": "A measure of how well the model's predictions match the actual data. Lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6343, "best_value": 0.6177}]}]}, {"metric_names": [{"metric_name": "Matthews correlation coefficient", "lower_is_better": false, "description": "A measure of the quality of binary classifications, ranging from -1 to +1.", "data": [{"dataset_name": "TRAIN DATASET", "final_value": 0.3681, "best_value": 0.3681}, {"dataset_name": "VALIDATION DATASET", "final_value": 0.3653, "best_value": 0.3653}, {"dataset_name": "TEST DATASET", "final_value": 0.3712, "best_value": 0.3712}]}, {"metric_name": "loss", "lower_is_better": true, "description": "A measure of error in the model's predictions, where lower values indicate better performance.", "data": [{"dataset_name": "TRAIN DATASET", "final_value": 0.6211, "best_value": 0.6211}, {"dataset_name": "VALIDATION DATASET", "final_value": 0.6399, "best_value": 0.6399}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6162, "best_value": 0.6162}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6343, "best_value": 0.6343}]}, {"metric_name": "training Matthews correlation coefficient (MCC)", "lower_is_better": false, "description": "Measures the quality of classification for training. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3716, "best_value": 0.3716}]}, {"metric_name": "validation Matthews correlation coefficient (MCC)", "lower_is_better": false, "description": "Measures the quality of classification for validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3733, "best_value": 0.3733}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Measures the F1 score across all classes for training. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6832, "best_value": 0.6832}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Measures the F1 score across all classes for validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6849, "best_value": 0.6849}]}, {"metric_name": "test Matthews correlation coefficient (MCC)", "lower_is_better": false, "description": "Measures the quality of classification on the test set. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3772, "best_value": 0.3772}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Measures the F1 score across all classes on the test set. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6878, "best_value": 0.6878}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6162, "best_value": 0.6162}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6345, "best_value": 0.6345}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6862, "best_value": 0.6862}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6872, "best_value": 0.6872}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6901, "best_value": 0.6901}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3811, "best_value": 0.3811}]}]}, {"metric_names": [{"metric_name": "MCC", "lower_is_better": false, "description": "Matthew's Correlation Coefficient, a performance metric for classification tasks.", "data": [{"dataset_name": "Training", "final_value": 0.3534, "best_value": 0.3658}, {"dataset_name": "Validation", "final_value": 0.3479, "best_value": 0.3614}, {"dataset_name": "Test", "final_value": 0.3671, "best_value": 0.3671}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "Loss measures the error in predictions; lower values indicate better performance.", "data": [{"dataset_name": "Training", "final_value": 0.6279, "best_value": 0.6202}, {"dataset_name": "Validation", "final_value": 0.6406, "best_value": 0.6344}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or difference between predicted and actual values. Lower values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.6191, "best_value": 0.6191}, {"dataset_name": "validation", "final_value": 0.6363, "best_value": 0.6363}]}, {"metric_name": "MCC", "lower_is_better": false, "description": "Matthews correlation coefficient, used for evaluating classification problems. Higher values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.3657, "best_value": 0.3657}, {"dataset_name": "validation", "final_value": 0.377, "best_value": 0.377}, {"dataset_name": "test", "final_value": 0.3772, "best_value": 0.3772}]}, {"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score, used for evaluating classification performance. Higher values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.68, "best_value": 0.68}, {"dataset_name": "validation", "final_value": 0.6872, "best_value": 0.6872}, {"dataset_name": "test", "final_value": 0.6879, "best_value": 0.6879}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6162, "best_value": 0.6162}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6345, "best_value": 0.6345}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6862, "best_value": 0.6862}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6872, "best_value": 0.6872}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6901, "best_value": 0.6901}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "Matthews correlation coefficient during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3811, "best_value": 0.3811}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6162, "best_value": 0.6162}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6345, "best_value": 0.6345}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6862, "best_value": 0.6862}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6872, "best_value": 0.6872}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6901, "best_value": 0.6901}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "Matthews correlation coefficient during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3811, "best_value": 0.3811}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model fits the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6162, "best_value": 0.6162}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6345, "best_value": 0.6345}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score during training, measuring the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6862, "best_value": 0.6862}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation, measuring the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6872, "best_value": 0.6872}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score on the test dataset, measuring the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6901, "best_value": 0.6901}]}, {"metric_name": "test Matthews correlation coefficient", "lower_is_better": false, "description": "The Matthews correlation coefficient on the test dataset, measuring the quality of binary classifications.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3811, "best_value": 0.3811}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, true, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_mcc_curve.png", "../../logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_5e-04_curves.png", "../../logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_1e-03_curves.png", "../../logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_2e-03_curves.png", "../../logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_test_MCC_bar.png"], ["../../logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_MCC_curves.png", "../../logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_test_MCC_bar.png"], ["../../logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_mcc_curves.png", "../../logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_val_mcc_bar.png", "../../logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_loss_curve.png", "../../logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_mcc_curve.png", "../../logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_f1_curve.png", "../../logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_test_summary.png"], ["../../logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_MCC_curves.png", "../../logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_final_val_MCC_bar.png", "../../logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_MCC_F1_curve.png", "../../logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_test_MCC_bar.png"], ["../../logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_test_metrics.png"], ["../../logs/0-run/experiment_results/seed_aggregation_50bcc09e09b24661aaef1dc0f0f4cbe4/spr_bench_aggregated_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_50bcc09e09b24661aaef1dc0f0f4cbe4/spr_bench_aggregated_f1_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_50bcc09e09b24661aaef1dc0f0f4cbe4/spr_bench_aggregated_test_metrics.png"]], "plot_paths": [["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_mcc_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_5e-04_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_1e-03_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_2e-03_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_test_MCC_bar.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_MCC_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_test_MCC_bar.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_mcc_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_val_mcc_bar.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_loss_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_mcc_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_f1_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_test_summary.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_f1_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_test_metrics.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_MCC_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_final_val_MCC_bar.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_MCC_F1_curve.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_test_MCC_bar.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_f1_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_test_metrics.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_f1_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_test_metrics.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_f1_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_test_metrics.png"], ["experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_50bcc09e09b24661aaef1dc0f0f4cbe4/spr_bench_aggregated_loss_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_50bcc09e09b24661aaef1dc0f0f4cbe4/spr_bench_aggregated_f1_curves.png", "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/seed_aggregation_50bcc09e09b24661aaef1dc0f0f4cbe4/spr_bench_aggregated_test_metrics.png"]], "plot_analyses": [[{"analysis": "The plot shows the training loss and validation loss over five epochs. Both the training and validation losses decrease over time, indicating that the model is learning effectively. However, the validation loss plateaus and slightly increases after the third epoch, which may indicate the onset of overfitting. The gap between the training and validation loss is relatively small, suggesting that the model is generalizing well to unseen data at this stage.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot compares the Matthews Correlation Coefficient (MCC) for training and validation over five epochs. Both metrics improve significantly during the first two epochs, with validation MCC closely tracking training MCC. After the second epoch, validation MCC plateaus and slightly decreases, while training MCC continues to improve marginally. This divergence may indicate slight overfitting, as the model performs better on training data than on validation data.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_mcc_curve.png"}, {"analysis": "The confusion matrix provides a breakdown of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for the test set. The high counts of TP and TN indicate that the model is correctly classifying a large proportion of the sequences. However, the presence of FP and FN suggests room for improvement in precision and recall. Balancing these metrics will be critical for optimizing the model's performance on complex symbolic rules.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_9758a0f0729e4e8ea8ab995545640add_proc_3327675/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The loss curves indicate that both training and validation loss decrease steadily across epochs, suggesting that the model is learning effectively without overfitting. The MCC (Matthews Correlation Coefficient) curves show an upward trend, with validation MCC peaking and stabilizing by the third epoch. This suggests that the learning rate of 5e-04 is effective in achieving stable performance.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_5e-04_curves.png"}, {"analysis": "The loss curves show a consistent decrease in training loss, but validation loss exhibits slight fluctuations after the second epoch. This could indicate minor overfitting or sensitivity to the learning rate of 1e-03. The MCC curves highlight instability in validation MCC, with a peak at epoch 3 followed by a drop, while training MCC continues to improve. This suggests that the learning rate might be slightly too high, leading to unstable generalization.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_1e-03_curves.png"}, {"analysis": "The training loss decreases consistently, while validation loss shows a slight increase in later epochs, indicating potential overfitting with the learning rate of 2e-03. The MCC curves show that while training MCC improves steadily, validation MCC peaks at epoch 3 and then declines. This suggests the learning rate may be too aggressive, leading to suboptimal generalization.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_lr_2e-03_curves.png"}, {"analysis": "The bar plot compares test MCC across different learning rates. All three learning rates (5e-04, 1e-03, 2e-03) achieve similar test MCC scores, with only marginal differences. This indicates that the model's performance is relatively robust to changes in the learning rate within this range. However, fine-tuning might still be necessary to optimize performance further.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_6af0f8e6f31e4367b7191d11b2f759f8_proc_3331033/SPR_BENCH_test_MCC_bar.png"}], [{"analysis": "This plot illustrates the training and validation Binary Cross-Entropy (BCE) loss for different batch sizes (64, 128, 256, and 512). The training loss decreases steadily for all batch sizes, indicating that the model is learning effectively. However, the validation loss shows different trends. For smaller batch sizes (64 and 128), the validation loss stabilizes or slightly decreases, suggesting better generalization. In contrast, for larger batch sizes (256 and 512), the validation loss either stagnates or increases after the initial epochs, indicating potential overfitting or insufficient generalization. Smaller batch sizes seem more favorable for this task as they balance training and validation performance.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the Matthews Correlation Coefficient (MCC) for training and validation across different batch sizes. Smaller batch sizes (64 and 128) achieve higher MCC values for validation, indicating better generalization. Larger batch sizes (256 and 512) show slower improvements in validation MCC and sometimes even diverge from the training MCC, suggesting overfitting. The MCC trends suggest that smaller batch sizes are more effective in capturing the complex patterns in the SPR_BENCH dataset.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_MCC_curves.png"}, {"analysis": "This plot compares test MCC values across different batch sizes. The MCC values are similar across all batch sizes, with marginal differences. This suggests that while training and validation trends differ for various batch sizes, the final test performance does not vary significantly. However, smaller batch sizes might still be preferred due to their stronger generalization during validation.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_757cfabf9b4342a789cb1c94a908a45e_proc_3331034/SPR_BENCH_test_MCC_bar.png"}], [{"analysis": "The plot shows the loss curves for different weight decay values in both training and validation phases. Lower loss values indicate better performance. The training losses generally decrease smoothly across epochs for all weight decay values, with some performing slightly better than others. Validation losses, however, show more variability, with some configurations (e.g., wd=0.001) maintaining lower loss values across epochs. This suggests that higher weight decay values might lead to better generalization, as lower validation losses are indicative of reduced overfitting.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the Matthews Correlation Coefficient (MCC) across epochs for various weight decay values. MCC is a balanced metric for classification tasks, particularly useful for imbalanced datasets. The curves show that MCC improves with training for most configurations, stabilizing after a few epochs. Among them, certain configurations (e.g., wd=0.001) achieve slightly higher MCC values, indicating better classification performance. However, some configurations (e.g., wd=0.0005) display significant fluctuations, suggesting instability or overfitting.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_mcc_curves.png"}, {"analysis": "This bar chart summarizes the final validation MCC values for different weight decay configurations. The configuration with wd=0.001 achieves the highest MCC, as highlighted in red, outperforming all other configurations. This indicates that wd=0.001 is the optimal weight decay value for this experiment, leading to the best generalization and classification performance on the validation set.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_val_mcc_bar.png"}, {"analysis": "The confusion matrix provides an overview of the model's performance on the test set. True positives (376) and true negatives (310) are relatively higher than false positives (176) and false negatives (138), indicating that the model performs reasonably well in distinguishing between the two classes. However, there is still room for improvement in reducing false predictions, especially false positives, to enhance overall accuracy.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_d9b071101cfe4456a015c06520093cf9_proc_3331035/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The BCE loss curve shows a general downward trend for the training loss, indicating that the model is learning and fitting the training data. However, the validation loss exhibits significant fluctuations, suggesting potential overfitting or instability in training. The gap between training and validation loss remains moderate, but the validation loss's erratic behavior warrants further investigation. Adjusting the learning rate or regularization parameters might stabilize the training process.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_loss_curve.png"}, {"analysis": "The MCC curve for both training and validation displays significant oscillations across epochs, with no clear upward trend. This indicates that the model's ability to correctly classify positive and negative samples is inconsistent. The validation MCC, in particular, shows sharp drops, suggesting that the model's performance on unseen data is unstable. Further tuning of hyperparameters or the use of techniques like cross-validation might help improve stability.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_mcc_curve.png"}, {"analysis": "The Macro-F1 curve for training and validation also shows fluctuating performance across epochs. While the training Macro-F1 shows slight improvements, the validation Macro-F1 exhibits sharp drops and lacks consistency. This indicates that the model struggles to generalize well across all classes. Strategies like data augmentation or rebalancing the dataset could be explored to enhance performance consistency.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_f1_curve.png"}, {"analysis": "The bar chart summarizing the best test performance reveals a significant gap between the MCC (0.377) and Macro-F1 (0.688) scores. While the Macro-F1 score is relatively decent, the low MCC score indicates poor overall classification performance, particularly in distinguishing between true positive and true negative samples. This discrepancy highlights the need for further optimization, particularly focusing on improving the model's ability to handle imbalanced data and edge cases.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_90b300b94306499b913559e343d198e6_proc_3331032/spr_bench_test_summary.png"}], [{"analysis": "The loss curves suggest that the model's training loss decreases steadily over epochs, which is indicative of learning. However, the validation loss exhibits periodic fluctuations, which could point to overfitting or sensitivity to the validation set. The periodic spikes in loss might also indicate instability in the training process or an issue with the learning rate schedule. The overall trend shows slight improvement, but the unstable validation loss raises concerns about generalization.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_loss_curves.png"}, {"analysis": "The Macro-F1 curves show a relatively stable performance across epochs for both training and validation sets. However, the fluctuations in the validation curve suggest that the model's performance is not consistently improving and might be sensitive to specific validation samples. The close alignment between training and validation performance indicates that the model is not severely overfitting, but the lack of a clear upward trend suggests limited gains from current hyperparameter settings.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_f1_curves.png"}, {"analysis": "The bar chart comparing Macro-F1 and MCC on the test set reveals that while the Macro-F1 score is relatively high (0.686), the MCC score is significantly lower (0.373). This discrepancy suggests that the model may be performing well on the majority class but struggles with balanced performance across all classes. The low MCC indicates limited correlation between predicted and true labels, highlighting potential issues with class imbalance or misclassification in minority classes.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_b9a30e0c00cd4e55acef01fc62a846c2_proc_3331034/spr_bench_test_metrics.png"}], [{"analysis": "This plot illustrates the training and validation loss (BCE Loss) across different epochs for various dropout rates (p=0.1, 0.3, 0.5). The training loss consistently decreases for all dropout rates, indicating that the model is learning effectively. However, the validation loss shows different trends: for p=0.1 and p=0.3, the validation loss decreases initially and stabilizes, suggesting a good fit. For p=0.5, the validation loss decreases initially but starts to increase after epoch 3, indicating potential overfitting or suboptimal generalization at higher dropout rates.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the Matthews Correlation Coefficient (MCC) for training and validation across epochs for different dropout rates. The MCC increases consistently up to epoch 4 for all dropout rates, demonstrating improving classification performance. Beyond epoch 4, the MCC stabilizes for p=0.1 and p=0.3, while it slightly decreases for p=0.5, indicating that a lower dropout rate might be more effective for this task.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_MCC_curves.png"}, {"analysis": "This bar chart compares the final validation MCC for different dropout rates. The MCC values are relatively close across the dropout rates, with p=0.1 slightly outperforming the others. This suggests that while all dropout rates perform similarly, a lower dropout rate might provide a marginal advantage in terms of classification performance.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_final_val_MCC_bar.png"}, {"analysis": "This confusion matrix represents the performance of the best model (p=0.1) on the test set. The model achieves a good balance between true positives (371) and true negatives (313), but there are still a notable number of false positives (173) and false negatives (143). This indicates that while the model performs reasonably well, there is room for improvement in minimizing misclassifications.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f02d936032c7486f96603e87959cdb14_proc_3331033/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves show a consistent decrease in both training and validation losses over epochs, indicating that the model is learning effectively. However, the sharp spikes in loss at certain intervals suggest a potential issue with learning rate scheduling or batch normalization, which could be causing temporary destabilization. The gap between training and validation loss remains small, which is a good sign of minimal overfitting.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_loss_curve.png"}, {"analysis": "The performance curves indicate steady improvements in both MCC and macro-F1 scores over epochs for both training and validation sets. The sudden drops in performance metrics align with the spikes observed in the loss curves, further suggesting an issue with optimization hyperparameters. Overall, the metrics show convergence and a good balance between training and validation performance, reflecting the model's generalization ability.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_MCC_F1_curve.png"}, {"analysis": "The bar chart highlights the incremental improvements in test MCC as the training budget increases from 10 to 30 epochs. While the gains are relatively modest, they indicate that longer training can lead to better performance. The diminishing returns suggest that further increases in training budget might not yield significant improvements without other changes to the model or training process.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_343ca29ec2e840d896f9ee4a8f551cd4_proc_3331035/SPR_BENCH_test_MCC_bar.png"}], [{"analysis": "The loss curves indicate a general downward trend in the training loss over epochs, which is expected as the model learns. However, the oscillations in both training and validation losses suggest instability in the training process, possibly caused by inappropriate hyperparameters such as a high learning rate or insufficient regularization. The validation loss does not consistently decrease, indicating potential overfitting or that the model struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_loss_curves.png"}, {"analysis": "The Macro-F1 curves show fluctuations but an overall improvement in both training and validation scores over epochs. The validation curve closely follows the training curve, indicating that the model's performance on the validation set is not significantly worse than on the training set. This suggests that the model is not severely overfitting, but the instability in the curves still points to possible hyperparameter tuning issues or dataset noise.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_f1_curves.png"}, {"analysis": "The bar chart comparing Macro-F1 and MCC on the test set shows a Macro-F1 score of 0.686 and an MCC of 0.373. While the Macro-F1 score is relatively decent, the MCC score is considerably lower. This discrepancy indicates that the model's predictions might not be well-balanced across classes, and further investigation into class-wise performance and potential class imbalance is recommended.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/spr_bench_test_metrics.png"}], [{"analysis": "The loss curves indicate a steady decrease in training loss, which is expected as the model learns during training. However, the validation loss exhibits periodic spikes, suggesting possible overfitting or instability in the learning process. This could be due to the choice of hyperparameters such as learning rate or batch size. The consistent decrease in training loss, coupled with the fluctuations in validation loss, suggests that the model is learning patterns in the training data but may be struggling to generalize effectively to unseen validation data.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_loss_curves.png"}, {"analysis": "The Macro-F1 curves show an overall increasing trend for both training and validation sets, indicating that the model is improving in terms of balanced classification performance across all classes. However, the fluctuations in the validation Macro-F1 scores suggest inconsistent generalization, which might be linked to the instability observed in the loss curves. Despite this, the upward trend is promising and indicates progress in model tuning.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_f1_curves.png"}, {"analysis": "The bar chart comparing Macro-F1 and MCC on the test set reveals that the model achieves a Macro-F1 score of 0.686, which is relatively high, indicating good performance across all classes. However, the MCC score is significantly lower at 0.373, suggesting that the model's ability to correctly predict positive and negative classes is limited. This discrepancy might point to class imbalance or issues with the model's ability to capture certain patterns in the data.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/spr_bench_test_metrics.png"}], [{"analysis": "The loss curves indicate that the model's training loss decreases steadily over the epochs, suggesting effective learning. However, the validation loss exhibits periodic spikes, which could indicate overfitting or sensitivity to specific validation data points. The general downward trend in validation loss is promising, but the oscillations suggest that the model's generalization ability might be inconsistent. Tuning hyperparameters such as learning rate or implementing regularization techniques might help stabilize the validation loss.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that both the training and validation scores improve over time, reflecting the model's increasing ability to classify sequences correctly. The fluctuations in the validation curve suggest that the model's performance might be sensitive to certain validation data subsets. This could be mitigated by fine-tuning the batch size or applying cross-validation to ensure more consistent performance.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_f1_curves.png"}, {"analysis": "The bar chart comparing Macro-F1 and MCC on the test set highlights a significant disparity between the two metrics. While the Macro-F1 score is relatively high (0.686), the MCC score is lower (0.373). This indicates that while the model performs well in terms of balanced precision and recall, its ability to handle class imbalance or correlation between predictions and ground truth is weaker. This discrepancy suggests a need to focus on improving the model's robustness to imbalanced datasets or exploring alternate loss functions to better align these metrics.", "plot_path": "experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/spr_bench_test_metrics.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model is learning effectively, with decreasing\nlosses and improving MCC during early epochs. However, slight overfitting is\nnoticeable as the validation metrics plateau and diverge from training metrics.\nThe confusion matrix highlights good classification performance but reveals\nareas for improvement in reducing false positives and false negatives.", "[]", "The results demonstrate that the model is learning effectively, with clear\ntrends in loss and MCC metrics across different learning rates. However, higher\nlearning rates show signs of overfitting or instability in validation\nperformance, suggesting that further tuning is necessary. The test MCC\ncomparison indicates robustness to learning rate changes, but additional\noptimization could improve results.", "The analysis indicates that smaller batch sizes (64 and 128) perform better in\nterms of validation loss and MCC, suggesting better generalization. Larger batch\nsizes (256 and 512) show signs of overfitting or insufficient generalization\nduring validation. Despite this, the final test MCC values are similar across\nall batch sizes, implying that the choice of batch size has a limited impact on\nthe final test performance. Smaller batch sizes are recommended due to their\nvalidation performance advantages.", "The experimental results highlight the importance of weight decay in improving\nmodel generalization and classification performance. The optimal weight decay\nvalue (wd=0.001) achieves the best MCC score and maintains stability across\nepochs. The confusion matrix indicates reasonable classification performance,\nthough there is room for improvement in reducing false predictions.", "The experimental results reveal that the model demonstrates inconsistent\nperformance across metrics, with significant fluctuations observed in validation\nloss, MCC, and Macro-F1 scores. The best test performance shows a notable gap\nbetween MCC and Macro-F1 scores, indicating room for improvement in handling\nimbalanced data and generalization. Further hyperparameter tuning,\nregularization, and dataset rebalancing are recommended.", "The results indicate that while the model shows promise, there are concerns with\ngeneralization and balanced classification performance. The loss curves reveal\ninstability and potential overfitting, while the Macro-F1 and MCC metrics point\nto challenges with class balance and overall prediction quality. Addressing\nthese issues through hyperparameter tuning or data augmentation could improve\nmodel robustness.", "The results indicate that the model performs well with lower dropout rates\n(p=0.1 and p=0.3), achieving stable validation loss and high MCC. However,\nhigher dropout rates (p=0.5) lead to signs of overfitting and slightly lower\nMCC. The confusion matrix highlights areas for improvement in reducing false\npositives and false negatives.", "The plots reveal that the model is learning effectively with minimal\noverfitting, as evidenced by the steady decrease in loss and the convergence of\nperformance metrics. However, there are optimization issues, such as spikes in\nloss and metric drops, that need to be addressed. Incremental improvements in\ntest MCC suggest that the current training setup is effective, but further gains\nmay require additional tuning or architectural changes.", "The experimental results show promise but highlight areas for improvement. The\nloss curves suggest training instability, possibly due to hyperparameter issues.\nThe Macro-F1 curves indicate moderate generalization but also exhibit\nfluctuations that need addressing. The test metrics reveal a gap between\nMacro-F1 and MCC, suggesting potential class imbalance or prediction\ninconsistencies.", "The results show promise in improving the model's performance on the SPR_BENCH\ntask. The training loss decreases steadily, but validation loss spikes indicate\npotential overfitting or instability. Macro-F1 scores show improvement, but the\ntest MCC score highlights limitations in predictive performance. Further\nhyperparameter tuning and addressing class imbalance or data representation\nissues may enhance results.", "The plots reveal promising trends in model performance, with steady improvements\nin loss and Macro-F1 over time. However, fluctuations in validation metrics and\na significant gap between Macro-F1 and MCC on the test set suggest areas for\nimprovement, particularly in generalization and handling of class imbalance.", "[]"], "exec_time": [4.861910820007324, 2.780441999435425, 4.738370418548584, 5.254851818084717, 10.01436448097229, 7.911975383758545, 6.376748085021973, 4.147345781326294, 10.224529266357422, 11.541270971298218, 6.313736915588379, 6.415578603744507, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], [], ["['5e-04']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[]"], ["\"\""], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[]"], ["\"\""], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import matthews_corrcoef\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- Load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    data = experiment_data.get(\"SPR_BENCH\", {})\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    train_mcc = data.get(\"metrics\", {}).get(\"train\", [])\n    val_mcc = data.get(\"metrics\", {}).get(\"val\", [])\n    preds = np.array(data.get(\"predictions\", []))\n    gts = np.array(data.get(\"ground_truth\", []))\n\n    # ---------- Plot 1: Loss curves ----------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(train_losses) + 1)\n        plt.plot(epochs, train_losses, label=\"Train Loss\")\n        plt.plot(epochs, val_losses, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.title(\"SPR_BENCH Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ---------- Plot 2: MCC curves ----------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(train_mcc) + 1)\n        plt.plot(epochs, train_mcc, label=\"Train MCC\")\n        plt.plot(epochs, val_mcc, label=\"Val MCC\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MCC\")\n        plt.title(\"SPR_BENCH Training vs Validation MCC\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_mcc_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MCC curve: {e}\")\n        plt.close()\n\n    # ---------- Plot 3: Confusion-matrix style bar plot ----------\n    try:\n        plt.figure()\n        tp = np.sum((preds == 1) & (gts == 1))\n        tn = np.sum((preds == 0) & (gts == 0))\n        fp = np.sum((preds == 1) & (gts == 0))\n        fn = np.sum((preds == 0) & (gts == 1))\n        bars = [tp, fp, fn, tn]\n        labels = [\"TP\", \"FP\", \"FN\", \"TN\"]\n        plt.bar(labels, bars, color=[\"g\", \"r\", \"r\", \"g\"])\n        plt.ylabel(\"Count\")\n        plt.title(\"SPR_BENCH Test Confusion Matrix (bar)\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion plot: {e}\")\n        plt.close()\n\n    # ---------- Print key metrics ----------\n    if val_mcc:\n        print(f\"Final Validation MCC: {val_mcc[-1]:.4f}\")\n    if preds.size and gts.size:\n        test_mcc = matthews_corrcoef(gts, preds)\n        print(f\"Test MCC: {test_mcc:.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ------------------------------------------------------------------\n# Helper: MCC in pure numpy\ndef np_mcc(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denom = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n    return 0.0 if denom == 0 else (tp * tn - fp * fn) / denom\n\n\n# ------------------------------------------------------------------\n# 1) Per-learning-rate curves\ntest_mcc_scores = []\nlr_labels = []\n\nfor lr_key, lr_dict in experiment_data.get(\"learning_rate\", {}).items():\n    losses = lr_dict[\"losses\"]\n    metrics = lr_dict[\"metrics\"]\n    epochs = range(1, len(losses[\"train\"]) + 1)\n    try:\n        plt.figure(figsize=(10, 4))\n        # Loss subplot\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, losses[\"train\"], label=\"Train\")\n        plt.plot(epochs, losses[\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.title(\"Loss\")\n        plt.legend()\n        # MCC subplot\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, metrics[\"train\"], label=\"Train\")\n        plt.plot(epochs, metrics[\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MCC\")\n        plt.title(\"MCC\")\n        plt.legend()\n        # Overall figure title\n        plt.suptitle(f\"SPR_BENCH | Learning Rate {lr_key} | Left: Loss, Right: MCC\")\n        save_name = f\"SPR_BENCH_{lr_key}_curves.png\"\n        plt.savefig(os.path.join(working_dir, save_name))\n    except Exception as e:\n        print(f\"Error creating plot for {lr_key}: {e}\")\n    finally:\n        plt.close()\n    # Compute and store test MCC\n    try:\n        y_true = np.array(lr_dict[\"ground_truth\"])\n        y_pred = np.array(lr_dict[\"predictions\"])\n        mcc_val = np_mcc(y_true, y_pred)\n        test_mcc_scores.append(mcc_val)\n        lr_labels.append(lr_key)\n        print(f\"Test MCC for {lr_key}: {mcc_val:.4f}\")\n    except Exception as e:\n        print(f\"Error computing test MCC for {lr_key}: {e}\")\n\n# ------------------------------------------------------------------\n# 2) Bar chart of test MCC across learning rates\ntry:\n    plt.figure()\n    plt.bar(lr_labels, test_mcc_scores, color=\"skyblue\")\n    plt.ylabel(\"Test MCC\")\n    plt.title(\"SPR_BENCH | Test MCC Across Learning Rates\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_MCC_bar.png\"))\nexcept Exception as e:\n    print(f\"Error creating test MCC bar chart: {e}\")\nfinally:\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_bs_dict = experiment_data.get(\"batch_size\", {}).get(\"SPR_BENCH\", {})\n\n# Collect keys and ensure consistent ordering\nbs_keys = sorted(spr_bs_dict.keys(), key=lambda x: int(x.split(\"_\")[-1]))\n\n# 1) Loss curves ----------------------------------------------------\ntry:\n    plt.figure(figsize=(8, 5))\n    for k in bs_keys:\n        losses = spr_bs_dict[k][\"losses\"]\n        plt.plot(losses[\"train\"], label=f\"{k}-train\")\n        plt.plot(losses[\"val\"], label=f\"{k}-val\", linestyle=\"--\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss (GRU baseline)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# 2) MCC curves -----------------------------------------------------\ntry:\n    plt.figure(figsize=(8, 5))\n    for k in bs_keys:\n        mccs = spr_bs_dict[k][\"metrics\"]\n        plt.plot(mccs[\"train\"], label=f\"{k}-train\")\n        plt.plot(mccs[\"val\"], label=f\"{k}-val\", linestyle=\"--\")\n    plt.title(\"SPR_BENCH: Training vs Validation MCC\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Correlation Coefficient\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_MCC_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curve plot: {e}\")\n    plt.close()\n\n# 3) Test MCC bar chart --------------------------------------------\ntry:\n    plt.figure(figsize=(6, 4))\n    test_mccs = [spr_bs_dict[k].get(\"test_mcc\", np.nan) for k in bs_keys]\n    plt.bar(\n        range(len(bs_keys)), test_mccs, tick_label=[k.split(\"_\")[-1] for k in bs_keys]\n    )\n    plt.title(\"SPR_BENCH: Test MCC by Batch Size\")\n    plt.ylabel(\"Matthews Correlation Coefficient\")\n    plt.xlabel(\"Batch Size\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_MCC_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Test MCC bar plot: {e}\")\n    plt.close()\n\n# Print numeric results --------------------------------------------\nfor k in bs_keys:\n    print(f\"{k}: Test MCC = {spr_bs_dict[k].get('test_mcc', 'NA')}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nwd_data = experiment_data.get(\"weight_decay_tuning\", {})\nweights = [\n    k\n    for k in wd_data.keys()\n    if k not in {\"best_wd\", \"predictions\", \"ground_truth\", \"test_mcc\"}\n]\nbest_wd = wd_data.get(\"best_wd\", None)\ntest_mcc = wd_data.get(\"test_mcc\", None)\nprint(f\"Best weight_decay: {best_wd}, Test MCC: {test_mcc}\")\n\n# 1) Loss curves -----------------------------------------------------------\ntry:\n    plt.figure()\n    for w in weights:\n        tr = wd_data[w][\"losses\"][\"train\"]\n        vl = wd_data[w][\"losses\"][\"val\"]\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"train wd={w}\")\n        plt.plot(epochs, vl, \"--\", label=f\"val wd={w}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.title(\"SPR_BENCH Loss Curves for Different Weight Decays\")\n    plt.legend(fontsize=6)\n    fpath = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fpath, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) MCC curves ------------------------------------------------------------\ntry:\n    plt.figure()\n    for w in weights:\n        tr = wd_data[w][\"metrics\"][\"train\"]\n        vl = wd_data[w][\"metrics\"][\"val\"]\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"train wd={w}\")\n        plt.plot(epochs, vl, \"--\", label=f\"val wd={w}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews Correlation Coef\")\n    plt.title(\"SPR_BENCH MCC Curves for Different Weight Decays\")\n    plt.legend(fontsize=6)\n    fpath = os.path.join(working_dir, \"SPR_BENCH_mcc_curves.png\")\n    plt.savefig(fpath, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curves: {e}\")\n    plt.close()\n\n# 3) Final val MCC bar chart ----------------------------------------------\ntry:\n    plt.figure()\n    final_vals = [wd_data[w][\"metrics\"][\"val\"][-1] for w in weights]\n    plt.bar(\n        weights,\n        final_vals,\n        color=[\"red\" if w == str(best_wd) else \"gray\" for w in weights],\n    )\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Val MCC\")\n    plt.title(f\"SPR_BENCH Final Validation MCC (best wd={best_wd})\")\n    plt.xticks(rotation=45)\n    fpath = os.path.join(working_dir, \"SPR_BENCH_val_mcc_bar.png\")\n    plt.tight_layout()\n    plt.savefig(fpath, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# 4) Confusion matrix heat-map --------------------------------------------\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    preds = wd_data.get(\"predictions\", None)\n    gts = wd_data.get(\"ground_truth\", None)\n    if preds is not None and gts is not None:\n        cm = confusion_matrix(gts, preds, labels=[0, 1])\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.title(\n            \"SPR_BENCH Test Confusion Matrix\\nLeft: Ground Truth 0/1, Right: Predicted 0/1\"\n        )\n        plt.colorbar(im)\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        fpath = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fpath, dpi=150)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nds_name = \"SPR_BENCH\"\ndata = experiment_data.get(ds_name, {})\n\n\n# helper to get arrays safely\ndef get_arr(path, default=None):\n    cur = data\n    for key in path:\n        cur = cur.get(key, {})\n    return np.array(cur if isinstance(cur, (list, np.ndarray)) else default)\n\n\nepochs = np.arange(1, len(get_arr([\"losses\", \"train\"])) + 1)\n\n# 1) loss curve\ntry:\n    plt.figure()\n    plt.plot(epochs, get_arr([\"losses\", \"train\"]), label=\"Train\")\n    plt.plot(epochs, get_arr([\"losses\", \"val\"]), label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BCE Loss\")\n    plt.title(f\"{ds_name} Loss Curve\\nTrain vs. Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name.lower()}_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) MCC curve\ntry:\n    plt.figure()\n    plt.plot(epochs, get_arr([\"metrics\", \"train_mcc\"]), label=\"Train\")\n    plt.plot(epochs, get_arr([\"metrics\", \"val_mcc\"]), label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Matthews CorrCoef\")\n    plt.title(f\"{ds_name} MCC Curve\\nTrain vs. Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name.lower()}_mcc_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MCC curve: {e}\")\n    plt.close()\n\n# 3) Macro-F1 curve\ntry:\n    plt.figure()\n    plt.plot(epochs, get_arr([\"metrics\", \"train_f1\"]), label=\"Train\")\n    plt.plot(epochs, get_arr([\"metrics\", \"val_f1\"]), label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(f\"{ds_name} Macro-F1 Curve\\nTrain vs. Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name.lower()}_f1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 4) summary test bar plot (best run)\ntry:\n    test_info = data.get(\"test\", {})\n    metrics = [\"mcc\", \"macro_f1\"]\n    values = [test_info.get(m, np.nan) for m in metrics]\n    plt.figure()\n    plt.bar(metrics, values, color=[\"skyblue\", \"salmon\"])\n    plt.ylim(0, 1)\n    for i, v in enumerate(values):\n        plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n    plt.title(f\"{ds_name} Best Test Performance\")\n    fname = os.path.join(working_dir, f\"{ds_name.lower()}_test_summary.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(\n        f\"Best Test MCC: {test_info.get('mcc', 'n/a'):.4f}, \"\n        f\"Best Test macro-F1: {test_info.get('macro_f1', 'n/a'):.4f}\"\n    )\nexcept Exception as e:\n    print(f\"Error creating test summary plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, matthews_corrcoef\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------#\nfor dname, dct in experiment_data.items():\n    losses_tr = np.array(dct[\"losses\"][\"train\"])\n    losses_val = np.array(dct[\"losses\"][\"val\"])\n    f1_tr = np.array(dct[\"metrics\"][\"train\"])\n    f1_val = np.array(dct[\"metrics\"][\"val\"])\n\n    # --------------------- Loss curves ----------------------------#\n    try:\n        plt.figure()\n        plt.plot(losses_tr, label=\"Train\")\n        plt.plot(losses_val, label=\"Validation\")\n        plt.title(f\"{dname} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------------------- F1 curves -------------------------------#\n    try:\n        plt.figure()\n        plt.plot(f1_tr, label=\"Train\")\n        plt.plot(f1_val, label=\"Validation\")\n        plt.title(f\"{dname} Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ----------------- Test metrics bar chart ---------------------#\n    try:\n        preds = np.array(dct[\"predictions\"][0]).flatten()\n        gts = np.array(dct[\"ground_truth\"][0]).flatten()\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        test_mcc = matthews_corrcoef(gts, preds)\n\n        plt.figure()\n        plt.bar([\"Macro-F1\", \"MCC\"], [test_f1, test_mcc], color=[\"steelblue\", \"orange\"])\n        plt.ylim(0, 1)\n        plt.title(f\"{dname} Test Metrics\\nMacro-F1 vs MCC\")\n        for i, v in enumerate([test_f1, test_mcc]):\n            plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n        fname = os.path.join(working_dir, f\"{dname.lower()}_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"{dname}  |  Test Macro-F1: {test_f1:.4f}  |  Test MCC: {test_mcc:.4f}\")\n    except Exception as e:\n        print(f\"Error creating metrics bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load experiment data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    exp_dict = experiment_data[\"dropout_rate\"]\n    keys = sorted(exp_dict.keys(), key=lambda x: float(x.split(\"=\")[1]))\n    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n\n    # ---------- Figure 1: loss curves ----------\n    try:\n        plt.figure()\n        for i, k in enumerate(keys):\n            ls_tr = exp_dict[k][\"losses\"][\"train\"]\n            ls_val = exp_dict[k][\"losses\"][\"val\"]\n            epochs = np.arange(1, len(ls_tr) + 1)\n            plt.plot(\n                epochs,\n                ls_tr,\n                linestyle=\"--\",\n                color=colors[i % len(colors)],\n                label=f\"{k} train\",\n            )\n            plt.plot(\n                epochs,\n                ls_val,\n                linestyle=\"-\",\n                color=colors[i % len(colors)],\n                label=f\"{k} val\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 2: MCC curves ----------\n    try:\n        plt.figure()\n        for i, k in enumerate(keys):\n            mcc_tr = exp_dict[k][\"metrics\"][\"train\"]\n            mcc_val = exp_dict[k][\"metrics\"][\"val\"]\n            epochs = np.arange(1, len(mcc_tr) + 1)\n            plt.plot(\n                epochs,\n                mcc_tr,\n                linestyle=\"--\",\n                color=colors[i % len(colors)],\n                label=f\"{k} train\",\n            )\n            plt.plot(\n                epochs,\n                mcc_val,\n                linestyle=\"-\",\n                color=colors[i % len(colors)],\n                label=f\"{k} val\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Matthews Corrcoef\")\n        plt.title(\"SPR_BENCH: Training vs Validation MCC\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_MCC_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MCC curve plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 3: final val MCC bar ----------\n    try:\n        plt.figure()\n        final_val_mcc = [exp_dict[k][\"metrics\"][\"val\"][-1] for k in keys]\n        plt.bar(keys, final_val_mcc, color=colors[: len(keys)])\n        plt.ylabel(\"Final Validation MCC\")\n        plt.title(\"SPR_BENCH: Final Validation MCC per Dropout Rate\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_MCC_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final val MCC bar plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 4: confusion matrix for best model ----------\n    try:\n        # detect best key (has 'ground_truth')\n        best_key = [k for k in keys if \"ground_truth\" in exp_dict[k]][0]\n        y_true = exp_dict[best_key][\"ground_truth\"]\n        y_pred = exp_dict[best_key][\"predictions\"]\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fp = np.sum((y_true == 0) & (y_pred == 1))\n        fn = np.sum((y_true == 1) & (y_pred == 0))\n        tn = np.sum((y_true == 0) & (y_pred == 0))\n        cm = np.array([[tn, fp], [fn, tp]])\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n        plt.xticks([0, 1], [\"True 0\", \"True 1\"])\n        plt.yticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n        plt.title(f\"SPR_BENCH: Confusion Matrix (Best Model {best_key})\")\n        plt.colorbar()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import matthews_corrcoef, f1_score  # only used for bar chart\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data is not None:\n    # ---------- gather per-epoch loss & metric sequences ---------- #\n    train_losses = np.asarray(spr_data[\"losses\"][\"train\"])\n    val_losses = np.asarray(spr_data[\"losses\"][\"val\"])\n    train_mcc = np.asarray([m[\"mcc\"] for m in spr_data[\"metrics\"][\"train\"]])\n    val_mcc = np.asarray([m[\"mcc\"] for m in spr_data[\"metrics\"][\"val\"]])\n    train_f1 = np.asarray([m[\"macro_f1\"] for m in spr_data[\"metrics\"][\"train\"]])\n    val_f1 = np.asarray([m[\"macro_f1\"] for m in spr_data[\"metrics\"][\"val\"]])\n    epochs = np.arange(1, len(train_losses) + 1)\n\n    # ------------------- PLOT 1: loss curves ---------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, train_losses, label=\"Train\")\n        plt.plot(epochs, val_losses, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCEWithLogits Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # --------------- PLOT 2: MCC & macro-F1 curves --------------- #\n    try:\n        fig, ax1 = plt.subplots()\n        ax1.plot(epochs, train_mcc, \"b-\", label=\"Train MCC\")\n        ax1.plot(epochs, val_mcc, \"c-\", label=\"Val MCC\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"MCC\", color=\"b\")\n        ax1.tick_params(axis=\"y\", labelcolor=\"b\")\n        ax2 = ax1.twinx()\n        ax2.plot(epochs, train_f1, \"r--\", label=\"Train macro-F1\")\n        ax2.plot(epochs, val_f1, \"m--\", label=\"Val macro-F1\")\n        ax2.set_ylabel(\"macro-F1\", color=\"r\")\n        ax2.tick_params(axis=\"y\", labelcolor=\"r\")\n        lines, labels = [], []\n        for ax in [ax1, ax2]:\n            line, label = ax.get_legend_handles_labels()\n            lines += line\n            labels += label\n        plt.legend(lines, labels, loc=\"lower right\")\n        plt.title(\"SPR_BENCH Performance Curves\\nLeft: MCC, Right: macro-F1\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_MCC_F1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MCC/F1 curve: {e}\")\n        plt.close()\n\n    # ----------- PLOT 3: bar chart of test MCC per run ----------- #\n    try:\n        preds_list = spr_data[\"predictions\"]\n        labels_list = spr_data[\"ground_truth\"]\n        test_mccs = [matthews_corrcoef(l, p) for p, l in zip(preds_list, labels_list)]\n        run_tags = [f\"{b}ep\" for b in spr_data[\"epoch_budgets\"]]\n        plt.figure()\n        plt.bar(run_tags, test_mccs, color=\"skyblue\")\n        plt.ylim(0, 1)\n        for i, v in enumerate(test_mccs):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(\"SPR_BENCH Test MCC by Training Budget\\nBar heights = MCC scores\")\n        plt.ylabel(\"Test MCC\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_MCC_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(\n            \"Test MCC per run:\", dict(zip(run_tags, [round(x, 4) for x in test_mccs]))\n        )\n    except Exception as e:\n        print(f\"Error creating test MCC bar plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, matthews_corrcoef\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------#\nfor dname, dct in experiment_data.items():\n    losses_tr = np.array(dct[\"losses\"][\"train\"])\n    losses_val = np.array(dct[\"losses\"][\"val\"])\n    f1_tr = np.array(dct[\"metrics\"][\"train\"])\n    f1_val = np.array(dct[\"metrics\"][\"val\"])\n\n    # --------------------- Loss curves ----------------------------#\n    try:\n        plt.figure()\n        plt.plot(losses_tr, label=\"Train\")\n        plt.plot(losses_val, label=\"Validation\")\n        plt.title(f\"{dname} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------------------- F1 curves -------------------------------#\n    try:\n        plt.figure()\n        plt.plot(f1_tr, label=\"Train\")\n        plt.plot(f1_val, label=\"Validation\")\n        plt.title(f\"{dname} Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ----------------- Test metrics bar chart ---------------------#\n    try:\n        preds = np.array(dct[\"predictions\"][0]).flatten()\n        gts = np.array(dct[\"ground_truth\"][0]).flatten()\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        test_mcc = matthews_corrcoef(gts, preds)\n\n        plt.figure()\n        plt.bar([\"Macro-F1\", \"MCC\"], [test_f1, test_mcc], color=[\"steelblue\", \"orange\"])\n        plt.ylim(0, 1)\n        plt.title(f\"{dname} Test Metrics\\nMacro-F1 vs MCC\")\n        for i, v in enumerate([test_f1, test_mcc]):\n            plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n        fname = os.path.join(working_dir, f\"{dname.lower()}_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"{dname}  |  Test Macro-F1: {test_f1:.4f}  |  Test MCC: {test_mcc:.4f}\")\n    except Exception as e:\n        print(f\"Error creating metrics bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, matthews_corrcoef\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------#\nfor dname, dct in experiment_data.items():\n    losses_tr = np.array(dct[\"losses\"][\"train\"])\n    losses_val = np.array(dct[\"losses\"][\"val\"])\n    f1_tr = np.array(dct[\"metrics\"][\"train\"])\n    f1_val = np.array(dct[\"metrics\"][\"val\"])\n\n    # --------------------- Loss curves ----------------------------#\n    try:\n        plt.figure()\n        plt.plot(losses_tr, label=\"Train\")\n        plt.plot(losses_val, label=\"Validation\")\n        plt.title(f\"{dname} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------------------- F1 curves -------------------------------#\n    try:\n        plt.figure()\n        plt.plot(f1_tr, label=\"Train\")\n        plt.plot(f1_val, label=\"Validation\")\n        plt.title(f\"{dname} Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ----------------- Test metrics bar chart ---------------------#\n    try:\n        preds = np.array(dct[\"predictions\"][0]).flatten()\n        gts = np.array(dct[\"ground_truth\"][0]).flatten()\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        test_mcc = matthews_corrcoef(gts, preds)\n\n        plt.figure()\n        plt.bar([\"Macro-F1\", \"MCC\"], [test_f1, test_mcc], color=[\"steelblue\", \"orange\"])\n        plt.ylim(0, 1)\n        plt.title(f\"{dname} Test Metrics\\nMacro-F1 vs MCC\")\n        for i, v in enumerate([test_f1, test_mcc]):\n            plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n        fname = os.path.join(working_dir, f\"{dname.lower()}_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"{dname}  |  Test Macro-F1: {test_f1:.4f}  |  Test MCC: {test_mcc:.4f}\")\n    except Exception as e:\n        print(f\"Error creating metrics bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, matthews_corrcoef\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------#\nfor dname, dct in experiment_data.items():\n    losses_tr = np.array(dct[\"losses\"][\"train\"])\n    losses_val = np.array(dct[\"losses\"][\"val\"])\n    f1_tr = np.array(dct[\"metrics\"][\"train\"])\n    f1_val = np.array(dct[\"metrics\"][\"val\"])\n\n    # --------------------- Loss curves ----------------------------#\n    try:\n        plt.figure()\n        plt.plot(losses_tr, label=\"Train\")\n        plt.plot(losses_val, label=\"Validation\")\n        plt.title(f\"{dname} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------------------- F1 curves -------------------------------#\n    try:\n        plt.figure()\n        plt.plot(f1_tr, label=\"Train\")\n        plt.plot(f1_val, label=\"Validation\")\n        plt.title(f\"{dname} Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ----------------- Test metrics bar chart ---------------------#\n    try:\n        preds = np.array(dct[\"predictions\"][0]).flatten()\n        gts = np.array(dct[\"ground_truth\"][0]).flatten()\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        test_mcc = matthews_corrcoef(gts, preds)\n\n        plt.figure()\n        plt.bar([\"Macro-F1\", \"MCC\"], [test_f1, test_mcc], color=[\"steelblue\", \"orange\"])\n        plt.ylim(0, 1)\n        plt.title(f\"{dname} Test Metrics\\nMacro-F1 vs MCC\")\n        for i, v in enumerate([test_f1, test_mcc]):\n            plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n        fname = os.path.join(working_dir, f\"{dname.lower()}_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"{dname}  |  Test Macro-F1: {test_f1:.4f}  |  Test MCC: {test_mcc:.4f}\")\n    except Exception as e:\n        print(f\"Error creating metrics bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, matthews_corrcoef\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\n# Absolute paths are resolved via AI_SCIENTIST_ROOT\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_5248448e90484b89b3dfd35fb4810f71_proc_3331035/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_f0f0d90379624347a5edd10be9d6689d_proc_3331034/experiment_data.npy\",\n    \"experiments/2025-08-17_18-47-59_symblic_polyrule_reasoning_attempt_0/logs/0-run/experiment_results/experiment_a0774ba2f08842f08cbf2e4d38bb4683_proc_3331033/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n    for p in experiment_data_path_list:\n        full_p = os.path.join(root, p)\n        data = np.load(full_p, allow_pickle=True).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# ------------------------------------------------------------------#\n# Aggregate by dataset name\nagg = {}\nfor run_data in all_experiment_data:\n    for dname, dct in run_data.items():\n        entry = agg.setdefault(\n            dname,\n            {\n                \"losses_tr\": [],\n                \"losses_val\": [],\n                \"f1_tr\": [],\n                \"f1_val\": [],\n                \"test_f1\": [],\n                \"test_mcc\": [],\n            },\n        )\n        try:\n            entry[\"losses_tr\"].append(np.array(dct[\"losses\"][\"train\"]))\n            entry[\"losses_val\"].append(np.array(dct[\"losses\"][\"val\"]))\n            entry[\"f1_tr\"].append(np.array(dct[\"metrics\"][\"train\"]))\n            entry[\"f1_val\"].append(np.array(dct[\"metrics\"][\"val\"]))\n\n            preds = np.array(dct[\"predictions\"][0]).flatten()\n            gts = np.array(dct[\"ground_truth\"][0]).flatten()\n            entry[\"test_f1\"].append(f1_score(gts, preds, average=\"macro\"))\n            entry[\"test_mcc\"].append(matthews_corrcoef(gts, preds))\n        except Exception as e:\n            print(f\"Skipped run for {dname} due to missing keys: {e}\")\n\n# ------------------------------------------------------------------#\nfor dname, dct in agg.items():\n    n_runs = len(dct[\"losses_tr\"])\n    if n_runs == 0:\n        continue\n\n    # Align epoch lengths to the shortest run\n    min_len_loss = min(len(x) for x in dct[\"losses_tr\"])\n    min_len_f1 = min(len(x) for x in dct[\"f1_tr\"])\n    epochs_loss = np.arange(min_len_loss)\n    epochs_f1 = np.arange(min_len_f1)\n\n    # Stack & compute statistics\n    def mean_se(arr_list, trim_len):\n        arr = np.stack([a[:trim_len] for a in arr_list], axis=0)\n        mean = arr.mean(axis=0)\n        se = arr.std(axis=0, ddof=1) / np.sqrt(arr.shape[0])\n        return mean, se\n\n    loss_tr_mu, loss_tr_se = mean_se(dct[\"losses_tr\"], min_len_loss)\n    loss_val_mu, loss_val_se = mean_se(dct[\"losses_val\"], min_len_loss)\n    f1_tr_mu, f1_tr_se = mean_se(dct[\"f1_tr\"], min_len_f1)\n    f1_val_mu, f1_val_se = mean_se(dct[\"f1_val\"], min_len_f1)\n\n    # --------------------- Aggregated Loss curves ----------------------------#\n    try:\n        plt.figure()\n        plt.plot(epochs_loss, loss_tr_mu, label=\"Train Mean\", color=\"steelblue\")\n        plt.fill_between(\n            epochs_loss,\n            loss_tr_mu - loss_tr_se,\n            loss_tr_mu + loss_tr_se,\n            alpha=0.3,\n            color=\"steelblue\",\n            label=\"Train \u00b1 SE\",\n        )\n        plt.plot(epochs_loss, loss_val_mu, label=\"Val Mean\", color=\"orange\")\n        plt.fill_between(\n            epochs_loss,\n            loss_val_mu - loss_val_se,\n            loss_val_mu + loss_val_se,\n            alpha=0.3,\n            color=\"orange\",\n            label=\"Val \u00b1 SE\",\n        )\n        plt.title(f\"{dname} Aggregated Loss Curves\\nMean \u00b1 SE over {n_runs} runs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BCE Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_aggregated_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dname}: {e}\")\n        plt.close()\n\n    # -------------------- Aggregated F1 curves -------------------------------#\n    try:\n        plt.figure()\n        plt.plot(epochs_f1, f1_tr_mu, label=\"Train Mean\", color=\"steelblue\")\n        plt.fill_between(\n            epochs_f1,\n            f1_tr_mu - f1_tr_se,\n            f1_tr_mu + f1_tr_se,\n            alpha=0.3,\n            color=\"steelblue\",\n            label=\"Train \u00b1 SE\",\n        )\n        plt.plot(epochs_f1, f1_val_mu, label=\"Val Mean\", color=\"orange\")\n        plt.fill_between(\n            epochs_f1,\n            f1_val_mu - f1_val_se,\n            f1_val_mu + f1_val_se,\n            alpha=0.3,\n            color=\"orange\",\n            label=\"Val \u00b1 SE\",\n        )\n        plt.title(f\"{dname} Aggregated Macro-F1 Curves\\nMean \u00b1 SE over {n_runs} runs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname.lower()}_aggregated_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated F1 plot for {dname}: {e}\")\n        plt.close()\n\n    # ----------------- Aggregated Test metrics bar chart ---------------------#\n    try:\n        test_f1_arr = np.array(dct[\"test_f1\"])\n        test_mcc_arr = np.array(dct[\"test_mcc\"])\n        metrics_mean = [test_f1_arr.mean(), test_mcc_arr.mean()]\n        metrics_se = [\n            test_f1_arr.std(ddof=1) / np.sqrt(n_runs),\n            test_mcc_arr.std(ddof=1) / np.sqrt(n_runs),\n        ]\n\n        plt.figure()\n        bars = plt.bar(\n            [\"Macro-F1\", \"MCC\"],\n            metrics_mean,\n            yerr=metrics_se,\n            capsize=5,\n            color=[\"steelblue\", \"orange\"],\n        )\n        plt.ylim(0, 1)\n        plt.title(f\"{dname} Test Metrics (Mean \u00b1 SE)\\nAggregated over {n_runs} runs\")\n        for i, v in enumerate(metrics_mean):\n            plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n        fname = os.path.join(\n            working_dir, f\"{dname.lower()}_aggregated_test_metrics.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n\n        print(\n            f\"{dname} | Test Macro-F1: {metrics_mean[0]:.4f}\u00b1{metrics_se[0]:.4f} | \"\n            f\"Test MCC: {metrics_mean[1]:.4f}\u00b1{metrics_se[1]:.4f}\"\n        )\n    except Exception as e:\n        print(f\"Error creating aggregated test metrics for {dname}: {e}\")\n        plt.close()\n"], "plot_plan": ["We will load the saved experiment_data.npy, extract loss and MCC records, and compute the final test MCC from stored predictions and ground-truth labels. Three plots will be produced: (1) training vs. validation loss across epochs, (2) training vs. validation MCC across epochs, and (3) a 2\u00d72 bar confusion-matrix style plot for the test set. Each plot is wrapped in an independent try-except block to avoid cascading failures; after saving, figures are explicitly closed. Filenames contain the dataset name (\u201cSPR_BENCH\u201d) and plot description, and all outputs are written to the pre-created \u2018working\u2019 directory. We handle the case where experiment_data.npy cannot be found, printing an informative message. Finally, the script prints the final validation MCC and computed test MCC so users can quickly see key metrics without opening plots.", null, "Load the saved experiment_data.npy, then iterate over each learning\u2010rate sub-dict. For every LR, retrieve train/val losses and MCC metrics, create a two-panel figure (left: losses, right: MCC) with epoch on the x-axis, add titles and legends, and save the plot to working_dir using a descriptive name containing \u201cSPR_BENCH\u201d, the LR key and \u201ccurves\u201d. Afterward, compute the final test MCC for each LR using the stored predictions/ground_truth (via the MCC formula implemented in pure NumPy) and collect them in a list. Create one additional bar-chart figure summarizing these test MCC values across LRs, give it an informative title, and save it similarly. Wrap each figure-creation block in try-except, always closing figures in finally/except. The script starts with the required imports and working_dir definition, and prints the computed test MCCs so the user can immediately see evaluation results.", "We will first load experiment_data.npy from the working directory and safely extract the dictionary with allow_pickle=True. From it, we gather every batch-size experiment under the SPR_BENCH dataset, retrieving per-epoch arrays for training/validation loss and MCC plus the final test MCC values. We will prepare three plots (\u22645 total): 1) a multi-line figure showing loss curves (train vs. val) for each batch size, 2) a similar multi-line plot for MCC curves, and 3) a bar chart summarising the final test MCC for every batch size. Each plot lives inside its own try-except block to satisfy robustness requirements; on any error we print the exception and still close the figure. All figures are titled descriptively, saved to working_dir with clear filenames, and then closed. Finally, after plotting, we print the test MCC table to stdout so that quantitative evaluation is visible in logs. Only data found in experiment_data is used\u2014nothing is simulated\u2014and we cap ourselves at three figures, well below the five-figure limit.", "We will load experiment_data.npy from the working directory, pull out the logged training/validation losses and MCCs for every weight-decay setting, and visualize four aspects: (1) loss curves, (2) MCC curves, (3) bar chart of the final validation MCC for each weight-decay, and (4) a confusion-matrix heat-map on the test split for the best model. Each figure is created in its own try-except block, saved to working_dir with descriptive filenames, and the figure is closed afterwards. We limit ourselves to these four plots to stay under the five-figure ceiling. All computations rely strictly on the arrays stored inside experiment_data.npy, and any missing data is handled gracefully inside the try-except wrappers. The script prints the overall best weight-decay and test MCC so the user can verify the plotted values.", "The solution loads experiment_data.npy, extracts metrics for the single dataset (SPR_BENCH), and plots standard curves\u2014training vs. validation loss, MCC, and macro-F1\u2014each in its own try-except block. Epoch indices are generated from 1 to the length of the stored arrays, covering all epochs across the three hyper-parameter runs. Every figure has clear titles and legends, is saved with descriptive filenames inside working_dir, and the figure is always closed to free memory. A final bar plot summarizes the best test MCC and macro-F1 achieved during the sweep, giving a quick snapshot of generalization performance. Only data present in the file are used; nothing is fabricated or simulated. The script finishes by printing those best test metrics to stdout for easy inspection.", "We will load the serialized experiment_data.npy that the training script already produced and unwrap its dict structure.  \nFrom the dict we extract (i) training/validation loss history, (ii) training/validation macro-F1 history, and (iii) the first stored test-set predictions and ground-truth labels.  \nWe will generate at most three plots: 1) loss curves, 2) macro-F1 curves, and 3) a bar chart comparing overall test F1 and MCC.  \nEach figure will be wrapped in its own try-except so that any failure does not prevent the rest of the plotting routine; every figure is closed after saving.  \nTitles will include the dataset name (SPR_BENCH) and clear subtitles such as \u201cLeft: Training, Right: Validation\u201d.  \nThe script computes the final macro-F1 and MCC for the stored predictions using sklearn and prints them to stdout.  \nAll plots are saved under working_dir with descriptive filenames that embed the dataset name and the plot type.  \nNo synthetic data are produced\u2014everything comes directly from experiment_data.npy.  \nThe code is concise, uses only matplotlib and numpy (plus sklearn for the metrics), and obeys the folder creation and closing requirements.", "We will load experiment_data.npy from the working directory, extract the per-epoch losses and MCC scores stored for each dropout setting, and plot four concise figures (\u22645 as required). Figure 1 overlays training and validation loss curves for all dropout rates, using solid lines for validation and dashed lines for training. Figure 2 overlays MCC curves in the same manner. Figure 3 is a bar chart comparing the final validation MCC at the last epoch for each dropout probability. Figure 4 shows a 2\u00d72 confusion-matrix heat-map for the best model on the test set (ground truth vs. predictions). Each figure is wrapped in a try-except block, saved with descriptive names inside working_dir, given clear titles and subtitles that mention the SPR_BENCH dataset, and the figure is closed afterward. No artificial data are created, and we respect the \u201c\u22645 figures\u201d guideline.", "We first load experiment_data.npy from the working directory and extract the \u2018SPR_BENCH\u2019 entry. From it, we collect the train/val loss lists and the corresponding MCC and macro-F1 values that were recorded each epoch across all three training runs. We then generate three separate plots (each wrapped in its own try-except): (1) a line plot of train vs validation loss over epochs, (2) a dual-line plot of train vs validation MCC (with an auxiliary y-axis for macro-F1 to keep the figure readable), and (3) a bar chart that compares the final test MCC obtained for each of the three epoch-budget runs (computed from the saved predictions and ground-truth arrays). All plots are saved into the existing working_dir with descriptive names that include the dataset and plot type, and every figure is properly closed afterward. We make sure to title each figure and add clear subtitles indicating what is shown. The code follows the required imports order, uses only data that actually exists, and limits the total number of figures to three (well below the five-figure cap). Finally, we print the computed test MCC values so the user can view the key evaluation metric directly in stdout.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "epochs", "learning_rate", "batch_size", "weight_decay", null, null, "dropout_rate", null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script loads the saved NumPy dictionary, unpacks the nested metrics for\nevery dataset key (here only \u201cSPR_BENCH\u201d), and extracts the best values\u2014minimum\nloss and maximum Matthews correlation coefficient (MCC)\u2014across epochs. It also\nrecomputes the test-set MCC from the stored predictions and ground-truth labels.\nFinally, it prints the dataset name followed by clearly labelled metrics so that\neach value is easy to interpret.", "", "The code will load experiment_data.npy from the working directory, iterate over\nevery learning-rate sub-experiment, and compute the key summary statistics.\nFor each learning-rate \u201cdataset\u201d it prints the final training and validation\nlosses, the best training and validation MCC scores observed during training,\nand the test MCC recomputed from the saved predictions and ground-truth labels.\nAll code is kept at the global scope so it executes immediately when the script\nis run.", "We will load the serialized dictionary, traverse its nested structure\n(batch_size \u2192 dataset \u2192 setting), collect all per-epoch losses and Matthew\u2019s\ncorrelation coefficients, and finally pick the optimal value for each metric\n(highest MCC, lowest loss, and highest held-out test MCC). Results are printed\nwith explicit, self-explanatory metric names for every dataset encountered.", "Below is a concise plan followed by executable code.   The script locates the\nworking directory, loads experiment_data.npy, identifies the best weight-decay\nrun, then prints the final Matthews correlation coefficient (MCC) and loss for\nthe training and validation splits, along with the MCC on the test split. Each\ndataset name and metric name is printed explicitly, complying with the\nformatting requirements.", "The script will locate the numpy file in the working directory, load it, and\nconvert it back to the original Python dictionary. For every dataset stored\ninside (here it is \u201cSPR_BENCH\u201d), it will compute the best value for each list-\ntype metric (minimum for losses and maximum for all other metrics). It then\nprints the dataset name followed by clearly labelled, human-readable metric\nsummaries, including the test-set results that are already stored as scalars in\nthe file.", "The script will load the saved NumPy file from the \u201cworking\u201d directory, extract\nthe dictionary it contains, and focus on the single dataset key (\u201cSPR_BENCH\u201d).\nFor every metric list recorded during training (losses and F1 scores for both\ntraining and validation) the script will report the best (minimum loss or\nmaximum F1) value observed.   For the test set, the script will recompute\nMacro-F1 and MCC from the stored \u201cpredictions\u201d and \u201cground_truth\u201d arrays for\neach run and then report the best values achieved.   All values are printed with\nclear, fully-qualified names so there is no ambiguity about which split or\nmetric they refer to.", "The script will load the saved numpy dictionary from the working directory,\niterate over every dropout-rate experiment, and for each dataset (training,\nvalidation, and test when present) print only the final value of each recorded\nmetric. It extracts the last element of the stored metric/loss lists for\ntraining and validation, and directly reads the single test MCC that was stored\nfor the best model. The output is human-readable and clearly labels both the\ndataset and the specific metric being reported.", "We will load the saved NumPy dictionary, navigate its nested structure, and\ncompute/locate the relevant \u201cbest\u201d or \u201cfinal\u201d values.   For training we simply\nread the last recorded entry (final epoch of the last run).   For validation we\npick the epoch with the highest validation MCC (best model selection criterion)\nand report its metrics and loss.   For test we recompute MCC & macro-F1 from the\nprediction/ground-truth arrays of the final run.   Finally, we print every\nmetric with an explicit, self-describing label, preceded by the dataset name.", "The script will load the saved NumPy file from the \u201cworking\u201d directory, extract\nthe dictionary it contains, and focus on the single dataset key (\u201cSPR_BENCH\u201d).\nFor every metric list recorded during training (losses and F1 scores for both\ntraining and validation) the script will report the best (minimum loss or\nmaximum F1) value observed.   For the test set, the script will recompute\nMacro-F1 and MCC from the stored \u201cpredictions\u201d and \u201cground_truth\u201d arrays for\neach run and then report the best values achieved.   All values are printed with\nclear, fully-qualified names so there is no ambiguity about which split or\nmetric they refer to.", "The script will load the saved NumPy file from the \u201cworking\u201d directory, extract\nthe dictionary it contains, and focus on the single dataset key (\u201cSPR_BENCH\u201d).\nFor every metric list recorded during training (losses and F1 scores for both\ntraining and validation) the script will report the best (minimum loss or\nmaximum F1) value observed.   For the test set, the script will recompute\nMacro-F1 and MCC from the stored \u201cpredictions\u201d and \u201cground_truth\u201d arrays for\neach run and then report the best values achieved.   All values are printed with\nclear, fully-qualified names so there is no ambiguity about which split or\nmetric they refer to.", "The script will load the saved NumPy file from the \u201cworking\u201d directory, extract\nthe dictionary it contains, and focus on the single dataset key (\u201cSPR_BENCH\u201d).\nFor every metric list recorded during training (losses and F1 scores for both\ntraining and validation) the script will report the best (minimum loss or\nmaximum F1) value observed.   For the test set, the script will recompute\nMacro-F1 and MCC from the stored \u201cpredictions\u201d and \u201cground_truth\u201d arrays for\neach run and then report the best values achieved.   All values are printed with\nclear, fully-qualified names so there is no ambiguity about which split or\nmetric they refer to.", ""], "parse_metrics_code": ["import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate through datasets and report best/final metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}\")\n\n    # Losses\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n    best_train_loss = min(train_losses) if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n\n    # MCC metrics\n    train_mccs = data[\"metrics\"][\"train\"]\n    val_mccs = data[\"metrics\"][\"val\"]\n    best_train_mcc = max(train_mccs) if train_mccs else None\n    best_val_mcc = max(val_mccs) if val_mccs else None\n\n    # Test MCC (re-compute from stored predictions and labels)\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    test_mcc = matthews_corrcoef(gts, preds) if len(preds) and len(gts) else None\n\n    # Print metrics with explicit names\n    if best_train_loss is not None:\n        print(f\"  training loss: {best_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"  validation loss: {best_val_loss:.6f}\")\n    if best_train_mcc is not None:\n        print(f\"  training Matthews correlation coefficient: {best_train_mcc:.6f}\")\n    if best_val_mcc is not None:\n        print(f\"  validation Matthews correlation coefficient: {best_val_mcc:.6f}\")\n    if test_mcc is not None:\n        print(f\"  test Matthews correlation coefficient: {test_mcc:.6f}\")\n", "", "import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef\n\n# ------------------------------------------------------------------\n# 0. Locate and load the stored experiment dictionary --------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Iterate over learning-rate experiments and report metrics -----\nfor lr_key, lr_data in experiment_data[\"learning_rate\"].items():\n    print(f\"=== Dataset: Learning rate {lr_key} ===\")\n    # Losses: take the final value recorded\n    final_train_loss = lr_data[\"losses\"][\"train\"][-1]\n    final_val_loss = lr_data[\"losses\"][\"val\"][-1]\n\n    # MCCs: best value achieved during the sweep\n    best_train_mcc = max(lr_data[\"metrics\"][\"train\"])\n    best_val_mcc = max(lr_data[\"metrics\"][\"val\"])\n\n    # Test MCC: recompute from stored predictions/labels\n    test_preds = lr_data[\"predictions\"]\n    test_labels = lr_data[\"ground_truth\"]\n    test_mcc = matthews_corrcoef(test_labels, test_preds)\n\n    # Print with explicit metric names\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"training MCC: {best_train_mcc:.4f}\")\n    print(f\"validation MCC: {best_val_mcc:.4f}\")\n    print(f\"testing MCC: {test_mcc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to update best values\ndef update_best(current_value, best_value, compare_fn):\n    if best_value is None:\n        return current_value\n    return compare_fn(current_value, best_value)\n\n\n# ------------------------------------------------------------------\n# Traverse the data and collect best metrics\nfor dataset_name, settings in experiment_data[\"batch_size\"].items():\n    best_train_mcc = None\n    best_validation_mcc = None\n    lowest_training_loss = None\n    lowest_validation_loss = None\n    best_test_mcc = None\n\n    # iterate over the batch-size sub-experiments\n    for setting in settings.values():\n        # per-epoch arrays\n        train_mcc_list = setting[\"metrics\"][\"train\"]\n        val_mcc_list = setting[\"metrics\"][\"val\"]\n        train_loss_list = setting[\"losses\"][\"train\"]\n        val_loss_list = setting[\"losses\"][\"val\"]\n\n        # update best/lowest over all epochs and settings\n        for mcc in train_mcc_list:\n            best_train_mcc = update_best(mcc, best_train_mcc, max)\n        for mcc in val_mcc_list:\n            best_validation_mcc = update_best(mcc, best_validation_mcc, max)\n        for loss_val in train_loss_list:\n            lowest_training_loss = update_best(loss_val, lowest_training_loss, min)\n        for loss_val in val_loss_list:\n            lowest_validation_loss = update_best(loss_val, lowest_validation_loss, min)\n\n        # test MCC is a single value per setting\n        best_test_mcc = update_best(setting[\"test_mcc\"], best_test_mcc, max)\n\n    # ------------------------------------------------------------------\n    # Print results for this dataset\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"Best training Matthew's correlation coefficient: {best_train_mcc:.4f}\")\n    print(\n        f\"Best validation Matthew's correlation coefficient: {best_validation_mcc:.4f}\"\n    )\n    print(f\"Lowest training loss: {lowest_training_loss:.4f}\")\n    print(f\"Lowest validation loss: {lowest_validation_loss:.4f}\")\n    print(f\"Best test Matthew's correlation coefficient: {best_test_mcc:.4f}\")\n", "import os\nimport numpy as np\n\n# --------------------------- load data ------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# --------------------------- extract best run -----------------------------\nwd_section = experiment_data[\"weight_decay_tuning\"]\nbest_wd = wd_section[\"best_wd\"]  # float\nbest_run_key = str(best_wd)  # dict key is stringified\nbest_run = wd_section[best_run_key]\n\n# final epoch statistics for the best run\nfinal_train_mcc = best_run[\"metrics\"][\"train\"][-1]\nfinal_val_mcc = best_run[\"metrics\"][\"val\"][-1]\nfinal_train_loss = best_run[\"losses\"][\"train\"][-1]\nfinal_val_loss = best_run[\"losses\"][\"val\"][-1]\n\n# test statistics\ntest_mcc = wd_section[\"test_mcc\"]\n\n# --------------------------- reporting ------------------------------------\nprint(\"TRAIN DATASET\")\nprint(f\"training Matthews correlation coefficient: {final_train_mcc:.4f}\")\nprint(f\"training loss: {final_train_loss:.4f}\\n\")\n\nprint(\"VALIDATION DATASET\")\nprint(f\"validation Matthews correlation coefficient: {final_val_mcc:.4f}\")\nprint(f\"validation loss: {final_val_loss:.4f}\\n\")\n\nprint(\"TEST DATASET\")\nprint(f\"test Matthews correlation coefficient: {test_mcc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------#\n# Locate and load the saved experiment data\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------#\n# Helper functions to pick best values\n# ------------------------------------------------------------------#\ndef best_loss(loss_list):\n    \"\"\"Lower is better for loss.\"\"\"\n    return min(loss_list) if loss_list else None\n\n\ndef best_metric(metric_list):\n    \"\"\"Higher is better for MCC / F1, etc.\"\"\"\n    return max(metric_list) if metric_list else None\n\n\n# ------------------------------------------------------------------#\n# Iterate through datasets and print summaries\n# ------------------------------------------------------------------#\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Training / validation losses\n    train_losses = data[\"losses\"].get(\"train\", [])\n    val_losses = data[\"losses\"].get(\"val\", [])\n    print(f\"Best training loss: {best_loss(train_losses):.4f}\")\n    print(f\"Best validation loss: {best_loss(val_losses):.4f}\")\n\n    # Training / validation MCC\n    train_mccs = data[\"metrics\"].get(\"train_mcc\", [])\n    val_mccs = data[\"metrics\"].get(\"val_mcc\", [])\n    print(\n        f\"Best training Matthews correlation coefficient (MCC): {best_metric(train_mccs):.4f}\"\n    )\n    print(\n        f\"Best validation Matthews correlation coefficient (MCC): {best_metric(val_mccs):.4f}\"\n    )\n\n    # Training / validation macro-F1\n    train_f1s = data[\"metrics\"].get(\"train_f1\", [])\n    val_f1s = data[\"metrics\"].get(\"val_f1\", [])\n    print(f\"Best training macro F1 score: {best_metric(train_f1s):.4f}\")\n    print(f\"Best validation macro F1 score: {best_metric(val_f1s):.4f}\")\n\n    # Test results\n    test_results = data.get(\"test\", {})\n    if test_results:\n        print(\n            f\"Test Matthews correlation coefficient (MCC): {test_results.get('mcc', float('nan')):.4f}\"\n        )\n        print(f\"Test macro F1 score: {test_results.get('macro_f1', float('nan')):.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n# ------------------------------------------------------------------#\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------#\ndef print_metrics_for_dataset(name, data_dict):\n    \"\"\"Print the best/final metrics for one dataset entry.\"\"\"\n    print(f\"Dataset: {name}\")\n\n    # Training / validation losses and F1 scores\n    train_losses = data_dict[\"losses\"][\"train\"]\n    val_losses = data_dict[\"losses\"][\"val\"]\n    train_f1s = data_dict[\"metrics\"][\"train\"]\n    val_f1s = data_dict[\"metrics\"][\"val\"]\n\n    # Best (min for loss, max for F1)\n    best_train_loss = min(train_losses) if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n\n    if best_train_loss is not None:\n        print(f\"best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n    if best_train_f1 is not None:\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ------------------------------------------------------------------#\n    # Evaluate stored test predictions\n    preds_runs = data_dict.get(\"predictions\", [])\n    labels_runs = data_dict.get(\"ground_truth\", [])\n\n    best_test_f1 = None\n    best_test_mcc = None\n    for preds, labels in zip(preds_runs, labels_runs):\n        cur_f1 = f1_score(labels, preds, average=\"macro\")\n        cur_mcc = matthews_corrcoef(labels, preds)\n\n        if best_test_f1 is None or cur_f1 > best_test_f1:\n            best_test_f1 = cur_f1\n            best_test_mcc = cur_mcc\n\n    if best_test_f1 is not None:\n        print(f\"best test macro F1 score: {best_test_f1:.4f}\")\n    if best_test_mcc is not None:\n        print(f\"best test Matthews correlation coefficient: {best_test_mcc:.4f}\")\n\n\n# ------------------------------------------------------------------#\n# Iterate over each dataset contained in the experiment data\nfor dataset_name, dataset_dict in experiment_data.items():\n    print_metrics_for_dataset(dataset_name, dataset_dict)\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the saved experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper to print metrics neatly\n# ------------------------------------------------------------------\ndef print_dataset_metrics(dataset_name, metrics_dict):\n    for metric_name, metric_value in metrics_dict.items():\n        # ensure numeric formatting where appropriate\n        if isinstance(metric_value, (float, int)):\n            print(f\"{dataset_name} {metric_name}: {metric_value:.4f}\")\n        else:\n            print(f\"{dataset_name} {metric_name}: {metric_value}\")\n\n\n# ------------------------------------------------------------------\n# iterate through each dropout\u2010rate experiment\n# ------------------------------------------------------------------\nfor exp_name, exp_data in experiment_data.get(\"dropout_rate\", {}).items():\n    print(f\"\\n=== Dropout configuration: {exp_name} ===\")\n\n    # Training set metrics\n    if exp_data[\"metrics\"][\"train\"]:\n        train_metrics = {\n            \"MCC\": exp_data[\"metrics\"][\"train\"][-1],\n            \"loss\": exp_data[\"losses\"][\"train\"][-1],\n        }\n        print_dataset_metrics(\"Training\", train_metrics)\n\n    # Validation set metrics\n    if exp_data[\"metrics\"][\"val\"]:\n        val_metrics = {\n            \"MCC\": exp_data[\"metrics\"][\"val\"][-1],\n            \"loss\": exp_data[\"losses\"][\"val\"][-1],\n        }\n        print_dataset_metrics(\"Validation\", val_metrics)\n\n    # Test set metrics (only available for the best run)\n    if \"test_MCC\" in exp_data:\n        test_metrics = {\"MCC\": exp_data[\"test_MCC\"]}\n        print_dataset_metrics(\"Test\", test_metrics)\n", "import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n# ----------------- Paths & Loading --------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- Helper to pretty-print one dataset -------------#\ndef report_dataset(name: str, dct: dict):\n    print(name)\n\n    # ----- TRAIN (take final entry recorded) -----------------------#\n    train_metrics_all = dct[\"metrics\"][\"train\"]\n    train_losses_all = dct[\"losses\"][\"train\"]\n    train_final = train_metrics_all[-1]\n    train_loss_final = train_losses_all[-1]\n    print(f\"  train loss: {train_loss_final:.4f}\")\n    print(f\"  train MCC: {train_final['mcc']:.4f}\")\n    print(f\"  train macro F1 score: {train_final['macro_f1']:.4f}\")\n\n    # ----- VALIDATION (pick best MCC) ------------------------------#\n    val_metrics_all = dct[\"metrics\"][\"val\"]\n    val_losses_all = dct[\"losses\"][\"val\"]\n    # index with highest MCC\n    best_idx = int(np.argmax([m[\"mcc\"] for m in val_metrics_all]))\n    best_val = val_metrics_all[best_idx]\n    best_val_loss = val_losses_all[best_idx]\n    print(f\"  validation loss (best MCC): {best_val_loss:.4f}\")\n    print(f\"  validation MCC (best): {best_val['mcc']:.4f}\")\n    print(f\"  validation macro F1 score (best): {best_val['macro_f1']:.4f}\")\n\n    # ----- TEST (use results from final run) -----------------------#\n    preds_list = dct[\"predictions\"]\n    gts_list = dct[\"ground_truth\"]\n    if preds_list and gts_list:\n        y_pred = preds_list[-1].astype(int)\n        y_true = gts_list[-1].astype(int)\n        test_mcc = matthews_corrcoef(y_true, y_pred)\n        test_f1 = f1_score(y_true, y_pred, average=\"macro\")\n        print(f\"  test MCC: {test_mcc:.4f}\")\n        print(f\"  test macro F1 score: {test_f1:.4f}\")\n\n\n# ----------------- Iterate over datasets --------------------------#\nfor dataset_name, dataset_results in experiment_data.items():\n    report_dataset(dataset_name, dataset_results)\n", "import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n# ------------------------------------------------------------------#\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------#\ndef print_metrics_for_dataset(name, data_dict):\n    \"\"\"Print the best/final metrics for one dataset entry.\"\"\"\n    print(f\"Dataset: {name}\")\n\n    # Training / validation losses and F1 scores\n    train_losses = data_dict[\"losses\"][\"train\"]\n    val_losses = data_dict[\"losses\"][\"val\"]\n    train_f1s = data_dict[\"metrics\"][\"train\"]\n    val_f1s = data_dict[\"metrics\"][\"val\"]\n\n    # Best (min for loss, max for F1)\n    best_train_loss = min(train_losses) if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n\n    if best_train_loss is not None:\n        print(f\"best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n    if best_train_f1 is not None:\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ------------------------------------------------------------------#\n    # Evaluate stored test predictions\n    preds_runs = data_dict.get(\"predictions\", [])\n    labels_runs = data_dict.get(\"ground_truth\", [])\n\n    best_test_f1 = None\n    best_test_mcc = None\n    for preds, labels in zip(preds_runs, labels_runs):\n        cur_f1 = f1_score(labels, preds, average=\"macro\")\n        cur_mcc = matthews_corrcoef(labels, preds)\n\n        if best_test_f1 is None or cur_f1 > best_test_f1:\n            best_test_f1 = cur_f1\n            best_test_mcc = cur_mcc\n\n    if best_test_f1 is not None:\n        print(f\"best test macro F1 score: {best_test_f1:.4f}\")\n    if best_test_mcc is not None:\n        print(f\"best test Matthews correlation coefficient: {best_test_mcc:.4f}\")\n\n\n# ------------------------------------------------------------------#\n# Iterate over each dataset contained in the experiment data\nfor dataset_name, dataset_dict in experiment_data.items():\n    print_metrics_for_dataset(dataset_name, dataset_dict)\n", "import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n# ------------------------------------------------------------------#\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------#\ndef print_metrics_for_dataset(name, data_dict):\n    \"\"\"Print the best/final metrics for one dataset entry.\"\"\"\n    print(f\"Dataset: {name}\")\n\n    # Training / validation losses and F1 scores\n    train_losses = data_dict[\"losses\"][\"train\"]\n    val_losses = data_dict[\"losses\"][\"val\"]\n    train_f1s = data_dict[\"metrics\"][\"train\"]\n    val_f1s = data_dict[\"metrics\"][\"val\"]\n\n    # Best (min for loss, max for F1)\n    best_train_loss = min(train_losses) if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n\n    if best_train_loss is not None:\n        print(f\"best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n    if best_train_f1 is not None:\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ------------------------------------------------------------------#\n    # Evaluate stored test predictions\n    preds_runs = data_dict.get(\"predictions\", [])\n    labels_runs = data_dict.get(\"ground_truth\", [])\n\n    best_test_f1 = None\n    best_test_mcc = None\n    for preds, labels in zip(preds_runs, labels_runs):\n        cur_f1 = f1_score(labels, preds, average=\"macro\")\n        cur_mcc = matthews_corrcoef(labels, preds)\n\n        if best_test_f1 is None or cur_f1 > best_test_f1:\n            best_test_f1 = cur_f1\n            best_test_mcc = cur_mcc\n\n    if best_test_f1 is not None:\n        print(f\"best test macro F1 score: {best_test_f1:.4f}\")\n    if best_test_mcc is not None:\n        print(f\"best test Matthews correlation coefficient: {best_test_mcc:.4f}\")\n\n\n# ------------------------------------------------------------------#\n# Iterate over each dataset contained in the experiment data\nfor dataset_name, dataset_dict in experiment_data.items():\n    print_metrics_for_dataset(dataset_name, dataset_dict)\n", "import os\nimport numpy as np\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n# ------------------------------------------------------------------#\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------#\ndef print_metrics_for_dataset(name, data_dict):\n    \"\"\"Print the best/final metrics for one dataset entry.\"\"\"\n    print(f\"Dataset: {name}\")\n\n    # Training / validation losses and F1 scores\n    train_losses = data_dict[\"losses\"][\"train\"]\n    val_losses = data_dict[\"losses\"][\"val\"]\n    train_f1s = data_dict[\"metrics\"][\"train\"]\n    val_f1s = data_dict[\"metrics\"][\"val\"]\n\n    # Best (min for loss, max for F1)\n    best_train_loss = min(train_losses) if train_losses else None\n    best_val_loss = min(val_losses) if val_losses else None\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n\n    if best_train_loss is not None:\n        print(f\"best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n    if best_train_f1 is not None:\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ------------------------------------------------------------------#\n    # Evaluate stored test predictions\n    preds_runs = data_dict.get(\"predictions\", [])\n    labels_runs = data_dict.get(\"ground_truth\", [])\n\n    best_test_f1 = None\n    best_test_mcc = None\n    for preds, labels in zip(preds_runs, labels_runs):\n        cur_f1 = f1_score(labels, preds, average=\"macro\")\n        cur_mcc = matthews_corrcoef(labels, preds)\n\n        if best_test_f1 is None or cur_f1 > best_test_f1:\n            best_test_f1 = cur_f1\n            best_test_mcc = cur_mcc\n\n    if best_test_f1 is not None:\n        print(f\"best test macro F1 score: {best_test_f1:.4f}\")\n    if best_test_mcc is not None:\n        print(f\"best test Matthews correlation coefficient: {best_test_mcc:.4f}\")\n\n\n# ------------------------------------------------------------------#\n# Iterate over each dataset contained in the experiment data\nfor dataset_name, dataset_dict in experiment_data.items():\n    print_metrics_for_dataset(dataset_name, dataset_dict)\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', '  training loss: 0.617833', '\\n', '  validation loss:\n0.636186', '\\n', '  training Matthews correlation coefficient: 0.369434', '\\n',\n'  validation Matthews correlation coefficient: 0.364823', '\\n', '  test\nMatthews correlation coefficient: 0.379180', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "", "['=== Dataset: Learning rate lr_5e-04 ===', '\\n', 'training loss: 0.6223', '\\n',\n'validation loss: 0.6356', '\\n', 'training MCC: 0.3641', '\\n', 'validation MCC:\n0.3733', '\\n', 'testing MCC: 0.3715\\n', '\\n', '=== Dataset: Learning rate\nlr_1e-03 ===', '\\n', 'training loss: 0.6177', '\\n', 'validation loss: 0.6383',\n'\\n', 'training MCC: 0.3716', '\\n', 'validation MCC: 0.3733', '\\n', 'testing\nMCC: 0.3751\\n', '\\n', '=== Dataset: Learning rate lr_2e-03 ===', '\\n', 'training\nloss: 0.6150', '\\n', 'validation loss: 0.6388', '\\n', 'training MCC: 0.3729',\n'\\n', 'validation MCC: 0.3646', '\\n', 'testing MCC: 0.3712\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', \"Best training Matthew's correlation coefficient:\n0.3716\", '\\n', \"Best validation Matthew's correlation coefficient: 0.3733\",\n'\\n', 'Lowest training loss: 0.6177', '\\n', 'Lowest validation loss: 0.6343',\n'\\n', \"Best test Matthew's correlation coefficient: 0.3772\", '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['TRAIN DATASET', '\\n', 'training Matthews correlation coefficient: 0.3681',\n'\\n', 'training loss: 0.6211\\n', '\\n', 'VALIDATION DATASET', '\\n', 'validation\nMatthews correlation coefficient: 0.3653', '\\n', 'validation loss: 0.6399\\n',\n'\\n', 'TEST DATASET', '\\n', 'test Matthews correlation coefficient: 0.3712',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training loss: 0.6162', '\\n', 'Best\nvalidation loss: 0.6343', '\\n', 'Best training Matthews correlation coefficient\n(MCC): 0.3716', '\\n', 'Best validation Matthews correlation coefficient (MCC):\n0.3733', '\\n', 'Best training macro F1 score: 0.6832', '\\n', 'Best validation\nmacro F1 score: 0.6849', '\\n', 'Test Matthews correlation coefficient (MCC):\n0.3772', '\\n', 'Test macro F1 score: 0.6878', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best training loss: 0.6162', '\\n', 'best\nvalidation loss: 0.6345', '\\n', 'best training macro F1 score: 0.6862', '\\n',\n'best validation macro F1 score: 0.6872', '\\n', 'best test macro F1 score:\n0.6901', '\\n', 'best test Matthews correlation coefficient: 0.3811', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== Dropout configuration: p=0.1 ===', '\\n', 'Training MCC: 0.3658', '\\n',\n'Training loss: 0.6202', '\\n', 'Validation MCC: 0.3609', '\\n', 'Validation loss:\n0.6371', '\\n', 'Test MCC: 0.3671', '\\n', '\\n=== Dropout configuration: p=0.3\n===', '\\n', 'Training MCC: 0.3568', '\\n', 'Training loss: 0.6257', '\\n',\n'Validation MCC: 0.3614', '\\n', 'Validation loss: 0.6344', '\\n', '\\n=== Dropout\nconfiguration: p=0.5 ===', '\\n', 'Training MCC: 0.3534', '\\n', 'Training loss:\n0.6279', '\\n', 'Validation MCC: 0.3479', '\\n', 'Validation loss: 0.6406', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  train loss: 0.6191', '\\n', '  train MCC: 0.3657', '\\n', '\ntrain macro F1 score: 0.6800', '\\n', '  validation loss (best MCC): 0.6363',\n'\\n', '  validation MCC (best): 0.3770', '\\n', '  validation macro F1 score\n(best): 0.6872', '\\n', '  test MCC: 0.3772', '\\n', '  test macro F1 score:\n0.6879', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best training loss: 0.6162', '\\n', 'best\nvalidation loss: 0.6345', '\\n', 'best training macro F1 score: 0.6862', '\\n',\n'best validation macro F1 score: 0.6872', '\\n', 'best test macro F1 score:\n0.6901', '\\n', 'best test Matthews correlation coefficient: 0.3811', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best training loss: 0.6162', '\\n', 'best\nvalidation loss: 0.6345', '\\n', 'best training macro F1 score: 0.6862', '\\n',\n'best validation macro F1 score: 0.6872', '\\n', 'best test macro F1 score:\n0.6901', '\\n', 'best test Matthews correlation coefficient: 0.3811', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best training loss: 0.6162', '\\n', 'best\nvalidation loss: 0.6345', '\\n', 'best training macro F1 score: 0.6862', '\\n',\n'best validation macro F1 score: 0.6872', '\\n', 'best test macro F1 score:\n0.6901', '\\n', 'best test Matthews correlation coefficient: 0.3811', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}