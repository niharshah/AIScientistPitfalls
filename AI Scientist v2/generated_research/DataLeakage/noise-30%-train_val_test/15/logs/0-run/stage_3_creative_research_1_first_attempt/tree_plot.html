<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 2], [0, 4], [0, 3], [3, 7], [3, 5], [3, 8], [3, 6], [6, 12], [6, 9], [6, 10], [6, 11]], "layout": [[0.42857142857142855, 0.0], [0.0, 0.33333333333333337], [0.2857142857142857, 0.33333333333333337], [0.5714285714285714, 0.33333333333333337], [0.8571428571428571, 0.33333333333333337], [0.14285714285714285, 0.6666666666666667], [0.42857142857142855, 0.6666666666666667], [0.7142857142857143, 0.6666666666666667], [1.0, 0.6666666666666667], [0.0, 1.0], [0.2857142857142857, 1.0], [0.5714285714285714, 1.0], [0.8571428571428571, 1.0]], "plan": ["We build a minimal yet end-to-end baseline: a character-level Transformer\nencoder that reads each SPR sequence as a string of symbols, embeds the\ncharacters, adds sinusoidal positions, and feeds them through two\nTransformerEncoder layers to predict the hidden rule label with a linear head.\nThe vocabulary is created from the training split, reserving id 0 for padding,\nso no external tokeniser is needed. DataLoaders pad / truncate sequences to a\nfixed length and supply attention masks; everything is moved to GPU when\navailable. We train for a handful of epochs with cross-entropy, monitor\nvalidation loss and Macro-F1 after every epoch, and keep per-epoch metrics,\nlosses, predictions and ground-truth in the experiment_data dict, which is\nfinally saved to ./working. If the SPR_BENCH folder is missing, a tiny synthetic\ndataset is generated so the script always runs. Evaluation on the dev split\nprints the final Macro-F1, and predictions for the held-out test split are\nstored for later analysis. The code follows all device, logging and saving\nrequirements and finishes comfortably within the time budget.", "We extend the baseline character-level Transformer by adding an explicit\nsymbolic-reasoning channel: for every sequence we build a count-vector that\nrecords how many times each vocabulary symbol occurs (a simple yet powerful\nglobal logical feature for parity, majority, and arithmetic-style rules).   A\nsmall MLP turns this symbolic vector into a dense reasoning embedding, which is\nthen concatenated with the pooled Transformer representation before\nclassification.   This hybrid architecture forces the network to simultaneously\nexploit local sequential patterns (via self-attention) and global symbolic\nstatistics (via counts), providing a cheap but expressive form of neural-\nsymbolic integration.   The dataloader is modified to return both token IDs and\ntheir count vectors; the model, optimiser, metric tracking, and saving logic\nfollow the required guidelines, including GPU handling, Macro-F1 evaluation, and\nwriting experiment_data.npy to ./working.   We keep everything in a single\nscript, train for a handful of epochs (configurable), and print validation loss\nand Macro-F1 each epoch so we can compare against the 70 % SOTA target.   The\nsolution remains lightweight (\u2248 200 k parameters), runs comfortably within 30\nminutes on CPU/GPU, and can be a foundation for richer symbolic modules in later\nstages.", "We augment the baseline character-level Transformer with an explicit symbolic\nmodule that computes a differentiable histogram (i.e. bag-of-symbols) for every\nsequence.   The histogram captures global counting statistics that are often\nsufficient to identify the hidden PolyRule and thus plays the role of a light-\nweight reasoning engine.   Both sources of information are processed in\nparallel: (1) contextual embeddings go through a shallow Transformer encoder and\nmean\u2013pooled, (2) the histogram is passed through a small MLP.   The two\nrepresentations are concatenated and classified, letting the network combine\nsequence order information with global symbolic regularities.   This neuro-\nsymbolic hybrid remains simple, runs fast, but gives the optimiser direct access\nto rule-level features that plain self-attention must discover implicitly, so we\nexpect higher validation Macro-F1 and better generalisation.   We keep all good\npractices (device handling, monitoring, saving artefacts) from the prior run and\nonly replace the model definition; everything else stays identical.   The script\nbelow trains for a few epochs, prints train / dev losses and Macro-F1, evaluates\non the test set, and stores all artefacts in ./working/experiment_data.npy.", "We augment the baseline character-level transformer with an explicit symbolic\nbranch that computes exact symbol-count vectors for every sequence; these\nvectors are passed through a small MLP and concatenated with the pooled\ntransformer representation before classification.  This hybrid architecture\ninvites the network to reason over global, order-invariant statistics (parity,\nmajority, equal-count relations, etc.) while the transformer continues modelling\nlocal context, giving it a simple yet effective \u201csymbolic reasoning\u201d capability.\nThe rest of the pipeline (data loading, vocabulary, padding, training loop,\nmetrics and checkpointing) stays identical, so we can directly compare Macro-F1\nto the previous baseline.  Below is a self-contained script that trains the\nHybridSymbolicTransformer, prints per-epoch loss and Macro-F1, evaluates on the\ntest split and stores all metrics in ./working/experiment_data.npy.", "We propose a hybrid neuro-symbolic classifier that combines a character-level\nTransformer encoder with an explicit symbolic module based on n-gram frequency\nvectors.   The Transformer captures contextual patterns, while the symbolic\nbranch provides discrete rule-like statistics (bigrams + trigrams) extracted\nfrom the 256 most frequent n-grams in the training split.   Both representations\nare projected and concatenated before the final softmax, enabling the network to\nmarry continuous and discrete reasoning.   We reuse earlier data handling, add a\nsmall routine to build the n-gram vocabulary and pre-compute n-gram count\nvectors, and insert an extra Linear\u2192ReLU layer for the symbolic path.   Training\nfollows the same cross-entropy objective, but we add stronger regularisation\n(dropout 0.2 and weight-decay) and early-stopping on validation Macro-F1.   The\nscript prints per-epoch losses and Macro-F1, saves full metrics to\n./working/experiment_data.npy, and should run within the time budget while\nstaying fully GPU-compatible.", "Our next attempt enriches the hybrid neuro-symbolic architecture with (1)\nexplicit bigram statistics in addition to unigram counts, and (2) a learnable\ngating mechanism that blends the pure-neural (Transformer) and symbolic\nrepresentations before classification.  By giving the model access to limited\nrelational features (bigrams) and allowing it to decide how much to trust\nsymbolic vs. neural evidence per example, we expect better generalization with\nonly a small parameter increase.  We extract the K = 200 most frequent bigrams\nfrom the training split, append their normalised counts to the existing unigram\nvector, and feed the resulting vector through an MLP.  The symbolic and\nTransformer embeddings are each projected to a common size; a scalar gate\n(sigmoid) mixes them, and the mixed vector is fed to the final classifier.  A\nshort 6-epoch run is shown; metrics (loss + Macro-F1) are tracked, printed, and\nsaved.  The whole script is self-contained, GPU-aware, and falls back to a tiny\nsynthetic dataset if SPR_BENCH is absent.", "To probe the impact of richer symbolic features and tighter neural-symbolic\nfusion, I will (1) extend the symbolic branch with a 64-dimensional hashed\nbigram histogram and a normalised length feature, (2) learn a gating vector from\nthe symbolic embedding that multiplicatively modulates the pooled Transformer\nrepresentation, and (3) train with label-smoothed cross-entropy and a cosine-\ndecay schedule to fight over-fitting. The resulting \u201cGated-Hybrid Transformer\u201d\nkeeps parameter count modest while giving the network global statistical cues\n(unigrams, bigrams, length) and an explicit mechanism to let symbolic evidence\nemphasise or suppress token-level patterns. We retain the previous lightweight\ncharacter-level encoder, regularise with dropout and weight-decay, and report\nMacro-F1 each epoch. The script below is self-contained: it loads SPR_BENCH if\npresent (otherwise synthesises toy data), trains for a few epochs on GPU/CPU,\nprints validation loss and Macro-F1, evaluates on the test split, and stores all\nmetrics/predictions in ./working/experiment_data.npy for later analysis.", "To address overfitting and strengthen global-pattern reasoning, we enrich the\nsymbolic branch with three additional hand-crafted signals\u2014sequence length,\nunique-symbol count, and Shannon entropy\u2014then fuse this richer vector with the\nTransformer representation through a learnable gate that decides, per sample,\nhow much symbolic versus neural evidence to trust.  We further apply dropout to\nboth branches, label-smoothing cross-entropy, and a cosine-annealing learning-\nrate scheduler with warm-up to stabilise training.  Early-stopping on dev\nMacro-F1 avoids wasted epochs.  This lightweight upgrade keeps parameter count\nmodest (<300 k) and training speed fast while giving the model more principled\naccess to rule-level statistics.  The script below trains, validates, prints\nper-epoch losses and Macro-F1, evaluates on the test split, and stores all\nmetrics/predictions in working/experiment_data.npy for later analysis.  It runs\non GPU if available and falls back to a small synthetic dataset when the real\nSPR_BENCH folder is absent.", "We add an auxiliary self-supervised objective that forces the transformer to\nreconstruct the per-symbol histogram of each input.  The hidden rule in SPR is\nfully expressed in those histograms, so encouraging the encoder to embed enough\ninformation to recover them should sharpen generalization while keeping the\nsymbolic branch lightweight.  Concretely, a single linear decoder tries to\npredict the normalized count-vector from the mean-pooled transformer states, and\nits MSE is combined with the normal cross-entropy (\u03bb = 0.3).  This couples the\nneural and symbolic pathways, encourages disentangled features, and acts as a\nstrong regularizer that mitigates the overfitting previously observed.  We also\nraise model capacity slightly, apply dropout and weight-decay, and use a cosine-\nannealed LR schedule.  Nothing else changes in data handling, so the code below\nis drop-in and trains in <10 min on GPU.  The script prints per-epoch Macro-F1\nand finally reports the test score while saving all metrics for later analysis.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, time, math, json, numpy as np, torch\nfrom typing import List, Dict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ----------------------- house-keeping & GPU -----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------- load SPR_BENCH or fallback ----------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndataset_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif dataset_path.exists():\n    spr = load_spr_bench(dataset_path)\nelse:\n    # synthetic fallback (small)\n    print(\"SPR_BENCH not found, generating synthetic data \u2026\")\n\n    def synth_split(n_rows, n_labels=5, max_len=20):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alphabet = list(\"ABCDEXYZUVW\")\n        for i in range(n_rows):\n            length = random.randint(5, max_len)\n            seq = \"\".join(random.choices(alphabet, k=length))\n            label = random.randint(0, n_labels - 1)\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(seq)\n            data[\"label\"].append(label)\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict(\n        {\"train\": synth_split(500), \"dev\": synth_split(100), \"test\": synth_split(100)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Loaded dataset with {num_labels} labels.\")\n\n# ----------------------- vocabulary & encoding ---------------------------\nPAD_ID = 0\n\n\ndef build_vocab(dataset) -> Dict[str, int]:\n    chars = set()\n    for s in dataset[\"sequence\"]:\n        chars.update(list(s))\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}  # reserve 0 for PAD\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nid2char = {i: c for c, i in vocab.items()}\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = min(64, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\n# ----------------------- dataset wrapper ---------------------------------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = torch.tensor(encode(self.seqs[idx], self.max_len), dtype=torch.long)\n        attn = (ids != PAD_ID).long()\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\"input_ids\": ids, \"attention_mask\": attn, \"labels\": label}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], MAX_LEN)\ndev_ds = SPRTorchDataset(spr[\"dev\"], MAX_LEN)\ntest_ds = SPRTorchDataset(spr[\"test\"], MAX_LEN)\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------- model ------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        num_layers=2,\n        dim_ff=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)  # (B, L, d)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = x.transpose(1, 2)  # for pooling\n        x = self.pool(x).squeeze(-1)\n        logits = self.classifier(x)\n        return logits\n\n\nmodel = SPRTransformer(vocab_size, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# ----------------------- experiment data dict ---------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ----------------------- training loop ----------------------------------\nEPOCHS = 5\n\n\ndef run_epoch(loader, train: bool):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, val_preds, val_gts = run_epoch(dev_loader, False)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_F1={tr_f1:.4f} val_F1={val_f1:.4f}\"\n    )\n\n# ----------------------- final evaluation on test -----------------------\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test  : loss={test_loss:.4f}  MacroF1={test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\n# ----------------------- save experiment data ---------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "import os, pathlib, random, math, time, json, numpy as np, torch\nfrom typing import Dict, List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ------------------- working dir & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------- load SPR_BENCH (with fallback) -----------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(name):  # helper\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _l(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr = load_spr_bench(DATA_PATH)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating small synthetic dataset as fallback.\")\n\n    def synth(n, max_len=20, n_labels=5):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alpha = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n        for i in range(n):\n            seq_len = random.randint(5, max_len)\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\"\".join(random.choices(alpha, k=seq_len)))\n            data[\"label\"].append(random.randrange(n_labels))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict({\"train\": synth(800), \"dev\": synth(200), \"test\": synth(200)})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Loaded data. #labels = {num_labels}\")\n\n# ------------------- build vocabulary ------------------------------------\nPAD_ID = 0\n\n\ndef build_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {ch: i + 1 for i, ch in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nid2char = {i: c for c, i in vocab.items()}\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ------------------- helpers ---------------------------------------------\ndef encode(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(c, PAD_ID) for c in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = min(64, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\n# ------------------- Dataset wrapper -------------------------------------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = torch.tensor(encode(self.seqs[idx], self.max_len), dtype=torch.long)\n        attn = (ids != PAD_ID).long()\n        # symbolic count vector (ignore PAD=0)\n        counts = (\n            torch.bincount(ids[ids > 0], minlength=vocab_size).clamp_max(255).float()\n        )\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\n            \"input_ids\": ids,\n            \"attention_mask\": attn,\n            \"counts\": counts,\n            \"labels\": label,\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], MAX_LEN)\ndev_ds = SPRTorchDataset(spr[\"dev\"], MAX_LEN)\ntest_ds = SPRTorchDataset(spr[\"test\"], MAX_LEN)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------- Model -----------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass HybridSPRModel(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        num_layers=2,\n        symb_dim=64,\n        dim_ff=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True\n        )\n        self.trf = nn.TransformerEncoder(enc_layer, num_layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n\n        self.symb_mlp = nn.Sequential(\n            nn.Linear(vocab_size, symb_dim), nn.ReLU(), nn.LayerNorm(symb_dim)\n        )\n\n        self.classifier = nn.Sequential(nn.Linear(d_model + symb_dim, num_labels))\n\n    def forward(self, input_ids, attention_mask, counts):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.trf(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)  # (B, d_model)\n        s = self.symb_mlp(counts)\n        feats = torch.cat([x, s], dim=-1)\n        return self.classifier(feats)\n\n\nmodel = HybridSPRModel(vocab_size, num_labels).to(device)\n\n# ------------------- optimisation & loss ---------------------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# ------------------- experiment data dict --------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ------------------- training / eval loops -------------------------------\ndef run_epoch(loader, train: bool):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\nEPOCHS = 8\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, vp, vg = run_epoch(dev_loader, False)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | \"\n        f\"MacroF1 train={tr_f1:.4f} val={val_f1:.4f}\"\n    )\n\n# ------------------- final test ------------------------------------------\ntest_loss, test_f1, tp, tg = run_epoch(test_loader, False)\nprint(f\"Test : loss={test_loss:.4f}  MacroF1={test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = tp\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = tg\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\n# ------------------- save -------------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "import os, pathlib, random, math, numpy as np, torch\nfrom typing import Dict, List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ----------------------- working dir & device ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------- SPR loader --------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):  # helper\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nspr_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif spr_path.exists():\n    spr = load_spr_bench(spr_path)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH not found, creating synthetic toy data...\")\n\n    def synth(n, L=20, nlab=5):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alpha = list(\"ABCDEUVXYZ\")\n        for i in range(n):\n            ln = random.randint(5, L)\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(\"\".join(random.choices(alpha, k=ln)))\n            data[\"label\"].append(random.randint(0, nlab - 1))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict({\"train\": synth(500), \"dev\": synth(120), \"test\": synth(120)})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\n    f'Dataset sizes \u2013 train:{len(spr[\"train\"])}, dev:{len(spr[\"dev\"])}, test:{len(spr[\"test\"])} | labels:{num_labels}'\n)\n\n# ----------------------- vocab & encoding --------------------------------\nPAD_ID = 0\n\n\ndef build_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = min(64, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\n# ----------------------- torch dataset -----------------------------------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = torch.tensor(encode(self.seqs[idx], self.max_len), dtype=torch.long)\n        attn = (ids != PAD_ID).long()\n        lab = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\"input_ids\": ids, \"attention_mask\": attn, \"labels\": lab}\n\n\ndef collate(b):\n    return {k: torch.stack([d[k] for d in b]) for k in b[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorchDataset(spr[s], MAX_LEN) for s in [\"train\", \"dev\", \"test\"]\n)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------- model -------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):  # (B,L,D)\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SymbolicTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        layers=2,\n        dim_ff=256,\n        drop=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, drop, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        # symbolic histogram pathway\n        self.hist_mlp = nn.Sequential(\n            nn.Linear(vocab_size, d_model), nn.ReLU(), nn.Linear(d_model, d_model)\n        )\n        self.classifier = nn.Linear(2 * d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        # contextual branch\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.encoder(x, src_key_padding_mask=(attention_mask == 0))\n        pooled = self.pool(x.transpose(1, 2)).squeeze(-1)  # (B,d)\n        # symbolic histogram branch\n        one_hot = torch.nn.functional.one_hot(input_ids, num_classes=vocab_size).float()\n        mask = attention_mask.unsqueeze(-1)\n        hist = (one_hot * mask).sum(1)  # (B,V)\n        hist = hist / mask.sum(1).clamp(min=1).float()\n        hist_repr = self.hist_mlp(hist)\n        # fuse & classify\n        fused = torch.cat([pooled, hist_repr], dim=-1)\n        return self.classifier(fused)\n\n\nmodel = SymbolicTransformer(vocab_size, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# ----------------------- experiment bookkeeping --------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ----------------------- training / eval loops ---------------------------\ndef run(loader, train_mode: bool):\n    model.train() if train_mode else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run(train_loader, True)\n    val_loss, val_f1, _, _ = run(dev_loader, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_F1={tr_f1:.4f} val_F1={val_f1:.4f}\"\n    )\n\n# ----------------------- final test --------------------------------------\ntest_loss, test_f1, test_preds, test_gts = run(test_loader, False)\nprint(f\"Test : loss={test_loss:.4f} MacroF1={test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\n# ----------------------- save artefacts ----------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy in\", working_dir)\n", "import os, pathlib, random, math, numpy as np, torch\nfrom collections import Counter\nfrom typing import Dict, List\nfrom datasets import load_dataset, DatasetDict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ---------------------------------------------------------------------\n# work dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# SPR loader (unchanged)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndataset_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif dataset_path.exists():\n    spr = load_spr_bench(dataset_path)\nelse:\n    # tiny synthetic fallback\n    print(\"SPR_BENCH not found, creating synthetic toy data \u2026\")\n\n    def synth(n_rows=300, max_len=18, n_labels=5):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alphabet = list(\"ABCDELMNOPQR\")\n        for i in range(n_rows):\n            seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(seq)\n            data[\"label\"].append(random.randint(0, n_labels - 1))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict({\"train\": synth(1000), \"dev\": synth(200), \"test\": synth(200)})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Loaded dataset with {num_labels} labels.\")\n\n# ---------------------------------------------------------------------\n# vocab and helpers\nPAD_ID = 0\n\n\ndef build_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\n# ---------------------------------------------------------------------\n# torch dataset with count-vector feature\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n        self.count_dim = vocab_size - 1  # without PAD\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _count_vec(self, seq: str) -> torch.Tensor:\n        cnt = Counter(seq)\n        vec = torch.zeros(self.count_dim, dtype=torch.float)\n        for ch, n in cnt.items():\n            idx = vocab.get(ch, 0)\n            if idx > 0:\n                vec[idx - 1] = float(n)\n        vec = vec / len(seq)  # normalise counts\n        return vec\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        ids = torch.tensor(encode(seq, self.max_len), dtype=torch.long)\n        attn = (ids != PAD_ID).long()\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        counts = self._count_vec(seq)\n        return {\n            \"input_ids\": ids,\n            \"attention_mask\": attn,\n            \"symbol_counts\": counts,\n            \"labels\": label,\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], MAX_LEN)\ndev_ds = SPRTorchDataset(spr[\"dev\"], MAX_LEN)\ntest_ds = SPRTorchDataset(spr[\"test\"], MAX_LEN)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model: transformer + symbolic branch\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass HybridSymbolicTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        n_layers=2,\n        ff_dim=256,\n        sym_hidden=64,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff_dim, dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, n_layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        # symbolic branch\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(vocab_size - 1, sym_hidden), nn.ReLU(), nn.LayerNorm(sym_hidden)\n        )\n        self.classifier = nn.Linear(d_model + sym_hidden, num_labels)\n\n    def forward(self, input_ids, attention_mask, symbol_counts):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = x.transpose(1, 2)\n        x = self.pool(x).squeeze(-1)  # (B,d_model)\n        s = self.sym_mlp(symbol_counts)\n        concat = torch.cat([x, s], dim=-1)\n        logits = self.classifier(concat)\n        return logits\n\n\nmodel = HybridSymbolicTransformer(vocab_size, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n\n# ---------------------------------------------------------------------\n# experiment data dict\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\n# training / eval functions\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"symbol_counts\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 8\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, train=False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss = {val_loss:.4f}  MacroF1 = {val_f1:.4f}\")\n\n# ---------------------------------------------------------------------\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, train=False)\nprint(f\"Test: loss = {test_loss:.4f}  MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, math, time, numpy as np, torch\nfrom collections import Counter\nfrom typing import Dict, List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ---------- set up working dir & device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- load SPR_BENCH or synthetic fallback ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndataset_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif dataset_path.exists():\n    spr = load_spr_bench(dataset_path)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH not found \u2013 generating synthetic data.\")\n\n    def synth_split(n, n_labels=5, max_len=20):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        vocab = list(\"ABCDEFXYZUV\")\n        for i in range(n):\n            seq = \"\".join(random.choices(vocab, k=random.randint(5, max_len)))\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(seq)\n            data[\"label\"].append(random.randint(0, n_labels - 1))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict(\n        {\"train\": synth_split(500), \"dev\": synth_split(100), \"test\": synth_split(100)}\n    )\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Dataset loaded with {num_labels} labels.\")\n\n# ---------- build char vocabulary ----------\nPAD_ID = 0\n\n\ndef build_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    v = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    v[\"<PAD>\"] = PAD_ID\n    return v\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\n\n\n# ---------- n-gram vocabulary (bigrams & trigrams) ----------\ndef top_ngrams(ds, top_k=256):\n    cnt = Counter()\n    for seq in ds[\"sequence\"]:\n        for n in (2, 3):\n            cnt.update(seq[i : i + n] for i in range(len(seq) - n + 1))\n    most = [ng for ng, _ in cnt.most_common(top_k)]\n    return {ng: i for i, ng in enumerate(most)}\n\n\nng_vocab = top_ngrams(spr[\"train\"], top_k=256)\nng_dim = len(ng_vocab)\nprint(f\"Character vocab={vocab_size}, n-gram vocab={ng_dim}\")\n\n\n# ---------- helpers ----------\ndef encode_seq(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\ndef encode_ngrams(seq: str) -> np.ndarray:\n    vec = np.zeros(ng_dim, dtype=np.float32)\n    for n in (2, 3):\n        for i in range(len(seq) - n + 1):\n            ng = seq[i : i + n]\n            if ng in ng_vocab:\n                vec[ng_vocab[ng]] += 1.0\n    # optional normalisation:\n    if vec.sum() > 0:\n        vec /= vec.sum()\n    return vec\n\n\nMAX_LEN = min(64, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\n# ---------- torch dataset ----------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n        # pre-compute n-gram vectors for speed\n        self.ngram_feats = [encode_ngrams(s) for s in self.seqs]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        ids = torch.tensor(encode_seq(self.seqs[idx], self.max_len), dtype=torch.long)\n        attn = (ids != PAD_ID).long()\n        ng = torch.tensor(self.ngram_feats[idx], dtype=torch.float)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\n            \"input_ids\": ids,\n            \"attention_mask\": attn,\n            \"ngram_feats\": ng,\n            \"labels\": lbl,\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], MAX_LEN)\ndev_ds = SPRTorchDataset(spr[\"dev\"], MAX_LEN)\ntest_ds = SPRTorchDataset(spr[\"test\"], MAX_LEN)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridModel(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        ngram_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        num_layers=2,\n        dim_ff=256,\n        drop=0.2,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, drop, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.ng_proj = nn.Sequential(\n            nn.Linear(ngram_size, 64), nn.ReLU(), nn.Dropout(drop)\n        )\n        self.classifier = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, ngram_feats):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        ng = self.ng_proj(ngram_feats)\n        logits = self.classifier(torch.cat([x, ng], dim=-1))\n        return logits\n\n\nmodel = HybridModel(vocab_size, ng_dim, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n# ---------- experiment tracking ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- training / evaluation ----------\ndef run_epoch(loader, train_flag: bool):\n    model.train() if train_flag else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train_flag):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"ngram_feats\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train_flag:\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\nEPOCHS = 7\nbest_val = -1\npatience = 2\nstalls = 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_F1={tr_f1:.4f} val_F1={val_f1:.4f}\"\n    )\n    # simple early stopping\n    if val_f1 > best_val:\n        best_val = val_f1\n        stalls = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best.pt\"))\n    else:\n        stalls += 1\n        if stalls >= patience:\n            print(\"Early stopping.\")\n            break\n\n# ---------- test evaluation ----------\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best.pt\"), map_location=device)\n)\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test : loss={test_loss:.4f}  MacroF1={test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy.\")\n", "import os, pathlib, random, math, numpy as np, torch, itertools\nfrom collections import Counter, defaultdict\nfrom typing import Dict, List\nfrom datasets import load_dataset, DatasetDict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# dataset helper ------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndataset_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif dataset_path.exists():\n    spr = load_spr_bench(dataset_path)\nelse:\n    print(\"SPR_BENCH not found \u2013 using synthetic toy data\")\n\n    def synth(n_rows=500, max_len=18, n_labels=6):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n        for i in range(n_rows):\n            seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(seq)\n            data[\"label\"].append(random.randint(0, n_labels - 1))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict({\"train\": synth(1500), \"dev\": synth(300), \"test\": synth(300)})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"labels: {num_labels}\")\n\n# ------------------------ vocabulation -----------------------\nPAD_ID = 0\n\n\ndef build_char_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars |= set(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_char_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Char vocab:\", vocab_size)\n\n\n# --------------- build top-K bigram vocabulary ---------------\ndef get_top_bigrams(seqs: List[str], k: int = 200):\n    cnt = Counter()\n    for s in seqs:\n        cnt.update([\"\".join(pair) for pair in zip(s, s[1:])])\n    bigrams = [b for b, _ in cnt.most_common(k)]\n    return {bg: i for i, bg in enumerate(bigrams)}\n\n\nBIGRAM_K = 200\nbigram2idx = get_top_bigrams(spr[\"train\"][\"sequence\"], BIGRAM_K)\nbigram_dim = len(bigram2idx)\nprint(\"Bigram dim:\", bigram_dim)\n\n# -------------------- encode utilities -----------------------\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef encode_chars(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\n# -------------------- torch dataset --------------------------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n        self.char_dim = vocab_size - 1\n        self.bigram_dim = bigram_dim\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _sym_vec(self, seq: str) -> torch.Tensor:\n        char_cnt = Counter(seq)\n        bg_cnt = Counter([\"\".join(p) for p in zip(seq, seq[1:])])\n        vec = torch.zeros(self.char_dim + self.bigram_dim, dtype=torch.float)\n        for ch, n in char_cnt.items():\n            idx = vocab.get(ch, 0)\n            if idx > 0:\n                vec[idx - 1] = float(n)\n        for bg, n in bg_cnt.items():\n            idx = bigram2idx.get(bg, None)\n            if idx is not None:\n                vec[self.char_dim + idx] = float(n)\n        vec = vec / len(seq)  # normalise\n        return vec\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        ids = torch.tensor(encode_chars(seq, self.max_len), dtype=torch.long)\n        attn = (ids != PAD_ID).long()\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        sym = self._sym_vec(seq)\n        return {\n            \"input_ids\": ids,\n            \"attention_mask\": attn,\n            \"symbolic\": sym,\n            \"labels\": label,\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], MAX_LEN)\ndev_ds = SPRTorchDataset(spr[\"dev\"], MAX_LEN)\ntest_ds = SPRTorchDataset(spr[\"test\"], MAX_LEN)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------------- model ----------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass GatedHybrid(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        n_layers=2,\n        ff_dim=256,\n        sym_hidden=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff_dim, dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, n_layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        # symbolic branch\n        self.sym_mlp = nn.Sequential(\n            nn.Linear((vocab_size - 1) + bigram_dim, sym_hidden),\n            nn.ReLU(),\n            nn.LayerNorm(sym_hidden),\n        )\n        self.sym_proj = nn.Linear(sym_hidden, d_model)\n        # gating\n        self.gate_fc = nn.Linear(d_model * 2, 1)\n        # classifier\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask, symbolic):\n        # transformer branch\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)  # (B,d_model)\n        # symbolic branch\n        s = self.sym_mlp(symbolic)\n        s = self.sym_proj(s)  # (B,d_model)\n        # gating\n        g = torch.sigmoid(self.gate_fc(torch.cat([x, s], dim=-1)))  # (B,1)\n        fused = g * x + (1 - g) * s\n        logits = self.cls(fused)\n        return logits\n\n\nmodel = GatedHybrid(vocab_size, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n\n# -------------------- experiment logging ---------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- training / eval funcs ------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"symbolic\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------- training loop --------------------------\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, train=False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}  MacroF1 = {val_f1:.4f}\")\n\n# -------------------- final test -----------------------------\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, train=False)\nprint(f\"Test: loss = {test_loss:.4f}  MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\n# -------------------- save log -------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Metrics saved at\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "import os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ---------------------------------------------------------------------\n# mandatory working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# 1. Load SPR_BENCH or synthetic fallback\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndataset_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif dataset_path.exists():\n    spr = load_spr_bench(dataset_path)\nelse:\n    print(\"SPR_BENCH not found, generating tiny synthetic data \u2026\")\n\n    def synth(n=500):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alphabet = list(\"ABCDELMNOPQR\")\n        for i in range(n):\n            seq_len = random.randint(5, 18)\n            seq = \"\".join(random.choices(alphabet, k=seq_len))\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(seq)\n            data[\"label\"].append(random.randint(0, 4))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict({\"train\": synth(800), \"dev\": synth(200), \"test\": synth(200)})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"n_labels = {num_labels}\")\n\n# ---------------------------------------------------------------------\n# 2. Build char vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nprint(f\"vocab_size={vocab_size}, MAX_LEN={MAX_LEN}\")\n\n\ndef encode(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------------\n# 3. Torch dataset with enriched symbolic features\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n        self.count_dim = vocab_size - 1\n\n    def _count_vec(self, seq: str) -> torch.Tensor:\n        cnt = Counter(seq)\n        vec = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in cnt.items():\n            idx = vocab.get(ch, 0)\n            if idx > 0:\n                vec[idx - 1] = n / len(seq)\n        return vec\n\n    def _extra_feats(self, seq: str) -> torch.Tensor:\n        length = len(seq) / self.max_len\n        uniq = len(set(seq)) / self.count_dim\n        # Shannon entropy scaled to [0,1]\n        p = np.array(list(Counter(seq).values())) / len(seq)\n        entropy = -(p * np.log2(p + 1e-9)).sum() / math.log2(self.count_dim + 1e-9)\n        return torch.tensor([length, uniq, entropy], dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq, self.max_len), dtype=torch.long),\n            \"attention_mask\": torch.tensor(\n                [1 if i != PAD_ID else 0 for i in encode(seq, self.max_len)],\n                dtype=torch.long,\n            ),\n            \"symbol_counts\": self._count_vec(seq),\n            \"extra_feats\": self._extra_feats(seq),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRTorchDataset(spr[s], MAX_LEN) for s in [\"train\", \"dev\", \"test\"]\n)\ntrain_loader = torch.utils.data.DataLoader(\n    train_ds, batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = torch.utils.data.DataLoader(\n    dev_ds, batch_size=256, shuffle=False, collate_fn=collate\n)\ntest_loader = torch.utils.data.DataLoader(\n    test_ds, batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------------------------------------------------------------\n# 4. Model: Transformer + gated symbolic vector\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass Model(torch.nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, n_layers=2, ff=256, sym_h=64\n    ):\n        super().__init__()\n        self.embed = torch.nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = torch.nn.TransformerEncoderLayer(\n            d_model, nhead, ff, dropout=0.1, batch_first=True\n        )\n        self.transformer = torch.nn.TransformerEncoder(enc_layer, n_layers)\n        self.pool = torch.nn.AdaptiveAvgPool1d(1)\n        # symbolic branch\n        self.sym_mlp = torch.nn.Sequential(\n            torch.nn.Linear(vocab_size - 1 + 3, sym_h),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.1),\n        )\n        # gating\n        self.gate = torch.nn.Linear(d_model + sym_h, 1)\n        self.classifier = torch.nn.Linear(d_model + sym_h, num_labels)\n\n    def forward(self, input_ids, attention_mask, symbol_counts, extra_feats):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = x.transpose(1, 2)\n        x = self.pool(x).squeeze(-1)  # (B,d_model)\n        sym = torch.cat([symbol_counts, extra_feats], dim=-1)\n        sym = self.sym_mlp(sym)  # (B,sym_h)\n        concat = torch.cat([x, sym], dim=-1)  # (B,d_model+sym_h)\n        g = torch.sigmoid(self.gate(concat))  # (B,1)\n        mixed = torch.cat([x * g, sym * (1 - g)], dim=-1)  # simple gated fusion\n        return self.classifier(mixed)  # logits\n\n\nmodel = Model(vocab_size, num_labels).to(device)\n\n\n# ---------------------------------------------------------------------\n# 5. Loss, optimiser, scheduler\nclass LabelSmoothingCE(torch.nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_classes = logits.size(-1)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_classes - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n\n\ncriterion = LabelSmoothingCE()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=12)\n\n# ---------------------------------------------------------------------\n# 6. experiment tracking dict\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\n# 7. Train / eval loops\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        logits = model(\n            batch[\"input_ids\"],\n            batch[\"attention_mask\"],\n            batch[\"symbol_counts\"],\n            batch[\"extra_feats\"],\n        )\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\nbest_val = -1\npatience, wait = 3, 0\nEPOCHS = 15\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    scheduler.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: val_loss={val_loss:.4f}  MacroF1={val_f1:.4f}  time={time.time()-t0:.1f}s\"\n    )\n    if val_f1 > best_val:\n        best_val, wait = val_f1, 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# ---------------------------------------------------------------------\n# 8. Test evaluation\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best.pt\"), map_location=device)\n)\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss={test_loss:.4f}  MacroF1={test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\n# ---------------------------------------------------------------------\n# 9. Save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, math, numpy as np, torch\nfrom collections import Counter\nfrom typing import Dict, List\nfrom datasets import load_dataset, DatasetDict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\n\n# ------------------- boiler-plate & device ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------- data loading ------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndataset_path = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif dataset_path.exists():\n    spr = load_spr_bench(dataset_path)\nelse:  # tiny synthetic fallback\n    print(\"SPR_BENCH not found, creating synthetic toy data \u2026\")\n\n    def synth(n_rows=400, max_len=18, n_labels=5):\n        data = {\"id\": [], \"sequence\": [], \"label\": []}\n        alphabet = list(\"ABCDEFGHIJKL\")\n        for i in range(n_rows):\n            seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n            data[\"id\"].append(str(i))\n            data[\"sequence\"].append(seq)\n            data[\"label\"].append(random.randint(0, n_labels - 1))\n        return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n    spr = DatasetDict({\"train\": synth(2000), \"dev\": synth(400), \"test\": synth(400)})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Dataset has {num_labels} labels.\")\n\n# ------------------- vocab & helpers ---------------------------------\nPAD_ID = 0\n\n\ndef build_vocab(ds) -> Dict[str, int]:\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\nid2char = {i: c for c, i in vocab.items()}\n\n\ndef encode(seq: str, max_len: int) -> List[int]:\n    ids = [vocab.get(ch, PAD_ID) for ch in seq[:max_len]]\n    ids += [PAD_ID] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n\n# ------------------- torch Datasets ----------------------------------\nclass SPRDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.max_len = max_len\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _count_vec(self, seq: str) -> torch.Tensor:\n        cnt = Counter(seq)\n        vec = torch.zeros(self.count_dim)\n        for ch, n in cnt.items():\n            idx = vocab.get(ch, 0)\n            if idx > 0:\n                vec[idx - 1] = float(n)\n        return vec / len(seq)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq, self.max_len), dtype=torch.long),\n            \"attention_mask\": torch.tensor(\n                [1 if i != PAD_ID else 0 for i in encode(seq, self.max_len)],\n                dtype=torch.long,\n            ),\n            \"symbol_counts\": self._count_vec(seq).float(),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"], MAX_LEN), batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"], MAX_LEN), batch_size=256, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"], MAX_LEN), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------- model -------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridReconTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=160,\n        nhead=4,\n        layers=3,\n        ff=256,\n        sym_hidden=80,\n        drop=0.15,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PositionalEncoding(d_model, MAX_LEN)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, dropout=drop, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(vocab_size - 1, sym_hidden), nn.ReLU(), nn.LayerNorm(sym_hidden)\n        )\n        self.classifier = nn.Linear(d_model + sym_hidden, num_labels)\n        # auxiliary decoder\n        self.recon = nn.Linear(d_model, vocab_size - 1)\n\n    def forward(self, input_ids, attention_mask, symbol_counts):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.encoder(x, src_key_padding_mask=(attention_mask == 0))\n        pooled = self.pool(x.transpose(1, 2)).squeeze(-1)  # (B,d)\n        logits_cls = self.classifier(\n            torch.cat([pooled, self.sym_mlp(symbol_counts)], dim=-1)\n        )\n        logits_recon = self.recon(pooled)\n        return logits_cls, logits_recon\n\n\nmodel = HybridReconTransformer(vocab_size, num_labels).to(device)\nce_loss = nn.CrossEntropyLoss()\nmse_loss = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-2)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\n# ------------------- experiment store --------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ------------------- training utils ----------------------------------\ndef run_epoch(loader, train_mode=False, lambda_aux=0.3):\n    model.train() if train_mode else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out_cls, out_recon = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"symbol_counts\"]\n            )\n            loss_cls = ce_loss(out_cls, batch[\"labels\"])\n            loss_recon = mse_loss(torch.sigmoid(out_recon), batch[\"symbol_counts\"])\n            loss = loss_cls + lambda_aux * loss_recon\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(torch.argmax(out_cls, dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macroF1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macroF1, preds, gts\n\n\n# ------------------- main training loop ------------------------------\nEPOCHS = 15\nbest_f1 = 0.0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    scheduler.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}  MacroF1 = {val_f1:.4f}\")\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best.pt\"))\n\n# ------------------- evaluation on test ------------------------------\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best.pt\"), map_location=device)\n)\ntest_loss, test_f1, preds, gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f}  MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 140861.90\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 114442.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 200387.18\nexamples/s]', '\\n', 'Loaded dataset with 2 labels.', '\\n', 'Vocab size: 10',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.5653\nval_loss=0.9587 train_F1=0.6913 val_F1=0.6059', '\\n', 'Epoch 2:\ntrain_loss=0.1564 val_loss=1.9590 train_F1=0.9505 val_F1=0.6632', '\\n', 'Epoch\n3: train_loss=0.1137 val_loss=1.9051 train_F1=0.9705 val_F1=0.6757', '\\n',\n'Epoch 4: train_loss=0.0843 val_loss=1.8856 train_F1=0.9815 val_F1=0.6838',\n'\\n', 'Epoch 5: train_loss=0.0637 val_loss=1.9662 train_F1=0.9810\nval_F1=0.6920', '\\n', 'Test  : loss=1.9695  MacroF1=0.6958', '\\n', 'Saved\nexperiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/0-\nrun/process_ForkProcess-4/working', '\\n', 'Execution time: 5 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 140835.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 153099.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 250495.94\nexamples/s]', '\\n', 'Loaded data. #labels = 2', '\\n', 'Vocab size:', ' ', '10',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.7411\n| MacroF1 train=0.6134 val=0.6548', '\\n', 'Epoch 2: validation_loss = 1.6390 |\nMacroF1 train=0.9575 val=0.6800', '\\n', 'Epoch 3: validation_loss = 1.6438 |\nMacroF1 train=0.9495 val=0.6549', '\\n', 'Epoch 4: validation_loss = 1.6179 |\nMacroF1 train=0.9680 val=0.6738', '\\n', 'Epoch 5: validation_loss = 1.6036 |\nMacroF1 train=0.9730 val=0.6820', '\\n', 'Epoch 6: validation_loss = 1.7456 |\nMacroF1 train=0.9775 val=0.6778', '\\n', 'Epoch 7: validation_loss = 1.7592 |\nMacroF1 train=0.9820 val=0.6838', '\\n', 'Epoch 8: validation_loss = 1.9551 |\nMacroF1 train=0.9860 val=0.6939', '\\n', 'Test : loss=1.9537  MacroF1=0.6958',\n'\\n', 'Saved experiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n11/working', '\\n', 'Execution time: 7 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 113316.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 91795.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 93723.27\nexamples/s]', '\\n', 'Dataset sizes \u2013 train:2000, dev:500, test:1000 | labels:2',\n'\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.3429\nval_loss=1.6011 train_F1=0.8654 val_F1=0.6680', '\\n', 'Epoch 2:\ntrain_loss=0.1343 val_loss=1.7416 train_F1=0.9595 val_F1=0.6612', '\\n', 'Epoch\n3: train_loss=0.1168 val_loss=1.6265 train_F1=0.9625 val_F1=0.6632', '\\n',\n'Epoch 4: train_loss=0.1093 val_loss=1.6723 train_F1=0.9680 val_F1=0.6780',\n'\\n', 'Epoch 5: train_loss=0.1017 val_loss=1.7019 train_F1=0.9710\nval_F1=0.6778', '\\n', 'Epoch 6: train_loss=0.0825 val_loss=1.7767\ntrain_F1=0.9795 val_F1=0.6738', '\\n', 'Test : loss=1.7194 MacroF1=0.6946', '\\n',\n'Saved experiment_data.npy in', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n12/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 140651.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 129798.35\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 214071.56\nexamples/s]', '\\n', 'Loaded dataset with 2 labels.', '\\n', 'Vocab size:', ' ',\n'10', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss = 0.6493\nMacroF1 = 0.6840', '\\n', 'Epoch 2: val_loss = 1.5523  MacroF1 = 0.6960', '\\n',\n'Epoch 3: val_loss = 1.5827  MacroF1 = 0.6877', '\\n', 'Epoch 4: val_loss =\n1.9302  MacroF1 = 0.6899', '\\n', 'Epoch 5: val_loss = 1.9724  MacroF1 = 0.6960',\n'\\n', 'Epoch 6: val_loss = 1.8374  MacroF1 = 0.6940', '\\n', 'Epoch 7: val_loss =\n1.8368  MacroF1 = 0.6980', '\\n', 'Epoch 8: val_loss = 2.0503  MacroF1 = 0.6960',\n'\\n', 'Test: loss = 2.0359  MacroF1 = 0.6969', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 112812.28\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 92304.23\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 138521.88\nexamples/s]', '\\n', 'Dataset loaded with 2 labels.', '\\n', 'Character vocab=10,\nn-gram vocab=72', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.5392\nval_loss=1.1687 train_F1=0.7369 val_F1=0.6549', '\\n', 'Epoch 2:\ntrain_loss=0.1609 val_loss=1.6724 train_F1=0.9460 val_F1=0.6780', '\\n', 'Epoch\n3: train_loss=0.1239 val_loss=1.8578 train_F1=0.9670 val_F1=0.6613', '\\n',\n'Epoch 4: train_loss=0.1127 val_loss=1.7507 train_F1=0.9665 val_F1=0.6779',\n'\\n', 'Early stopping.', '\\n', 'Test : loss=1.6238  MacroF1=0.6839', '\\n',\n'Saved experiment_data.npy.', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'labels: 2', '\\n', 'Char vocab:', ' ', '10', '\\n',\n'Bigram dim:', ' ', '24', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 1.1831\nMacroF1 = 0.6859', '\\n', 'Epoch 2: validation_loss = 1.9428  MacroF1 = 0.6960',\n'\\n', 'Epoch 3: validation_loss = 1.8189  MacroF1 = 0.6980', '\\n', 'Epoch 4:\nvalidation_loss = 1.5642  MacroF1 = 0.6839', '\\n', 'Epoch 5: validation_loss =\n2.0609  MacroF1 = 0.6940', '\\n', 'Epoch 6: validation_loss = 1.6634  MacroF1 =\n0.6857', '\\n', 'Test: loss = 1.6498  MacroF1 = 0.6854', '\\n', 'Metrics saved\nat', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.7373\n| MacroF1 = 0.6007', '\\n', 'Epoch 2: validation_loss = 0.8751 | MacroF1 =\n0.6899', '\\n', 'Epoch 3: validation_loss = 0.8708 | MacroF1 = 0.6980', '\\n',\n'Epoch 4: validation_loss = 0.8634 | MacroF1 = 0.6899', '\\n', 'Epoch 5:\nvalidation_loss = 0.8477 | MacroF1 = 0.6960', '\\n', 'Epoch 6: validation_loss =\n0.8493 | MacroF1 = 0.6960', '\\n', 'Epoch 7: validation_loss = 0.8340 | MacroF1 =\n0.6980', '\\n', 'Epoch 8: validation_loss = 0.8866 | MacroF1 = 0.6980', '\\n',\n'Epoch 9: validation_loss = 0.8674 | MacroF1 = 0.7000', '\\n', 'Epoch 10:\nvalidation_loss = 0.8694 | MacroF1 = 0.7000', '\\n', 'Test: loss = 0.8654 |\nMacroF1 = 0.6999', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Total runtime: 18.3s', '\\n', 'Execution\ntime: 21 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'n_labels = 2', '\\n', 'vocab_size=10, MAX_LEN=95',\n'\\n', 'Epoch 1: val_loss=0.6903  MacroF1=0.3316  time=0.7s', '\\n', 'Epoch 2:\nval_loss=0.9043  MacroF1=0.6919  time=0.3s', '\\n', 'Epoch 3: val_loss=0.9542\nMacroF1=0.6960  time=0.3s', '\\n', 'Epoch 4: val_loss=0.9287  MacroF1=0.6960\ntime=0.3s', '\\n', 'Epoch 5: val_loss=0.8653  MacroF1=0.6960  time=0.3s', '\\n',\n'Epoch 6: val_loss=0.8589  MacroF1=0.6940  time=0.3s', '\\n', 'Early stopping.',\n'\\n', 'Test: loss=0.9454  MacroF1=0.6959', '\\n', 'Saved metrics to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset has 2 labels.', '\\n', 'Vocab size:', ' ',\n'10', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.9567\nMacroF1 = 0.6920', '\\n', 'Epoch 2: validation_loss = 1.4288  MacroF1 = 0.6940',\n'\\n', 'Epoch 3: validation_loss = 1.7550  MacroF1 = 0.6920', '\\n', 'Epoch 4:\nvalidation_loss = 1.6583  MacroF1 = 0.6899', '\\n', 'Epoch 5: validation_loss =\n1.9517  MacroF1 = 0.6837', '\\n', 'Epoch 6: validation_loss = 1.7334  MacroF1 =\n0.6898', '\\n', 'Epoch 7: validation_loss = 1.8095  MacroF1 = 0.6839', '\\n',\n'Epoch 8: validation_loss = 1.5934  MacroF1 = 0.7000', '\\n', 'Epoch 9:\nvalidation_loss = 2.0024  MacroF1 = 0.6859', '\\n', 'Epoch 10: validation_loss =\n2.0224  MacroF1 = 0.6859', '\\n', 'Epoch 11: validation_loss = 2.0688  MacroF1 =\n0.6920', '\\n', 'Epoch 12: validation_loss = 2.0332  MacroF1 = 0.6960', '\\n',\n'Epoch 13: validation_loss = 2.0502  MacroF1 = 0.6960', '\\n', 'Epoch 14:\nvalidation_loss = 2.0263  MacroF1 = 0.6940', '\\n', 'Epoch 15: validation_loss =\n2.0194  MacroF1 = 0.6960', '\\n', 'Test: loss = 1.6184  MacroF1 = 0.6939', '\\n',\n'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_23-44-27_conceptual_generalization_poly_rule_attempt_0/0-\nrun/process_ForkProcess-13/working/experiment_data.npy', '\\n', 'Execution time:\n15 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.7388\n| MacroF1 = 0.6920', '\\n', 'Epoch 2: validation_loss = 0.8271 | MacroF1 =\n0.6920', '\\n', 'Epoch 3: validation_loss = 0.8779 | MacroF1 = 0.6940', '\\n',\n'Epoch 4: validation_loss = 0.8392 | MacroF1 = 0.6980', '\\n', 'Epoch 5:\nvalidation_loss = 0.8989 | MacroF1 = 0.6839', '\\n', 'Epoch 6: validation_loss =\n0.8657 | MacroF1 = 0.6960', '\\n', 'Epoch 7: validation_loss = 0.8626 | MacroF1 =\n0.6940', '\\n', 'Epoch 8: validation_loss = 0.8660 | MacroF1 = 0.6960', '\\n',\n'Epoch 9: validation_loss = 0.8798 | MacroF1 = 0.6960', '\\n', 'Epoch 10:\nvalidation_loss = 0.8754 | MacroF1 = 0.6960', '\\n', 'Test: loss = 0.8716 |\nMacroF1 = 0.6969', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Total runtime: 21.9s', '\\n', 'Execution\ntime: 26 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6610\n| MacroF1 = 0.6879', '\\n', 'Epoch 2: validation_loss = 0.8787 | MacroF1 =\n0.6778', '\\n', 'Epoch 3: validation_loss = 0.9000 | MacroF1 = 0.6879', '\\n',\n'Epoch 4: validation_loss = 0.8902 | MacroF1 = 0.6920', '\\n', 'Epoch 5:\nvalidation_loss = 0.8694 | MacroF1 = 0.6960', '\\n', 'Epoch 6: validation_loss =\n0.8852 | MacroF1 = 0.6899', '\\n', 'Epoch 7: validation_loss = 0.8671 | MacroF1 =\n0.6960', '\\n', 'Epoch 8: validation_loss = 0.8755 | MacroF1 = 0.6960', '\\n',\n'Epoch 9: validation_loss = 0.8731 | MacroF1 = 0.6960', '\\n', 'Epoch 10:\nvalidation_loss = 0.8716 | MacroF1 = 0.6960', '\\n', 'Test: loss = 0.8684 |\nMacroF1 = 0.6959', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Total runtime: 19.0s', '\\n', 'Execution\ntime: 22 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6595\n| MacroF1 = 0.6745', '\\n', 'Epoch 2: validation_loss = 0.9036 | MacroF1 =\n0.6960', '\\n', 'Epoch 3: validation_loss = 0.8843 | MacroF1 = 0.6960', '\\n',\n'Epoch 4: validation_loss = 0.8762 | MacroF1 = 0.6858', '\\n', 'Epoch 5:\nvalidation_loss = 0.8436 | MacroF1 = 0.6980', '\\n', 'Epoch 6: validation_loss =\n0.8716 | MacroF1 = 0.6980', '\\n', 'Epoch 7: validation_loss = 0.8884 | MacroF1 =\n0.6940', '\\n', 'Epoch 8: validation_loss = 0.8822 | MacroF1 = 0.6940', '\\n',\n'Epoch 9: validation_loss = 0.8662 | MacroF1 = 0.6980', '\\n', 'Epoch 10:\nvalidation_loss = 0.8781 | MacroF1 = 0.6980', '\\n', 'Test: loss = 0.8766 |\nMacroF1 = 0.6939', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Total runtime: 18.6s', '\\n', 'Execution\ntime: 22 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution of the training script was successful without any errors or bugs.\nThe training process completed all epochs, and the model achieved a test MacroF1\nscore of 0.6958, indicating that the implementation is functional. The\nexperiment data was saved successfully as well.", "", "", "The training and evaluation of the HybridSymbolicTransformer model executed\nsuccessfully without any errors or bugs. The results show that the model\nachieved a MacroF1 score of 0.6969 on the test set, which is slightly below the\nstate-of-the-art accuracy of 70.0%. The execution time was well within the\nallowed limit. Future improvements could focus on hyperparameter tuning or\narchitectural changes to surpass the benchmark.", "", "", "The execution ran successfully without any errors or bugs. The\nGatedHybridTransformer model was trained and evaluated on the SPR_BENCH dataset.\nThe model achieved a test MacroF1 score of 0.6999, which is very close to the\nstate-of-the-art benchmark of 70.0%. The experiment data was saved successfully,\nand the total runtime was within the expected limits.", "", "", "The execution of the training script completed successfully without any errors\nor bugs. The model achieved a test MacroF1 score of 0.6969, which is slightly\nbelow the state-of-the-art benchmark of 70.0%. Further tuning of\nhyperparameters, model architecture, or data preprocessing might help in\nimproving the performance.", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss measures the error on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0637, "best_value": 0.0637}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss measures the error on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9662, "best_value": 1.9662}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Final training macro F1 score measures the F1 score on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.981, "best_value": 0.981}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Final validation macro F1 score measures the F1 score on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Test loss measures the error on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9695, "best_value": 1.9695}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Test macro F1 score measures the F1 score on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6958, "best_value": 0.6958}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro F1 score measures the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6958, "best_value": 0.6939}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the model's error during training and evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9537, "best_value": 0.7411}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during the training phase, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0825, "best_value": 0.0825}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during the validation phase, used to evaluate the model's performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.7767, "best_value": 1.7767}]}, {"metric_name": "training Macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training, measuring the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9795, "best_value": 0.9795}]}, {"metric_name": "validation Macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation, measuring the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6738, "best_value": 0.6738}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing, indicating how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.7194, "best_value": 1.7194}]}, {"metric_name": "test Macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing, measuring the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6946, "best_value": 0.6946}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.6969}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Represents the error of the model; lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.0359, "best_value": 0.0227}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "The macro-average F1 score, which is the harmonic mean of precision and recall, equally weighted across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9665, "best_value": 0.9665}, {"dataset_name": "SPR_BENCH", "final_value": 0.6839, "best_value": 0.678}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The loss value, which measures the error or deviation from the ideal output.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1127, "best_value": 0.1127}, {"dataset_name": "SPR_BENCH", "final_value": 1.6238, "best_value": 1.1687}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score measures the harmonic mean of precision and recall, averaged across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6854, "best_value": 0.698}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.6498, "best_value": 1.1831}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3282, "best_value": 0.3282}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8694, "best_value": 0.7373}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during testing", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8654, "best_value": 0.8654}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the macro-averaged F1 score, indicating the balance between precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6959, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or deviation from the expected outcome. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9454, "best_value": 0.3353}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated during training of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.019, "best_value": 0.019}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated during training of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9965, "best_value": 0.9965}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated during validation of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9567, "best_value": 0.9567}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated during validation of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss calculated during testing of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.6184, "best_value": 1.6184}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated during testing of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6939, "best_value": 0.6939}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "A measure of a model's accuracy considering both precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.698}]}, {"metric_name": "loss", "lower_is_better": true, "description": "A measure of the model's prediction error.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8716, "best_value": 0.3365}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.997, "best_value": 0.997}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.333, "best_value": 0.333}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8716, "best_value": 0.661}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6959, "best_value": 0.6959}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8684, "best_value": 0.8684}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3346, "best_value": 0.3346}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8781, "best_value": 0.6595}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8766, "best_value": 0.8766}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, true, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/comparison_test_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_metric_curve.png", "../../logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/seed_aggregation_68b069385f7948279247350f699fcde7/SPR_BENCH_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_68b069385f7948279247350f699fcde7/SPR_BENCH_aggregated_macroF1_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_68b069385f7948279247350f699fcde7/cross_dataset_val_macroF1_aggregated.png"]], "plot_paths": [["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_class_distribution.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_class_distribution.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/comparison_test_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_metric_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_68b069385f7948279247350f699fcde7/SPR_BENCH_aggregated_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_68b069385f7948279247350f699fcde7/SPR_BENCH_aggregated_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_68b069385f7948279247350f699fcde7/cross_dataset_val_macroF1_aggregated.png"]], "plot_analyses": [[{"analysis": "The plot shows the training and validation loss curves over 5 epochs. Training loss steadily decreases, indicating that the model is learning from the training data. However, the validation loss initially increases sharply and then stabilizes at a high value, suggesting overfitting. The model performs well on the training set but struggles to generalize to the validation set. This discrepancy indicates the need for regularization techniques or better hyperparameter tuning to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot represents the training and validation Macro-F1 scores over 5 epochs. The training Macro-F1 score quickly approaches 1.0, indicating excellent performance on the training set. Meanwhile, the validation Macro-F1 score improves gradually but remains significantly lower than the training score, further emphasizing the overfitting problem. Although there is some improvement in validation performance, the gap suggests that the model is not effectively generalizing to unseen data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix highlights the distribution of predictions against the ground truth. The matrix indicates that the model achieves relatively balanced performance across the classes, as the diagonal entries (representing correct predictions) are prominent. However, the off-diagonal entries suggest some misclassifications, which could be further analyzed to identify specific weaknesses in the model's reasoning capabilities. Fine-tuning the model or incorporating additional reasoning modules could help address these issues.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_179195b55c1b49c89b65435ec46aee6a_proc_3458581/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning effectively on the training data. However, the validation loss initially decreases but starts to increase after a few epochs, suggesting overfitting. The divergence between the training and validation losses in later epochs highlights the need for regularization techniques or early stopping.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score rapidly improves and approaches 1.0, indicating that the model achieves near-perfect performance on the training data. However, the validation Macro-F1 score remains relatively stagnant, with only a slight upward trend. This discrepancy further emphasizes the overfitting issue, as the model struggles to generalize to unseen validation data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The confusion matrix shows some imbalance in the predictions, with one class being predicted more accurately than the other. Normalized values reveal that the model has a bias towards one class, which could be due to class imbalance in the training data or inadequate learning of features for the other class. This requires further investigation and potentially rebalancing the dataset or adjusting the loss function.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ab5f0d62779a48348bc9c737e38ff97e_proc_3469031/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss over six epochs. The training loss decreases steadily, indicating that the model is learning from the training data. However, the validation loss starts higher than the training loss and shows an increasing trend after the second epoch, suggesting potential overfitting. This indicates the model may not generalize well to unseen data beyond the training set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_loss_curve.png"}, {"analysis": "The plot shows the training and validation Macro-F1 scores over six epochs. The training Macro-F1 score improves consistently, nearing 1.0, which indicates excellent performance on the training data. However, the validation Macro-F1 score remains relatively flat and significantly lower than the training score, further confirming overfitting and poor generalization to the validation set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix indicates that the model performs better at predicting certain classes while struggling with others. The diagonal elements represent correct predictions, and the off-diagonal elements reflect misclassifications. The uneven intensity of the diagonal elements suggests class imbalance or difficulty in learning specific patterns for certain classes.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The class distribution plot compares the number of samples per class in the ground truth and predicted outputs. The predicted class distribution closely matches the ground truth, indicating that the model captures the overall class distribution well. However, this does not necessarily reflect the accuracy or quality of individual predictions.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5561463ebd2a47938f6346145441cd31_proc_3469032/SPR_BENCH_class_distribution.png"}], [{"analysis": "The training loss decreases rapidly and approaches zero within a few epochs, indicating that the model learns the training data effectively. However, the validation loss increases after the initial epochs, suggesting overfitting. The divergence between training and validation loss highlights a lack of generalization, which could be addressed by regularization techniques or data augmentation.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score quickly reaches nearly perfect performance, which aligns with the rapid decrease in training loss. However, the validation Macro-F1 score plateaus around 0.7 and does not improve significantly, further emphasizing the overfitting issue. The disparity between training and validation performance suggests that the model struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The confusion matrix shows a clear imbalance in the model's predictions. Although the model achieves some level of accuracy, it may struggle with certain classes or exhibit bias. This imbalance needs to be addressed to ensure fair and reliable predictions across all classes.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The class distribution plot indicates that the predicted class distribution closely matches the ground truth distribution. This suggests that the model performs well in capturing the overall class proportions. However, this does not necessarily mean that individual predictions are accurate, as other metrics and analyses (e.g., confusion matrix) must be considered.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_c219004876cc409c9fa9d94f75babbf7_proc_3469033/SPR_BENCH_class_distribution.png"}], [{"analysis": "The training loss decreases steadily over the epochs, reaching a very low value by the fourth epoch, indicating that the model is fitting the training data well. However, the validation loss increases significantly after the first epoch, suggesting overfitting. The model is not generalizing well to unseen data, as evidenced by the divergence between training and validation losses.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_loss_curve.png"}, {"analysis": "The training macro-F1 score increases rapidly and approaches a value close to 1.0, demonstrating that the model achieves near-perfect performance on the training set. However, the validation macro-F1 score remains relatively low and shows minimal improvement across epochs. This further confirms overfitting and indicates poor generalization to the validation set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix shows a significant disparity between the ground truth and predictions. The model appears to struggle with correctly classifying certain labels, as indicated by the uneven distribution of prediction probabilities. This suggests that the model's performance is biased or inconsistent across different classes.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ebd1630395fa4458acb60783ced9246e_proc_3469034/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss across epochs. The training loss decreases steadily and stabilizes, indicating that the model is learning effectively from the training data. However, the validation loss starts high, decreases slightly, and then increases again, suggesting potential overfitting after a few epochs. This behavior indicates that the model may not generalize well to unseen data, and regularization techniques like dropout or early stopping could help mitigate this issue.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot illustrates the macro-F1 scores for training and validation across epochs. The training macro-F1 score increases rapidly and stabilizes near 1.0, showcasing excellent performance on the training data. However, the validation macro-F1 score remains relatively stable but low, hovering around 0.7, which is approximately the state-of-the-art performance threshold. This indicates that while the model performs well on the training set, it struggles to generalize to the validation set, possibly due to overfitting or insufficient learning of the underlying rules.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The confusion matrix provides a visual representation of the model's predictions compared to the ground truth labels. The normalized values indicate that the model performs well on certain classes but struggles with others, as seen in the lighter shades in some parts of the matrix. This imbalance suggests that the model might be biased towards certain labels or that the dataset might have an uneven distribution of classes. Addressing this issue could involve rebalancing the dataset or adjusting the loss function to account for class imbalances.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a97da25972d4448aca44bafc55d978d_proc_3469034/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning effectively during training. However, the validation loss remains relatively high and does not decrease significantly after an initial drop, suggesting potential overfitting or insufficient generalization to the validation set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score quickly converges to 1.0, indicating excellent performance on the training data. However, the validation Macro-F1 score plateaus around 0.7, which, while better than random guessing, shows a gap in generalization performance between the training and validation datasets.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix indicates that the model performs well on both classes, as the diagonal elements dominate. However, there is still some level of misclassification, as indicated by the off-diagonal elements.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_conf_matrix.png"}, {"analysis": "The ground-truth label distribution is balanced, with an equal number of samples for each class. This balance ensures that the model is not biased due to class imbalance, and the performance metrics are reflective of the model's actual ability to generalize.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_label_distribution.png"}, {"analysis": "The final validation Macro-F1 score for SPR_BENCH is approximately 0.7, which aligns with the results observed in the training vs. validation Macro-F1 plot. This score indicates that while the model performs reasonably well, it has room for improvement to achieve state-of-the-art performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/cross_dataset_val_macroF1.png"}], [{"analysis": "This plot shows the training and validation loss curves over six epochs. The training loss decreases steadily, indicating that the model is learning effectively during training. However, the validation loss initially increases before decreasing, which could suggest some overfitting during the early epochs. The eventual decrease in validation loss indicates that the model generalizes better after a few epochs, but the gap between training and validation loss suggests that further regularization techniques might be needed to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot depicts the training and validation Macro-F1 scores over six epochs. The training Macro-F1 score improves rapidly and plateaus close to 1.0, indicating excellent performance on the training set. The validation Macro-F1 score also improves and stabilizes around 0.7, showing reasonable generalization to unseen data. However, the gap between training and validation scores suggests that the model might still be overfitting to the training data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix provides insights into the model's classification performance. The matrix shows a strong diagonal pattern, indicating that the model performs well in correctly predicting the true labels. However, there is still a non-negligible proportion of misclassifications, which could be addressed by further fine-tuning or by incorporating additional symbolic reasoning modules to handle challenging cases.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/SPR_BENCH_confusion_matrix.png"}, {"analysis": "This plot shows the Test Macro-F1 score as a single bar, indicating a value of approximately 0.7. This aligns with the validation performance observed earlier, suggesting that the model generalizes well to the test set. However, this score is at the benchmark state-of-the-art level, and achieving a higher score would require further architectural innovations or optimization strategies.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_16ece0618b654ee8bd69eb4e7a766f5f_proc_3469032/comparison_test_macroF1.png"}], [{"analysis": "The training loss decreases steadily and converges to near-zero, indicating that the model is learning effectively on the training data. However, the validation loss increases after an initial drop, suggesting overfitting. This implies that the model generalizes poorly to unseen data and may require regularization techniques or adjustments to the training process.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score quickly reaches near-perfect values, reflecting excellent performance on the training set. In contrast, the validation Macro-F1 score remains relatively stagnant and significantly lower, hovering around the benchmark state-of-the-art performance (70%). This further corroborates the overfitting issue observed in the loss plot. Efforts should be directed towards improving generalization, such as hyperparameter tuning or introducing symbolic reasoning enhancements.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_metric_curve.png"}, {"analysis": "The confusion matrix indicates a strong diagonal pattern, suggesting that the model performs well overall. However, the normalized values hint at potential class imbalance or difficulty in predicting certain classes. Analyzing the distribution of predictions and errors could provide insights into specific areas where the model struggles. This might also point towards the need for better handling of class imbalances or more representative training data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_251bdb3c925c494a930f51f8072ddfcc_proc_3469033/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows a clear divergence between training and validation loss as training progresses. The training loss decreases steadily and stabilizes around epoch 4, while the validation loss increases after an initial dip. This indicates potential overfitting of the model to the training data. Further regularization or data augmentation may help reduce overfitting.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_loss_curve.png"}, {"analysis": "The training macro-F1 score rapidly increases and plateaus near 1.0, indicating excellent performance on the training set. However, the validation macro-F1 score remains consistently low, showing minimal improvement over epochs. This suggests poor generalization to unseen data and reinforces the observation of overfitting.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix suggests that the model predictions are highly imbalanced. The generated samples do not align well with the ground truth, indicating that the model struggles to capture the underlying patterns in the data. This could be due to insufficient training or the complexity of the task.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_conf_matrix.png"}, {"analysis": "The ground-truth label distribution is balanced, with equal frequency for both labels. This ensures that the dataset is not inherently biased, and the observed performance issues are likely due to model limitations rather than data imbalance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/SPR_BENCH_label_distribution.png"}, {"analysis": "The final validation macro-F1 score for SPR_BENCH is significantly lower than the target state-of-the-art benchmark of 70%. This highlights a gap in the model's performance and suggests that further architectural improvements or training strategies are needed to achieve competitive results.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/cross_dataset_val_macroF1.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating successful learning by the model. However, the validation loss remains relatively high and almost flat after an initial increase, suggesting potential overfitting or difficulty in generalizing to the validation data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score improves rapidly and saturates close to 1.0, demonstrating that the model performs well on the training data. In contrast, the validation Macro-F1 score remains relatively stable around 0.7, indicating that the model struggles to generalize effectively to unseen data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix shows a strong diagonal pattern for both ground truth and generated samples, indicating that the model achieves reasonable accuracy in its predictions. However, the off-diagonal values suggest room for improvement in correctly classifying certain classes.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_conf_matrix.png"}, {"analysis": "The label distribution in the ground truth is balanced, with an equal frequency of labels. This ensures that the dataset does not introduce bias in favor of any particular class, making it suitable for evaluating the model's performance across all classes.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/SPR_BENCH_label_distribution.png"}, {"analysis": "The final validation Macro-F1 score comparison shows that the model achieves a score of approximately 0.7 on the SPR_BENCH dataset. This is below the desired state-of-the-art benchmark of 0.70, indicating that further improvements are needed, particularly in the model's generalization capabilities.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/cross_dataset_val_macroF1.png"}], [{"analysis": "This plot shows the training and validation loss over 10 epochs. The training loss decreases steadily, indicating that the model is learning effectively during training. However, the validation loss initially increases and then stabilizes, suggesting potential overfitting or a mismatch between the training and validation distributions. The gap between training and validation loss at the end of training indicates that the model may not generalize well to unseen data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot compares the training and validation Macro-F1 scores over 10 epochs. The training Macro-F1 improves rapidly and reaches near-perfect performance, whereas the validation Macro-F1 plateaus at a much lower level. This discrepancy further supports the observation of overfitting, as the model performs well on the training data but struggles to generalize to the validation set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix indicates that the model is better at predicting one class over the other. The imbalance in prediction accuracy suggests that the model might be biased or that the training process is not adequately capturing the complexities of the task.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_conf_matrix.png"}, {"analysis": "The ground-truth label distribution is balanced, with an equal frequency of both labels. This indicates that the dataset itself is not biased, and the observed performance issues are likely due to the model's architecture or training process rather than the data distribution.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/SPR_BENCH_label_distribution.png"}, {"analysis": "This plot compares the final validation Macro-F1 score across datasets. The performance on the SPR_BENCH dataset is below the state-of-the-art benchmark of 70.0%, indicating that the current model configuration is not yet competitive. This suggests the need for further optimization or architectural changes to improve performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/cross_dataset_val_macroF1.png"}], []], "vlm_feedback_summary": ["The results indicate that the model is learning effectively on the training data\nbut is overfitting, as evidenced by the disparity between training and\nvalidation performance. Validation loss remains high, and Macro-F1 scores show a\nsignificant gap between training and validation. The confusion matrix indicates\nbalanced but imperfect classification, suggesting room for improvement in\ngeneralization and reasoning capabilities.", "The experimental results indicate significant overfitting, as evidenced by the\ndivergence between training and validation metrics. The confusion matrix\nhighlights class imbalance in predictions, suggesting the need for dataset\nrebalancing or improved feature learning. Regularization techniques, early\nstopping, or architectural adjustments could improve generalization.", "The plots suggest that the model is overfitting to the training data, as\nevidenced by the increasing validation loss and flat validation Macro-F1 score\ndespite improved training performance. The confusion matrix highlights potential\nclass-specific prediction issues, while the class distribution plot shows that\nthe model captures the overall class distribution well but does not guarantee\nindividual prediction accuracy. Further experimentation is needed to address\noverfitting and improve generalization.", "The plots reveal significant overfitting issues, as evidenced by the divergence\nbetween training and validation performance. While the model learns the training\ndata effectively, its generalization to unseen data is limited. The class\ndistribution suggests reasonable balance, but the confusion matrix highlights\npotential biases or weaknesses in certain classes. Regularization techniques,\nimproved validation strategies, and a deeper analysis of the symbolic reasoning\nmodule's impact are recommended to address these challenges.", "The results indicate severe overfitting of the model to the training data, as\nevidenced by the divergence between training and validation metrics. While\ntraining performance is excellent, validation performance remains suboptimal,\nhighlighting poor generalization. The confusion matrix further reveals potential\nissues with class imbalance or misclassification, which need to be addressed to\nimprove overall model reliability.", "The results highlight that the model shows strong performance on the training\nset but struggles with generalization to the validation set, as evidenced by\nincreasing validation loss and stable but low validation macro-F1 scores. The\nconfusion matrix further indicates potential class imbalance or bias, which may\nrequire addressing to improve overall performance.", "The analysis highlights a general trend of effective training but limited\ngeneralization, as evidenced by the gap between training and validation\nperformance metrics. The balanced dataset ensures unbiased evaluation, but the\nmodel's validation performance suggests a need for further optimization or\narchitectural improvements to achieve state-of-the-art results.", "The experimental results indicate that the model performs well on the training\ndata and achieves reasonable generalization on the validation and test sets. The\ntraining and validation loss curves highlight some overfitting, while the\nMacro-F1 scores demonstrate stable performance. The confusion matrix confirms\ngood classification accuracy but reveals room for improvement in edge cases.\nOverall, the results are promising, but additional efforts are needed to surpass\nthe state-of-the-art benchmark.", "The results indicate that the model is overfitting, as evidenced by the\ndivergence between training and validation metrics. While the training\nperformance is excellent, validation performance stagnates, remaining close to\nthe benchmark state-of-the-art. The confusion matrix suggests overall good\nperformance but highlights areas for improvement in handling class imbalances or\ndifficult classes. Future experiments should focus on addressing overfitting and\nimproving generalization through methods like regularization, data augmentation,\nor enhanced model architecture.", "The provided plots reveal significant issues with model generalization, as\nevidenced by the divergence between training and validation metrics. Overfitting\nis a primary concern, and the model's inability to achieve competitive\nvalidation macro-F1 scores suggests that further experimentation with\nregularization, architecture, or training data is necessary.", "The results indicate that while the model performs well on training data, it\nstruggles to generalize effectively to validation data, as evidenced by the\ndisparity between training and validation metrics. The confusion matrix and\nlabel distribution suggest that the dataset is balanced and the model has\nreasonable accuracy but requires enhancements to improve generalization and\nachieve state-of-the-art performance.", "The provided plots indicate that the model is overfitting to the training data\nand has not achieved state-of-the-art performance on the SPR_BENCH dataset. The\ntraining process effectively minimizes loss and maximizes Macro-F1 on the\ntraining set, but the validation performance remains suboptimal. The balanced\ndataset distribution rules out data bias as a cause, pointing to model\nimprovements as the primary area of focus.", "[]"], "exec_time": [5.932438611984253, 7.795186996459961, 3.828740358352661, 5.07481050491333, 6.233036279678345, 8.106271505355835, 21.212788820266724, 5.1865150928497314, 15.116793632507324, 26.051698684692383, 22.387029886245728, 22.28656244277954, null], "exec_time_feedback": ["", "", "", "", "Implementation works but runs too quickly (0.08 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["[]"], ["[]"], ["[]"], ["[]"], ["[]"], ["['SPR_BENCH']"], ["<list(test_scores.keys())>"], ["[]"], ["[]"], ["[]"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------- Load experiment data & print test metric ---------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data, spr = None, None\n\nif spr:\n    print(f\"Test Macro-F1 (SPR_BENCH): {spr.get('test_macroF1', 'N/A'):.4f}\")\n\n    epochs = spr[\"epochs\"]\n    tr_loss = spr[\"losses\"][\"train\"]\n    val_loss = spr[\"losses\"][\"val\"]\n    tr_f1 = spr[\"metrics\"][\"train\"]\n    val_f1 = spr[\"metrics\"][\"val\"]\n    preds = spr.get(\"predictions\", [])\n    gts = spr.get(\"ground_truth\", [])\n\n    # ------------------------ Loss curve ---------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ------------------------ Macro-F1 curve -----------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 curve: {e}\")\n        plt.close()\n\n    # ------------------------ Confusion matrix ---------------------------\n    try:\n        from sklearn.metrics import confusion_matrix\n\n        if preds and gts:\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            plt.title(\n                \"SPR_BENCH: Normalized Confusion Matrix\\n(Left: Ground Truth, Right: Predictions)\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------- setup & load data -----------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------- per-dataset plots -----------------------------------\nval_curves = {}\nfor name, rec in experiment_data.items():\n    try:\n        epochs = rec[\"epochs\"]\n        tr_loss = rec[\"losses\"][\"train\"]\n        val_loss = rec[\"losses\"][\"val\"]\n        tr_f1 = rec[\"metrics\"][\"train\"]\n        val_f1 = rec[\"metrics\"][\"val\"]\n        preds = rec.get(\"predictions\", [])\n        gts = rec.get(\"ground_truth\", [])\n        test_f1 = rec.get(\"test_macroF1\", None)\n\n        if test_f1 is not None:\n            print(f\"{name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n        # 1) Loss curve\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{name}: Training vs Validation Loss\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{name}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error plotting loss for {name}: {e}\")\n            plt.close()\n\n        # 2) Macro-F1 curve\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n            plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{name}: Training vs Validation Macro-F1\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{name}_macroF1_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error plotting f1 for {name}: {e}\")\n            plt.close()\n\n        # 3) Confusion matrix\n        try:\n            if preds and gts:\n                cm = confusion_matrix(gts, preds, normalize=\"true\")\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im, fraction=0.046, pad=0.04)\n                plt.xlabel(\"Predicted Label\")\n                plt.ylabel(\"True Label\")\n                plt.title(\n                    f\"{name}: Normalized Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\"\n                )\n                plt.tight_layout()\n                plt.savefig(os.path.join(working_dir, f\"{name}_confusion_matrix.png\"))\n                plt.close()\n        except Exception as e:\n            print(f\"Error plotting confusion matrix for {name}: {e}\")\n            plt.close()\n\n        val_curves[name] = (epochs, val_f1)\n    except Exception as e:\n        print(f\"Error processing dataset {name}: {e}\")\n\n# ------------------- multi-dataset comparison ----------------------------\nif len(val_curves) >= 2:\n    try:\n        plt.figure()\n        for ds_name, (ep, f1s) in val_curves.items():\n            plt.plot(ep, f1s, label=f\"{ds_name} Val Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Validation Macro-F1 Comparison Across Datasets\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"comparison_val_macroF1.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting comparison curve: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- per-dataset plots ----------\ntest_scores = {}\nfor dname, d in experiment_data.items():\n    epochs = d.get(\"epochs\", [])\n    tr_loss = d[\"losses\"].get(\"train\", [])\n    val_loss = d[\"losses\"].get(\"val\", [])\n    tr_f1 = d[\"metrics\"].get(\"train\", [])\n    val_f1 = d[\"metrics\"].get(\"val\", [])\n    preds = d.get(\"predictions\", [])\n    gts = d.get(\"ground_truth\", [])\n    test_f1 = d.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        test_scores[dname] = test_f1\n        print(f\"{dname} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # ---- loss curve ----\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname}: Train vs Val Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {dname}: {e}\")\n        plt.close()\n\n    # ---- Macro-F1 curve ----\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dname}: Train vs Val Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 curve for {dname}: {e}\")\n        plt.close()\n\n    # ---- confusion matrix ----\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Normalized Confusion Matrix\\nLeft: GT, Right: Pred\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # ---- class distribution ----\n    try:\n        if preds and gts:\n            unique = sorted(set(gts + preds))\n            gt_cnt = [gts.count(u) for u in unique]\n            pr_cnt = [preds.count(u) for u in unique]\n            x = np.arange(len(unique))\n            plt.figure()\n            plt.bar(x - 0.2, gt_cnt, width=0.4, label=\"Ground Truth\")\n            plt.bar(x + 0.2, pr_cnt, width=0.4, label=\"Predictions\")\n            plt.xlabel(\"Class\")\n            plt.ylabel(\"# Samples\")\n            plt.title(f\"{dname}: Class Distribution\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_class_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating class distribution for {dname}: {e}\")\n        plt.close()\n\n# ---------- cross-dataset comparison ----------\nif len(test_scores) > 1:\n    try:\n        names, scores = zip(*test_scores.items())\n        plt.figure()\n        plt.bar(names, scores)\n        plt.ylabel(\"Test Macro-F1\")\n        plt.ylim(0, 1)\n        plt.title(\"Comparison of Test Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_test_macroF1_comparison.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating comparison plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------------- setup and data loading ------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ----------------------------- plotting --------------------------------\nfor dset_name, d in experiment_data.items():\n    try:\n        print(f\"{dset_name}  Test Macro-F1: {d.get('test_macroF1', np.nan):.4f}\")\n    except Exception as e:\n        print(f\"Could not read test metric for {dset_name}: {e}\")\n\n    epochs = d.get(\"epochs\", [])\n    tr_loss = d.get(\"losses\", {}).get(\"train\", [])\n    val_loss = d.get(\"losses\", {}).get(\"val\", [])\n    tr_f1 = d.get(\"metrics\", {}).get(\"train\", [])\n    val_f1 = d.get(\"metrics\", {}).get(\"val\", [])\n    preds = d.get(\"predictions\", [])\n    gts = d.get(\"ground_truth\", [])\n\n    # 1) Loss curve ------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dset_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dset_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"{dset_name} loss curve error: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curve --------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dset_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dset_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"{dset_name} Macro-F1 curve error: {e}\")\n        plt.close()\n\n    # 3) Confusion matrix ------------------------------------------------\n    if preds and gts:\n        try:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            plt.title(\n                f\"{dset_name}: Normalized Confusion Matrix\\n\"\n                \"Left: Ground Truth, Right: Predictions\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset_name}_confusion_matrix.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"{dset_name} confusion matrix error: {e}\")\n            plt.close()\n\n    # 4) Class distribution bar chart -----------------------------------\n    if preds and gts:\n        try:\n            from collections import Counter\n\n            true_cnt = Counter(gts)\n            pred_cnt = Counter(preds)\n            labels = sorted(set(list(true_cnt.keys()) + list(pred_cnt.keys())))\n            true_vals = [true_cnt.get(l, 0) for l in labels]\n            pred_vals = [pred_cnt.get(l, 0) for l in labels]\n\n            x = np.arange(len(labels))\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, true_vals, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, pred_vals, width, label=\"Predicted\")\n            plt.xlabel(\"Class Label\")\n            plt.ylabel(\"Count\")\n            plt.title(\n                f\"{dset_name}: Class Distribution\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.xticks(x, labels)\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dset_name}_class_distribution.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"{dset_name} class distribution error: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- container for comparison plot ----------\nval_curves = {}\n\n# ---------- per-dataset visualisation ----------\nfor dset, blob in experiment_data.items():\n    print(f\"{dset}  Test Macro-F1: {blob.get('test_macroF1', 'N/A'):.4f}\")\n    epochs = blob.get(\"epochs\", [])\n    tr_loss = blob.get(\"losses\", {}).get(\"train\", [])\n    val_loss = blob.get(\"losses\", {}).get(\"val\", [])\n    tr_f1 = blob.get(\"metrics\", {}).get(\"train\", [])\n    val_f1 = blob.get(\"metrics\", {}).get(\"val\", [])\n    preds = blob.get(\"predictions\", [])\n    gts = blob.get(\"ground_truth\", [])\n\n    # store for later comparison plot\n    if epochs and val_f1:\n        val_curves[dset] = (epochs, val_f1)\n\n    # ---------- loss curve ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dset}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {dset}: {e}\")\n        plt.close()\n\n    # ---------- macro-F1 curve ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dset}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dset}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating macro-F1 curve for {dset}: {e}\")\n        plt.close()\n\n    # ---------- confusion matrix ----------\n    try:\n        from sklearn.metrics import confusion_matrix\n\n        if preds and gts:\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            plt.title(\n                f\"{dset}: Normalized Confusion Matrix\\n\"\n                \"Left: Ground Truth, Right: Predictions\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n# ---------- comparison plot across datasets ----------\nif len(val_curves) > 1:\n    try:\n        plt.figure()\n        for dset, (ep, vf1) in val_curves.items():\n            plt.plot(ep, vf1, label=dset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Macro-F1\")\n        plt.title(\"Validation Macro-F1 Comparison Across Datasets\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"all_datasets_val_macroF1.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating comparison plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------ prepare paths ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ load experiment log ------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------ iterate over datasets ----------\nfor ds_name, ds in experiment_data.items():\n    test_f1 = ds.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n    epochs = ds.get(\"epochs\", [])\n    tr_loss = ds.get(\"losses\", {}).get(\"train\", [])\n    val_loss = ds.get(\"losses\", {}).get(\"val\", [])\n    tr_f1 = ds.get(\"metrics\", {}).get(\"train\", [])\n    val_f1 = ds.get(\"metrics\", {}).get(\"val\", [])\n    preds = ds.get(\"predictions\", [])\n    gts = ds.get(\"ground_truth\", [])\n\n    # -------- loss curve ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- macro-F1 curve ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 curve for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- confusion matrix ----------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            plt.title(\n                f\"{ds_name}: Normalized Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- per-dataset plots ----------\ntest_scores = {}\nfor dname, dct in experiment_data.items():\n    try:\n        epochs = dct[\"epochs\"]\n        tr_loss = dct[\"losses\"][\"train\"]\n        val_loss = dct[\"losses\"][\"val\"]\n        tr_f1 = dct[\"metrics\"][\"train\"]\n        val_f1 = dct[\"metrics\"][\"val\"]\n        preds = dct.get(\"predictions\", [])\n        gts = dct.get(\"ground_truth\", [])\n        test_macro = dct.get(\"test_macroF1\", None)\n        if test_macro is not None:\n            test_scores[dname] = test_macro\n            print(f\"{dname} \u2011 Test Macro-F1: {test_macro:.4f}\")\n    except Exception as e:\n        print(f\"Dataset {dname} missing keys: {e}\")\n        continue\n\n    # ---- 1. Loss curve ----\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {dname}: {e}\")\n        plt.close()\n\n    # ---- 2. Macro-F1 curve ----\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dname}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 curve for {dname}: {e}\")\n        plt.close()\n\n    # ---- 3. Confusion matrix ----\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{dname}: Normalized Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n# ---------- comparison bar chart ----------\ntry:\n    if test_scores:\n        plt.figure()\n        names = list(test_scores.keys())\n        scores = [test_scores[n] for n in names]\n        plt.bar(names, scores, color=\"skyblue\")\n        plt.ylabel(\"Test Macro-F1\")\n        plt.title(\"Dataset Comparison: Test Macro-F1\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"comparison_test_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nbest_val_overall = {}\n\nfor dname, d in experiment_data.items():\n    print(\n        f\"{dname}: Test {d.get('primary_metric','Macro-F1')} = {d.get('test_macroF1','N/A'):.4f}\"\n    )\n    epochs = d.get(\"epochs\", [])\n    tr_loss = d.get(\"losses\", {}).get(\"train\", [])\n    val_loss = d.get(\"losses\", {}).get(\"val\", [])\n    tr_met = d.get(\"metrics\", {}).get(\"train\", [])\n    val_met = d.get(\"metrics\", {}).get(\"val\", [])\n    preds = d.get(\"predictions\", [])\n    gts = d.get(\"ground_truth\", [])\n    best_val_overall[dname] = max(val_met) if val_met else np.nan\n\n    # -------- loss curve --------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {dname}: {e}\")\n        plt.close()\n\n    # -------- metric curve -------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_met, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_met, label=\"Val Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dname}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_metric_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting metric for {dname}: {e}\")\n        plt.close()\n\n    # -------- confusion matrix ---\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{dname}: Normalized Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {dname}: {e}\")\n        plt.close()\n\n# -------- comparison plot across datasets ----------\ntry:\n    if len(best_val_overall) > 1:\n        plt.figure()\n        names = list(best_val_overall.keys())\n        scores = [best_val_overall[n] for n in names]\n        plt.bar(names, scores)\n        plt.ylabel(\"Best Validation Macro-F1\")\n        plt.title(\"Dataset Comparison: Best Val Macro-F1\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_best_val_metric.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom math import sqrt\n\n# ---------------------------------------------------------\n# Prepare working directory\n# ---------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------\n# Paths to all experiment_data.npy files (provided)\n# ---------------------------------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d7213184e2734b10b3b559c2186faf68_proc_3469034/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_357ab890370d42178e3bcfa1e9dda187_proc_3469032/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_60f7274c4ba14a2fb3b2b0f5fdb3912b_proc_3469033/experiment_data.npy\",\n]\n\n# ---------------------------------------------------------\n# Load every experiment file\n# ---------------------------------------------------------\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp_data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# ---------------------------------------------------------\n# Aggregate runs by dataset\n# ---------------------------------------------------------\ndatasets_runs = {}  # {ds_name: [run_dict1, run_dict2, ...]}\nfor run in all_experiment_data:\n    for ds_name, ds_dict in run.items():\n        datasets_runs.setdefault(ds_name, []).append(ds_dict)\n\n# Will be used for cross-dataset comparison\ncross_final_val_mean = {}\ncross_final_val_se = {}\n\n# ---------------------------------------------------------\n# Iterate over datasets and aggregate plots\n# ---------------------------------------------------------\nfor ds_name, run_list in datasets_runs.items():\n    # Collect arrays for each metric\n    train_loss_curves, val_loss_curves = [], []\n    train_f1_curves, val_f1_curves = [], []\n\n    for r in run_list:\n        # Retrieve metrics if present\n        epochs = r.get(\"epochs\", [])\n        losses = r.get(\"losses\", {})\n        metrics = r.get(\"metrics\", {})\n\n        if epochs and losses.get(\"train\") and losses.get(\"val\"):\n            min_len = min(len(epochs), len(losses[\"train\"]), len(losses[\"val\"]))\n            train_loss_curves.append(np.array(losses[\"train\"][:min_len]))\n            val_loss_curves.append(np.array(losses[\"val\"][:min_len]))\n\n        if epochs and metrics.get(\"train\") and metrics.get(\"val\"):\n            min_len = min(len(epochs), len(metrics[\"train\"]), len(metrics[\"val\"]))\n            train_f1_curves.append(np.array(metrics[\"train\"][:min_len]))\n            val_f1_curves.append(np.array(metrics[\"val\"][:min_len]))\n\n    # -----------------------------------------------------\n    # Aggregated LOSS curve\n    # -----------------------------------------------------\n    try:\n        if train_loss_curves and val_loss_curves:\n            # Align all curves to the shortest one\n            min_len = min([c.shape[0] for c in train_loss_curves + val_loss_curves])\n            train_stack = np.vstack([c[:min_len] for c in train_loss_curves])\n            val_stack = np.vstack([c[:min_len] for c in val_loss_curves])\n            epochs_axis = np.arange(1, min_len + 1)\n\n            tr_mean = train_stack.mean(axis=0)\n            tr_se = train_stack.std(axis=0) / sqrt(train_stack.shape[0])\n            val_mean = val_stack.mean(axis=0)\n            val_se = val_stack.std(axis=0) / sqrt(val_stack.shape[0])\n\n            plt.figure()\n            plt.plot(epochs_axis, tr_mean, label=\"Train Loss (mean)\")\n            plt.fill_between(\n                epochs_axis,\n                tr_mean - tr_se,\n                tr_mean + tr_se,\n                alpha=0.3,\n                label=\"Train \u00b11SE\",\n            )\n            plt.plot(epochs_axis, val_mean, label=\"Validation Loss (mean)\")\n            plt.fill_between(\n                epochs_axis,\n                val_mean - val_se,\n                val_mean + val_se,\n                alpha=0.3,\n                label=\"Val \u00b11SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name}: Aggregated Training vs Validation Loss\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_aggregated_loss_curve.png\")\n            )\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------\n    # Aggregated Macro-F1 curve\n    # -----------------------------------------------------\n    try:\n        if train_f1_curves and val_f1_curves:\n            min_len = min([c.shape[0] for c in train_f1_curves + val_f1_curves])\n            train_stack = np.vstack([c[:min_len] for c in train_f1_curves])\n            val_stack = np.vstack([c[:min_len] for c in val_f1_curves])\n            epochs_axis = np.arange(1, min_len + 1)\n\n            tr_mean = train_stack.mean(axis=0)\n            tr_se = train_stack.std(axis=0) / sqrt(train_stack.shape[0])\n            val_mean = val_stack.mean(axis=0)\n            val_se = val_stack.std(axis=0) / sqrt(val_stack.shape[0])\n\n            plt.figure()\n            plt.plot(epochs_axis, tr_mean, label=\"Train Macro-F1 (mean)\")\n            plt.fill_between(\n                epochs_axis,\n                tr_mean - tr_se,\n                tr_mean + tr_se,\n                alpha=0.3,\n                label=\"Train \u00b11SE\",\n            )\n            plt.plot(epochs_axis, val_mean, label=\"Validation Macro-F1 (mean)\")\n            plt.fill_between(\n                epochs_axis,\n                val_mean - val_se,\n                val_mean + val_se,\n                alpha=0.3,\n                label=\"Val \u00b11SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{ds_name}: Aggregated Training vs Validation Macro-F1\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_aggregated_macroF1_curve.png\")\n            )\n            plt.close()\n\n            # store final val stats for cross-dataset bar chart\n            cross_final_val_mean[ds_name] = val_stack[:, -1].mean()\n            cross_final_val_se[ds_name] = val_stack[:, -1].std() / sqrt(\n                val_stack.shape[0]\n            )\n    except Exception as e:\n        print(f\"Error creating aggregated f1 plot for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------------------------------------------------\n# Cross-dataset comparison bar chart (final val Macro-F1)\n# ---------------------------------------------------------\ntry:\n    if cross_final_val_mean:\n        plt.figure(figsize=(6, max(2, len(cross_final_val_mean) * 0.4)))\n        names = list(cross_final_val_mean.keys())\n        means = np.array([cross_final_val_mean[n] for n in names])\n        errors = np.array([cross_final_val_se[n] for n in names])\n\n        y_pos = np.arange(len(names))\n        plt.barh(\n            y_pos,\n            means,\n            xerr=errors,\n            align=\"center\",\n            color=\"skyblue\",\n            ecolor=\"darkblue\",\n            capsize=4,\n        )\n        plt.yticks(y_pos, names)\n        plt.xlabel(\"Final Validation Macro-F1 (mean \u00b11SE)\")\n        plt.title(\"Aggregated Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"cross_dataset_val_macroF1_aggregated.png\")\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating cross-dataset comparison plot: {e}\")\n    plt.close()\n"], "plot_plan": ["This snippet loads the saved experiment_data, extracts epochs, losses, and macro-F1 scores for the SPR_BENCH dataset, prints the final test macro-F1, and produces three standard visualizations: (1) training vs. validation loss across epochs; (2) training vs. validation macro-F1 across epochs; and (3) a normalized confusion matrix on the test set.  Each figure is generated inside its own try-except block to guarantee robustness, titled clearly with the dataset name, and saved to working_dir using descriptive filenames.  All figures are closed after saving to avoid memory leaks.  If sklearn is unavailable the confusion-matrix plot is silently skipped.  No synthetic data are invented\u2014only values present in experiment_data.npy are used.  At most three figures are produced, well within the five-figure limit.  The code is concise, follows the mandated import order, and gracefully handles any missing file or plotting error while still printing the final evaluation metric.  Running it will leave the resulting PNG files in the working directory for easy inspection.", "We load experiment_data.npy, iterate through each dataset contained, and retrieve epochs, training/validation losses, Macro-F1 scores, predictions and ground-truth labels. For every dataset we generate three visuals inside independent try-except blocks: (1) train vs. validation loss curve, (2) train vs. validation Macro-F1 curve, and (3) a normalized confusion-matrix heat-map (if predictions exist). All figures are saved to working_dir with descriptive filenames that contain the dataset name and plot type, and each figure is explicitly closed after saving. We also print the final test Macro-F1 for every dataset. After looping over individual datasets we create one optional overlay plot comparing validation Macro-F1 curves across all datasets (only plotted if at least two datasets exist). The code adheres to the base imports, uses only data found in experiment_data.npy, limits the total number of plots, and keeps the implementation concise. No additional data are fabricated and every step is wrapped defensively to avoid crashes. Finally, the script can be executed as-is to regenerate the visuals and metrics.", "We will load experiment_data.npy, iterate through every dataset key stored, and for each one extract epochs, training/validation loss and Macro-F1, predictions, and ground-truth labels. For every dataset we create three independent plots\u2014loss curve, Macro-F1 curve, and normalized confusion matrix\u2014each wrapped in its own try-except block, titled clearly, saved to working_dir, and closed. While looping we also collect the final test Macro-F1 for an optional cross-dataset comparison; if two or more datasets exist we create a bar chart comparing their test scores (also inside try-except). A final optional per-dataset class-distribution bar plot is generated when predictions/labels are available. All figures follow a consistent \u201c<dataset>_<plotname>.png\u201d naming convention, use basic matplotlib, and respect the five-figure limit by skipping extra epochs if necessary (only plotting full curves once). The script prints the test Macro-F1 for every dataset so the user can immediately read performance. Everything lives in one concise, self-contained Python snippet starting with the required imports and working_dir definition.", "We will reload the saved experiment_data.npy, iterate through every dataset entry, and extract epochs, losses, Macro-F1 scores, predictions, and ground-truth labels. For each dataset we create up to four separate figures: (1) train/validation loss curves, (2) train/validation Macro-F1 curves, (3) a normalized confusion matrix on the test split, and (4) a bar chart comparing true versus predicted class frequencies. Each figure is wrapped in its own try-except block and always closed afterwards. File names embed the dataset name and plot type to stay descriptive and unique. We print the stored test Macro-F1 for quick inspection before plotting. The script is fully general, so if additional datasets are added to experiment_data.npy the same plots are produced for them automatically while respecting the \u201c\u22645 figures per dataset\u201d rule. All plots are saved in the working directory created earlier, and only information actually present in experiment_data.npy is used\u2014no dummy data are generated. Finally, the code is concise and uses only standard matplotlib and numpy.", "The solution loads the stored experiment_data.npy, iterates over every dataset inside it and extracts epochs, losses, macro-F1 scores, predictions and ground-truth labels. For each dataset it draws three standard figures\u2014loss curve, macro-F1 curve and (when predictions exist) a normalized confusion matrix\u2014each wrapped in its own try-except block to guarantee robustness and automatic figure closing. File names always contain the dataset tag plus the plot type and are written into the working directory created at the start. After the per-dataset plots, the script creates an additional comparison plot that overlays validation macro-F1 curves from all datasets so users can directly compare performance trajectories (only appearing if two or more datasets are present). Whenever data is missing the corresponding plot is skipped gracefully with an informative message. Finally the script prints out every dataset\u2019s test macro-F1 so the key metric is visible in the notebook / console. All matplotlib calls use the basic style; no invented values are generated\u2014everything comes strictly from experiment_data.npy. The whole plotting routine stays concise and self-contained inside a single Python file that can run after training finishes.", "We need to load the numpy dictionary written by the training script, loop over every dataset key inside it (currently only \u201cSPR_BENCH\u201d), and extract epochs, losses, macro-F1 scores, predictions and ground-truth labels.  \nFor each dataset we will: (1) print the stored test macro-F1, (2) plot training vs validation loss, (3) plot training vs validation macro-F1, and (4) \u2013 if predictions exist \u2013 plot a normalized confusion matrix.  \nEach figure is created in its own try-except block, saved into the pre-defined \u2018working\u2019 directory with a descriptive filename, and then immediately closed.  \nWe follow the base plotting style from the instructions, keep matplotlib defaults, and never invent data.  \nPlot titles include the dataset name and a short subtitle clarifying what is shown; for the confusion matrix we explicitly mention \u201cLeft: Ground Truth, Right: Predictions\u201d.  \nIf a dataset has more than, say, 30 epochs we will down-sample the epoch ticks to at most 5 points, but plotting all points is acceptable here because EPOCHS=6.  \nAll paths are assembled with os.path.join to remain platform-independent, and the working directory is created if missing.  \nFinally the script echoes the test macro-F1 for quick inspection and exits without errors when experiment_data.npy is absent.", "We load experiment_data.npy, gracefully handle missing files, and iterate over every dataset key present (currently only SPR_BENCH but the loop generalises). For each dataset we extract epochs, training/validation loss and macro-F1, predictions and ground-truth. We then create: (1) loss curves, (2) macro-F1 curves, (3) normalised confusion matrices if predictions are available, and (4) a simple class-distribution bar chart. Each plot is wrapped in its own try-except to ensure the script never crashes and all figures are closed. After the per-dataset plots, we build an across-dataset comparison bar chart of the final validation macro-F1 so that future runs with multiple datasets are visualised consistently. All figures are saved to working_dir with descriptive names. Finally, we print the test macro-F1 for every dataset for quick inspection.", "We will load experiment_data.npy, iterate over all dataset keys, and for each dataset extract epochs, train/val losses, macro-F1 scores, predictions, and ground-truth. Inside individual try-except blocks (one per figure) we create three plots per dataset: a loss curve, a macro-F1 curve, and a normalized confusion matrix (only if predictions are available). All figures get explicit titles mentioning the dataset and plot type; files are saved into the working directory with consistent names. After processing all datasets we build a single comparison bar chart showing the final test macro-F1 of every dataset so users can quickly gauge overall performance. Every plot is closed after saving to avoid memory leaks, and any plotting error is caught and reported without stopping subsequent plots. The code keeps the original numpy data structure, uses basic matplotlib only, and respects the \u22645-figures-per-dataset guidance. Finally, the script prints test Macro-F1 values for each dataset so metrics are visible in stdout.", "We will load the saved NumPy dictionary, iterate over every dataset entry and, for each, generate (1) a loss curve, (2) a primary-metric curve, and (3) a normalized confusion-matrix (only if predictions are present). All figures are wrapped in try-except blocks, explicitly closed, titled, and saved with descriptive names in working_dir. If multiple datasets exist we additionally create a bar plot comparing the best validation metric across datasets. The code prints the test metric for each dataset so users immediately see headline results.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved NumPy dictionary from the working directory,\niterate over every stored dataset (e.g., \u201cSPR_BENCH\u201d) and extract the lists of\nrecorded losses and F1 scores. It then selects the final epoch\u2019s values for\ntraining and validation statistics, as well as the stored test statistics.\nFinally, it prints the dataset name followed by clear, explicit metric names and\ntheir corresponding values. All code is placed at global scope so that it runs\nimmediately when the file is executed.", "The script will load the experiment data file from the working directory,\niterate over each dataset contained in the dictionary, compute (a) the final\ntraining macro-F1, (b) the best validation macro-F1, (c) the final training\nloss, (d) the best validation loss, plus the stored test macro-F1 and test loss.\nIt then prints each value with an explicit, self-descriptive label, preceded by\nthe dataset name. All code runs immediately at the top level, with no special\nentry point.", "The script loads the saved NumPy dictionary, iterates through each stored\ndataset, and extracts the last (i.e., final) entry for every logged metric. It\nthen prints the dataset name followed by clearly labelled values for the final\ntraining loss, final validation loss, final training Macro F1 score, final\nvalidation Macro F1 score, test loss, and test Macro F1 score. No plots are\ngenerated and the code runs immediately upon execution.", "The script will locate the saved NumPy file in the \u201cworking\u201d directory, load it\nas a Python dictionary, and iterate over every dataset it contains.   For each\ndataset, it inspects the recorded arrays of training/validation losses and\nmacro-F1 scores collected across epochs, selects the best value (minimum loss,\nmaximum F1), and fetches the final test metrics stored separately.   Each metric\nis printed with an explicit, descriptive label so that its meaning is\nunambiguous (e.g., \u201cvalidation loss\u201d instead of just \u201cval\u201d).   The code is\nwritten at global scope\u2014no `if __name__ == \"__main__\":` guard\u2014so it executes\nimmediately when run.", "We will load the saved NumPy file from the working directory, convert it back to\na Python dictionary, and iterate over every dataset key it contains (e.g.,\n\u201cSPR_BENCH\u201d).   For each dataset we will compute: the final training macro-F1\nscore and training loss (last epoch), the best validation macro-F1 score\n(maximum across epochs) and the best validation loss (minimum across epochs),\nand finally the stored test macro-F1 score and test loss.   Each value is\nprinted with a clear, explicit metric name so that there is no ambiguity, and no\nplots are created.", "The script loads the saved numpy file from the working directory, extracts the\ndictionary that contains all logged information, and then iterates through each\ndataset entry (e.g., \u201cSPR_BENCH\u201d). For every dataset it gathers the stored lists\nof training/validation losses and macro-F1 scores, as well as the separate test\nmetrics. It then prints the final training values, the best (min/ max)\nvalidation values, and the test values with clear, fully-qualified metric names.\nAll code runs immediately at import time and does not rely on any special entry\npoint.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", "The script will load the saved NumPy file from the working directory, parse its\nnested dict structure, locate the final-epoch training metrics and the epoch\nwith the highest validation macro-F1, and then print those values together with\nthe stored test results. It follows the naming conventions requested, prints one\ndataset at a time, and executes immediately when run.", "The script will locate the working directory, load the saved NumPy file, and\niterate over all datasets contained in the experiment_data dictionary. For each\ndataset it will compute the final training values (last epoch), the best\nvalidation values (min loss and max macro-F1), and read the recorded test\nvalues. It then prints the dataset name followed by each metric with clear,\ndescriptive labels. No plots are generated, and everything runs at global scope\nwithout any special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------- load experiment data -----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------- print metrics ------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Extract metric trajectories\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    train_f1s = data.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Safeguard for empty lists\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n    if train_f1s:\n        print(f\"Final training macro F1 score: {train_f1s[-1]:.4f}\")\n    if val_f1s:\n        print(f\"Final validation macro F1 score: {val_f1s[-1]:.4f}\")\n\n    # Test metrics (already single values)\n    if \"test_loss\" in data:\n        print(f\"Test loss: {data['test_loss']:.4f}\")\n    if \"test_macroF1\" in data:\n        print(f\"Test macro F1 score: {data['test_macroF1']:.4f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------------\n# 0. Locate and load the saved experiment data\n# --------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# --------------------------------------------------------------------------\n# 1. Iterate through datasets and report key metrics\n# --------------------------------------------------------------------------\nfor dataset_name, data_dict in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract sequences of metrics / losses\n    train_f1_history = data_dict[\"metrics\"][\"train\"]\n    val_f1_history = data_dict[\"metrics\"][\"val\"]\n    train_loss_hist = data_dict[\"losses\"][\"train\"]\n    val_loss_hist = data_dict[\"losses\"][\"val\"]\n\n    # Compute requested summaries\n    final_train_f1 = train_f1_history[-1] if train_f1_history else None\n    best_val_f1 = max(val_f1_history) if val_f1_history else None\n    final_train_loss = train_loss_hist[-1] if train_loss_hist else None\n    best_val_loss = min(val_loss_hist) if val_loss_hist else None\n    test_macro_f1 = data_dict.get(\"test_macroF1\", None)\n    test_loss = data_dict.get(\"test_loss\", None)\n\n    # ----------------------------------------------------------------------\n    # 2. Print results with explicit metric names\n    # ----------------------------------------------------------------------\n    if final_train_f1 is not None:\n        print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.4f}\")\n    if test_macro_f1 is not None:\n        print(f\"Test macro F1 score: {test_macro_f1:.4f}\")\n    if test_loss is not None:\n        print(f\"Test loss: {test_loss:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the stored experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# print the final values for each metric in each dataset\n# ------------------------------------------------------------------\nfor dataset_name, results in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # losses\n    final_training_loss = (\n        results[\"losses\"][\"train\"][-1] if results[\"losses\"][\"train\"] else None\n    )\n    final_validation_loss = (\n        results[\"losses\"][\"val\"][-1] if results[\"losses\"][\"val\"] else None\n    )\n\n    # macro-F1 metrics\n    final_training_macro_f1 = (\n        results[\"metrics\"][\"train\"][-1] if results[\"metrics\"][\"train\"] else None\n    )\n    final_validation_macro_f1 = (\n        results[\"metrics\"][\"val\"][-1] if results[\"metrics\"][\"val\"] else None\n    )\n\n    # print losses\n    if final_training_loss is not None:\n        print(f\"final training loss: {final_training_loss:.4f}\")\n    if final_validation_loss is not None:\n        print(f\"final validation loss: {final_validation_loss:.4f}\")\n\n    # print macro-F1 scores\n    if final_training_macro_f1 is not None:\n        print(f\"final training Macro F1 score: {final_training_macro_f1:.4f}\")\n    if final_validation_macro_f1 is not None:\n        print(f\"final validation Macro F1 score: {final_validation_macro_f1:.4f}\")\n\n    # test set statistics (if present)\n    if \"test_loss\" in results:\n        print(f\"test loss: {results['test_loss']:.4f}\")\n    if \"test_macroF1\" in results:\n        print(f\"test Macro F1 score: {results['test_macroF1']:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# iterate through datasets and report best/final metrics\nfor dataset_name, ds in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- training metrics --------------------------------------------------\n    train_f1s = ds.get(\"metrics\", {}).get(\"train\", [])\n    if train_f1s:\n        best_train_f1 = max(train_f1s)\n        print(f\"train macro F1 score: {best_train_f1:.4f}\")\n\n    train_losses = ds.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        best_train_loss = min(train_losses)\n        print(f\"train loss: {best_train_loss:.4f}\")\n\n    # ---- validation metrics ------------------------------------------------\n    val_f1s = ds.get(\"metrics\", {}).get(\"val\", [])\n    if val_f1s:\n        best_val_f1 = max(val_f1s)\n        print(f\"validation macro F1 score: {best_val_f1:.4f}\")\n\n    val_losses = ds.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"validation loss: {best_val_loss:.4f}\")\n\n    # ---- test metrics ------------------------------------------------------\n    if \"test_macroF1\" in ds:\n        print(f\"test macro F1 score: {ds['test_macroF1']:.4f}\")\n    if \"test_loss\" in ds:\n        print(f\"test loss: {ds['test_loss']:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load stored experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- extract and print metrics ----------\nfor dataset_name, ds in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # Final training metrics (last epoch)\n    final_train_f1 = ds[\"metrics\"][\"train\"][-1]\n    final_train_loss = ds[\"losses\"][\"train\"][-1]\n\n    # Best validation metrics across epochs\n    best_val_f1 = max(ds[\"metrics\"][\"val\"])\n    best_val_loss = min(ds[\"losses\"][\"val\"])\n\n    # Test metrics (recorded after selecting best model)\n    test_macro_f1 = ds.get(\"test_macroF1\", None)\n    test_loss = ds.get(\"test_loss\", None)\n\n    # Print all metrics with explicit names\n    print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n    if test_macro_f1 is not None:\n        print(f\"Test macro F1 score: {test_macro_f1:.4f}\")\n    if test_loss is not None:\n        print(f\"Test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Utility helpers\n# ------------------------------------------------------------------\ndef best_idx(vals, mode=\"max\"):\n    \"\"\"Return index of best value according to mode ('max' or 'min').\"\"\"\n    return vals.index(max(vals)) if mode == \"max\" else vals.index(min(vals))\n\n\n# ------------------------------------------------------------------\n# Print metrics\n# ------------------------------------------------------------------\nfor dataset_name, ds in experiment_data.items():\n    print(dataset_name)  # Requirement 3\n\n    # Extract series\n    train_f1_series = ds[\"metrics\"][\"train\"]\n    val_f1_series = ds[\"metrics\"][\"val\"]\n    train_loss_series = ds[\"losses\"][\"train\"]\n    val_loss_series = ds[\"losses\"][\"val\"]\n\n    # Final values\n    final_train_f1 = train_f1_series[-1]\n    final_train_loss = train_loss_series[-1]\n\n    # Best validation values\n    best_val_f1 = max(val_f1_series)\n    best_val_loss = min(val_loss_series)\n\n    # Test metrics\n    test_f1 = ds.get(\"test_macroF1\", None)\n    test_loss = ds.get(\"test_loss\", None)\n\n    # Printing \u2013 Requirement 4\n    print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n    print(f\"Test macro F1 score: {test_f1:.4f}\" if test_f1 is not None else \"\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n    print(f\"Test loss: {test_loss:.4f}\" if test_loss is not None else \"\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# 1. Iterate over datasets and print requested metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract arrays ---------------------------------------------------\n    train_f1_history = data[\"metrics\"][\"train\"]\n    val_f1_history = data[\"metrics\"][\"val\"]\n    train_loss_hist = data[\"losses\"][\"train\"]\n    val_loss_hist = data[\"losses\"][\"val\"]\n\n    # Final epoch values ----------------------------------------------\n    final_train_macro_f1 = train_f1_history[-1]\n    final_train_loss = train_loss_hist[-1]\n\n    # Best validation epoch -------------------------------------------\n    best_val_idx = int(np.argmax(val_f1_history))\n    best_val_macro_f1 = val_f1_history[best_val_idx]\n    val_loss_at_best_f1 = val_loss_hist[best_val_idx]\n\n    # Test set values -------------------------------------------------\n    test_macro_f1 = data.get(\"test_macroF1\", None)\n    test_loss = data.get(\"test_loss\", None)\n\n    # -----------------------------------------------------------------\n    # 2. Print metrics with explicit names\n    print(f\"Final train macro F1 score: {final_train_macro_f1:.4f}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n\n    print(f\"Best validation macro F1 score: {best_val_macro_f1:.4f}\")\n    print(f\"Validation loss at best macro F1 score: {val_loss_at_best_f1:.4f}\")\n\n    # Only print test metrics if they exist\n    if test_macro_f1 is not None:\n        print(f\"Test macro F1 score: {test_macro_f1:.4f}\")\n    if test_loss is not None:\n        print(f\"Test loss: {test_loss:.4f}\")\n\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the stored experiment results\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment data at {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper: pretty-print a single metric if it exists\n# ---------------------------------------------------------------------\ndef _print_metric(label: str, value):\n    if value is None:\n        print(f\"{label}: N/A\")\n    else:\n        # Try to format as float; fall back to raw representation\n        try:\n            print(f\"{label}: {float(value):.4f}\")\n        except Exception:\n            print(f\"{label}: {value}\")\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and report metrics\n# ---------------------------------------------------------------------\nfor dataset_name, record in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # --- training metrics (final epoch) ---\n    final_train_loss = (\n        record.get(\"losses\", {}).get(\"train\", [])[-1]\n        if record.get(\"losses\", {}).get(\"train\")\n        else None\n    )\n    final_train_f1 = (\n        record.get(\"metrics\", {}).get(\"train\", [])[-1]\n        if record.get(\"metrics\", {}).get(\"train\")\n        else None\n    )\n\n    _print_metric(\"Final training loss\", final_train_loss)\n    _print_metric(\"Final training macro F1 score\", final_train_f1)\n\n    # --- validation metrics (best across epochs) ---\n    val_losses = record.get(\"losses\", {}).get(\"val\", [])\n    val_f1s = record.get(\"metrics\", {}).get(\"val\", [])\n\n    best_val_loss = min(val_losses) if val_losses else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n\n    _print_metric(\"Best validation loss\", best_val_loss)\n    _print_metric(\"Best validation macro F1 score\", best_val_f1)\n\n    # --- test metrics ---\n    _print_metric(\"Test loss\", record.get(\"test_loss\"))\n    _print_metric(\"Test macro F1 score\", record.get(\"test_macroF1\"))\n\n    # Spacer for readability when multiple datasets are present\n    print()\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.0637', '\\n', 'Final\nvalidation loss: 1.9662', '\\n', 'Final training macro F1 score: 0.9810', '\\n',\n'Final validation macro F1 score: 0.6920', '\\n', 'Test loss: 1.9695', '\\n',\n'Test macro F1 score: 0.6958', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training macro F1 score: 0.9860', '\\n',\n'Best validation macro F1 score: 0.6939', '\\n', 'Final training loss: 0.0463',\n'\\n', 'Best validation loss: 0.7411', '\\n', 'Test macro F1 score: 0.6958', '\\n',\n'Test loss: 1.9537', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.0825', '\\n', 'final\nvalidation loss: 1.7767', '\\n', 'final training Macro F1 score: 0.9795', '\\n',\n'final validation Macro F1 score: 0.6738', '\\n', 'test loss: 1.7194', '\\n',\n'test Macro F1 score: 0.6946', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train macro F1 score: 0.9955', '\\n', 'train loss: 0.0227',\n'\\n', 'validation macro F1 score: 0.6980', '\\n', 'validation loss: 0.6493',\n'\\n', 'test macro F1 score: 0.6969', '\\n', 'test loss: 2.0359', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training macro F1 score: 0.9665', '\\n', 'Final\ntraining loss: 0.1127', '\\n', 'Best validation macro F1 score: 0.6780', '\\n',\n'Best validation loss: 1.1687', '\\n', 'Test macro F1 score: 0.6839', '\\n', 'Test\nloss: 1.6238', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'Final training macro F1 score: 0.9725', '\\n', 'Best\nvalidation macro F1 score: 0.6980', '\\n', 'Test macro F1 score: 0.6854', '\\n',\n'Final training loss: 0.1063', '\\n', 'Best validation loss: 1.1831', '\\n', 'Test\nloss: 1.6498', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '1.0000',\n'\\n', '  final training macro F1 score:', ' ', '1.0000', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.7000', '\\n', '  final validation macro F1\nscore:', ' ', '0.7000', '\\n', '  lowest training loss:', ' ', '0.3282', '\\n', '\nfinal training loss:', ' ', '0.3282', '\\n', '  lowest validation loss:', ' ',\n'0.7373', '\\n', '  final validation loss:', ' ', '0.8694', '\\n', '  test macro\nF1 score:', ' ', '0.6999', '\\n', '  test loss:', ' ', '0.8654', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final train macro F1 score: 0.9930', '\\n', 'Final\ntrain loss: 0.3353', '\\n', 'Best validation macro F1 score: 0.6960', '\\n',\n'Validation loss at best macro F1 score: 0.9542', '\\n', 'Test macro F1 score:\n0.6959', '\\n', 'Test loss: 0.9454', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training loss: 0.0190', '\\n', 'Final\ntraining macro F1 score: 0.9965', '\\n', 'Best validation loss: 0.9567', '\\n',\n'Best validation macro F1 score: 0.7000', '\\n', 'Test loss: 1.6184', '\\n', 'Test\nmacro F1 score: 0.6939', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '0.9945',\n'\\n', '  final training macro F1 score:', ' ', '0.9945', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.6980', '\\n', '  final validation macro F1\nscore:', ' ', '0.6960', '\\n', '  lowest training loss:', ' ', '0.3365', '\\n', '\nfinal training loss:', ' ', '0.3365', '\\n', '  lowest validation loss:', ' ',\n'0.7388', '\\n', '  final validation loss:', ' ', '0.8754', '\\n', '  test macro\nF1 score:', ' ', '0.6969', '\\n', '  test loss:', ' ', '0.8716', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '0.9970',\n'\\n', '  final training macro F1 score:', ' ', '0.9970', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.6960', '\\n', '  final validation macro F1\nscore:', ' ', '0.6960', '\\n', '  lowest training loss:', ' ', '0.3330', '\\n', '\nfinal training loss:', ' ', '0.3330', '\\n', '  lowest validation loss:', ' ',\n'0.6610', '\\n', '  final validation loss:', ' ', '0.8716', '\\n', '  test macro\nF1 score:', ' ', '0.6959', '\\n', '  test loss:', ' ', '0.8684', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '0.9960',\n'\\n', '  final training macro F1 score:', ' ', '0.9960', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.6980', '\\n', '  final validation macro F1\nscore:', ' ', '0.6980', '\\n', '  lowest training loss:', ' ', '0.3346', '\\n', '\nfinal training loss:', ' ', '0.3346', '\\n', '  lowest validation loss:', ' ',\n'0.6595', '\\n', '  final validation loss:', ' ', '0.8781', '\\n', '  test macro\nF1 score:', ' ', '0.6939', '\\n', '  test loss:', ' ', '0.8766', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
