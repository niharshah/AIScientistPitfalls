
% Papers #2 and #4 provide foundational insights into neural-symbolic integration. Paper #4 surveys methodologies for combining symbolic reasoning and neural networks, offering a principled framework for explainable AI, which aligns with the goal of integrating symbolic reasoning modules into Transformer architectures. Paper #2 elaborates on the promises and challenges of neural-symbolic integration, particularly focusing on bridging symbolic and subsymbolic paradigms, which is directly relevant to the research aims. These citations will be used to establish the theoretical background for neural-symbolic integration in the introduction and related work sections of the paper.
@article{hitzler2020neuralsymbolicia,
 author = {P. Hitzler and Federico Bianchi and Monireh Ebrahimi and Md Kamruzzaman Sarker},
 booktitle = {Semantic Web},
 journal = {Semantic Web},
 pages = {3-11},
 title = {Neural-symbolic integration and the Semantic Web},
 volume = {11},
 year = {2020}
}

@article{garcez2019neuralsymbolicca,
 author = {A. Garcez and M. Gori and L. Lamb and L. Serafini and Michael Spranger and S. Tran},
 booktitle = {FLAP},
 journal = {FLAP},
 pages = {611-632},
 title = {Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning},
 volume = {6},
 year = {2019}
}

% This paper discusses the integration of transformer-based language models with symbolic formulas for enhancing reasoning in tasks involving first-order logic. It highlights the challenges transformers face in learning semantic information from logical expressions and proposes a method to improve their reasoning capabilities. This citation will be used to support the related work section, emphasizing the application of transformers in symbolic reasoning tasks and highlighting the gaps addressed by the current research.
@article{sheng2024integratinglm,
 author = {Yu Sheng and Linjing Li and Yifei Wang and D. Zeng},
 booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
 journal = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 pages = {11586-11590},
 title = {Integrating Language Models with Symbolic Formulas for First-Order Logic Reasoning},
 year = {2024}
}

% The KANDY Benchmark paper introduces a framework for symbolic reasoning tasks, providing insights into challenges related to symbolic compositionality and the integration of neuro-symbolic methods. This is highly relevant to contextualizing the SPR_BENCH dataset and the challenges addressed in the proposed research. It should be cited in the dataset and related work sections to highlight existing benchmarks for symbolic reasoning.
@article{lorello2024thekb,
 author = {Luca Salvatore Lorello and Marco Lippi and S. Melacci},
 booktitle = {Machine-mediated learning},
 journal = {Mach. Learn.},
 pages = {161},
 title = {The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns},
 volume = {114},
 year = {2024}
}

% The paper 'Transformer Interpretability Beyond Attention Visualization' introduces a novel method for explaining transformer models by assigning local relevance using Deep Taylor Decomposition principles and propagating relevancy scores through attention layers. This is directly relevant to the interpretability analysis in the proposed research, particularly for understanding attention visualization and the decision-making processes of augmented transformer models. It should be cited in the experiments section to support the interpretability methods used in analyzing the models' decisions.
@article{chefer2020transformerib,
 author = {Hila Chefer and Shir Gur and Lior Wolf},
 booktitle = {Computer Vision and Pattern Recognition},
 journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {782-791},
 title = {Transformer Interpretability Beyond Attention Visualization},
 year = {2020}
}

% Paper 0, 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models,' investigates the limitations of transformer models in logical reasoning tasks, which provides critical context to the generalization challenges in symbolic reasoning discussed in the proposed research. Paper 2, 'Symbolic Working Memory Enhances Language Models for Complex Rule Application,' introduces a symbolic working memory framework to improve multi-step rule application in language models, directly aligning with the symbolic reasoning integration explored in this work. Both papers will be cited in the related work section to highlight existing challenges and complementary approaches in symbolic reasoning with transformers.
@article{pirozelli2023assessinglr,
 author = {Paulo Pirozelli and M. M. Jos'e and Paulo de Tarso P. Filho and A. Brand√£o and F. G. Cozman},
 booktitle = {International Workshop on Neural-Symbolic Learning and Reasoning},
 pages = {29-46},
 title = {Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models},
 year = {2023}
}

@article{wang2024symbolicwm,
 author = {Siyuan Wang and Zhongyu Wei and Yejin Choi and Xiang Ren},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 journal = {ArXiv},
 title = {Symbolic Working Memory Enhances Language Models for Complex Rule Application},
 volume = {abs/2408.13654},
 year = {2024}
}
