import matplotlib.pyplot as plt
import numpy as np
import os
from math import sqrt

# mandatory working dir
working_dir = os.path.join(os.getcwd(), "working")
os.makedirs(working_dir, exist_ok=True)

# ------------------------------------------------------------------
# 1) Load all experiment_data.npy files that were generated by runs
# ------------------------------------------------------------------
experiment_data_path_list = [
    "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2e10d3d94ff3484884a15718e2f33ea2_proc_3458557/experiment_data.npy",
    "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_633cab2785164d228c48a28784cc6fd8_proc_3458558/experiment_data.npy",
    "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_00f8a92c851e4f53ab3c32aa3d093198_proc_3458559/experiment_data.npy",
]

all_experiment_data = []
for pth in experiment_data_path_list:
    try:
        full_path = os.path.join(os.getenv("AI_SCIENTIST_ROOT", ""), pth)
        data = np.load(full_path, allow_pickle=True).item()
        all_experiment_data.append(data)
    except Exception as e:
        print(f"Error loading {pth}: {e}")

n_runs = len(all_experiment_data)
if n_runs == 0:
    print("No experiment files loaded; exiting.")
    exit()

# ------------------------------------------------------------------
# 2) Iterate over datasets and aggregate metrics across runs
# ------------------------------------------------------------------
# Use dataset keys from the first successfully loaded run
reference_dset_names = all_experiment_data[0].keys()

for dset_name in reference_dset_names:
    # gather arrays across runs; skip run if dataset missing
    train_losses_runs, val_losses_runs = [], []
    train_f1s_runs, val_f1s_runs = [], []
    epoch_vector = None  # we will assume epochs are identical across runs

    for run_idx, run_data in enumerate(all_experiment_data):
        if dset_name not in run_data:
            print(f"Dataset {dset_name} not found in run {run_idx}, skipping that run.")
            continue
        dset_data = run_data[dset_name]

        # extract per-epoch metrics
        train_losses = [d["loss"] for d in dset_data["losses"]["train"]]
        val_losses = [d["loss"] for d in dset_data["losses"]["val"]]
        train_f1s = [d["macro_f1"] for d in dset_data["metrics"]["train"]]
        val_f1s = [d["macro_f1"] for d in dset_data["metrics"]["val"]]
        epochs = [d["epoch"] for d in dset_data["metrics"]["train"]]

        if epoch_vector is None:
            epoch_vector = np.array(epochs)
        # ensure equal length, otherwise pad with NaNs
        max_len = len(epoch_vector)

        def pad(a):
            if len(a) < max_len:
                return a + [np.nan] * (max_len - len(a))
            return a[:max_len]

        train_losses_runs.append(pad(train_losses))
        val_losses_runs.append(pad(val_losses))
        train_f1s_runs.append(pad(train_f1s))
        val_f1s_runs.append(pad(val_f1s))

    # convert to numpy for easier math
    train_losses_runs = np.array(train_losses_runs, dtype=float)
    val_losses_runs = np.array(val_losses_runs, dtype=float)
    train_f1s_runs = np.array(train_f1s_runs, dtype=float)
    val_f1s_runs = np.array(val_f1s_runs, dtype=float)

    # compute mean and SEM (ignoring NaNs)
    mean_train_loss = np.nanmean(train_losses_runs, axis=0)
    sem_train_loss = np.nanstd(train_losses_runs, axis=0) / np.sqrt(
        np.sum(~np.isnan(train_losses_runs), axis=0)
    )

    mean_val_loss = np.nanmean(val_losses_runs, axis=0)
    sem_val_loss = np.nanstd(val_losses_runs, axis=0) / np.sqrt(
        np.sum(~np.isnan(val_losses_runs), axis=0)
    )

    mean_train_f1 = np.nanmean(train_f1s_runs, axis=0)
    sem_train_f1 = np.nanstd(train_f1s_runs, axis=0) / np.sqrt(
        np.sum(~np.isnan(train_f1s_runs), axis=0)
    )

    mean_val_f1 = np.nanmean(val_f1s_runs, axis=0)
    sem_val_f1 = np.nanstd(val_f1s_runs, axis=0) / np.sqrt(
        np.sum(~np.isnan(val_f1s_runs), axis=0)
    )

    # ----------------------------------------------------------
    # 2a) Aggregated Loss plot with SEM shading
    # ----------------------------------------------------------
    try:
        plt.figure()
        plt.plot(
            epoch_vector, mean_train_loss, label="Train Loss (mean)", color="tab:blue"
        )
        plt.fill_between(
            epoch_vector,
            mean_train_loss - sem_train_loss,
            mean_train_loss + sem_train_loss,
            color="tab:blue",
            alpha=0.2,
            label="Train SEM",
        )
        plt.plot(
            epoch_vector, mean_val_loss, label="Val Loss (mean)", color="tab:orange"
        )
        plt.fill_between(
            epoch_vector,
            mean_val_loss - sem_val_loss,
            mean_val_loss + sem_val_loss,
            color="tab:orange",
            alpha=0.2,
            label="Val SEM",
        )
        plt.title(
            f"{dset_name} Aggregated Loss Curves\n(mean ± SEM across {n_runs} runs)"
        )
        plt.xlabel("Epoch")
        plt.ylabel("Cross-Entropy Loss")
        plt.legend()
        plt.tight_layout()
        fname = os.path.join(working_dir, f"{dset_name}_agg_loss_curves.png")
        plt.savefig(fname)
        plt.close()
    except Exception as e:
        print(f"Error creating aggregated loss plot for {dset_name}: {e}")
        plt.close()

    # ----------------------------------------------------------
    # 2b) Aggregated Macro-F1 plot with SEM shading
    # ----------------------------------------------------------
    try:
        plt.figure()
        plt.plot(
            epoch_vector,
            mean_train_f1,
            label="Train Macro-F1 (mean)",
            color="tab:green",
        )
        plt.fill_between(
            epoch_vector,
            mean_train_f1 - sem_train_f1,
            mean_train_f1 + sem_train_f1,
            color="tab:green",
            alpha=0.2,
            label="Train SEM",
        )
        plt.plot(
            epoch_vector, mean_val_f1, label="Val Macro-F1 (mean)", color="tab:red"
        )
        plt.fill_between(
            epoch_vector,
            mean_val_f1 - sem_val_f1,
            mean_val_f1 + sem_val_f1,
            color="tab:red",
            alpha=0.2,
            label="Val SEM",
        )
        plt.title(
            f"{dset_name} Aggregated Macro-F1 Curves\n(mean ± SEM across {n_runs} runs)"
        )
        plt.xlabel("Epoch")
        plt.ylabel("Macro-F1")
        plt.legend()
        plt.tight_layout()
        fname = os.path.join(working_dir, f"{dset_name}_agg_f1_curves.png")
        plt.savefig(fname)
        plt.close()
    except Exception as e:
        print(f"Error creating aggregated F1 plot for {dset_name}: {e}")
        plt.close()

    # ----------------------------------------------------------
    # 2c) Print final epoch validation Macro-F1 mean ± SEM
    # ----------------------------------------------------------
    if len(mean_val_f1) > 0:
        final_epoch = -1
        final_mean = mean_val_f1[final_epoch]
        final_sem = sem_val_f1[final_epoch]
        print(
            f"{dset_name}: final-epoch validation Macro-F1 = {final_mean:.4f} ± {final_sem:.4f} (SEM)"
        )
