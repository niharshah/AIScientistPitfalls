
% The paper 'Attention is All You Need' introduces the Transformer architecture, which forms the foundation for the proposed transformer-based model in this research. It should be cited when discussing the theoretical and architectural basis of Transformers and their application to symbolic reasoning tasks.
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}

% The paper 'Neural Turing Machines' by Alex Graves et al. (2014) introduces a foundational framework for integrating neural networks with external memory resources, enabling the inference of algorithms such as copying, sorting, and associative recall. It is a seminal work in neural-symbolic integration and should be cited when summarizing the potential of combining neural networks with symbolic systems, especially in the context of algorithmic reasoning and memory-augmented models.
@article{graves2014neuraltm,
 author = {Alex Graves and Greg Wayne and Ivo Danihelka},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Neural Turing Machines},
 volume = {abs/1410.5401},
 year = {2014}
}

% The paper 'Pretrained Language Models are Symbolic Mathematics Solvers too!' (Noorbakhsh et al., 2021) demonstrates how transformer-based language models can be pre-trained on language translation tasks and fine-tuned to solve symbolic mathematics problems with improved sample efficiency. This work is relevant to discussions of transformers' capabilities in symbolic reasoning tasks, providing experimental evidence for their application in solving complex symbolic problems. It should be cited when discussing the state-of-the-art methods in symbolic reasoning and the potential of leveraging pretraining to improve performance in such tasks.
@article{noorbakhsh2021pretrainedlm,
 author = {Kimia Noorbakhsh and Modar Sulaiman and M. Sharifi and Kallol Roy and Pooyan Jamshidi},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Pretrained Language Models are Symbolic Mathematics Solvers too!},
 volume = {abs/2110.03501},
 year = {2021}
}

% Paper 0 introduces FinChain, a symbolic benchmark for verifiable chain-of-thought financial reasoning, highlighting the importance of systematic evaluation in symbolic reasoning tasks. Paper 4 discusses a proposal for common datasets in neural-symbolic reasoning studies, emphasizing the need for standardization and benchmarking in this area. These works are relevant for contextualizing the SPR_BENCH dataset and its role in advancing symbolic reasoning benchmarks.
@article{xie2025finchainas,
 author = {Zhuohan Xie and Dhruv Sahnan and Debopriyo Banerjee and Georgi N. Georgiev and Rushil Thareja and Hachem Madmoun and Jinyan Su and Aaryamonvikram Singh and Yuxia Wang and Rui Xing and Fajri Koto and Haonan Li and Ivan Koychev and Tanmoy Chakraborty and Salem Lahlou and Veselin Stoyanov and Preslav Nakov},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning},
 volume = {abs/2506.02515},
 year = {2025}
}

@article{ylmaz2016apf,
 author = {Özgür Yılmaz and A. Garcez and Daniel L. Silver},
 booktitle = {NeSy@HLAI},
 title = {A Proposal for Common Dataset in Neural-Symbolic Reasoning Studies},
 year = {2016}
}

% The paper 'A Symbolic Rule Integration Framework with Logic Transformer for Inductive Relation Prediction' introduces a framework that integrates symbolic rule reasoning into a transformer-based architecture. This work is particularly relevant for our research as it provides methodologies and insights for embedding symbolic reasoning capabilities within transformers, a key component of our proposed model. It should be cited when discussing the design and integration of symbolic reasoning modules into transformer architectures.
@article{pan2024asr,
 author = {Yudai Pan and Jun Liu and Tianzhe Zhao and Lingling Zhang and Yun Lin and J. Dong},
 booktitle = {The Web Conference},
 journal = {Proceedings of the ACM Web Conference 2024},
 title = {A Symbolic Rule Integration Framework with Logic Transformer for Inductive Relation Prediction},
 year = {2024}
}
