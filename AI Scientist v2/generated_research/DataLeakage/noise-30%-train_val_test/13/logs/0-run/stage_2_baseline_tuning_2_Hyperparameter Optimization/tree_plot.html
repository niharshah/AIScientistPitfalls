<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[5, 9], [5, 11], [5, 8], [5, 10]], "layout": [[0.0, 0.0], [0.14285714285714285, 0.0], [0.2857142857142857, 0.0], [0.42857142857142855, 0.0], [0.5714285714285714, 0.0], [0.7142857142857143, 0.0], [0.8571428571428571, 0.0], [1.0, 0.0], [0.5, 1.0], [0.6428571428571429, 1.0], [0.7857142857142857, 1.0], [0.9285714285714286, 1.0]], "plan": ["Hyperparam tuning name: num_epochs. We will tune the single hyper-parameter\n\u201cnum_epochs\u201d: increase the maximum epochs to 30 while monitoring validation\nmacro-F1 and stopping early if it fails to improve for five consecutive epochs.\nNo other hyper-parameters (e.g., learning-rate) are touched, so training simply\nproceeds longer, letting the 1e-3 Adam optimizer converge further. All\nintermediate metrics, losses, predictions, etc. are stored under the key\n\"num_epochs\" and finally saved to experiment_data.npy.", "Hyperparam tuning name: learning_rate. We introduce a simple learning-rate grid\nsearch: for every candidate in {3e-4, 5e-4, 7e-4, 1.5e-3, 2e-3} we re-initialise\nthe model and train it for eight epochs, recording train/validation losses and\nmacro-F1 scores. After each run we evaluate on the test split and store all\nplottable data under experiment_data['lr_search']['SPR_BENCH'] so that results\nacross learning-rates are comparable. The rest of the pipeline (dataset loading,\nmodel, training loop) is unchanged; only the optimiser\u2019s lr is varied.\nEverything is saved to experiment_data.npy for later analysis.", "Hyperparam tuning name: batch_size. We add a loop that trains a fresh model for\neach candidate batch size (32, 64, 128, 256, 512).   For every run we build new\nDataLoaders with the chosen batch size, train the network for a few epochs,\nevaluate after each epoch, and finally evaluate on the test split.   All per-run\nlosses, F1 scores, predictions and ground-truth labels are stored inside a\nhierarchical experiment_data dictionary under the key \"batch_size_tuning\" \u2192\n\"SPR_BENCH\" \u2192 \"<batch_size>\".   The whole script is self-contained, releases GPU\nmemory between runs and saves the collected data to experiment_data.npy at the\nend.", "Hyperparam tuning name: d_model (embedding/hidden size). The solution trains\nthree separate Transformer classifiers with embedding sizes d_model \u2208 {64, 128,\n256}.   For each value it rebuilds the network, trains for 8 epochs, tracks\nlosses and macro-F1 on train/validation splits, then evaluates on the test\nsplit.   All results are stored in a nested experiment_data dictionary under the\nkey \u201cd_model_tuning\u201d and finally saved to experiment_data.npy.   The script\nprints per-epoch statistics and reports the best validation F1 as well as the\ntest F1 for every d_model tried.", "Hyperparam tuning name: dropout_rate. We sweep several dropout probabilities\n(0.0 \u2013 0.3) for the Transformer encoder, re-initialising and training a fresh\nmodel for each value, logging train/validation losses and macro-F1 per epoch and\nthen evaluating on the test set. Results for every configuration are stored in a\nhierarchical experiment_data dictionary under the hyper-parameter name\n\u201cdropout_rate\u201d and finally persisted as experiment_data.npy.", "Hyperparam tuning name: num_layers. The solution trains four Transformer\nclassifiers that differ only in the encoder depth (num_layers = 1, 2, 3, 4).\nFor every setting we train for eight epochs, track train/validation losses and\nmacro-F1, evaluate on the test set, and store all plottable data in a nested\nexperiment_data dictionary that is finally saved to \u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: weight_decay. We grid-search weight_decay values {0,\n1e-5, 1e-4, 1e-3}.   For every setting a fresh transformer, loss and Adam\noptimizer (with the current weight_decay) are instantiated, trained for a few\nepochs, evaluated on the dev and test splits, and all per-epoch metrics are\nstored in a nested experiment_data dict under the key \"weight_decay\".   The\nscript keeps everything in one file, mirrors the original training loop,\nreleases GPU memory between trials, and finally saves experiment_data.npy\ncontaining losses, F1 scores, predictions and ground-truth for every\nweight_decay tried.", "Hyperparam tuning name: nhead (number_of_attention_heads). The solution performs\na small grid-search over the number of attention heads {2, 4, 8}.   For every\nvalue it builds a fresh Transformer, trains for eight epochs exactly as in the\nbaseline, evaluates on the dev set each epoch, and stores per-run and per-epoch\nlosses, macro-F1 scores, predictions and ground-truth in the required\nexperiment_data structure.   After all sweeps it prints the best nhead setting\n(highest dev macro-F1) and saves the whole experiment_data dictionary to\nexperiment_data.npy.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["# num_epochs hyper-parameter tuning on SPR_BENCH\nimport os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- reproducibility --------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic, torch.backends.cudnn.benchmark = True, False\n\n# -------------------- experiment dict --------------------\nexperiment_data = {\n    \"num_epochs\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------- dataset --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(fname):  # csv loader helper\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    ds = DatasetDict()\n    for split_file, split_name in [\n        (\"train.csv\", \"train\"),\n        (\"dev.csv\", \"dev\"),\n        (\"test.csv\", \"test\"),\n    ]:\n        ds[split_name] = _load(split_file)\n    return ds\n\n\ndata_root = None\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        data_root = p\n        break\nif data_root is None:\n    raise FileNotFoundError(\"SPR_BENCH folder not found\")\n\nspr = load_spr_bench(data_root)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocab --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = {ch for seq in spr[\"train\"][\"sequence\"] for ch in seq}\nvocab = [PAD, UNK] + sorted(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\n\n# -------------------- labels --------------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\n\n# -------------------- encoders --------------------\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\n# -------------------- torch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size, shuffle=False)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model=128, nhead=4, layers=2, n_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, layers)\n        self.fc = nn.Linear(d_model, n_classes)\n\n    def forward(self, ids):\n        mask = ids == 0\n        x = self.pos(self.embed(ids))\n        x = self.encoder(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\nmodel = TransformerClassifier(vocab_size, n_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# -------------------- train / eval --------------------\ndef run_epoch(dloader, train_mode=True):\n    model.train() if train_mode else model.eval()\n    tot_loss, preds, labs = 0.0, [], []\n    for batch in dloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train_mode):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds, labs = torch.cat(preds).numpy(), torch.cat(labs).numpy()\n    macro_f1 = f1_score(labs, preds, average=\"macro\")\n    return tot_loss / len(dloader.dataset), macro_f1, preds, labs\n\n\n# -------------------- training loop with early stopping --------------------\nMAX_EPOCHS, PATIENCE = 30, 5\nbest_f1, patience_cnt = -1.0, 0\nbest_state = None\n\nfor epoch in range(1, MAX_EPOCHS + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, True)\n    val_loss, val_f1, _, _ = run_epoch(val_dl, False)\n    ed = experiment_data[\"num_epochs\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n    ed[\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: val_loss={val_loss:.4f}  val_F1={val_f1:.4f} \"\n        f\"(train_loss={tr_loss:.4f})  [{time.time()-t0:.1f}s]\"\n    )\n    # early stopping\n    if val_f1 > best_f1:\n        best_f1, patience_cnt, best_state = val_f1, 0, model.state_dict()\n    else:\n        patience_cnt += 1\n        if patience_cnt >= PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\n# -------------------- test evaluation (best model) --------------------\nmodel.load_state_dict(best_state)\ntest_loss, test_f1, test_preds, test_labels = run_epoch(test_dl, False)\ned = experiment_data[\"num_epochs\"][\"SPR_BENCH\"]\ned[\"predictions\"] = test_preds.tolist()\ned[\"ground_truth\"] = test_labels.tolist()\nprint(f\"Best validation F1: {best_f1:.4f} | Test F1: {test_f1:.4f}\")\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n", "import os, pathlib, math, time, random, json\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- reproducibility --------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"lr_search\": {\"SPR_BENCH\": []}}  # list of dicts, one per lr\n\n\n# -------------------- dataset load --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocabulary & labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = set()\nfor s in spr[\"train\"][\"sequence\"]:\n    vocab.update(list(s))\nvocab = [PAD, UNK] + sorted(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Num classes: {num_classes}\")\n\n# -------------------- encoding helpers --------------------\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in list(seq)[:MAX_LEN]]\n    if len(ids) < MAX_LEN:\n        ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\n# -------------------- torch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size, shuffle=False)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model=128, nhead=4, num_layers=2, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=256,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- training helpers --------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, dataloader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, labs = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).detach().cpu())\n        labs.append(batch[\"labels\"].detach().cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    macro_f1 = f1_score(labs, preds, average=\"macro\")\n    return tot_loss / len(dataloader.dataset), macro_f1, preds, labs\n\n\n# -------------------- learning-rate grid search --------------------\nlr_grid = [3e-4, 5e-4, 7e-4, 1.5e-3, 2e-3]\nEPOCHS = 8\n\nfor lr in lr_grid:\n    print(f\"\\n===== Training with learning rate {lr} =====\")\n    model = TransformerClassifier(vocab_size, num_classes=num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    run_record = {\n        \"lr\": lr,\n        \"epochs\": [],\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"test_f1\": None,\n    }\n    best_val = -1.0\n    for ep in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, optimizer=None)\n        run_record[\"epochs\"].append(ep)\n        run_record[\"metrics\"][\"train_f1\"].append(tr_f1)\n        run_record[\"metrics\"][\"val_f1\"].append(vl_f1)\n        run_record[\"losses\"][\"train\"].append(tr_loss)\n        run_record[\"losses\"][\"val\"].append(vl_loss)\n        print(\n            f\"Epoch {ep}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} \"\n            f\"(train_loss={tr_loss:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n    # final test evaluation\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, optimizer=None)\n    run_record[\"predictions\"] = ts_preds.tolist()\n    run_record[\"ground_truth\"] = ts_labels.tolist()\n    run_record[\"test_f1\"] = ts_f1\n    print(f\"Test macro-F1 with lr={lr}: {ts_f1:.4f}\")\n    experiment_data[\"lr_search\"][\"SPR_BENCH\"].append(run_record)\n\n# -------------------- save data --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- reproducibility --------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -------------------- device & work dir --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"batch_size_tuning\": {\n        \"SPR_BENCH\": {\n            # each batch size key will be added here\n        }\n    }\n}\n\n\n# -------------------- dataset load --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocabulary & labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = set()\nfor s in spr[\"train\"][\"sequence\"]:\n    vocab.update(list(s))\nvocab = [PAD, UNK] + sorted(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\nlabels = sorted(list(set(spr[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Num classes: {num_classes}\")\n\n# -------------------- encoding helpers --------------------\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in list(seq)[:MAX_LEN]]\n    if len(ids) < MAX_LEN:\n        ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labs = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_sz, d_model=128, nhead=4, num_layers=2, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=256,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        x = x.sum(dim=1) / (~mask).sum(dim=1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\n# -------------------- training helpers --------------------\ndef run_epoch(model, dataloader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).detach().cpu())\n        labels.append(batch[\"labels\"].detach().cpu())\n    preds = torch.cat(preds).numpy()\n    labels = torch.cat(labels).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyperparameter sweep --------------------\nbatch_sizes = [32, 64, 128, 256, 512]\nEPOCHS = 6\n\nfor bs in batch_sizes:\n    bs_key = str(bs)\n    print(f\"\\n==== Training with batch_size={bs} ====\")\n    experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"][bs_key] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    try:\n        train_dl = DataLoader(\n            SPRTorchDataset(spr[\"train\"]), batch_size=bs, shuffle=True\n        )\n        val_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=bs, shuffle=False)\n        test_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=bs, shuffle=False)\n\n        model = TransformerClassifier(vocab_size, num_classes=num_classes).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        for epoch in range(1, EPOCHS + 1):\n            t0 = time.time()\n            tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, optimizer)\n            vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, optimizer=None)\n\n            entry = experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"][bs_key]\n            entry[\"losses\"][\"train\"].append(tr_loss)\n            entry[\"losses\"][\"val\"].append(vl_loss)\n            entry[\"metrics\"][\"train_f1\"].append(tr_f1)\n            entry[\"metrics\"][\"val_f1\"].append(vl_f1)\n            entry[\"epochs\"].append(epoch)\n\n            print(\n                f\"Epoch {epoch}/{EPOCHS} | bs={bs} | val_F1={vl_f1:.4f} | tr_F1={tr_f1:.4f} | {time.time()-t0:.1f}s\"\n            )\n\n        # final test evaluation\n        tst_loss, tst_f1, preds, gts = run_epoch(model, test_dl, optimizer=None)\n        entry[\"predictions\"] = preds.tolist()\n        entry[\"ground_truth\"] = gts.tolist()\n        entry[\"test_f1\"] = tst_f1\n        print(f\"Test macro-F1 with bs={bs}: {tst_f1:.4f}\")\n\n    except RuntimeError as e:\n        warnings.warn(f\"Skipping bs={bs} due to runtime error: {str(e)}\")\n        experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"][bs_key][\"error\"] = str(e)\n    finally:\n        # free GPU memory\n        del model, optimizer\n        torch.cuda.empty_cache()\n\n# -------------------- save experiment data --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random, warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------- basic imports --------------------\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"d_model_tuning\": {}}  # will hold one entry per d_model value\n\n\n# -------------------- dataset load --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found in expected locations.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocabulary --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = set()\nfor s in spr[\"train\"][\"sequence\"]:\n    vocab.update(list(s))\nvocab = [PAD, UNK] + sorted(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n# -------------------- label mapping --------------------\nlabels = sorted(list(set(spr[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Num classes: {num_classes}\")\n\n# -------------------- encoding helpers --------------------\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in list(seq)[:MAX_LEN]]\n    if len(ids) < MAX_LEN:\n        ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labs = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl_full = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl_full = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False\n)\ntest_dl_full = DataLoader(\n    SPRTorchDataset(spr[\"test\"]), batch_size=batch_size, shuffle=False\n)\n\n\n# -------------------- model definitions --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model=128, nhead=4, num_layers=2, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        x = x.sum(dim=1) / (~mask).sum(dim=1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- training helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, all_preds, all_labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        all_preds.append(logits.argmax(-1).detach().cpu())\n        all_labels.append(batch[\"labels\"].detach().cpu())\n    all_preds = torch.cat(all_preds).numpy()\n    all_labels = torch.cat(all_labels).numpy()\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    avg_loss = total_loss / len(dataloader.dataset)\n    return avg_loss, macro_f1, all_preds, all_labels\n\n\n# -------------------- hyperparameter grid --------------------\nd_model_grid = [64, 128, 256]\nEPOCHS = 8\n\nfor dm in d_model_grid:\n    print(f\"\\n===== Training with d_model={dm} =====\")\n    # prepare logging dict\n    experiment_data[\"d_model_tuning\"][str(dm)] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    # initialize model, loss, optim\n    model = TransformerClassifier(\n        vocab_size, d_model=dm, nhead=4, num_layers=2, num_classes=num_classes\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_val_f1 = -1.0\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl_full, criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, val_dl_full, criterion)\n        # logging\n        ed = experiment_data[\"d_model_tuning\"][str(dm)]\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_f1\"].append(val_f1)\n        ed[\"epochs\"].append(epoch)\n        print(\n            f\"d_model {dm} | Epoch {epoch}: val_loss={val_loss:.4f} val_F1={val_f1:.4f} (train_loss={tr_loss:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n    # --------------- test evaluation with best model ---------------\n    model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_labels = run_epoch(\n        model, test_dl_full, criterion\n    )\n    ed = experiment_data[\"d_model_tuning\"][str(dm)]\n    ed[\"predictions\"] = test_preds.tolist()\n    ed[\"ground_truth\"] = test_labels.tolist()\n    ed[\"test_loss\"] = test_loss\n    ed[\"test_f1\"] = test_f1\n    ed[\"best_val_f1\"] = best_val_f1\n    print(f\"--> d_model {dm}: Best Val F1={best_val_f1:.4f} | Test F1={test_f1:.4f}\")\n\n# -------------------- save experiment data --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"\\nSaved experiment_data.npy\")\n", "import os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- reproducibility --------------------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"dropout_rate\": {\"SPR_BENCH\": {}}}\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# -------------------- dataset load --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d_split = _load(f\"{split}.csv\")\n        d[split if split != \"dev\" else \"dev\"] = d_split\n    return d\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocabulary & labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = set(ch for seq in spr[\"train\"][\"sequence\"] for ch in seq)\nvocab = [PAD, UNK] + sorted(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size, shuffle=False)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(\n        self, vocab, d_model=128, nhead=4, layers=2, num_classes=2, dropout=0.1\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=256, dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask[..., None], 0).sum(1) / (~mask).sum(\n            1, keepdim=True\n        ).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).detach().cpu())\n        labels.append(batch[\"labels\"].cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        tot_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyper-parameter sweep --------------------\ndropout_grid = [0.0, 0.05, 0.1, 0.2, 0.3]\nEPOCHS = 8\ncriterion = nn.CrossEntropyLoss()\n\nfor dr in dropout_grid:\n    print(f\"\\n=== Training with dropout={dr} ===\")\n    data_key = f\"dr_{dr}\"\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][data_key] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": list(range(1, EPOCHS + 1)),\n    }\n\n    model = TransformerClassifier(vocab_size, num_classes=num_classes, dropout=dr).to(\n        device\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, criterion, optimizer)\n        va_loss, va_f1, _, _ = run_epoch(model, val_dl, criterion)\n        exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][data_key]\n        exp[\"losses\"][\"train\"].append(tr_loss)\n        exp[\"losses\"][\"val\"].append(va_loss)\n        exp[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp[\"metrics\"][\"val_f1\"].append(va_f1)\n        print(\n            f\"Epoch {epoch} | val_loss={va_loss:.4f} val_F1={va_f1:.4f} \"\n            f\"(train_loss={tr_loss:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n    # final test evaluation\n    te_loss, te_f1, te_pred, te_gt = run_epoch(model, test_dl, criterion)\n    exp[\"predictions\"] = te_pred.tolist()\n    exp[\"ground_truth\"] = te_gt.tolist()\n    exp[\"test_loss\"] = te_loss\n    exp[\"test_f1\"] = te_f1\n    print(f\"Test macro-F1 (dropout={dr}): {te_f1:.4f}\")\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- directories / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"num_layers\": {\"SPR_BENCH\": {}}}  # hyper-parameter tuned\n\n\n# -------------------- load SPR_BENCH --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):  # tiny helper\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocab / labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"Vocab size: {vocab_size}  Num classes: {num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- train / eval helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labels.append(batch[\"labels\"].cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyper-parameter sweep --------------------\nEPOCHS = 8\nfor nl in [1, 2, 3, 4]:\n    print(f\"\\n=== Training with num_layers={nl} ===\")\n    model = TransformerClassifier(vocab_size, 128, 4, nl, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    data_slot = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, criterion, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, criterion)\n        data_slot[\"losses\"][\"train\"].append(tr_loss)\n        data_slot[\"losses\"][\"val\"].append(vl_loss)\n        data_slot[\"metrics\"][\"train_f1\"].append(tr_f1)\n        data_slot[\"metrics\"][\"val_f1\"].append(vl_f1)\n        data_slot[\"epochs\"].append(epoch)\n        print(\n            f\"  Ep{epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} (train_F1={tr_f1:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n\n    # final test\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, criterion)\n    data_slot[\"predictions\"], data_slot[\"ground_truth\"] = (\n        ts_preds.tolist(),\n        ts_labels.tolist(),\n    )\n    print(f\"  --> Test macro-F1={ts_f1:.4f}\")\n\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][f\"nl_{nl}\"] = data_slot\n    # free memory before next run\n    del model\n    torch.cuda.empty_cache()\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random, gc\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- reproducibility --------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# -------------------- device & working dir --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"weight_decay\": {}}\n\n\n# -------------------- dataset load --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split if split != \"dev\" else \"dev\"] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocabulary & labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = set()\n[vocab.update(list(s)) for s in spr[\"train\"][\"sequence\"]]\nvocab = [PAD, UNK] + sorted(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\nlabels = sorted(list(set(spr[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Num classes: {num_classes}\")\n\n# -------------------- encoding helpers --------------------\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in list(seq)[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\n# -------------------- dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl_full = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl_full = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False\n)\ntest_dl_full = DataLoader(\n    SPRTorchDataset(spr[\"test\"]), batch_size=batch_size, shuffle=False\n)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1), :]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model=128, nhead=4, num_layers=2, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- train/val helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).detach().cpu())\n        labels.append(batch[\"labels\"].detach().cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        tot_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyperparameter search --------------------\nsearch_wd = [0.0, 1e-5, 1e-4, 1e-3]\nEPOCHS = 8\nfor wd in search_wd:\n    key = f\"wd_{wd}\"\n    print(f\"\\n================ Weight Decay = {wd} =================\")\n    experiment_data[\"weight_decay\"][key] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    model = TransformerClassifier(vocab_size, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    best_val_f1 = -1\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl_full, criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, val_dl_full, criterion)\n        exp = experiment_data[\"weight_decay\"][key]\n        exp[\"losses\"][\"train\"].append(tr_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp[\"metrics\"][\"val_f1\"].append(val_f1)\n        exp[\"epochs\"].append(epoch)\n        print(\n            f\"Epoch {epoch}: val_loss={val_loss:.4f} val_F1={val_f1:.4f} \"\n            f\"(train_loss={tr_loss:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n    # final test evaluation\n    test_loss, test_f1, test_preds, test_labels = run_epoch(\n        model, test_dl_full, criterion\n    )\n    exp[\"predictions\"] = test_preds.tolist()\n    exp[\"ground_truth\"] = test_labels.tolist()\n    exp[\"test_f1\"] = test_f1\n    exp[\"test_loss\"] = test_loss\n    print(f\"Test macro-F1 (wd={wd}): {test_f1:.4f}\")\n    # cleanup\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# -------------------- save experiment data --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- misc & reproducibility --------------------\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------- experiment-data container --------------------\nexperiment_data = {\"nhead_tuning\": {\"SPR_BENCH\": {}}}\n\n\n# -------------------- dataset load --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split if split != \"dev\" else \"dev\"] = _load(f\"{split}.csv\")\n    return d\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocabulary & label mapping --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for seq in spr[\"train\"][\"sequence\"] for ch in seq})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(\"Vocab size:\", vocab_size, \"| Num classes:\", num_classes)\n\n# -------------------- encoding helpers --------------------\nMAX_LEN = 64\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\n# -------------------- torch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(encode_label(self.labs[idx]), dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size, shuffle=False)\n\n\n# -------------------- model definition --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model=128, nhead=4, num_layers=2, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, dhead := nhead, dim_feedforward=256, dropout=0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- training helpers --------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, dataloader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, labs = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        tot_loss / len(dataloader.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# -------------------- hyperparameter sweep --------------------\nnhead_options = [2, 4, 8]\nEPOCHS = 8\nbest_val_f1, best_nhead = -1, None\n\nfor nhead in nhead_options:\n    print(f\"\\n==== Training with nhead={nhead} ====\")\n    model = TransformerClassifier(vocab_size, nhead=nhead, num_classes=num_classes).to(\n        device\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    run_dict = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n        run_dict[\"losses\"][\"train\"].append(tr_loss)\n        run_dict[\"losses\"][\"val\"].append(vl_loss)\n        run_dict[\"metrics\"][\"train_f1\"].append(tr_f1)\n        run_dict[\"metrics\"][\"val_f1\"].append(vl_f1)\n        run_dict[\"epochs\"].append(epoch)\n        print(\n            f\"epoch {epoch}: val_f1={vl_f1:.4f} | train_f1={tr_f1:.4f} ({time.time()-t0:.1f}s)\"\n        )\n\n    # final test evaluation for this setting\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\n    run_dict[\"predictions\"] = ts_preds.tolist()\n    run_dict[\"ground_truth\"] = ts_labels.tolist()\n    run_dict[\"test_loss\"], run_dict[\"test_f1\"] = ts_loss, ts_f1\n    print(f\"Test macro-F1 with nhead={nhead}: {ts_f1:.4f}\")\n\n    experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][f\"nhead_{nhead}\"] = run_dict\n\n    if vl_f1 > best_val_f1:\n        best_val_f1, best_nhead = vl_f1, nhead\n\nprint(f\"\\nBest nhead based on dev macro-F1: {best_nhead} (dev F1={best_val_f1:.4f})\")\n\n# -------------------- save --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- directories / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"num_layers\": {\"SPR_BENCH\": {}}}  # hyper-parameter tuned\n\n\n# -------------------- load SPR_BENCH --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):  # tiny helper\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocab / labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"Vocab size: {vocab_size}  Num classes: {num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- train / eval helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labels.append(batch[\"labels\"].cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyper-parameter sweep --------------------\nEPOCHS = 8\nfor nl in [1, 2, 3, 4]:\n    print(f\"\\n=== Training with num_layers={nl} ===\")\n    model = TransformerClassifier(vocab_size, 128, 4, nl, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    data_slot = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, criterion, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, criterion)\n        data_slot[\"losses\"][\"train\"].append(tr_loss)\n        data_slot[\"losses\"][\"val\"].append(vl_loss)\n        data_slot[\"metrics\"][\"train_f1\"].append(tr_f1)\n        data_slot[\"metrics\"][\"val_f1\"].append(vl_f1)\n        data_slot[\"epochs\"].append(epoch)\n        print(\n            f\"  Ep{epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} (train_F1={tr_f1:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n\n    # final test\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, criterion)\n    data_slot[\"predictions\"], data_slot[\"ground_truth\"] = (\n        ts_preds.tolist(),\n        ts_labels.tolist(),\n    )\n    print(f\"  --> Test macro-F1={ts_f1:.4f}\")\n\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][f\"nl_{nl}\"] = data_slot\n    # free memory before next run\n    del model\n    torch.cuda.empty_cache()\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- directories / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"num_layers\": {\"SPR_BENCH\": {}}}  # hyper-parameter tuned\n\n\n# -------------------- load SPR_BENCH --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):  # tiny helper\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocab / labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"Vocab size: {vocab_size}  Num classes: {num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- train / eval helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labels.append(batch[\"labels\"].cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyper-parameter sweep --------------------\nEPOCHS = 8\nfor nl in [1, 2, 3, 4]:\n    print(f\"\\n=== Training with num_layers={nl} ===\")\n    model = TransformerClassifier(vocab_size, 128, 4, nl, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    data_slot = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, criterion, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, criterion)\n        data_slot[\"losses\"][\"train\"].append(tr_loss)\n        data_slot[\"losses\"][\"val\"].append(vl_loss)\n        data_slot[\"metrics\"][\"train_f1\"].append(tr_f1)\n        data_slot[\"metrics\"][\"val_f1\"].append(vl_f1)\n        data_slot[\"epochs\"].append(epoch)\n        print(\n            f\"  Ep{epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} (train_F1={tr_f1:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n\n    # final test\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, criterion)\n    data_slot[\"predictions\"], data_slot[\"ground_truth\"] = (\n        ts_preds.tolist(),\n        ts_labels.tolist(),\n    )\n    print(f\"  --> Test macro-F1={ts_f1:.4f}\")\n\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][f\"nl_{nl}\"] = data_slot\n    # free memory before next run\n    del model\n    torch.cuda.empty_cache()\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- directories / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"num_layers\": {\"SPR_BENCH\": {}}}  # hyper-parameter tuned\n\n\n# -------------------- load SPR_BENCH --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):  # tiny helper\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocab / labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"Vocab size: {vocab_size}  Num classes: {num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- train / eval helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labels.append(batch[\"labels\"].cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyper-parameter sweep --------------------\nEPOCHS = 8\nfor nl in [1, 2, 3, 4]:\n    print(f\"\\n=== Training with num_layers={nl} ===\")\n    model = TransformerClassifier(vocab_size, 128, 4, nl, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    data_slot = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, criterion, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, criterion)\n        data_slot[\"losses\"][\"train\"].append(tr_loss)\n        data_slot[\"losses\"][\"val\"].append(vl_loss)\n        data_slot[\"metrics\"][\"train_f1\"].append(tr_f1)\n        data_slot[\"metrics\"][\"val_f1\"].append(vl_f1)\n        data_slot[\"epochs\"].append(epoch)\n        print(\n            f\"  Ep{epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} (train_F1={tr_f1:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n\n    # final test\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, criterion)\n    data_slot[\"predictions\"], data_slot[\"ground_truth\"] = (\n        ts_preds.tolist(),\n        ts_labels.tolist(),\n    )\n    print(f\"  --> Test macro-F1={ts_f1:.4f}\")\n\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][f\"nl_{nl}\"] = data_slot\n    # free memory before next run\n    del model\n    torch.cuda.empty_cache()\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 144526.52\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 94063.78\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 152592.28\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.3330\nval_F1=0.6514 (train_loss=0.4457)  [1.0s]', '\\n', 'Epoch 2: val_loss=1.9464\nval_F1=0.6632 (train_loss=0.1340)  [0.5s]', '\\n', 'Epoch 3: val_loss=1.6964\nval_F1=0.6760 (train_loss=0.1317)  [0.4s]', '\\n', 'Epoch 4: val_loss=1.8611\nval_F1=0.6653 (train_loss=0.0995)  [0.4s]', '\\n', 'Epoch 5: val_loss=1.9116\nval_F1=0.6798 (train_loss=0.0909)  [0.4s]', '\\n', 'Epoch 6: val_loss=1.9055\nval_F1=0.6879 (train_loss=0.0872)  [0.4s]', '\\n', 'Epoch 7: val_loss=2.1047\nval_F1=0.6899 (train_loss=0.0563)  [0.5s]', '\\n', 'Epoch 8: val_loss=2.4149\nval_F1=0.6919 (train_loss=0.0205)  [0.5s]', '\\n', 'Epoch 9: val_loss=2.3228\nval_F1=0.6960 (train_loss=0.0297)  [0.4s]', '\\n', 'Epoch 10: val_loss=2.3136\nval_F1=0.6980 (train_loss=0.0259)  [0.4s]', '\\n', 'Epoch 11: val_loss=2.3783\nval_F1=0.7000 (train_loss=0.0134)  [0.4s]', '\\n', 'Epoch 12: val_loss=2.6519\nval_F1=0.6899 (train_loss=0.0170)  [0.4s]', '\\n', 'Epoch 13: val_loss=2.5300\nval_F1=0.6960 (train_loss=0.0169)  [0.4s]', '\\n', 'Epoch 14: val_loss=2.3107\nval_F1=0.6960 (train_loss=0.0183)  [0.4s]', '\\n', 'Epoch 15: val_loss=2.5878\nval_F1=0.6960 (train_loss=0.0293)  [0.4s]', '\\n', 'Epoch 16: val_loss=2.7737\nval_F1=0.6980 (train_loss=0.0073)  [0.4s]', '\\n', 'Early stopping triggered.',\n'\\n', 'Best validation F1: 0.7000 | Test F1: 0.6999', '\\n', 'Execution time: 10\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 121854.82\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 42270.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 127061.62\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize: 11', '\\n', 'Num classes: 2', '\\n', '\\n===== Training with learning rate\n0.0003 =====', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7555\nval_F1=0.6579 (train_loss=0.5314) [0.4s]', '\\n', 'Epoch 2: val_loss=1.6810\nval_F1=0.6653 (train_loss=0.1834) [0.1s]', '\\n', 'Epoch 3: val_loss=1.9316\nval_F1=0.6495 (train_loss=0.1538) [0.1s]', '\\n', 'Epoch 4: val_loss=1.6690\nval_F1=0.6759 (train_loss=0.1456) [0.1s]', '\\n', 'Epoch 5: val_loss=1.7356\nval_F1=0.6799 (train_loss=0.1065) [0.1s]', '\\n', 'Epoch 6: val_loss=1.8264\nval_F1=0.6677 (train_loss=0.0991) [0.1s]', '\\n', 'Epoch 7: val_loss=1.8377\nval_F1=0.6677 (train_loss=0.0924) [0.1s]', '\\n', 'Epoch 8: val_loss=1.8709\nval_F1=0.6697 (train_loss=0.0870) [0.1s]', '\\n', 'Test macro-F1 with lr=0.0003:\n0.6967', '\\n', '\\n===== Training with learning rate 0.0005 =====', '\\n', 'Epoch\n1: val_loss=0.6917 val_F1=0.6640 (train_loss=0.5990) [0.1s]', '\\n', 'Epoch 2:\nval_loss=1.6771 val_F1=0.6740 (train_loss=0.2092) [0.1s]', '\\n', 'Epoch 3:\nval_loss=1.8775 val_F1=0.6697 (train_loss=0.1221) [0.1s]', '\\n', 'Epoch 4:\nval_loss=1.7618 val_F1=0.6800 (train_loss=0.1034) [0.1s]', '\\n', 'Epoch 5:\nval_loss=1.9095 val_F1=0.6899 (train_loss=0.0743) [0.1s]', '\\n', 'Epoch 6:\nval_loss=2.0650 val_F1=0.6939 (train_loss=0.0352) [0.1s]', '\\n', 'Epoch 7:\nval_loss=2.2954 val_F1=0.6939 (train_loss=0.0295) [0.1s]', '\\n', 'Epoch 8:\nval_loss=2.3200 val_F1=0.6939 (train_loss=0.0336) [0.1s]', '\\n', 'Test macro-F1\nwith lr=0.0005: 0.6957', '\\n', '\\n===== Training with learning rate 0.0007\n=====', '\\n', 'Epoch 1: val_loss=0.7820 val_F1=0.6277 (train_loss=0.5789)\n[0.1s]', '\\n', 'Epoch 2: val_loss=1.6395 val_F1=0.6840 (train_loss=0.1802)\n[0.1s]', '\\n', 'Epoch 3: val_loss=1.8663 val_F1=0.6778 (train_loss=0.1095)\n[0.1s]', '\\n', 'Epoch 4: val_loss=1.8893 val_F1=0.6777 (train_loss=0.0913)\n[0.1s]', '\\n', 'Epoch 5: val_loss=1.8506 val_F1=0.6879 (train_loss=0.0764)\n[0.1s]', '\\n', 'Epoch 6: val_loss=1.9868 val_F1=0.6879 (train_loss=0.0589)\n[0.1s]', '\\n', 'Epoch 7: val_loss=2.1824 val_F1=0.6879 (train_loss=0.0386)\n[0.1s]', '\\n', 'Epoch 8: val_loss=2.2272 val_F1=0.6980 (train_loss=0.0339)\n[0.1s]', '\\n', 'Test macro-F1 with lr=0.0007: 0.6979', '\\n', '\\n===== Training\nwith learning rate 0.0015 =====', '\\n', 'Epoch 1: val_loss=0.9805 val_F1=0.6654\n(train_loss=0.5538) [0.1s]', '\\n', 'Epoch 2: val_loss=1.5761 val_F1=0.6799\n(train_loss=0.1708) [0.1s]', '\\n', 'Epoch 3: val_loss=1.8097 val_F1=0.6675\n(train_loss=0.1049) [0.1s]', '\\n', 'Epoch 4: val_loss=1.9159 val_F1=0.6838\n(train_loss=0.0673) [0.1s]', '\\n', 'Epoch 5: val_loss=1.9771 val_F1=0.6899\n(train_loss=0.0542) [0.1s]', '\\n', 'Epoch 6: val_loss=2.1106 val_F1=0.6919\n(train_loss=0.0406) [0.1s]', '\\n', 'Epoch 7: val_loss=2.1875 val_F1=0.6919\n(train_loss=0.0419) [0.1s]', '\\n', 'Epoch 8: val_loss=2.2832 val_F1=0.7000\n(train_loss=0.0309) [0.1s]', '\\n', 'Test macro-F1 with lr=0.0015: 0.6969', '\\n',\n'\\n===== Training with learning rate 0.002 =====', '\\n', 'Epoch 1:\nval_loss=0.6548 val_F1=0.6642 (train_loss=0.8222) [0.1s]', '\\n', 'Epoch 2:\nval_loss=1.7277 val_F1=0.6506 (train_loss=0.2225) [0.1s]', '\\n', 'Epoch 3:\nval_loss=1.5925 val_F1=0.6879 (train_loss=0.1280) [0.1s]', '\\n', 'Epoch 4:\nval_loss=1.8538 val_F1=0.6838 (train_loss=0.0636) [0.1s]', '\\n', 'Epoch 5:\nval_loss=2.1067 val_F1=0.6899 (train_loss=0.0244) [0.1s]', '\\n', 'Epoch 6:\nval_loss=2.2233 val_F1=0.6980 (train_loss=0.0213) [0.1s]', '\\n', 'Epoch 7:\nval_loss=2.2046 val_F1=0.6980 (train_loss=0.0091) [0.1s]', '\\n', 'Epoch 8:\nval_loss=2.3305 val_F1=0.7020 (train_loss=0.0112) [0.1s]', '\\n', 'Test macro-F1\nwith lr=0.002: 0.6969', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution\ntime: 8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 125487.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 114673.67\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 82927.44\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize: 11', '\\n', 'Num classes: 2', '\\n', '\\n==== Training with batch_size=32\n====', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/6 | bs=32 | val_F1=0.6659\n| tr_F1=0.8450 | 1.3s', '\\n', 'Epoch 2/6 | bs=32 | val_F1=0.6780 | tr_F1=0.9555\n| 0.5s', '\\n', 'Epoch 3/6 | bs=32 | val_F1=0.6791 | tr_F1=0.9790 | 0.5s', '\\n',\n'Epoch 4/6 | bs=32 | val_F1=0.6878 | tr_F1=0.9835 | 0.5s', '\\n', 'Epoch 5/6 |\nbs=32 | val_F1=0.6939 | tr_F1=0.9920 | 0.6s', '\\n', 'Epoch 6/6 | bs=32 |\nval_F1=0.6960 | tr_F1=0.9930 | 0.4s', '\\n', 'Test macro-F1 with bs=32: 0.6978',\n'\\n', '\\n==== Training with batch_size=64 ====', '\\n', 'Epoch 1/6 | bs=64 |\nval_F1=0.6711 | tr_F1=0.7429 | 0.2s', '\\n', 'Epoch 2/6 | bs=64 | val_F1=0.6653 |\ntr_F1=0.9625 | 0.2s', '\\n', 'Epoch 3/6 | bs=64 | val_F1=0.6589 | tr_F1=0.9685 |\n0.2s', '\\n', 'Epoch 4/6 | bs=64 | val_F1=0.6653 | tr_F1=0.9620 | 0.2s', '\\n',\n'Epoch 5/6 | bs=64 | val_F1=0.6919 | tr_F1=0.9780 | 0.2s', '\\n', 'Epoch 6/6 |\nbs=64 | val_F1=0.6960 | tr_F1=0.9935 | 0.2s', '\\n', 'Test macro-F1 with bs=64:\n0.6978', '\\n', '\\n==== Training with batch_size=128 ====', '\\n', 'Epoch 1/6 |\nbs=128 | val_F1=0.6800 | tr_F1=0.6969 | 0.1s', '\\n', 'Epoch 2/6 | bs=128 |\nval_F1=0.6512 | tr_F1=0.9520 | 0.1s', '\\n', 'Epoch 3/6 | bs=128 | val_F1=0.6464\n| tr_F1=0.9530 | 0.1s', '\\n', 'Epoch 4/6 | bs=128 | val_F1=0.6736 | tr_F1=0.9765\n| 0.1s', '\\n', 'Epoch 5/6 | bs=128 | val_F1=0.6818 | tr_F1=0.9810 | 0.1s', '\\n',\n'Epoch 6/6 | bs=128 | val_F1=0.6838 | tr_F1=0.9840 | 0.1s', '\\n', 'Test macro-F1\nwith bs=128: 0.6917', '\\n', '\\n==== Training with batch_size=256 ====', '\\n',\n'Epoch 1/6 | bs=256 | val_F1=0.5858 | tr_F1=0.5322 | 0.2s', '\\n', 'Epoch 2/6 |\nbs=256 | val_F1=0.6426 | tr_F1=0.8776 | 0.1s', '\\n', 'Epoch 3/6 | bs=256 |\nval_F1=0.6738 | tr_F1=0.9319 | 0.1s', '\\n', 'Epoch 4/6 | bs=256 | val_F1=0.6720\n| tr_F1=0.9445 | 0.1s', '\\n', 'Epoch 5/6 | bs=256 | val_F1=0.6716 | tr_F1=0.9580\n| 0.1s', '\\n', 'Epoch 6/6 | bs=256 | val_F1=0.6694 | tr_F1=0.9660 | 0.1s', '\\n',\n'Test macro-F1 with bs=256: 0.6834', '\\n', '\\n==== Training with batch_size=512\n====', '\\n', 'Epoch 1/6 | bs=512 | val_F1=0.3316 | tr_F1=0.5205 | 0.1s', '\\n',\n'Epoch 2/6 | bs=512 | val_F1=0.3690 | tr_F1=0.5923 | 0.1s', '\\n', 'Epoch 3/6 |\nbs=512 | val_F1=0.5199 | tr_F1=0.7080 | 0.1s', '\\n', 'Epoch 4/6 | bs=512 |\nval_F1=0.6373 | tr_F1=0.8145 | 0.1s', '\\n', 'Epoch 5/6 | bs=512 | val_F1=0.6720\n| tr_F1=0.9184 | 0.1s', '\\n', 'Epoch 6/6 | bs=512 | val_F1=0.6700 | tr_F1=0.9424\n| 0.1s', '\\n', 'Test macro-F1 with bs=512: 0.6780', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 36644.76\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 110011.65\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 181650.24\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize: 11', '\\n', 'Num classes: 2', '\\n', '\\n===== Training with d_model=64\n=====', '\\n', 'd_model 64 | Epoch 1: val_loss=0.6878 val_F1=0.6332\n(train_loss=0.5849) [0.9s]', '\\n', 'd_model 64 | Epoch 2: val_loss=1.5334\nval_F1=0.6694 (train_loss=0.2134) [0.5s]', '\\n', 'd_model 64 | Epoch 3:\nval_loss=1.7120 val_F1=0.6633 (train_loss=0.1206) [0.5s]', '\\n', 'd_model 64 |\nEpoch 4: val_loss=1.6437 val_F1=0.6859 (train_loss=0.1077) [0.5s]', '\\n',\n'd_model 64 | Epoch 5: val_loss=1.5854 val_F1=0.6778 (train_loss=0.1053)\n[0.5s]', '\\n', 'd_model 64 | Epoch 6: val_loss=1.6825 val_F1=0.6736\n(train_loss=0.0936) [0.5s]', '\\n', 'd_model 64 | Epoch 7: val_loss=1.6737\nval_F1=0.6737 (train_loss=0.0775) [0.5s]', '\\n', 'd_model 64 | Epoch 8:\nval_loss=1.6255 val_F1=0.6716 (train_loss=0.1217) [0.5s]', '\\n', '--> d_model\n64: Best Val F1=0.6859 | Test F1=0.6968', '\\n', '\\n===== Training with\nd_model=128 =====', '\\n', 'd_model 128 | Epoch 1: val_loss=0.6621 val_F1=0.6248\n(train_loss=0.7181) [0.6s]', '\\n', 'd_model 128 | Epoch 2: val_loss=1.5203\nval_F1=0.6457 (train_loss=0.2707) [0.6s]', '\\n', 'd_model 128 | Epoch 3:\nval_loss=1.5740 val_F1=0.6655 (train_loss=0.1605) [0.6s]', '\\n', 'd_model 128 |\nEpoch 4: val_loss=1.7644 val_F1=0.6675 (train_loss=0.0960) [0.6s]', '\\n',\n'd_model 128 | Epoch 5: val_loss=1.8327 val_F1=0.6776 (train_loss=0.0850)\n[0.5s]', '\\n', 'd_model 128 | Epoch 6: val_loss=1.9470 val_F1=0.6959\n(train_loss=0.0459) [0.6s]', '\\n', 'd_model 128 | Epoch 7: val_loss=2.1339\nval_F1=0.6919 (train_loss=0.0251) [0.6s]', '\\n', 'd_model 128 | Epoch 8:\nval_loss=2.2043 val_F1=0.6959 (train_loss=0.0273) [0.6s]', '\\n', '--> d_model\n128: Best Val F1=0.6959 | Test F1=0.6957', '\\n', '\\n===== Training with\nd_model=256 =====', '\\n', 'd_model 256 | Epoch 1: val_loss=0.6720 val_F1=0.5534\n(train_loss=1.0207) [0.9s]', '\\n', 'd_model 256 | Epoch 2: val_loss=1.3014\nval_F1=0.6396 (train_loss=0.4789) [0.8s]', '\\n', 'd_model 256 | Epoch 3:\nval_loss=1.9052 val_F1=0.5628 (train_loss=0.1725) [0.8s]', '\\n', 'd_model 256 |\nEpoch 4: val_loss=1.4760 val_F1=0.6692 (train_loss=0.1883) [0.8s]', '\\n',\n'd_model 256 | Epoch 5: val_loss=2.2625 val_F1=0.6920 (train_loss=0.0524)\n[0.8s]', '\\n', 'd_model 256 | Epoch 6: val_loss=2.5553 val_F1=0.6899\n(train_loss=0.0259) [0.8s]', '\\n', 'd_model 256 | Epoch 7: val_loss=2.6066\nval_F1=0.6980 (train_loss=0.0213) [0.8s]', '\\n', 'd_model 256 | Epoch 8:\nval_loss=2.4482 val_F1=0.6960 (train_loss=0.0457) [0.8s]', '\\n', '--> d_model\n256: Best Val F1=0.6980 | Test F1=0.6949', '\\n', '\\nSaved experiment_data.npy',\n'\\n', 'Execution time: 20 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', '\\n=== Training with dropout=0.0 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1 | val_loss=1.7288\nval_F1=0.6444 (train_loss=0.4143) [0.8s]', '\\n', 'Epoch 2 | val_loss=1.6709\nval_F1=0.6699 (train_loss=0.1735) [0.5s]', '\\n', 'Epoch 3 | val_loss=1.6220\nval_F1=0.6697 (train_loss=0.1240) [0.5s]', '\\n', 'Epoch 4 | val_loss=1.6411\nval_F1=0.6738 (train_loss=0.1183) [0.4s]', '\\n', 'Epoch 5 | val_loss=1.8889\nval_F1=0.6797 (train_loss=0.0829) [0.4s]', '\\n', 'Epoch 6 | val_loss=1.9854\nval_F1=0.6838 (train_loss=0.0591) [0.5s]', '\\n', 'Epoch 7 | val_loss=2.0727\nval_F1=0.6919 (train_loss=0.0319) [0.4s]', '\\n', 'Epoch 8 | val_loss=2.2792\nval_F1=0.6939 (train_loss=0.0215) [0.5s]', '\\n', 'Test macro-F1 (dropout=0.0):\n0.6988', '\\n', '\\n=== Training with dropout=0.05 ===', '\\n', 'Epoch 1 |\nval_loss=0.7408 val_F1=0.6536 (train_loss=0.6673) [0.5s]', '\\n', 'Epoch 2 |\nval_loss=1.5921 val_F1=0.6421 (train_loss=0.2186) [0.5s]', '\\n', 'Epoch 3 |\nval_loss=1.7116 val_F1=0.6591 (train_loss=0.1372) [0.4s]', '\\n', 'Epoch 4 |\nval_loss=1.7017 val_F1=0.6696 (train_loss=0.1187) [0.4s]', '\\n', 'Epoch 5 |\nval_loss=1.8832 val_F1=0.6716 (train_loss=0.0931) [0.4s]', '\\n', 'Epoch 6 |\nval_loss=1.8896 val_F1=0.6838 (train_loss=0.0772) [0.4s]', '\\n', 'Epoch 7 |\nval_loss=1.8951 val_F1=0.6878 (train_loss=0.0834) [0.4s]', '\\n', 'Epoch 8 |\nval_loss=2.0003 val_F1=0.6920 (train_loss=0.0421) [0.4s]', '\\n', 'Test macro-F1\n(dropout=0.05): 0.6949', '\\n', '\\n=== Training with dropout=0.1 ===', '\\n',\n'Epoch 1 | val_loss=0.6862 val_F1=0.6552 (train_loss=0.6523) [0.4s]', '\\n',\n'Epoch 2 | val_loss=1.6488 val_F1=0.6708 (train_loss=0.2149) [0.4s]', '\\n',\n'Epoch 3 | val_loss=1.5377 val_F1=0.6698 (train_loss=0.1657) [0.4s]', '\\n',\n'Epoch 4 | val_loss=1.7404 val_F1=0.6483 (train_loss=0.1297) [0.4s]', '\\n',\n'Epoch 5 | val_loss=1.6368 val_F1=0.6800 (train_loss=0.1289) [0.4s]', '\\n',\n'Epoch 6 | val_loss=1.8636 val_F1=0.6716 (train_loss=0.0936) [0.4s]', '\\n',\n'Epoch 7 | val_loss=1.9283 val_F1=0.6777 (train_loss=0.0775) [0.4s]', '\\n',\n'Epoch 8 | val_loss=2.1030 val_F1=0.6939 (train_loss=0.0438) [0.4s]', '\\n',\n'Test macro-F1 (dropout=0.1): 0.6968', '\\n', '\\n=== Training with dropout=0.2\n===', '\\n', 'Epoch 1 | val_loss=0.7142 val_F1=0.6514 (train_loss=0.6390)\n[0.4s]', '\\n', 'Epoch 2 | val_loss=1.7320 val_F1=0.6546 (train_loss=0.2096)\n[0.4s]', '\\n', 'Epoch 3 | val_loss=1.6505 val_F1=0.6549 (train_loss=0.1503)\n[0.4s]', '\\n', 'Epoch 4 | val_loss=1.6931 val_F1=0.6654 (train_loss=0.1162)\n[0.4s]', '\\n', 'Epoch 5 | val_loss=1.8435 val_F1=0.6695 (train_loss=0.0985)\n[0.4s]', '\\n', 'Epoch 6 | val_loss=1.8597 val_F1=0.6757 (train_loss=0.0907)\n[0.4s]', '\\n', 'Epoch 7 | val_loss=2.0201 val_F1=0.6838 (train_loss=0.0685)\n[0.4s]', '\\n', 'Epoch 8 | val_loss=2.3405 val_F1=0.6939 (train_loss=0.0241)\n[0.4s]', '\\n', 'Test macro-F1 (dropout=0.2): 0.6978', '\\n', '\\n=== Training with\ndropout=0.3 ===', '\\n', 'Epoch 1 | val_loss=0.6797 val_F1=0.6598\n(train_loss=0.6592) [0.4s]', '\\n', 'Epoch 2 | val_loss=1.7003 val_F1=0.6694\n(train_loss=0.2205) [0.4s]', '\\n', 'Epoch 3 | val_loss=1.5889 val_F1=0.6570\n(train_loss=0.1708) [0.4s]', '\\n', 'Epoch 4 | val_loss=1.5330 val_F1=0.6849\n(train_loss=0.1551) [0.4s]', '\\n', 'Epoch 5 | val_loss=1.7667 val_F1=0.6716\n(train_loss=0.1042) [0.4s]', '\\n', 'Epoch 6 | val_loss=1.8895 val_F1=0.6838\n(train_loss=0.0849) [0.4s]', '\\n', 'Epoch 7 | val_loss=2.0209 val_F1=0.6899\n(train_loss=0.0520) [0.4s]', '\\n', 'Epoch 8 | val_loss=2.1871 val_F1=0.6919\n(train_loss=0.0380) [0.4s]', '\\n', 'Test macro-F1 (dropout=0.3): 0.6958', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 21 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 11  Num classes: 2', '\\n', '\\n=== Training with num_layers=1 ===',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Ep1: val_loss=0.7619\nval_F1=0.6658 (train_F1=0.6967) [0.4s]', '\\n', '  Ep2: val_loss=1.7147\nval_F1=0.6527 (train_F1=0.9360) [0.1s]', '\\n', '  Ep3: val_loss=1.5587\nval_F1=0.6717 (train_F1=0.9455) [0.1s]', '\\n', '  Ep4: val_loss=1.7727\nval_F1=0.6760 (train_F1=0.9695) [0.1s]', '\\n', '  Ep5: val_loss=1.8685\nval_F1=0.6632 (train_F1=0.9680) [0.1s]', '\\n', '  Ep6: val_loss=1.8670\nval_F1=0.6696 (train_F1=0.9710) [0.1s]', '\\n', '  Ep7: val_loss=2.0020\nval_F1=0.6656 (train_F1=0.9765) [0.1s]', '\\n', '  Ep8: val_loss=1.9902\nval_F1=0.6819 (train_F1=0.9735) [0.1s]', '\\n', '  --> Test macro-F1=0.6927',\n'\\n', '\\n=== Training with num_layers=2 ===', '\\n', '  Ep1: val_loss=0.7793\nval_F1=0.6240 (train_F1=0.6341) [0.1s]', '\\n', '  Ep2: val_loss=1.6605\nval_F1=0.6592 (train_F1=0.9339) [0.1s]', '\\n', '  Ep3: val_loss=1.6827\nval_F1=0.6675 (train_F1=0.9545) [0.1s]', '\\n', '  Ep4: val_loss=1.6880\nval_F1=0.6840 (train_F1=0.9645) [0.1s]', '\\n', '  Ep5: val_loss=1.7551\nval_F1=0.6860 (train_F1=0.9735) [0.1s]', '\\n', '  Ep6: val_loss=1.7615\nval_F1=0.6838 (train_F1=0.9765) [0.1s]', '\\n', '  Ep7: val_loss=1.9592\nval_F1=0.6960 (train_F1=0.9910) [0.1s]', '\\n', '  Ep8: val_loss=2.1754\nval_F1=0.7020 (train_F1=0.9925) [0.1s]', '\\n', '  --> Test macro-F1=0.7009',\n'\\n', '\\n=== Training with num_layers=3 ===', '\\n', '  Ep1: val_loss=0.7980\nval_F1=0.6259 (train_F1=0.6859) [0.2s]', '\\n', '  Ep2: val_loss=1.5874\nval_F1=0.6631 (train_F1=0.9290) [0.1s]', '\\n', '  Ep3: val_loss=1.6538\nval_F1=0.6759 (train_F1=0.9670) [0.1s]', '\\n', '  Ep4: val_loss=1.7894\nval_F1=0.6715 (train_F1=0.9765) [0.1s]', '\\n', '  Ep5: val_loss=1.8209\nval_F1=0.6858 (train_F1=0.9835) [0.1s]', '\\n', '  Ep6: val_loss=1.9672\nval_F1=0.6859 (train_F1=0.9845) [0.2s]', '\\n', '  Ep7: val_loss=2.0552\nval_F1=0.6980 (train_F1=0.9870) [0.2s]', '\\n', '  Ep8: val_loss=2.0251\nval_F1=0.6939 (train_F1=0.9845) [0.1s]', '\\n', '  --> Test macro-F1=0.6977',\n'\\n', '\\n=== Training with num_layers=4 ===', '\\n', '  Ep1: val_loss=0.9241\nval_F1=0.6450 (train_F1=0.6612) [0.2s]', '\\n', '  Ep2: val_loss=1.5868\nval_F1=0.6798 (train_F1=0.9380) [0.2s]', '\\n', '  Ep3: val_loss=1.6443\nval_F1=0.6760 (train_F1=0.9745) [0.2s]', '\\n', '  Ep4: val_loss=1.5723\nval_F1=0.6757 (train_F1=0.9575) [0.2s]', '\\n', '  Ep5: val_loss=1.7957\nval_F1=0.6695 (train_F1=0.9705) [0.2s]', '\\n', '  Ep6: val_loss=1.7929\nval_F1=0.6675 (train_F1=0.9735) [0.2s]', '\\n', '  Ep7: val_loss=1.6983\nval_F1=0.6858 (train_F1=0.9565) [0.2s]', '\\n', '  Ep8: val_loss=1.8660\nval_F1=0.6858 (train_F1=0.9745) [0.2s]', '\\n', '  --> Test macro-F1=0.6947',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 11', '\\n', 'Num classes: 2', '\\n', '\\n================ Weight Decay\n= 0.0 =================', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.3330\nval_F1=0.6514 (train_loss=0.4457) [0.4s]', '\\n', 'Epoch 2: val_loss=1.9464\nval_F1=0.6632 (train_loss=0.1340) [0.1s]', '\\n', 'Epoch 3: val_loss=1.6964\nval_F1=0.6760 (train_loss=0.1317) [0.1s]', '\\n', 'Epoch 4: val_loss=1.8611\nval_F1=0.6653 (train_loss=0.0995) [0.1s]', '\\n', 'Epoch 5: val_loss=1.9116\nval_F1=0.6798 (train_loss=0.0909) [0.1s]', '\\n', 'Epoch 6: val_loss=1.9055\nval_F1=0.6879 (train_loss=0.0872) [0.1s]', '\\n', 'Epoch 7: val_loss=2.1047\nval_F1=0.6899 (train_loss=0.0563) [0.1s]', '\\n', 'Epoch 8: val_loss=2.4149\nval_F1=0.6919 (train_loss=0.0205) [0.1s]', '\\n', 'Test macro-F1 (wd=0.0):\n0.6937', '\\n', '\\n================ Weight Decay = 1e-05 =================',\n'\\n', 'Epoch 1: val_loss=0.7363 val_F1=0.6781 (train_loss=0.6153) [0.1s]', '\\n',\n'Epoch 2: val_loss=1.7398 val_F1=0.6800 (train_loss=0.1922) [0.1s]', '\\n',\n'Epoch 3: val_loss=1.6673 val_F1=0.6718 (train_loss=0.1339) [0.1s]', '\\n',\n'Epoch 4: val_loss=1.7772 val_F1=0.6818 (train_loss=0.0975) [0.1s]', '\\n',\n'Epoch 5: val_loss=2.0559 val_F1=0.6939 (train_loss=0.0460) [0.1s]', '\\n',\n'Epoch 6: val_loss=1.7201 val_F1=0.6838 (train_loss=0.0827) [0.1s]', '\\n',\n'Epoch 7: val_loss=1.8092 val_F1=0.6838 (train_loss=0.0994) [0.1s]', '\\n',\n'Epoch 8: val_loss=2.1791 val_F1=0.6814 (train_loss=0.0609) [0.1s]', '\\n', 'Test\nmacro-F1 (wd=1e-05): 0.6934', '\\n', '\\n================ Weight Decay = 0.0001\n=================', '\\n', 'Epoch 1: val_loss=0.7963 val_F1=0.6174\n(train_loss=0.6138) [0.2s]', '\\n', 'Epoch 2: val_loss=1.6381 val_F1=0.6800\n(train_loss=0.1878) [0.2s]', '\\n', 'Epoch 3: val_loss=1.8055 val_F1=0.6799\n(train_loss=0.1115) [0.2s]', '\\n', 'Epoch 4: val_loss=1.8304 val_F1=0.6757\n(train_loss=0.0955) [0.2s]', '\\n', 'Epoch 5: val_loss=1.8228 val_F1=0.6880\n(train_loss=0.0804) [0.1s]', '\\n', 'Epoch 6: val_loss=1.9766 val_F1=0.6899\n(train_loss=0.0547) [0.1s]', '\\n', 'Epoch 7: val_loss=2.3001 val_F1=0.6919\n(train_loss=0.0326) [0.1s]', '\\n', 'Epoch 8: val_loss=2.3960 val_F1=0.6980\n(train_loss=0.0158) [0.1s]', '\\n', 'Test macro-F1 (wd=0.0001): 0.6959', '\\n',\n'\\n================ Weight Decay = 0.001 =================', '\\n', 'Epoch 1:\nval_loss=1.0740 val_F1=0.6675 (train_loss=0.4957) [0.1s]', '\\n', 'Epoch 2:\nval_loss=1.7557 val_F1=0.6758 (train_loss=0.1497) [0.1s]', '\\n', 'Epoch 3:\nval_loss=1.8365 val_F1=0.6675 (train_loss=0.1094) [0.1s]', '\\n', 'Epoch 4:\nval_loss=1.8613 val_F1=0.6797 (train_loss=0.0820) [0.1s]', '\\n', 'Epoch 5:\nval_loss=1.8897 val_F1=0.6838 (train_loss=0.0811) [0.1s]', '\\n', 'Epoch 6:\nval_loss=1.8482 val_F1=0.6817 (train_loss=0.0753) [0.1s]', '\\n', 'Epoch 7:\nval_loss=1.9200 val_F1=0.6980 (train_loss=0.0400) [0.1s]', '\\n', 'Epoch 8:\nval_loss=2.1099 val_F1=0.6980 (train_loss=0.0249) [0.1s]', '\\n', 'Test macro-F1\n(wd=0.001): 0.6999', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 8\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab size:', ' ', '11', ' ', '| Num classes:', ' ', '2', '\\n',\n'\\n==== Training with nhead=2 ====', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'epoch 1: val_f1=0.6351 |\ntrain_f1=0.7491 (0.7s)', '\\n', 'epoch 2: val_f1=0.6612 | train_f1=0.9505\n(0.4s)', '\\n', 'epoch 3: val_f1=0.6696 | train_f1=0.9535 (0.4s)', '\\n', 'epoch\n4: val_f1=0.6500 | train_f1=0.9640 (0.4s)', '\\n', 'epoch 5: val_f1=0.6738 |\ntrain_f1=0.9715 (0.4s)', '\\n', 'epoch 6: val_f1=0.6632 | train_f1=0.9695\n(0.4s)', '\\n', 'epoch 7: val_f1=0.6675 | train_f1=0.9690 (0.4s)', '\\n', 'epoch\n8: val_f1=0.6675 | train_f1=0.9770 (0.4s)', '\\n', 'Test macro-F1 with nhead=2:\n0.6854', '\\n', '\\n==== Training with nhead=4 ====', '\\n', 'epoch 1:\nval_f1=0.6760 | train_f1=0.6600 (0.4s)', '\\n', 'epoch 2: val_f1=0.6780 |\ntrain_f1=0.9470 (0.4s)', '\\n', 'epoch 3: val_f1=0.6759 | train_f1=0.9580\n(0.4s)', '\\n', 'epoch 4: val_f1=0.6818 | train_f1=0.9700 (0.4s)', '\\n', 'epoch\n5: val_f1=0.6939 | train_f1=0.9885 (0.4s)', '\\n', 'epoch 6: val_f1=0.6980 |\ntrain_f1=0.9920 (0.4s)', '\\n', 'epoch 7: val_f1=0.6960 | train_f1=0.9925\n(0.4s)', '\\n', 'epoch 8: val_f1=0.6960 | train_f1=0.9920 (0.4s)', '\\n', 'Test\nmacro-F1 with nhead=4: 0.6978', '\\n', '\\n==== Training with nhead=8 ====', '\\n',\n'epoch 1: val_f1=0.6407 | train_f1=0.6489 (0.4s)', '\\n', 'epoch 2: val_f1=0.6900\n| train_f1=0.9429 (0.4s)', '\\n', 'epoch 3: val_f1=0.6900 | train_f1=0.9590\n(0.4s)', '\\n', 'epoch 4: val_f1=0.6818 | train_f1=0.9760 (0.4s)', '\\n', 'epoch\n5: val_f1=0.6899 | train_f1=0.9860 (0.4s)', '\\n', 'epoch 6: val_f1=0.7000 |\ntrain_f1=0.9960 (0.4s)', '\\n', 'epoch 7: val_f1=0.6960 | train_f1=0.9890\n(0.4s)', '\\n', 'epoch 8: val_f1=0.7020 | train_f1=0.9955 (0.4s)', '\\n', 'Test\nmacro-F1 with nhead=8: 0.6999', '\\n', '\\nBest nhead based on dev macro-F1: 8\n(dev F1=0.7020)', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 12\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 11  Num classes: 2', '\\n', '\\n=== Training with num_layers=1 ===',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Ep1: val_loss=0.7389\nval_F1=0.6373 (train_F1=0.6577) [0.7s]', '\\n', '  Ep2: val_loss=1.6739\nval_F1=0.6639 (train_F1=0.9389) [0.3s]', '\\n', '  Ep3: val_loss=1.7123\nval_F1=0.6717 (train_F1=0.9580) [0.3s]', '\\n', '  Ep4: val_loss=1.6867\nval_F1=0.6757 (train_F1=0.9620) [0.3s]', '\\n', '  Ep5: val_loss=1.8986\nval_F1=0.6633 (train_F1=0.9745) [0.3s]', '\\n', '  Ep6: val_loss=1.9593\nval_F1=0.6715 (train_F1=0.9750) [0.3s]', '\\n', '  Ep7: val_loss=2.1252\nval_F1=0.6818 (train_F1=0.9825) [0.3s]', '\\n', '  Ep8: val_loss=2.3000\nval_F1=0.6919 (train_F1=0.9915) [0.3s]', '\\n', '  --> Test macro-F1=0.6958',\n'\\n', '\\n=== Training with num_layers=2 ===', '\\n', '  Ep1: val_loss=0.7451\nval_F1=0.6515 (train_F1=0.6094) [0.4s]', '\\n', '  Ep2: val_loss=1.6146\nval_F1=0.6421 (train_F1=0.9259) [0.4s]', '\\n', '  Ep3: val_loss=1.7002\nval_F1=0.6653 (train_F1=0.9610) [0.4s]', '\\n', '  Ep4: val_loss=1.7270\nval_F1=0.6697 (train_F1=0.9665) [0.4s]', '\\n', '  Ep5: val_loss=1.8850\nval_F1=0.6695 (train_F1=0.9795) [0.4s]', '\\n', '  Ep6: val_loss=1.8721\nval_F1=0.6860 (train_F1=0.9800) [0.4s]', '\\n', '  Ep7: val_loss=1.8725\nval_F1=0.6818 (train_F1=0.9700) [0.4s]', '\\n', '  Ep8: val_loss=1.9700\nval_F1=0.6940 (train_F1=0.9795) [0.4s]', '\\n', '  --> Test macro-F1=0.6980',\n'\\n', '\\n=== Training with num_layers=3 ===', '\\n', '  Ep1: val_loss=0.6699\nval_F1=0.6696 (train_F1=0.5834) [0.5s]', '\\n', '  Ep2: val_loss=1.6831\nval_F1=0.6729 (train_F1=0.9499) [0.5s]', '\\n', '  Ep3: val_loss=1.6168\nval_F1=0.6738 (train_F1=0.9615) [0.5s]', '\\n', '  Ep4: val_loss=1.6813\nval_F1=0.6320 (train_F1=0.9610) [0.5s]', '\\n', '  Ep5: val_loss=1.5961\nval_F1=0.6642 (train_F1=0.9439) [0.5s]', '\\n', '  Ep6: val_loss=1.7037\nval_F1=0.6612 (train_F1=0.9625) [0.5s]', '\\n', '  Ep7: val_loss=1.8135\nval_F1=0.6632 (train_F1=0.9725) [0.5s]', '\\n', '  Ep8: val_loss=1.8000\nval_F1=0.6736 (train_F1=0.9765) [0.5s]', '\\n', '  --> Test macro-F1=0.6874',\n'\\n', '\\n=== Training with num_layers=4 ===', '\\n', '  Ep1: val_loss=0.7255\nval_F1=0.6689 (train_F1=0.6238) [0.7s]', '\\n', '  Ep2: val_loss=1.3964\nval_F1=0.6759 (train_F1=0.9180) [0.7s]', '\\n', '  Ep3: val_loss=1.6529\nval_F1=0.6695 (train_F1=0.9700) [0.7s]', '\\n', '  Ep4: val_loss=1.6163\nval_F1=0.6757 (train_F1=0.9685) [0.6s]', '\\n', '  Ep5: val_loss=1.6101\nval_F1=0.6815 (train_F1=0.9700) [0.6s]', '\\n', '  Ep6: val_loss=1.7513\nval_F1=0.6879 (train_F1=0.9750) [0.6s]', '\\n', '  Ep7: val_loss=1.8668\nval_F1=0.6960 (train_F1=0.9855) [0.6s]', '\\n', '  Ep8: val_loss=2.2888\nval_F1=0.6936 (train_F1=0.9930) [0.6s]', '\\n', '  --> Test macro-F1=0.6966',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 19 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 11  Num classes: 2', '\\n', '\\n=== Training with num_layers=1 ===',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Ep1: val_loss=0.7285\nval_F1=0.6233 (train_F1=0.6960) [0.5s]', '\\n', '  Ep2: val_loss=1.5932\nval_F1=0.6692 (train_F1=0.9335) [0.1s]', '\\n', '  Ep3: val_loss=1.8116\nval_F1=0.6738 (train_F1=0.9665) [0.1s]', '\\n', '  Ep4: val_loss=1.8348\nval_F1=0.6757 (train_F1=0.9660) [0.1s]', '\\n', '  Ep5: val_loss=2.0110\nval_F1=0.6818 (train_F1=0.9725) [0.1s]', '\\n', '  Ep6: val_loss=2.2237\nval_F1=0.6919 (train_F1=0.9875) [0.1s]', '\\n', '  Ep7: val_loss=2.3129\nval_F1=0.6960 (train_F1=0.9940) [0.1s]', '\\n', '  Ep8: val_loss=2.6449\nval_F1=0.6960 (train_F1=0.9950) [0.1s]', '\\n', '  --> Test macro-F1=0.6967',\n'\\n', '\\n=== Training with num_layers=2 ===', '\\n', '  Ep1: val_loss=0.7439\nval_F1=0.6578 (train_F1=0.6704) [0.1s]', '\\n', '  Ep2: val_loss=1.6502\nval_F1=0.6384 (train_F1=0.9360) [0.1s]', '\\n', '  Ep3: val_loss=1.7042\nval_F1=0.6735 (train_F1=0.9535) [0.1s]', '\\n', '  Ep4: val_loss=1.8278\nval_F1=0.6736 (train_F1=0.9735) [0.1s]', '\\n', '  Ep5: val_loss=1.6825\nval_F1=0.6716 (train_F1=0.9665) [0.1s]', '\\n', '  Ep6: val_loss=1.8160\nval_F1=0.6675 (train_F1=0.9730) [0.1s]', '\\n', '  Ep7: val_loss=1.9003\nval_F1=0.6797 (train_F1=0.9805) [0.1s]', '\\n', '  Ep8: val_loss=2.0268\nval_F1=0.6899 (train_F1=0.9800) [0.1s]', '\\n', '  --> Test macro-F1=0.6947',\n'\\n', '\\n=== Training with num_layers=3 ===', '\\n', '  Ep1: val_loss=0.7328\nval_F1=0.3607 (train_F1=0.5525) [0.2s]', '\\n', '  Ep2: val_loss=1.5975\nval_F1=0.6655 (train_F1=0.8775) [0.2s]', '\\n', '  Ep3: val_loss=1.5467\nval_F1=0.6719 (train_F1=0.9495) [0.2s]', '\\n', '  Ep4: val_loss=1.6537\nval_F1=0.6820 (train_F1=0.9650) [0.2s]', '\\n', '  Ep5: val_loss=1.7520\nval_F1=0.6879 (train_F1=0.9715) [0.1s]', '\\n', '  Ep6: val_loss=1.9000\nval_F1=0.6859 (train_F1=0.9865) [0.1s]', '\\n', '  Ep7: val_loss=2.1282\nval_F1=0.6919 (train_F1=0.9885) [0.2s]', '\\n', '  Ep8: val_loss=2.0031\nval_F1=0.6918 (train_F1=0.9875) [0.2s]', '\\n', '  --> Test macro-F1=0.6936',\n'\\n', '\\n=== Training with num_layers=4 ===', '\\n', '  Ep1: val_loss=0.6528\nval_F1=0.6618 (train_F1=0.5751) [0.2s]', '\\n', '  Ep2: val_loss=1.4244\nval_F1=0.6632 (train_F1=0.9315) [0.2s]', '\\n', '  Ep3: val_loss=1.4633\nval_F1=0.6631 (train_F1=0.9213) [0.2s]', '\\n', '  Ep4: val_loss=1.5479\nval_F1=0.6588 (train_F1=0.9670) [0.2s]', '\\n', '  Ep5: val_loss=1.4558\nval_F1=0.6525 (train_F1=0.9545) [0.2s]', '\\n', '  Ep6: val_loss=1.5484\nval_F1=0.6780 (train_F1=0.9630) [0.2s]', '\\n', '  Ep7: val_loss=1.6597\nval_F1=0.6654 (train_F1=0.9735) [0.2s]', '\\n', '  Ep8: val_loss=1.7124\nval_F1=0.6675 (train_F1=0.9745) [0.2s]', '\\n', '  --> Test macro-F1=0.6874',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 11  Num classes: 2', '\\n', '\\n=== Training with num_layers=1 ===',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Ep1: val_loss=1.0390\nval_F1=0.6679 (train_F1=0.7644) [0.4s]', '\\n', '  Ep2: val_loss=1.7475\nval_F1=0.6609 (train_F1=0.9510) [0.2s]', '\\n', '  Ep3: val_loss=1.6312\nval_F1=0.6717 (train_F1=0.9485) [0.1s]', '\\n', '  Ep4: val_loss=1.7492\nval_F1=0.6675 (train_F1=0.9625) [0.1s]', '\\n', '  Ep5: val_loss=1.9451\nval_F1=0.6632 (train_F1=0.9760) [0.1s]', '\\n', '  Ep6: val_loss=1.9902\nval_F1=0.6838 (train_F1=0.9765) [0.1s]', '\\n', '  Ep7: val_loss=2.3445\nval_F1=0.6959 (train_F1=0.9875) [0.1s]', '\\n', '  Ep8: val_loss=2.4455\nval_F1=0.6980 (train_F1=0.9895) [0.1s]', '\\n', '  --> Test macro-F1=0.6959',\n'\\n', '\\n=== Training with num_layers=2 ===', '\\n', '  Ep1: val_loss=0.9760\nval_F1=0.6657 (train_F1=0.7274) [0.2s]', '\\n', '  Ep2: val_loss=1.6722\nval_F1=0.6592 (train_F1=0.9545) [0.1s]', '\\n', '  Ep3: val_loss=1.7340\nval_F1=0.6778 (train_F1=0.9645) [0.2s]', '\\n', '  Ep4: val_loss=1.7513\nval_F1=0.6779 (train_F1=0.9750) [0.2s]', '\\n', '  Ep5: val_loss=1.7975\nval_F1=0.6858 (train_F1=0.9775) [0.2s]', '\\n', '  Ep6: val_loss=2.0103\nval_F1=0.6940 (train_F1=0.9855) [0.2s]', '\\n', '  Ep7: val_loss=2.3142\nval_F1=0.6937 (train_F1=0.9920) [0.1s]', '\\n', '  Ep8: val_loss=2.2505\nval_F1=0.6939 (train_F1=0.9900) [0.1s]', '\\n', '  --> Test macro-F1=0.6968',\n'\\n', '\\n=== Training with num_layers=3 ===', '\\n', '  Ep1: val_loss=0.6923\nval_F1=0.6457 (train_F1=0.6084) [0.2s]', '\\n', '  Ep2: val_loss=1.5513\nval_F1=0.6757 (train_F1=0.9244) [0.2s]', '\\n', '  Ep3: val_loss=1.7667\nval_F1=0.6716 (train_F1=0.9725) [0.2s]', '\\n', '  Ep4: val_loss=1.7499\nval_F1=0.6858 (train_F1=0.9795) [0.2s]', '\\n', '  Ep5: val_loss=2.0242\nval_F1=0.6858 (train_F1=0.9835) [0.2s]', '\\n', '  Ep6: val_loss=1.9299\nval_F1=0.6878 (train_F1=0.9840) [0.2s]', '\\n', '  Ep7: val_loss=2.1000\nval_F1=0.6919 (train_F1=0.9925) [0.2s]', '\\n', '  Ep8: val_loss=1.9642\nval_F1=0.7020 (train_F1=0.9925) [0.2s]', '\\n', '  --> Test macro-F1=0.6990',\n'\\n', '\\n=== Training with num_layers=4 ===', '\\n', '  Ep1: val_loss=0.6653\nval_F1=0.6610 (train_F1=0.5875) [0.2s]', '\\n', '  Ep2: val_loss=1.5227\nval_F1=0.6588 (train_F1=0.9395) [0.2s]', '\\n', '  Ep3: val_loss=1.5533\nval_F1=0.6900 (train_F1=0.9700) [0.2s]', '\\n', '  Ep4: val_loss=1.5582\nval_F1=0.6840 (train_F1=0.9760) [0.2s]', '\\n', '  Ep5: val_loss=1.7927\nval_F1=0.6960 (train_F1=0.9890) [0.3s]', '\\n', '  Ep6: val_loss=1.9138\nval_F1=0.6960 (train_F1=0.9940) [0.3s]', '\\n', '  Ep7: val_loss=1.5711\nval_F1=0.6635 (train_F1=0.9590) [0.2s]', '\\n', '  Ep8: val_loss=1.4499\nval_F1=0.6880 (train_F1=0.9439) [0.2s]', '\\n', '  --> Test macro-F1=0.6907',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds\n(time limit is 30 minutes).']", ""], "analysis": ["", "The execution output shows that the training script successfully completed all\ntasks without any errors or bugs. The learning rate grid search was performed\nacross five different values, and the model's performance was evaluated using\nvalidation loss and macro-F1 scores. The results were saved to\n'experiment_data.npy'. No issues were identified in the execution.", "", "", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "F1 score is the harmonic mean of precision and recall. It is used to evaluate classification models.", "data": [{"dataset_name": "training", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "test", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error in predictions. Lower values indicate better performance.", "data": [{"dataset_name": "training", "final_value": 0.007296, "best_value": 0.007296}, {"dataset_name": "validation", "final_value": 2.773659, "best_value": 2.773659}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "Measures the F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9965, "best_value": 0.9965}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Measures the F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.702, "best_value": 0.702}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0112, "best_value": 0.0112}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6548, "best_value": 0.6548}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "Measures the F1 score achieved during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.6969}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score for the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.993, "best_value": 0.993}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score for the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score for the test dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6978, "best_value": 0.6978}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss for the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.024, "best_value": 0.024}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.4167, "best_value": 2.4167}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset.", "data": [{"dataset_name": "d_model=64", "final_value": 0.1217, "best_value": 0.1217}, {"dataset_name": "d_model=128", "final_value": 0.0273, "best_value": 0.0273}, {"dataset_name": "d_model=256", "final_value": 0.0457, "best_value": 0.0457}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score computed on the training dataset.", "data": [{"dataset_name": "d_model=64", "final_value": 0.962, "best_value": 0.962}, {"dataset_name": "d_model=128", "final_value": 0.9935, "best_value": 0.9935}, {"dataset_name": "d_model=256", "final_value": 0.9825, "best_value": 0.9825}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "d_model=64", "final_value": 1.6255, "best_value": 1.6255}, {"dataset_name": "d_model=128", "final_value": 2.2043, "best_value": 2.2043}, {"dataset_name": "d_model=256", "final_value": 2.4482, "best_value": 2.4482}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score computed on the validation dataset.", "data": [{"dataset_name": "d_model=64", "final_value": 0.6716, "best_value": 0.6859}, {"dataset_name": "d_model=128", "final_value": 0.6959, "best_value": 0.6959}, {"dataset_name": "d_model=256", "final_value": 0.696, "best_value": 0.698}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value computed on the test dataset.", "data": [{"dataset_name": "d_model=64", "final_value": 1.6037, "best_value": 1.6037}, {"dataset_name": "d_model=128", "final_value": 1.9511, "best_value": 1.9511}, {"dataset_name": "d_model=256", "final_value": 2.6313, "best_value": 2.6313}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score computed on the test dataset.", "data": [{"dataset_name": "d_model=64", "final_value": 0.6968, "best_value": 0.6968}, {"dataset_name": "d_model=128", "final_value": 0.6957, "best_value": 0.6957}, {"dataset_name": "d_model=256", "final_value": 0.6949, "best_value": 0.6949}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "The harmonic mean of precision and recall, used to measure classification performance.", "data": [{"dataset_name": "SPR_BENCH training", "final_value": 0.9945, "best_value": 0.9945}, {"dataset_name": "SPR_BENCH validation", "final_value": 0.6939, "best_value": 0.6939}, {"dataset_name": "SPR_BENCH test", "final_value": 0.6988, "best_value": 0.6988}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The value of the loss function, used to measure model error.", "data": [{"dataset_name": "SPR_BENCH training", "final_value": 0.0215, "best_value": 0.0215}, {"dataset_name": "SPR_BENCH validation", "final_value": 1.622, "best_value": 1.622}, {"dataset_name": "SPR_BENCH test", "final_value": 2.2856, "best_value": 2.2856}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value of the model during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.02, "best_value": 0.02}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the model during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.1754, "best_value": 2.1754}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score of the model during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9925, "best_value": 0.9925}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score of the model during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.702, "best_value": 0.702}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score achieved during training phase.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "wd_1e-05", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "wd_0.0001", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "wd_0.001", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value achieved during training phase.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.0205, "best_value": 0.0205}, {"dataset_name": "wd_1e-05", "final_value": 0.046, "best_value": 0.046}, {"dataset_name": "wd_0.0001", "final_value": 0.0158, "best_value": 0.0158}, {"dataset_name": "wd_0.001", "final_value": 0.0249, "best_value": 0.0249}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score achieved during validation phase.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.6919, "best_value": 0.6919}, {"dataset_name": "wd_1e-05", "final_value": 0.6939, "best_value": 0.6939}, {"dataset_name": "wd_0.0001", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "wd_0.001", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value achieved during validation phase.", "data": [{"dataset_name": "wd_0.0", "final_value": 1.333, "best_value": 1.333}, {"dataset_name": "wd_1e-05", "final_value": 0.7363, "best_value": 0.7363}, {"dataset_name": "wd_0.0001", "final_value": 0.7963, "best_value": 0.7963}, {"dataset_name": "wd_0.001", "final_value": 1.074, "best_value": 1.074}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score achieved during testing phase.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.6937, "best_value": 0.6937}, {"dataset_name": "wd_1e-05", "final_value": 0.6934, "best_value": 0.6934}, {"dataset_name": "wd_0.0001", "final_value": 0.6959, "best_value": 0.6959}, {"dataset_name": "wd_0.001", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss value achieved during testing phase.", "data": [{"dataset_name": "wd_0.0", "final_value": 2.4181, "best_value": 2.4181}, {"dataset_name": "wd_1e-05", "final_value": 2.1453, "best_value": 2.1453}, {"dataset_name": "wd_0.0001", "final_value": 2.4127, "best_value": 2.4127}, {"dataset_name": "wd_0.001", "final_value": 2.1014, "best_value": 2.1014}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "The harmonic mean of precision and recall, used to measure model accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "loss", "lower_is_better": true, "description": "A measure of error in the model's predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.4656, "best_value": 2.4656}]}]}, {"metric_names": [{"metric_name": "Training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0621, "best_value": 0.0621}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.97, "best_value": 1.97}]}, {"metric_name": "Training F1 score", "lower_is_better": false, "description": "F1 score during training. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9795, "best_value": 0.9795}]}, {"metric_name": "Validation F1 score", "lower_is_better": false, "description": "F1 score during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.694, "best_value": 0.694}]}]}, {"metric_names": [{"metric_name": "Training loss", "lower_is_better": true, "description": "The loss value during training, which indicates how well the model is performing on the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0244, "best_value": 0.0244}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "The loss value during validation, which indicates how well the model is generalizing to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.6449, "best_value": 2.6449}]}, {"metric_name": "Training F1 score", "lower_is_better": false, "description": "The F1 score during training, which is the harmonic mean of precision and recall for the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "Validation F1 score", "lower_is_better": false, "description": "The F1 score during validation, which is the harmonic mean of precision and recall for the validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset, indicating the model's error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0285, "best_value": 0.0285}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset, indicating the model's error on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9642, "best_value": 1.9642}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score computed on the training dataset, representing the harmonic mean of precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9925, "best_value": 0.9925}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score computed on the validation dataset, representing the harmonic mean of precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.702, "best_value": 0.702}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, true, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_confusion_matrix_dmodel256.png"], ["../../logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_testF1_bar.png", "../../logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_confusion_dr_0.0.png"], ["../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_confusion_nl_2.png"], ["../../logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_loss_curves_weight_decay.png", "../../logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_f1_curves_weight_decay.png", "../../logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_test_f1_weight_decay.png"], ["../../logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_confusion_nl_2.png"], ["../../logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_confusion_nl_1.png"], ["../../logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_confusion_nl_3.png"], ["../../logs/0-run/experiment_results/seed_aggregation_36115645b8bd4f8cb155aafb1a925e98/SPR_BENCH_agg_f1_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_36115645b8bd4f8cb155aafb1a925e98/SPR_BENCH_agg_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_36115645b8bd4f8cb155aafb1a925e98/SPR_BENCH_agg_test_f1_bar.png"]], "plot_paths": [["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_confusion_matrix_dmodel256.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_testF1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_confusion_dr_0.0.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_confusion_nl_2.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_loss_curves_weight_decay.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_f1_curves_weight_decay.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_test_f1_weight_decay.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_confusion_nl_2.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_confusion_nl_1.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_confusion_nl_3.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_36115645b8bd4f8cb155aafb1a925e98/SPR_BENCH_agg_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_36115645b8bd4f8cb155aafb1a925e98/SPR_BENCH_agg_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_36115645b8bd4f8cb155aafb1a925e98/SPR_BENCH_agg_test_f1_bar.png"]], "plot_analyses": [[{"analysis": "The first plot shows the training and validation loss across 16 epochs. The training loss decreases consistently and stabilizes at a low value, indicating that the model is learning effectively on the training dataset. However, the validation loss increases steadily after the initial epochs, suggesting overfitting. This indicates that the model is not generalizing well to unseen data, and further regularization techniques, such as dropout, weight decay, or early stopping, should be considered.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_loss_curve.png"}, {"analysis": "The second plot illustrates the training and validation macro-F1 scores across the same 16 epochs. The training F1 score quickly approaches 1.0 and remains stable, indicating excellent performance on the training dataset. However, the validation F1 score remains relatively low and shows only a slight upward trend, further confirming the overfitting observed in the loss plot. This suggests that while the model is memorizing the training data, it struggles to generalize to the validation set. Adjustments to the learning rate, batch size, or training duration may help mitigate this issue.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_f1_curve.png"}, {"analysis": "The third plot is a confusion matrix, which provides insight into the model's classification performance. The diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. The plot shows a strong diagonal pattern, suggesting that the model performs well overall. However, the intensity of the off-diagonal elements indicates room for improvement in certain classes. This could be addressed by rebalancing the dataset, modifying the loss function to penalize misclassifications more heavily, or refining the model's symbolic reasoning capabilities.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6752534a254a558377ca1d0c10bef1_proc_3464961/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The loss curves indicate that smaller batch sizes (e.g., 32 and 64) tend to result in better convergence for both training and validation losses. Larger batch sizes (e.g., 512) show slower convergence and higher validation loss, suggesting possible overfitting or insufficient gradient updates. The validation loss for batch sizes 32 and 64 is relatively stable, indicating better generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 score analysis suggests that smaller batch sizes (32 and 64) achieve higher and more stable validation F1 scores compared to larger batch sizes. The training F1 scores for all batch sizes converge to high values, but the generalization gap is more pronounced for larger batch sizes like 512, where the validation F1 lags significantly.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_f1_curves.png"}, {"analysis": "The bar plot shows that the test Macro-F1 score is relatively consistent across all batch sizes, with only slight variations. This indicates that while training and validation metrics vary significantly with batch size, the final test performance is less sensitive to batch size changes, though smaller batch sizes may still be preferable for stability.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5d24f237aa5e46c3857e8e340a4741cc_proc_3464963/SPR_BENCH_test_f1_bar.png"}], [{"analysis": "The loss curves for different batch sizes (64, 128, and 256) show that the training loss decreases consistently across epochs for all configurations. However, the validation loss exhibits different behaviors: for batch size 64, it stabilizes after an initial decrease; for batch size 128, it decreases but shows signs of overfitting as it starts to increase slightly after epoch 6; and for batch size 256, it fluctuates significantly, indicating instability in training. This suggests that smaller batch sizes provide more stable generalization, while larger batch sizes may require additional regularization techniques or adjustments to learning rate.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves indicate that smaller batch sizes (64 and 128) achieve better validation performance compared to batch size 256. Batch size 64 achieves the most stable and high validation F1 score, while batch size 256 shows significant instability in validation performance. This confirms that smaller batch sizes are better suited for this task, likely due to improved gradient updates and generalization capabilities.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_f1_curves.png"}, {"analysis": "The test F1 scores across different d_model values (64, 128, and 256) are very close, with all configurations achieving approximately 0.7. This suggests that the model's performance is not highly sensitive to the dimensionality of the model within this range, indicating that other hyperparameters or model components might have a more significant impact on performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for d_model=256 shows that the model performs reasonably well, with a good balance between true positives and true negatives. However, there are noticeable misclassifications in both classes, indicating that there is room for improvement in terms of precision and recall. This could be addressed by further fine-tuning hyperparameters or incorporating additional regularization to reduce overfitting.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2c7a7ff4a29f4c7ab3fbeaaf760025cc_proc_3464964/SPR_BENCH_confusion_matrix_dmodel256.png"}], [{"analysis": "This plot illustrates the Macro-F1 score progression over epochs for different dropout rates. The training Macro-F1 scores consistently improve across all dropout rates, indicating that the model is learning effectively. However, the validation Macro-F1 scores show divergence at higher dropout rates (e.g., 0.2 and 0.3), suggesting potential underfitting or insufficient learning capacity. Lower dropout rates (0.0 and 0.05) appear to achieve better validation performance, with minimal gap between training and validation scores, indicating better generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot depicts the cross-entropy loss progression over epochs for different dropout rates. Training losses decrease steadily for all configurations, confirming effective model learning. However, validation losses increase after an initial drop for higher dropout rates (e.g., 0.2, 0.3), signaling over-regularization or underfitting. Lower dropout rates (0.0, 0.05) maintain relatively stable validation losses, aligning with the observations from the Macro-F1 plot.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_loss_curves.png"}, {"analysis": "This bar chart presents the test Macro-F1 scores for different dropout rates. The results indicate that all dropout configurations achieve comparable test performance, with slight variations. Dropout rates around 0.0 and 0.05 seem to marginally outperform higher dropout rates, consistent with the trends observed in training and validation metrics.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_testF1_bar.png"}, {"analysis": "The confusion matrix for the best-performing dropout rate (0.0) shows a balanced distribution of true positives and true negatives. While the model achieves good performance, there are still notable false positives and false negatives, which could be addressed through further fine-tuning or architectural modifications. The overall performance suggests that the model is effective in capturing the underlying patterns but has room for improvement in handling edge cases.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_5892bf1260724c22baf62a7652ca83f6_proc_3464961/SPR_BENCH_confusion_dr_0.0.png"}], [{"analysis": "The plot shows the macro-F1 scores for both training and validation datasets across different epochs and numbers of layers (nl). Training macro-F1 scores quickly converge to near-perfect values (close to 1.0) by the second epoch, indicating that the model is learning well on the training data. However, the validation macro-F1 scores remain significantly lower, fluctuating between 0.65 and 0.75 across epochs and layers. This indicates potential overfitting, as the model performs well on the training data but struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_f1_curves.png"}, {"analysis": "The plot illustrates the cross-entropy loss for training and validation datasets across epochs and different numbers of layers (nl). Training loss steadily decreases and converges to near-zero values, again indicating effective learning on the training data. However, validation loss increases after the second epoch for all configurations, suggesting that overfitting is occurring. The gap between training and validation loss widens as the number of epochs increases, further confirming overfitting issues.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart presents the test macro-F1 scores for models with different numbers of layers (nl). While all configurations achieve similar macro-F1 scores in the range of 0.693 to 0.701, the model with nl=2 slightly outperforms the others. This suggests that increasing the number of layers beyond a certain point does not significantly improve performance and may even lead to diminishing returns.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for the best-performing model (nl=2) provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. The model correctly classifies a substantial number of samples in both classes, but there are still a notable number of misclassifications. This indicates that while the model performs well overall, there is room for improvement in reducing false positives and false negatives.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_confusion_nl_2.png"}], [{"analysis": "This plot shows the training and validation loss for various weight decay (wd) values over 8 epochs. The training loss generally decreases rapidly in the first two epochs and stabilizes afterward for all weight decay values. However, the validation loss exhibits different behaviors depending on the weight decay. For wd = 0.0 and wd = 1e-05, the validation loss increases slightly after epoch 2, indicating potential overfitting. For wd = 0.0001 and wd = 0.001, the validation loss remains relatively stable, suggesting better generalization. Overall, wd = 0.0001 and wd = 0.001 appear to provide the best trade-off between training and validation performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_loss_curves_weight_decay.png"}, {"analysis": "This plot depicts the training and validation Macro-F1 scores for different weight decay values over 8 epochs. Training Macro-F1 scores quickly reach near-perfect performance for all weight decay values, while validation Macro-F1 scores plateau at around 0.7. The gap between training and validation scores indicates that the model achieves high training performance but struggles to generalize to validation data. Weight decay values of 0.0001 and 0.001 show slightly better validation Macro-F1 scores, suggesting they may help mitigate overfitting.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_f1_curves_weight_decay.png"}, {"analysis": "This bar chart summarizes the test Macro-F1 scores for different weight decay values. The scores are very close across all configurations, with wd = 0.0001 and wd = 0.001 achieving a slightly higher Macro-F1 score of 0.70 compared to 0.69 for wd = 0.0 and wd = 1e-05. This indicates that while weight decay has a minor impact on test performance, wd = 0.0001 and wd = 0.001 provide a slight edge in generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e1bfc1833bdc400cba16fdf71c1f7996_proc_3464963/SPR_BENCH_test_f1_weight_decay.png"}], [{"analysis": "The Macro-F1 scores for training and validation show that for all configurations (nhead_2, nhead_4, nhead_8), the models converge rapidly within the first two epochs. After convergence, the training Macro-F1 scores remain consistently high (~1.0), indicating that the models are capable of fitting the training data well. However, the validation Macro-F1 scores plateau at slightly lower levels (~0.95), suggesting a small generalization gap. The nhead_8 configuration shows a marginally better validation performance compared to nhead_4 and nhead_2, but the differences are not substantial.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_f1_curves.png"}, {"analysis": "The loss curves indicate a rapid decrease in training loss for all configurations within the first two epochs, corresponding to the rapid convergence in Macro-F1 scores. However, the validation loss for nhead_2 and nhead_4 stabilizes at a higher level compared to nhead_8, which suggests that nhead_8 may generalize better to unseen data. Notably, the validation loss for nhead_8 increases slightly after epoch 6, potentially indicating the onset of overfitting.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_loss_curves.png"}, {"analysis": "The test Macro-F1 scores for all configurations (nhead_2, nhead_4, nhead_8) are very similar, with all configurations achieving approximately 0.65. This indicates that while the models perform well on training and validation data, their generalization to the test set is limited. The choice of nhead configuration does not appear to have a significant impact on test performance, highlighting a potential need for further optimization or architectural changes to improve test generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_044cb9aa8c404d73a4e7a08f57b00412_proc_3464964/SPR_BENCH_test_f1_bar.png"}], [{"analysis": "The training Macro-F1 scores for all configurations (nl_1 to nl_4) converge rapidly and reach near-perfect values by epoch 3. The validation Macro-F1 scores, however, show less pronounced improvements, with nl_1 and nl_2 achieving higher and more stable scores compared to nl_3 and nl_4. This indicates that nl_1 and nl_2 configurations might generalize better to unseen data, while nl_3 and nl_4 suffer from potential underfitting or inability to capture the patterns effectively.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_f1_curves.png"}, {"analysis": "The training loss decreases sharply for all configurations and stabilizes at low values by epoch 3. However, the validation loss increases for all configurations after an initial drop, with nl_1 and nl_2 showing a slower increase compared to nl_3 and nl_4. This divergence between training and validation loss suggests overfitting, particularly for nl_3 and nl_4. The results highlight the need for regularization techniques or early stopping to prevent overfitting.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_loss_curves.png"}, {"analysis": "The test Macro-F1 scores are relatively consistent across configurations, with nl_2 achieving the highest score (0.698). The differences between configurations are minor, indicating that the number of layers (nl) has a limited impact on the model's final performance. However, nl_2 appears to be the most balanced configuration based on its performance on both validation and test sets.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for nl_2 shows a balanced performance across both classes, with a slightly higher number of true positives and true negatives compared to false positives and false negatives. This indicates that the nl_2 configuration is effective at distinguishing between classes, although there is still room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/SPR_BENCH_confusion_nl_2.png"}], [{"analysis": "The plot compares training and validation Macro-F1 scores across epochs for different configurations of the number of layers (nl). Training Macro-F1 scores improve rapidly and converge to near-perfect values for all configurations. However, validation Macro-F1 scores plateau at lower levels, indicating potential overfitting. The nl=1 configuration achieves the highest validation Macro-F1 score, suggesting that simpler models generalize better for this task.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_f1_curves.png"}, {"analysis": "The plot shows the training and validation loss across epochs for different configurations of the number of layers (nl). Training loss decreases smoothly, indicating effective learning. However, validation loss increases after an initial decrease, especially for higher nl configurations, which is a clear sign of overfitting. The nl=1 configuration exhibits the most stable validation loss, further supporting its better generalization capabilities.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar plot compares the test Macro-F1 scores for different configurations of the number of layers (nl). All configurations achieve similar scores, with nl=1 slightly outperforming the others. This consistency suggests that the number of layers has a limited impact on test performance, but simpler models (nl=1) might still be preferable for better generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for the best-performing configuration (nl=1) shows a balanced performance across classes, with a slightly higher number of true positives compared to false positives and false negatives. This indicates that the model is relatively robust in its predictions, although there is still room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/SPR_BENCH_confusion_nl_1.png"}], [{"analysis": "This plot shows the Macro-F1 scores for training and validation over multiple epochs for different numbers of layers (nl). The training Macro-F1 scores improve steadily and reach near-perfect levels for all configurations, indicating that the model is capable of fitting the training data well. However, the validation Macro-F1 scores show a plateau or slight decline after the initial epochs, particularly for configurations with higher nl values. This suggests potential overfitting as the model complexity increases. Additionally, nl=1 and nl=2 appear to generalize slightly better compared to nl=3 and nl=4, as evidenced by their relatively stable validation performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot depicts the cross-entropy loss for training and validation over multiple epochs for different numbers of layers (nl). The training loss decreases consistently for all configurations, demonstrating effective learning. However, the validation loss increases after an initial decline, particularly for nl=3 and nl=4, suggesting overfitting. The higher validation losses for these configurations imply that increasing the number of layers may reduce the model's ability to generalize, likely due to increased model complexity.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_loss_curves.png"}, {"analysis": "This bar chart compares the test Macro-F1 scores for different numbers of layers (nl). The scores are relatively close, ranging from 0.691 to 0.699, with nl=3 achieving the highest Macro-F1 score. This indicates that while increasing the number of layers beyond a certain point does not significantly degrade performance, it also does not result in substantial improvements. nl=3 appears to strike a balance between model complexity and generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_test_f1_bar.png"}, {"analysis": "This confusion matrix evaluates the performance of the best-performing configuration (nl=3) on the test set. The matrix shows a reasonable balance between true positives and true negatives, but the presence of misclassifications (false positives and false negatives) indicates areas for improvement. The relatively higher number of false negatives suggests that the model might struggle with correctly identifying certain classes. This insight could guide future adjustments, such as refining the symbolic reasoning modules or optimizing hyperparameters further.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/SPR_BENCH_confusion_nl_3.png"}], []], "vlm_feedback_summary": ["The plots highlight significant overfitting in the model, as evidenced by the\ndivergence between training and validation metrics. While training performance\nis excellent, validation performance remains suboptimal, indicating poor\ngeneralization. The confusion matrix suggests that the model performs reasonably\nwell but still has room for improvement in handling specific classes. Further\nhyperparameter tuning and regularization techniques are recommended to address\nthese issues and improve overall performance.", "[]", "The analysis highlights that smaller batch sizes (32 and 64) are generally more\neffective in achieving stable and better-performing models, particularly in\nterms of validation loss and F1 score. Larger batch sizes may lead to\noverfitting and less stable validation performance, though the final test\nperformance remains relatively consistent across batch sizes.", "The analysis highlights that smaller batch sizes (e.g., 64) yield more stable\nand better generalization performance. Validation loss and F1 scores suggest\npotential overfitting with larger batch sizes and room for improvement in\nmisclassification rates. The model's performance is relatively insensitive to\nd_model values within the tested range, and additional fine-tuning or\nregularization may enhance results.", "The plots collectively suggest that lower dropout rates (0.0 and 0.05) lead to\nbetter generalization and performance on the SPR task. Training is effective\nacross all configurations, but higher dropout rates result in over-\nregularization and underfitting. The test results and confusion matrix highlight\nthe model's robustness but also indicate areas for optimization, such as\nreducing false positives and negatives.", "The analysis highlights overfitting as a major issue, with training performance\nsignificantly outpacing validation performance. While the model shows strong\nlearning capability on the training data, its generalization to validation and\ntest data is limited. The best-performing configuration (nl=2) achieves slightly\nbetter test macro-F1 scores, but further optimization is needed to address\noverfitting and improve generalization.", "The plots indicate that weight decay values of 0.0001 and 0.001 provide better\ngeneralization performance, as evidenced by lower validation loss and slightly\nhigher validation and test Macro-F1 scores. These configurations appear to\nstrike a balance between minimizing overfitting and maintaining good performance\non unseen data.", "The plots reveal that the models converge quickly and achieve high training\nperformance, but there is a noticeable generalization gap between training and\nvalidation/test performance. The nhead_8 configuration shows slightly better\nvalidation performance, but all configurations perform similarly on the test\nset, indicating limited test generalization. Further optimization or\narchitectural changes may be necessary to enhance test performance.", "The plots indicate that nl_2 is the most balanced configuration, achieving the\nbest trade-off between training, validation, and test performance. Overfitting\nis observed in validation loss trends for all configurations, particularly nl_3\nand nl_4, suggesting the need for further regularization. The confusion matrix\nconfirms that nl_2 performs well in classifying both classes, but\nmisclassifications still exist.", "The analysis reveals that simpler models (nl=1) generalize better, as evidenced\nby higher validation Macro-F1 scores and stable validation loss. Overfitting is\nobserved in deeper models (higher nl), as shown by increasing validation loss\ndespite decreasing training loss. Test performance is similar across\nconfigurations, but nl=1 offers a slight advantage. The confusion matrix\nhighlights a balanced performance but indicates some misclassification issues\nthat could be addressed in further optimization.", "The plots indicate that while the model performs well on training data, there\nare signs of overfitting, especially for configurations with higher numbers of\nlayers. The validation and test results suggest that nl=3 provides the best\nbalance between model complexity and generalization. However, the confusion\nmatrix highlights areas for further optimization, particularly in reducing false\nnegatives.", "[]"], "exec_time": [10.687866687774658, 8.325813055038452, 9.881422758102417, 20.30759286880493, 21.512779474258423, 7.63445258140564, 8.116342782974243, 12.344619989395142, 19.95716643333435, 8.122910261154175, 9.507100105285645, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"num_epochs\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed:\n    epochs = ed.get(\"epochs\", [])\n    train_loss = ed.get(\"losses\", {}).get(\"train\", [])\n    val_loss = ed.get(\"losses\", {}).get(\"val\", [])\n    train_f1 = ed.get(\"metrics\", {}).get(\"train_f1\", [])\n    val_f1 = ed.get(\"metrics\", {}).get(\"val_f1\", [])\n    preds = np.array(ed.get(\"predictions\", []))\n    gts = np.array(ed.get(\"ground_truth\", []))\n\n    # 1) Loss curve\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # 2) F1 curve\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation F1\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # 3) Confusion matrix\n    if preds.size and gts.size:\n        try:\n            num_classes = len(np.unique(np.concatenate([preds, gts])))\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\"SPR_BENCH \u2013 Confusion Matrix\")\n            plt.tight_layout()\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------- paths / load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# -------- helper to print summary ----------\ndef summarise_runs(runs):\n    header = f\"{'lr':>8} | {'best_val_f1':>12} | {'test_f1':>8}\"\n    print(header)\n    print(\"-\" * len(header))\n    for r in runs:\n        best_val = (\n            max(r[\"metrics\"][\"val_f1\"]) if r[\"metrics\"][\"val_f1\"] else float(\"nan\")\n        )\n        print(f\"{r['lr']:8.4g} | {best_val:12.4f} | {r['test_f1']:8.4f}\")\n\n\n# -------- iterate datasets ----------\nfor ds_name, lr_dict in experiment_data.items():\n    runs = lr_dict.get(ds_name, []) if isinstance(lr_dict, dict) else lr_dict\n    if not runs:\n        continue\n    summarise_runs(runs)\n\n    # --- per-lr plots (<=5) ---\n    for r in runs[:5]:  # ensure at most 5\n        lr = r[\"lr\"]\n        epochs = r[\"epochs\"]\n        # F1 curve\n        try:\n            plt.figure()\n            plt.plot(epochs, r[\"metrics\"][\"train_f1\"], label=\"Train F1\")\n            plt.plot(epochs, r[\"metrics\"][\"val_f1\"], label=\"Val F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{ds_name} F1 vs Epoch (lr={lr})\")\n            plt.legend()\n            fname = f\"{ds_name}_f1_curve_lr{lr}.png\".replace(\".\", \"_\")\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating F1 plot for lr={lr}: {e}\")\n            plt.close()\n\n        # Loss curve\n        try:\n            plt.figure()\n            plt.plot(epochs, r[\"losses\"][\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, r[\"losses\"][\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name} Loss vs Epoch (lr={lr})\")\n            plt.legend()\n            fname = f\"{ds_name}_loss_curve_lr{lr}.png\".replace(\".\", \"_\")\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating Loss plot for lr={lr}: {e}\")\n            plt.close()\n\n    # --- aggregated test-F1 bar plot ---\n    try:\n        plt.figure()\n        lrs = [r[\"lr\"] for r in runs]\n        testF = [r[\"test_f1\"] for r in runs]\n        plt.bar(range(len(lrs)), testF, tick_label=[str(lr) for lr in lrs])\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Test Macro-F1\")\n        plt.title(f\"{ds_name} Test F1 across Learning Rates\")\n        fname = f\"{ds_name}_testF1_vs_lr.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated bar plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- data load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_dict = experiment_data.get(\"batch_size_tuning\", {}).get(\"SPR_BENCH\", {})\n# keep only successful runs\nbs_keys = [k for k, v in spr_dict.items() if \"error\" not in v]\nbs_keys_sorted = sorted(bs_keys, key=lambda x: int(x))\n\n\n# helper: get lists for plotting\ndef get_list(bs, field, split):\n    return spr_dict[bs][field][split]\n\n\n# ---------- Plot 1: Loss curves ----------\ntry:\n    plt.figure(figsize=(10, 4))\n    epochs = spr_dict[bs_keys_sorted[0]][\"epochs\"]\n    # Left subplot: train loss\n    ax1 = plt.subplot(1, 2, 1)\n    for bs in bs_keys_sorted:\n        ax1.plot(epochs, get_list(bs, \"losses\", \"train\"), label=f\"bs={bs}\")\n    ax1.set_title(\"Train Loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n    # Right subplot: val loss\n    ax2 = plt.subplot(1, 2, 2)\n    for bs in bs_keys_sorted:\n        ax2.plot(epochs, get_list(bs, \"losses\", \"val\"), label=f\"bs={bs}\")\n    ax2.set_title(\"Validation Loss\")\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"Loss\")\n    ax2.legend()\n    plt.suptitle(\"SPR_BENCH Loss Curves\\nLeft: Train Loss, Right: Val Loss\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: F1 curves ----------\ntry:\n    plt.figure(figsize=(10, 4))\n    epochs = spr_dict[bs_keys_sorted[0]][\"epochs\"]\n    # Left subplot: train F1\n    ax1 = plt.subplot(1, 2, 1)\n    for bs in bs_keys_sorted:\n        ax1.plot(epochs, get_list(bs, \"metrics\", \"train_f1\"), label=f\"bs={bs}\")\n    ax1.set_title(\"Train Macro-F1\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"F1\")\n    ax1.legend()\n    # Right subplot: val F1\n    ax2 = plt.subplot(1, 2, 2)\n    for bs in bs_keys_sorted:\n        ax2.plot(epochs, get_list(bs, \"metrics\", \"val_f1\"), label=f\"bs={bs}\")\n    ax2.set_title(\"Validation Macro-F1\")\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"F1\")\n    ax2.legend()\n    plt.suptitle(\"SPR_BENCH Macro-F1 Curves\\nLeft: Train F1, Right: Val F1\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 curves plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: Test F1 bar ----------\ntry:\n    test_f1s = [spr_dict[bs][\"test_f1\"] for bs in bs_keys_sorted]\n    plt.figure(figsize=(6, 4))\n    plt.bar(range(len(bs_keys_sorted)), test_f1s, tick_label=bs_keys_sorted)\n    plt.title(\"SPR_BENCH Test Macro-F1 vs Batch Size\")\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Macro-F1\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating Test F1 bar plot: {e}\")\n    plt.close()\n\n# ---------- Print evaluation summary ----------\ntry:\n    for bs, f1 in zip(bs_keys_sorted, test_f1s):\n        print(f\"Batch size {bs}: Test macro-F1 = {f1:.4f}\")\n    best_idx = int(np.argmax(test_f1s))\n    print(\n        f\"Best batch size: {bs_keys_sorted[best_idx]} with macro-F1={test_f1s[best_idx]:.4f}\"\n    )\nexcept Exception as e:\n    print(f\"Error printing evaluation summary: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndm_data = experiment_data.get(\"d_model_tuning\", {})\nif not dm_data:\n    print(\"No d_model_tuning data found.\")\n    exit()\n\ndmodels = sorted(map(int, dm_data.keys()))\ncolors = plt.cm.tab10(np.linspace(0, 1, len(dmodels)))\n\n# ---------- PLOT 1: loss curves ----------\ntry:\n    plt.figure()\n    for c, dm in zip(colors, dmodels):\n        epochs = dm_data[str(dm)][\"epochs\"]\n        plt.plot(\n            epochs,\n            dm_data[str(dm)][\"losses\"][\"train\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"{dm}-train\",\n        )\n        plt.plot(\n            epochs,\n            dm_data[str(dm)][\"losses\"][\"val\"],\n            color=c,\n            linestyle=\"--\",\n            label=f\"{dm}-val\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curves\\nSolid: Train, Dashed: Val\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# ---------- PLOT 2: F1 curves ----------\ntry:\n    plt.figure()\n    for c, dm in zip(colors, dmodels):\n        epochs = dm_data[str(dm)][\"epochs\"]\n        plt.plot(\n            epochs,\n            dm_data[str(dm)][\"metrics\"][\"train_f1\"],\n            color=c,\n            linestyle=\"-\",\n            label=f\"{dm}-train\",\n        )\n        plt.plot(\n            epochs,\n            dm_data[str(dm)][\"metrics\"][\"val_f1\"],\n            color=c,\n            linestyle=\"--\",\n            label=f\"{dm}-val\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH \u2013 Macro-F1 Curves\\nSolid: Train, Dashed: Val\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves plot: {e}\")\n    plt.close()\n\n# ---------- PLOT 3: Test F1 bar chart ----------\ntry:\n    plt.figure()\n    test_f1s = [dm_data[str(dm)][\"test_f1\"] for dm in dmodels]\n    plt.bar(range(len(dmodels)), test_f1s, tick_label=dmodels, color=colors)\n    plt.ylabel(\"Test Macro F1\")\n    plt.xlabel(\"d_model\")\n    plt.title(\"SPR_BENCH \u2013 Test F1 vs d_model\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# ---------- identify best model ----------\nbest_dm = max(dmodels, key=lambda d: dm_data[str(d)][\"best_val_f1\"])\nbest_entry = dm_data[str(best_dm)]\nprint(\n    f'Best d_model = {best_dm} | Val F1 = {best_entry[\"best_val_f1\"]:.4f} | '\n    f'Test F1 = {best_entry[\"test_f1\"]:.4f}'\n)\n\n# ---------- PLOT 4: Confusion matrix for best model ----------\ntry:\n    preds = np.array(best_entry[\"predictions\"])\n    gts = np.array(best_entry[\"ground_truth\"])\n    num_cls = len(np.unique(gts))\n    conf = np.zeros((num_cls, num_cls), dtype=int)\n    for p, g in zip(preds, gts):\n        conf[g, p] += 1\n    plt.figure()\n    im = plt.imshow(conf, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(f\"SPR_BENCH \u2013 Confusion Matrix (d_model={best_dm})\")\n    for i in range(num_cls):\n        for j in range(num_cls):\n            plt.text(\n                j,\n                i,\n                conf[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if conf[i, j] > conf.max() / 2 else \"black\",\n                fontsize=8,\n            )\n    fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_dmodel{best_dm}.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_exp = {}\n\nif spr_exp:  # proceed only if data exists\n    dropouts = sorted(spr_exp.keys(), key=lambda x: float(x.split(\"_\")[1]))\n    epochs = spr_exp[dropouts[0]][\"epochs\"]\n\n    # ---------- 1. F1 curves ----------\n    try:\n        plt.figure()\n        for dr in dropouts:\n            plt.plot(epochs, spr_exp[dr][\"metrics\"][\"train_f1\"], label=f\"train {dr}\")\n            plt.plot(epochs, spr_exp[dr][\"metrics\"][\"val_f1\"], \"--\", label=f\"val {dr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Macro-F1 vs Epochs (dropout sweep)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curves: {e}\")\n        plt.close()\n\n    # ---------- 2. Loss curves ----------\n    try:\n        plt.figure()\n        for dr in dropouts:\n            plt.plot(epochs, spr_exp[dr][\"losses\"][\"train\"], label=f\"train {dr}\")\n            plt.plot(epochs, spr_exp[dr][\"losses\"][\"val\"], \"--\", label=f\"val {dr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Loss vs Epochs (dropout sweep)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # ---------- 3. Test F1 bar ----------\n    try:\n        plt.figure()\n        test_f1s = [spr_exp[dr][\"test_f1\"] for dr in dropouts]\n        plt.bar(dropouts, test_f1s, color=\"skyblue\")\n        plt.ylabel(\"Test Macro-F1\")\n        plt.title(\"SPR_BENCH: Test F1 by Dropout\")\n        plt.xticks(rotation=45)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_testF1_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test F1 bar: {e}\")\n        plt.close()\n\n    # ---------- 4. Confusion matrix for best dropout ----------\n    try:\n        best_idx = int(np.argmax(test_f1s))\n        best_dr = dropouts[best_idx]\n        y_true = spr_exp[best_dr][\"ground_truth\"]\n        y_pred = spr_exp[best_dr][\"predictions\"]\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH Confusion Matrix (best {best_dr})\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_dr}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ---------- summary print ----------\n    print(\"Test macro-F1 by dropout:\")\n    for dr, f1 in zip(dropouts, test_f1s):\n        print(f\"{dr}: {f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\nruns = experiment_data.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n\n# helper: fetch arrays\ndef extract(metric_type, split, nl_key):\n    return np.asarray(runs[nl_key][metric_type][split])\n\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(\n            epochs, extract(\"metrics\", \"train_f1\", nl_key), label=f\"train nl={nl_key}\"\n        )\n        plt.plot(\n            epochs,\n            extract(\"metrics\", \"val_f1\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(epochs, extract(\"losses\", \"train\", nl_key), label=f\"train nl={nl_key}\")\n        plt.plot(\n            epochs,\n            extract(\"losses\", \"val\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# compute test F1s\ntest_f1s = {}\nfor nl_key in runs:\n    preds = np.asarray(runs[nl_key][\"predictions\"])\n    gts = np.asarray(runs[nl_key][\"ground_truth\"])\n    test_f1s[nl_key] = f1_score(gts, preds, average=\"macro\")\n\n# ---------- plot 3: bar chart of test F1 ----------\ntry:\n    plt.figure()\n    keys = sorted(test_f1s, key=lambda x: int(x.split(\"_\")[1]))\n    vals = [test_f1s[k] for k in keys]\n    plt.bar(keys, vals, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 by num_layers\")\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# identify best model\nbest_key = max(test_f1s, key=test_f1s.get)\n\n# ---------- plot 4: confusion matrix for best model ----------\ntry:\n    preds = np.asarray(runs[best_key][\"predictions\"])\n    gts = np.asarray(runs[best_key][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(f\"SPR_BENCH Confusion Matrix (Best {best_key})\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_key}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Test Macro-F1 scores:\")\nfor k, v in sorted(test_f1s.items(), key=lambda x: int(x[0].split(\"_\")[1])):\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load experiment data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    wd_runs = experiment_data.get(\"weight_decay\", {})\n    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n    # ------------- Figure 1: loss curves ----------------\n    try:\n        plt.figure()\n        for i, (run_name, run_dict) in enumerate(wd_runs.items()):\n            epochs = run_dict[\"epochs\"]\n            plt.plot(\n                epochs,\n                run_dict[\"losses\"][\"train\"],\n                linestyle=\"-\",\n                marker=\"o\",\n                color=colors[i % len(colors)],\n                label=f\"{run_name} train\",\n            )\n            plt.plot(\n                epochs,\n                run_dict[\"losses\"][\"val\"],\n                linestyle=\"--\",\n                marker=\"x\",\n                color=colors[i % len(colors)],\n                label=f\"{run_name} val\",\n            )\n        plt.title(\n            \"SPR_BENCH: Training vs Validation Loss\\n(TransformerClassifier, varying weight decay)\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_weight_decay.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # ------------- Figure 2: F1 curves ------------------\n    try:\n        plt.figure()\n        for i, (run_name, run_dict) in enumerate(wd_runs.items()):\n            epochs = run_dict[\"epochs\"]\n            plt.plot(\n                epochs,\n                run_dict[\"metrics\"][\"train_f1\"],\n                linestyle=\"-\",\n                marker=\"o\",\n                color=colors[i % len(colors)],\n                label=f\"{run_name} train\",\n            )\n            plt.plot(\n                epochs,\n                run_dict[\"metrics\"][\"val_f1\"],\n                linestyle=\"--\",\n                marker=\"x\",\n                color=colors[i % len(colors)],\n                label=f\"{run_name} val\",\n            )\n        plt.title(\n            \"SPR_BENCH: Training vs Validation Macro-F1\\n(TransformerClassifier, varying weight decay)\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves_weight_decay.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curves: {e}\")\n        plt.close()\n\n    # ------------- Figure 3: test F1 bar chart ----------\n    try:\n        plt.figure()\n        run_names = list(wd_runs.keys())\n        test_f1s = [wd_runs[r][\"test_f1\"] for r in run_names]\n        x = np.arange(len(run_names))\n        plt.bar(x, test_f1s, color=colors[: len(run_names)])\n        plt.xticks(x, run_names, rotation=45)\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Test Macro-F1 by Weight Decay\")\n        for idx, val in enumerate(test_f1s):\n            plt.text(idx, val + 0.01, f\"{val:.2f}\", ha=\"center\", va=\"bottom\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_weight_decay.png\")\n        plt.tight_layout()\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test F1 bar chart: {e}\")\n        plt.close()\n\n    print(\"Plots saved to\", working_dir)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----- set up working dir -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load experiment data -----\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data.get(\"nhead_tuning\", {}).get(\"SPR_BENCH\", {})\n    if not runs:\n        print(\"No SPR_BENCH runs found inside experiment_data.\")\n    else:\n        # compute best dev F1 & gather test scores\n        best_nhead, best_dev_f1, best_test_f1 = None, -1.0, -1.0\n        test_scores = {}\n        for key, run in runs.items():\n            dev_f1 = run[\"metrics\"][\"val_f1\"][-1]\n            test_f1 = run[\"test_f1\"]\n            test_scores[key] = test_f1\n            if dev_f1 > best_dev_f1:\n                best_dev_f1, best_nhead, best_test_f1 = dev_f1, key, test_f1\n\n        # ----------- PLOT 1: F1 curves -----------\n        try:\n            plt.figure()\n            for key, run in runs.items():\n                epochs = run[\"epochs\"]\n                plt.plot(\n                    epochs,\n                    run[\"metrics\"][\"train_f1\"],\n                    label=f\"{key} train\",\n                    linestyle=\"--\",\n                )\n                plt.plot(epochs, run[\"metrics\"][\"val_f1\"], label=f\"{key} val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(\"SPR_BENCH: Train & Val Macro-F1 vs Epochs (nhead tuning)\")\n            plt.legend()\n            save_path = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n            plt.savefig(save_path)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating F1 curve plot: {e}\")\n            plt.close()\n\n        # ----------- PLOT 2: Loss curves -----------\n        try:\n            plt.figure()\n            for key, run in runs.items():\n                epochs = run[\"epochs\"]\n                plt.plot(\n                    epochs, run[\"losses\"][\"train\"], label=f\"{key} train\", linestyle=\"--\"\n                )\n                plt.plot(epochs, run[\"losses\"][\"val\"], label=f\"{key} val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\"SPR_BENCH: Train & Val Loss vs Epochs (nhead tuning)\")\n            plt.legend()\n            save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n            plt.savefig(save_path)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve plot: {e}\")\n            plt.close()\n\n        # ----------- PLOT 3: Test macro-F1 summary -----------\n        try:\n            plt.figure()\n            labels, values = zip(*test_scores.items())\n            plt.bar(range(len(values)), values, tick_label=labels)\n            plt.ylabel(\"Test Macro-F1\")\n            plt.title(\"SPR_BENCH: Test Macro-F1 by nhead configuration\")\n            plt.ylim(0, 1)\n            save_path = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n            plt.savefig(save_path)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating test F1 bar plot: {e}\")\n            plt.close()\n\n        # ----------- print evaluation metrics -----------\n        print(\"Test Macro-F1 per configuration:\")\n        for k, v in test_scores.items():\n            print(f\"  {k}: {v:.4f}\")\n        print(f\"\\nBest configuration based on dev Macro-F1: {best_nhead}\")\n        print(f\"  Dev Macro-F1:  {best_dev_f1:.4f}\")\n        print(f\"  Test Macro-F1: {best_test_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\nruns = experiment_data.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n\n# helper: fetch arrays\ndef extract(metric_type, split, nl_key):\n    return np.asarray(runs[nl_key][metric_type][split])\n\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(\n            epochs, extract(\"metrics\", \"train_f1\", nl_key), label=f\"train nl={nl_key}\"\n        )\n        plt.plot(\n            epochs,\n            extract(\"metrics\", \"val_f1\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(epochs, extract(\"losses\", \"train\", nl_key), label=f\"train nl={nl_key}\")\n        plt.plot(\n            epochs,\n            extract(\"losses\", \"val\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# compute test F1s\ntest_f1s = {}\nfor nl_key in runs:\n    preds = np.asarray(runs[nl_key][\"predictions\"])\n    gts = np.asarray(runs[nl_key][\"ground_truth\"])\n    test_f1s[nl_key] = f1_score(gts, preds, average=\"macro\")\n\n# ---------- plot 3: bar chart of test F1 ----------\ntry:\n    plt.figure()\n    keys = sorted(test_f1s, key=lambda x: int(x.split(\"_\")[1]))\n    vals = [test_f1s[k] for k in keys]\n    plt.bar(keys, vals, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 by num_layers\")\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# identify best model\nbest_key = max(test_f1s, key=test_f1s.get)\n\n# ---------- plot 4: confusion matrix for best model ----------\ntry:\n    preds = np.asarray(runs[best_key][\"predictions\"])\n    gts = np.asarray(runs[best_key][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(f\"SPR_BENCH Confusion Matrix (Best {best_key})\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_key}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Test Macro-F1 scores:\")\nfor k, v in sorted(test_f1s.items(), key=lambda x: int(x[0].split(\"_\")[1])):\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\nruns = experiment_data.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n\n# helper: fetch arrays\ndef extract(metric_type, split, nl_key):\n    return np.asarray(runs[nl_key][metric_type][split])\n\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(\n            epochs, extract(\"metrics\", \"train_f1\", nl_key), label=f\"train nl={nl_key}\"\n        )\n        plt.plot(\n            epochs,\n            extract(\"metrics\", \"val_f1\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(epochs, extract(\"losses\", \"train\", nl_key), label=f\"train nl={nl_key}\")\n        plt.plot(\n            epochs,\n            extract(\"losses\", \"val\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# compute test F1s\ntest_f1s = {}\nfor nl_key in runs:\n    preds = np.asarray(runs[nl_key][\"predictions\"])\n    gts = np.asarray(runs[nl_key][\"ground_truth\"])\n    test_f1s[nl_key] = f1_score(gts, preds, average=\"macro\")\n\n# ---------- plot 3: bar chart of test F1 ----------\ntry:\n    plt.figure()\n    keys = sorted(test_f1s, key=lambda x: int(x.split(\"_\")[1]))\n    vals = [test_f1s[k] for k in keys]\n    plt.bar(keys, vals, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 by num_layers\")\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# identify best model\nbest_key = max(test_f1s, key=test_f1s.get)\n\n# ---------- plot 4: confusion matrix for best model ----------\ntry:\n    preds = np.asarray(runs[best_key][\"predictions\"])\n    gts = np.asarray(runs[best_key][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(f\"SPR_BENCH Confusion Matrix (Best {best_key})\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_key}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Test Macro-F1 scores:\")\nfor k, v in sorted(test_f1s.items(), key=lambda x: int(x[0].split(\"_\")[1])):\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\nruns = experiment_data.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n\n# helper: fetch arrays\ndef extract(metric_type, split, nl_key):\n    return np.asarray(runs[nl_key][metric_type][split])\n\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(\n            epochs, extract(\"metrics\", \"train_f1\", nl_key), label=f\"train nl={nl_key}\"\n        )\n        plt.plot(\n            epochs,\n            extract(\"metrics\", \"val_f1\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(epochs, extract(\"losses\", \"train\", nl_key), label=f\"train nl={nl_key}\")\n        plt.plot(\n            epochs,\n            extract(\"losses\", \"val\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# compute test F1s\ntest_f1s = {}\nfor nl_key in runs:\n    preds = np.asarray(runs[nl_key][\"predictions\"])\n    gts = np.asarray(runs[nl_key][\"ground_truth\"])\n    test_f1s[nl_key] = f1_score(gts, preds, average=\"macro\")\n\n# ---------- plot 3: bar chart of test F1 ----------\ntry:\n    plt.figure()\n    keys = sorted(test_f1s, key=lambda x: int(x.split(\"_\")[1]))\n    vals = [test_f1s[k] for k in keys]\n    plt.bar(keys, vals, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 by num_layers\")\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# identify best model\nbest_key = max(test_f1s, key=test_f1s.get)\n\n# ---------- plot 4: confusion matrix for best model ----------\ntry:\n    preds = np.asarray(runs[best_key][\"predictions\"])\n    gts = np.asarray(runs[best_key][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(f\"SPR_BENCH Confusion Matrix (Best {best_key})\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_key}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Test Macro-F1 scores:\")\nfor k, v in sorted(test_f1s.items(), key=lambda x: int(x[0].split(\"_\")[1])):\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score\n\n# ----------------- basic set-up -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# paths provided in the prompt\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_7b4df9fbec364550a6f9cd010d46ea10_proc_3464963/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_510b055ead6f4748869474fa247f6a6f_proc_3464962/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f28d2d1233934773935c04c13333106d_proc_3464964/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        all_experiment_data.append(np.load(full, allow_pickle=True).item())\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# ----------------- aggregate -----------------\nagg = {}  # {nl_key: dict of lists}\nfor exp in all_experiment_data:\n    runs = exp.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\n    for nl_key, data in runs.items():\n        entry = agg.setdefault(\n            nl_key,\n            {\n                \"epochs\": np.asarray(data[\"epochs\"]),\n                \"train_f1\": [],\n                \"val_f1\": [],\n                \"train_loss\": [],\n                \"val_loss\": [],\n                \"test_f1\": [],\n            },\n        )\n        entry[\"train_f1\"].append(np.asarray(data[\"metrics\"][\"train_f1\"]))\n        entry[\"val_f1\"].append(np.asarray(data[\"metrics\"][\"val_f1\"]))\n        entry[\"train_loss\"].append(np.asarray(data[\"losses\"][\"train\"]))\n        entry[\"val_loss\"].append(np.asarray(data[\"losses\"][\"val\"]))\n\n        # test F1 for this run\n        preds = np.asarray(data.get(\"predictions\", []))\n        gts = np.asarray(data.get(\"ground_truth\", []))\n        if preds.size and gts.size:\n            entry[\"test_f1\"].append(f1_score(gts, preds, average=\"macro\"))\n\n\n# helper to get mean & se\ndef mean_se(stack):\n    arr = np.vstack(stack)\n    mean = arr.mean(axis=0)\n    se = arr.std(axis=0, ddof=1) / np.sqrt(arr.shape[0])\n    return mean, se\n\n\nsorted_keys = sorted(agg, key=lambda x: int(x.split(\"_\")[1]))\n\n# ----------------- plot 1: mean \u00b1 se F1 curves -----------------\ntry:\n    plt.figure()\n    for nl_key in sorted_keys:\n        d = agg[nl_key]\n        epochs = d[\"epochs\"]\n        if not d[\"train_f1\"] or not d[\"val_f1\"]:\n            continue\n        m_tr, se_tr = mean_se(d[\"train_f1\"])\n        m_val, se_val = mean_se(d[\"val_f1\"])\n\n        plt.plot(epochs, m_tr, label=f\"train {nl_key}\")\n        plt.fill_between(epochs, m_tr - se_tr, m_tr + se_tr, alpha=0.2)\n        plt.plot(epochs, m_val, linestyle=\"--\", label=f\"val {nl_key}\")\n        plt.fill_between(epochs, m_val - se_val, m_val + se_val, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Mean \u00b1 SE Training/Validation F1\")\n    plt.legend(title=\"Shaded: \u00b1SE\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_agg_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated F1 plot: {e}\")\n    plt.close()\n\n# ----------------- plot 2: mean \u00b1 se Loss curves -----------------\ntry:\n    plt.figure()\n    for nl_key in sorted_keys:\n        d = agg[nl_key]\n        epochs = d[\"epochs\"]\n        if not d[\"train_loss\"] or not d[\"val_loss\"]:\n            continue\n        m_tr, se_tr = mean_se(d[\"train_loss\"])\n        m_val, se_val = mean_se(d[\"val_loss\"])\n\n        plt.plot(epochs, m_tr, label=f\"train {nl_key}\")\n        plt.fill_between(epochs, m_tr - se_tr, m_tr + se_tr, alpha=0.2)\n        plt.plot(epochs, m_val, linestyle=\"--\", label=f\"val {nl_key}\")\n        plt.fill_between(epochs, m_val - se_val, m_val + se_val, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Mean \u00b1 SE Training/Validation Loss\")\n    plt.legend(title=\"Shaded: \u00b1SE\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_agg_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# ----------------- plot 3: test F1 bar with error -----------------\ntry:\n    plt.figure()\n    means, ses = [], []\n    for nl_key in sorted_keys:\n        scores = agg[nl_key][\"test_f1\"]\n        if scores:\n            means.append(np.mean(scores))\n            ses.append(np.std(scores, ddof=1) / np.sqrt(len(scores)))\n        else:\n            means.append(0.0)\n            ses.append(0.0)\n    x = np.arange(len(sorted_keys))\n    plt.bar(x, means, yerr=ses, capsize=5, color=\"skyblue\")\n    plt.xticks(x, sorted_keys, rotation=45)\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 (Mean \u00b1 SE) by num_layers\")\n    for i, v in enumerate(means):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_agg_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated test F1 bar plot: {e}\")\n    plt.close()\n\n# --------------- print summary ---------------\nprint(\"Aggregated Test Macro-F1 (mean \u00b1 SE):\")\nfor k, m, s in zip(sorted_keys, means, ses):\n    print(f\"{k}: {m:.4f} \u00b1 {s:.4f}\")\n"], "plot_plan": ["We load the stored experiment_data.npy, isolate the \u201cSPR_BENCH\u201d record, and retrieve the epochs, loss, F1, predictions and labels.  Three core visualizations are generated: (1) the training/validation loss curve across epochs, (2) the training/validation macro-F1 curve across epochs, and (3) a confusion-matrix heat-map for the final test split.  Each figure is wrapped in its own try-except block to ensure robustness and is saved into the pre-defined working_dir with descriptive names.  The code adheres strictly to basic matplotlib, closes every figure after saving, and never invents data\u2014only what is present in experiment_data.npy is used.  Titles explicitly name the dataset and clarify the subplot purpose (e.g. \u201cTraining vs Validation\u201d).  If the experiment file cannot be loaded, a graceful error message is printed and plotting is skipped.  No more than three figures are produced to satisfy the \u201cat most 5\u201d guideline.  The whole script is compact so that it can be executed directly after training has finished.  Finally, the script prints a short summary confirming which plots were successfully written.", "We will load the saved experiment_data.npy, extract the records for the single dataset (SPR_BENCH) and loop through the five learning-rate runs. For each run we will line-plot the training and validation macro-F1 versus epoch in one figure, and training/validation loss in another; these will be saved with filenames containing the dataset and the lr value. Because we have exactly five learning rates, we will create at most five figures of each type (10 total). Afterwards we will aggregate all runs and make a bar plot comparing the final test macro-F1 across learning rates. Every plotting block is wrapped in try-except, each figure is titled clearly, saved to working_dir, and closed. Finally, we print a concise table (stdout) that lists learning rate, best validation F1, and test F1 to provide quick numerical insight. No fabricated data are used; everything comes directly from experiment_data.npy.", "We load experiment_data.npy and extract per-batch-size arrays for epochs, training/validation loss, training/validation macro-F1 and final test macro-F1. Three plots are produced: (1) side-by-side line charts of train and val loss across epochs, (2) side-by-side line charts of train and val F1 across epochs, and (3) a bar chart of test F1 versus batch size. Only the five recorded batch sizes are used and any run that contains an \u201cerror\u201d field is skipped. Each figure is enclosed in its own try-except block, includes clear titles/subtitles (e.g. \u201cLeft: Train Loss, Right: Val Loss\u201d), is saved with descriptive names in working_dir, and is closed afterward. Finally, the script prints the collected test F1 scores and highlights the best batch size. All plotting relies solely on the real data found in experiment_data.npy. This meets the requirement of at most five figures, one for each visualization type.", "We will load the saved experiment_data.npy, iterate over the \u201cd_model_tuning\u201d group and extract per-epoch losses, F1 scores, and test statistics. Four plots are produced (all wrapped in try-except blocks, max five plots): (1) train/validation loss curves for every d_model, (2) train/validation macro-F1 curves, (3) a bar chart of final test F1 versus d_model, and (4) a confusion-matrix heat-map for the single model that achieved the highest validation F1. Every figure is titled with the dataset name (\u201cSPR_BENCH\u201d) and a clear subtitle; filenames are descriptive and saved inside working_dir. After plotting we also print the best d_model and its corresponding val/test F1 so the notebook logs include numerical metrics. All figures are closed explicitly to avoid memory leaks.", "We will load the saved experiment_data.npy, focus on the SPR_BENCH results across the five dropout settings, and visualize four things: (1) training/validation macro-F1 curves, (2) training/validation loss curves, (3) a bar chart of final test macro-F1 for every dropout, and (4) a confusion matrix for the best-performing dropout. Each plot sits in its own try-except block, saved to working_dir with descriptive names and then closed. We first identify the dropout variant with the highest test F1 to choose the confusion matrix. After plotting we also print a small summary table of the test metrics so the script outputs something numerical. All plotting is done with basic matplotlib; nothing is synthesized\u2014only data present in experiment_data.npy is used, and no more than four figures are produced (well under the five-figure cap).", "We will load experiment_data.npy, identify the \u201cSPR_BENCH\u201d runs for each num_layers setting, and aggregate per-epoch losses and macro-F1 scores.  \nPlot 1 combines training/validation F1 curves for all four num_layers values; plot 2 does the same for losses.  \nPlot 3 is a bar-chart of final test macro-F1 for every setting, recomputed from stored predictions/ground_truth so the figure is self-contained.  \nFor the best model (highest test F1) we also draw a confusion matrix heat-map, limiting ourselves to at most four figures total.  \nAll plots are enclosed in independent try-except blocks, use clear titles/subtitles, are saved to working_dir with descriptive names, and figures are closed in every branch.  \nAfter plotting we print a small table of the test macro-F1 scores so the user can verify the numeric results quickly.  \nNo values are fabricated; every curve and metric comes directly from experiment_data.npy.  \nWe rely only on numpy, matplotlib, os and sklearn\u2019s confusion_matrix / f1_score which are already available in the experiment environment.", "We will load experiment_data.npy, iterate through the four weight-decay runs and collect epochs, train/val loss, train/val F1, and final test F1.  \nThree figures are created (each wrapped in its own try-except):  \n1) Line plot of training vs. validation loss across epochs, one color per weight decay.  \n2) Line plot of training vs. validation macro-F1 across epochs, again one color per weight decay.  \n3) Bar chart comparing final test macro-F1 for each weight decay.  \nEach figure has a clear title, axis labels, legend, and is saved into working_dir with descriptive filenames that mention SPR_BENCH and the plot type.  \nOnly the content contained in experiment_data is plotted; we cap to these three figures (<5 as required).  \nFigures are always closed after saving, and any exceptions are caught and printed so the script never crashes prematurely.  \nFinally, the code prints a short confirmation that plots were generated successfully.", "Load the saved experiment_data.npy file and focus on the \u201cnhead_tuning \u2192 SPR_BENCH\u201d section. For every n-head configuration collect the per-epoch arrays for training/validation macro-F1 and losses as well as the final test macro-F1. Produce three standard visualizations: (1) line plot of training/validation macro-F1 across epochs for all n-head values, (2) line plot of training/validation loss across epochs, and (3) bar chart summarising test macro-F1 for each n-head. Each plot is enclosed in its own try-except block, saved in working_dir with descriptive names, titled with the dataset and the kind of plot, and the figure is always closed. After plotting, compute and print the best n-head setting based on the highest final dev macro-F1 together with its corresponding test macro-F1; also print the full table of test scores so users can compare configurations at a glance. No synthetic data are generated\u2014everything comes directly from experiment_data.npy. The code is concise, uses only matplotlib and numpy, and adheres to the provided plotting guidelines.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["num_epochs", "learning_rate", "batch_size", "d_model (embedding/hidden size)", "dropout_rate", "num_layers", "weight_decay", "nhead (number_of_attention_heads)", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate the \u201cworking\u201d directory, load the serialized NumPy\ndictionary, and pull out the stored entries for the single dataset\n(\u201cSPR_BENCH\u201d). It then selects the last recorded values for training/validation\nlosses and F1 scores (representing the final epoch) and computes the macro-\naveraged test F1 score from the saved predictions and ground-truth labels.\nFinally, it prints the dataset name followed by each metric with clear, explicit\nlabels.", "The script will load the saved numpy file from the working directory, inspect\nthe nested structure, pick out the run whose validation F1 score is the best for\neach dataset, and then report the most relevant final / best numbers with clear\nmetric names. All code is at global scope so it executes immediately.", "We will load the saved numpy dictionary from the working directory, navigate\nthrough its nested structure, and focus on the single dataset (SPR_BENCH)\ncontained in the batch-size sweep.   For every batch-size configuration we\nretrieve the final (last-epoch) metrics; the configuration whose final\nvalidation-F1 is highest is selected as the \u201cbest\u201d.   Finally, we print the\ndataset name once, then the best batch-size and the requested final metrics\n(training F1, validation F1, test F1, training loss, validation loss), each\npreceded by a clear descriptive label.", "The script loads the saved experiment_data.npy file from the working directory,\niterates through every d_model configuration recorded under the \"d_model_tuning\"\nkey, and prints the final training loss/F1, final validation loss/F1, best\nvalidation F1, and the test loss/F1. Each configuration is treated as a separate\ndataset whose name is printed before its metrics, and every metric line is\npreceded by a clear, descriptive label. The code runs immediately upon execution\nwithout requiring any special entry point.", "The script loads the serialized experiment dictionary from the working\ndirectory, iterates over every dataset stored inside it (here, \u201cSPR_BENCH\u201d), and\nlooks through all the hyper-parameter runs (different dropout rates).   For each\nrun it finds the best validation F1 score across epochs; the run that achieves\nthe overall highest validation F1 score is selected as the representative model\nfor that dataset.   From this winning run the script gathers: best training F1,\nbest validation F1, final test F1, lowest training loss, lowest validation loss,\nand final test loss.   Finally, it prints the dataset name followed by each\nmetric name and its value, using explicit labels such as \u201ctraining loss\u201d or\n\u201ctest F1 score,\u201d exactly as required.", "The script will load the numpy file from the working directory, navigate the\nnested dictionary to find every experiment run for each dataset, pick the run\nthat finishes with the highest validation F1 score, and then print the final-\nepoch values for training loss, validation loss, training F1 score, and\nvalidation F1 score. Each group of metrics is preceded by the corresponding\ndataset name, and each printed line includes an explicit metric label.", "The script will load the stored NumPy dictionary, iterate over each weight-decay\nexperiment (\u201cdataset\u201d), compute the best (maximum) F1 scores and the lowest\nlosses observed during training and validation, and then print those alongside\nthe final test metrics. All prints clearly label the dataset and each metric.", "The script will load the saved numpy dictionary, locate the \u201cSPR_BENCH\u201d results,\npick the run (n-head setting) with the highest final validation F1 score, and\nthen print the final (last-epoch) values of each relevant metric with clear,\nexplicit names. All logic is executed at the top level so the file runs\nimmediately.", "The script will load the numpy file from the working directory, navigate the\nnested dictionary to find every experiment run for each dataset, pick the run\nthat finishes with the highest validation F1 score, and then print the final-\nepoch values for training loss, validation loss, training F1 score, and\nvalidation F1 score. Each group of metrics is preceded by the corresponding\ndataset name, and each printed line includes an explicit metric label.", "The script will load the numpy file from the working directory, navigate the\nnested dictionary to find every experiment run for each dataset, pick the run\nthat finishes with the highest validation F1 score, and then print the final-\nepoch values for training loss, validation loss, training F1 score, and\nvalidation F1 score. Each group of metrics is preceded by the corresponding\ndataset name, and each printed line includes an explicit metric label.", "The script will load the numpy file from the working directory, navigate the\nnested dictionary to find every experiment run for each dataset, pick the run\nthat finishes with the highest validation F1 score, and then print the final-\nepoch values for training loss, validation loss, training F1 score, and\nvalidation F1 score. Each group of metrics is preceded by the corresponding\ndataset name, and each printed line includes an explicit metric label.", ""], "parse_metrics_code": ["import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ------------------------------------------------------------------\n# 1. Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 2. Extract metrics for each dataset\n# ------------------------------------------------------------------\nfor dataset_name, dataset_dict in experiment_data[\"num_epochs\"].items():\n    ed = dataset_dict[\"SPR_BENCH\"] if \"SPR_BENCH\" in dataset_dict else dataset_dict\n\n    # Final epoch values\n    final_train_f1 = ed[\"metrics\"][\"train_f1\"][-1]\n    final_val_f1 = ed[\"metrics\"][\"val_f1\"][-1]\n    final_train_loss = ed[\"losses\"][\"train\"][-1]\n    final_val_loss = ed[\"losses\"][\"val\"][-1]\n\n    # Test F1 from saved predictions/ground truth\n    preds = np.array(ed[\"predictions\"])\n    gts = np.array(ed[\"ground_truth\"])\n    test_f1 = f1_score(gts, preds, average=\"macro\") if preds.size else float(\"nan\")\n\n    # ------------------------------------------------------------------\n    # 3. Print results\n    # ------------------------------------------------------------------\n    print(dataset_name)\n    print(f\"training F1 score: {final_train_f1:.4f}\")\n    print(f\"validation F1 score: {final_val_f1:.4f}\")\n    print(f\"training loss: {final_train_loss:.6f}\")\n    print(f\"validation loss: {final_val_loss:.6f}\")\n    print(f\"test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# -------------------- analyse --------------------\nfor dataset_name, runs in experiment_data.get(\"lr_search\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # pick the run that achieved the highest validation F1 score (best epoch)\n    best_run = None\n    best_val_f1 = -1.0\n    for run in runs:\n        run_best_val_f1 = max(run[\"metrics\"][\"val_f1\"])\n        if run_best_val_f1 > best_val_f1:\n            best_val_f1 = run_best_val_f1\n            best_run = run\n\n    if best_run is None:\n        print(\"  No runs found for this dataset.\")\n        continue\n\n    # extract desired values\n    learning_rate = best_run[\"lr\"]\n    final_train_f1 = best_run[\"metrics\"][\"train_f1\"][-1]\n    best_validation_f1 = max(best_run[\"metrics\"][\"val_f1\"])\n    final_train_loss = best_run[\"losses\"][\"train\"][-1]\n    best_validation_loss = min(best_run[\"losses\"][\"val\"])\n    test_f1_score = best_run[\"test_f1\"]\n\n    # print metrics with clear names\n    print(f\"  learning rate: {learning_rate}\")\n    print(f\"  final training F1 score: {final_train_f1:.4f}\")\n    print(f\"  best validation F1 score: {best_validation_f1:.4f}\")\n    print(f\"  final training loss: {final_train_loss:.4f}\")\n    print(f\"  best validation loss: {best_validation_loss:.4f}\")\n    print(f\"  test F1 score: {test_f1_score:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------\n# Load experiment data\n# ---------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------\n# Process and print metrics\n# ---------------------------------------------------------\nbatch_size_section = experiment_data.get(\"batch_size_tuning\", {})\n\nfor dataset_name, bs_dict in batch_size_section.items():\n    best_val_f1 = -float(\"inf\")\n    best_bs = None\n    best_entry = None\n\n    # Find the batch-size run with the highest FINAL validation-F1\n    for bs_key, entry in bs_dict.items():\n        if \"error\" in entry:  # skip failed runs\n            continue\n        val_f1_list = entry[\"metrics\"].get(\"val_f1\", [])\n        if not val_f1_list:\n            continue\n        final_val_f1 = val_f1_list[-1]\n        if final_val_f1 > best_val_f1:\n            best_val_f1 = final_val_f1\n            best_bs = bs_key\n            best_entry = entry\n\n    if best_entry is None:\n        continue  # nothing to report\n\n    # Gather final metrics from the best configuration\n    final_train_f1 = best_entry[\"metrics\"][\"train_f1\"][-1]\n    final_val_f1 = best_entry[\"metrics\"][\"val_f1\"][-1]\n    final_train_loss = best_entry[\"losses\"][\"train\"][-1]\n    final_val_loss = best_entry[\"losses\"][\"val\"][-1]\n    test_f1 = best_entry.get(\"test_f1\", float(\"nan\"))\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Best batch size: {best_bs}\")\n    print(f\"final training F1 score: {final_train_f1:.4f}\")\n    print(f\"final validation F1 score: {final_val_f1:.4f}\")\n    print(f\"test F1 score: {test_f1:.4f}\")\n    print(f\"final training loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Extract and print metrics\n# ------------------------------------------------------------------\nd_model_dict = experiment_data.get(\"d_model_tuning\", {})\n\nfor d_model_key, record in d_model_dict.items():\n    print(f\"Dataset: d_model={d_model_key}\")\n\n    # Final values (last epoch logged)\n    final_train_loss = (\n        record[\"losses\"][\"train\"][-1] if record[\"losses\"][\"train\"] else None\n    )\n    final_val_loss = record[\"losses\"][\"val\"][-1] if record[\"losses\"][\"val\"] else None\n    final_train_f1 = (\n        record[\"metrics\"][\"train_f1\"][-1] if record[\"metrics\"][\"train_f1\"] else None\n    )\n    final_val_f1 = (\n        record[\"metrics\"][\"val_f1\"][-1] if record[\"metrics\"][\"val_f1\"] else None\n    )\n\n    # Best / test values\n    best_val_f1 = record.get(\"best_val_f1\")\n    test_f1 = record.get(\"test_f1\")\n    test_loss = record.get(\"test_loss\")\n\n    # Print metrics with explicit labels\n    if final_train_loss is not None:\n        print(f\"  Final training loss: {final_train_loss:.4f}\")\n    if final_train_f1 is not None:\n        print(f\"  Final training F1 score: {final_train_f1:.4f}\")\n    if final_val_loss is not None:\n        print(f\"  Final validation loss: {final_val_loss:.4f}\")\n    if final_val_f1 is not None:\n        print(f\"  Final validation F1 score: {final_val_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"  Best validation F1 score: {best_val_f1:.4f}\")\n    if test_loss is not None:\n        print(f\"  Test loss: {test_loss:.4f}\")\n    if test_f1 is not None:\n        print(f\"  Test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------- extract and print metrics --------------------\nfor sweep_name, datasets in experiment_data.items():  # e.g. \"dropout_rate\"\n    for dataset_name, runs in datasets.items():  # e.g. \"SPR_BENCH\"\n        # Select the run (dropout value) with the best validation F1 score\n        best_run_key = None\n        best_val_f1 = -1.0\n        for run_key, run_data in runs.items():\n            this_best_val = max(run_data[\"metrics\"][\"val_f1\"])\n            if this_best_val > best_val_f1:\n                best_val_f1 = this_best_val\n                best_run_key = run_key\n\n        best_run = runs[best_run_key]\n\n        # Retrieve required metrics from the best run\n        training_f1 = max(best_run[\"metrics\"][\"train_f1\"])\n        validation_f1 = max(best_run[\"metrics\"][\"val_f1\"])\n        test_f1 = best_run[\"test_f1\"]\n\n        training_loss = min(best_run[\"losses\"][\"train\"])\n        validation_loss = min(best_run[\"losses\"][\"val\"])\n        test_loss = best_run[\"test_loss\"]\n\n        # -------------------- print results --------------------\n        print(dataset_name)\n        print(f\"training F1 score: {training_f1:.4f}\")\n        print(f\"validation F1 score: {validation_f1:.4f}\")\n        print(f\"test F1 score: {test_f1:.4f}\")\n        print(f\"training loss: {training_loss:.4f}\")\n        print(f\"validation loss: {validation_loss:.4f}\")\n        print(f\"test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to locate best run --------------------\ndef best_run_by_val_f1(experiments_dict):\n    \"\"\"\n    Return the data_slot (dict) corresponding to the run with the\n    highest final-epoch validation F1 score.\n    \"\"\"\n    best_slot, best_f1 = None, -float(\"inf\")\n    for run_name, slot in experiments_dict.items():\n        # guard against malformed entries\n        if not slot[\"metrics\"][\"val_f1\"]:\n            continue\n        final_val_f1 = slot[\"metrics\"][\"val_f1\"][-1]\n        if final_val_f1 > best_f1:\n            best_f1 = final_val_f1\n            best_slot = slot\n    return best_slot\n\n\n# -------------------- iterate over datasets --------------------\nhp_group = experiment_data.get(\"num_layers\", {})  # top-level hyper-param group\nfor dataset_name, runs in hp_group.items():  # e.g. \"SPR_BENCH\" : {nl_1:..., nl_2:...}\n    best_slot = best_run_by_val_f1(runs)\n    if best_slot is None:\n        print(f\"{dataset_name}: no valid runs found.\")\n        continue\n\n    # extract final-epoch metrics\n    train_loss = best_slot[\"losses\"][\"train\"][-1]\n    val_loss = best_slot[\"losses\"][\"val\"][-1]\n    train_f1 = best_slot[\"metrics\"][\"train_f1\"][-1]\n    val_f1 = best_slot[\"metrics\"][\"val_f1\"][-1]\n\n    # -------------------- print results --------------------\n    print(f\"{dataset_name}\")\n    print(f\"  Training loss: {train_loss:.4f}\")\n    print(f\"  Validation loss: {val_loss:.4f}\")\n    print(f\"  Training F1 score: {train_f1:.4f}\")\n    print(f\"  Validation F1 score: {val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load experiment results --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -------------------- iterate and report --------------------\nfor dataset_name, stats in experiment_data[\"weight_decay\"].items():\n    print(f\"\\nDataset: {dataset_name}\")  # dataset header\n\n    # ----- training metrics -----\n    train_f1_scores = stats[\"metrics\"][\"train_f1\"]\n    best_train_f1 = max(train_f1_scores)\n    print(f\"Best training F1 score: {best_train_f1:.4f}\")\n\n    train_losses = stats[\"losses\"][\"train\"]\n    lowest_train_loss = min(train_losses)\n    print(f\"Lowest training loss: {lowest_train_loss:.4f}\")\n\n    # ----- validation metrics -----\n    val_f1_scores = stats[\"metrics\"][\"val_f1\"]\n    best_val_f1 = max(val_f1_scores)\n    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n\n    val_losses = stats[\"losses\"][\"val\"]\n    lowest_val_loss = min(val_losses)\n    print(f\"Lowest validation loss: {lowest_val_loss:.4f}\")\n\n    # ----- test metrics -----\n    test_f1 = stats[\"test_f1\"]\n    print(f\"Test F1 score: {test_f1:.4f}\")\n\n    test_loss = stats[\"test_loss\"]\n    print(f\"Test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Locate and load the numpy file ----------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# 1. Helper to select the best run for a dataset based on final validation F1 ----------\ndef best_run_by_val_f1(runs_dict):\n    best_key, best_val = None, -1.0\n    for run_name, run in runs_dict.items():\n        final_val_f1 = run[\"metrics\"][\"val_f1\"][-1]  # last epoch value\n        if final_val_f1 > best_val:\n            best_val = final_val_f1\n            best_key = run_name\n    return best_key, runs_dict[best_key]\n\n\n# 2. Iterate through datasets and print metrics ----------------------------------------\nfor dataset_name, runs in experiment_data[\"nhead_tuning\"].items():\n    # choose the single best hyper-parameter configuration for this dataset\n    run_name, best_run = best_run_by_val_f1(runs)\n\n    # extract final epoch metrics and test set metrics\n    final_train_f1 = best_run[\"metrics\"][\"train_f1\"][-1]\n    final_val_f1 = best_run[\"metrics\"][\"val_f1\"][-1]\n    final_train_loss = best_run[\"losses\"][\"train\"][-1]\n    final_val_loss = best_run[\"losses\"][\"val\"][-1]\n    test_f1 = best_run[\"test_f1\"]\n    test_loss = best_run[\"test_loss\"]\n\n    # 3. Print results with explicit metric names --------------------------------------\n    print(dataset_name)  # dataset header\n    print(f\"final train F1 score: {final_train_f1:.4f}\")\n    print(f\"final validation F1 score: {final_val_f1:.4f}\")\n    print(f\"test F1 score: {test_f1:.4f}\")\n    print(f\"final train loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_val_loss:.4f}\")\n    print(f\"test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to locate best run --------------------\ndef best_run_by_val_f1(experiments_dict):\n    \"\"\"\n    Return the data_slot (dict) corresponding to the run with the\n    highest final-epoch validation F1 score.\n    \"\"\"\n    best_slot, best_f1 = None, -float(\"inf\")\n    for run_name, slot in experiments_dict.items():\n        # guard against malformed entries\n        if not slot[\"metrics\"][\"val_f1\"]:\n            continue\n        final_val_f1 = slot[\"metrics\"][\"val_f1\"][-1]\n        if final_val_f1 > best_f1:\n            best_f1 = final_val_f1\n            best_slot = slot\n    return best_slot\n\n\n# -------------------- iterate over datasets --------------------\nhp_group = experiment_data.get(\"num_layers\", {})  # top-level hyper-param group\nfor dataset_name, runs in hp_group.items():  # e.g. \"SPR_BENCH\" : {nl_1:..., nl_2:...}\n    best_slot = best_run_by_val_f1(runs)\n    if best_slot is None:\n        print(f\"{dataset_name}: no valid runs found.\")\n        continue\n\n    # extract final-epoch metrics\n    train_loss = best_slot[\"losses\"][\"train\"][-1]\n    val_loss = best_slot[\"losses\"][\"val\"][-1]\n    train_f1 = best_slot[\"metrics\"][\"train_f1\"][-1]\n    val_f1 = best_slot[\"metrics\"][\"val_f1\"][-1]\n\n    # -------------------- print results --------------------\n    print(f\"{dataset_name}\")\n    print(f\"  Training loss: {train_loss:.4f}\")\n    print(f\"  Validation loss: {val_loss:.4f}\")\n    print(f\"  Training F1 score: {train_f1:.4f}\")\n    print(f\"  Validation F1 score: {val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to locate best run --------------------\ndef best_run_by_val_f1(experiments_dict):\n    \"\"\"\n    Return the data_slot (dict) corresponding to the run with the\n    highest final-epoch validation F1 score.\n    \"\"\"\n    best_slot, best_f1 = None, -float(\"inf\")\n    for run_name, slot in experiments_dict.items():\n        # guard against malformed entries\n        if not slot[\"metrics\"][\"val_f1\"]:\n            continue\n        final_val_f1 = slot[\"metrics\"][\"val_f1\"][-1]\n        if final_val_f1 > best_f1:\n            best_f1 = final_val_f1\n            best_slot = slot\n    return best_slot\n\n\n# -------------------- iterate over datasets --------------------\nhp_group = experiment_data.get(\"num_layers\", {})  # top-level hyper-param group\nfor dataset_name, runs in hp_group.items():  # e.g. \"SPR_BENCH\" : {nl_1:..., nl_2:...}\n    best_slot = best_run_by_val_f1(runs)\n    if best_slot is None:\n        print(f\"{dataset_name}: no valid runs found.\")\n        continue\n\n    # extract final-epoch metrics\n    train_loss = best_slot[\"losses\"][\"train\"][-1]\n    val_loss = best_slot[\"losses\"][\"val\"][-1]\n    train_f1 = best_slot[\"metrics\"][\"train_f1\"][-1]\n    val_f1 = best_slot[\"metrics\"][\"val_f1\"][-1]\n\n    # -------------------- print results --------------------\n    print(f\"{dataset_name}\")\n    print(f\"  Training loss: {train_loss:.4f}\")\n    print(f\"  Validation loss: {val_loss:.4f}\")\n    print(f\"  Training F1 score: {train_f1:.4f}\")\n    print(f\"  Validation F1 score: {val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to locate best run --------------------\ndef best_run_by_val_f1(experiments_dict):\n    \"\"\"\n    Return the data_slot (dict) corresponding to the run with the\n    highest final-epoch validation F1 score.\n    \"\"\"\n    best_slot, best_f1 = None, -float(\"inf\")\n    for run_name, slot in experiments_dict.items():\n        # guard against malformed entries\n        if not slot[\"metrics\"][\"val_f1\"]:\n            continue\n        final_val_f1 = slot[\"metrics\"][\"val_f1\"][-1]\n        if final_val_f1 > best_f1:\n            best_f1 = final_val_f1\n            best_slot = slot\n    return best_slot\n\n\n# -------------------- iterate over datasets --------------------\nhp_group = experiment_data.get(\"num_layers\", {})  # top-level hyper-param group\nfor dataset_name, runs in hp_group.items():  # e.g. \"SPR_BENCH\" : {nl_1:..., nl_2:...}\n    best_slot = best_run_by_val_f1(runs)\n    if best_slot is None:\n        print(f\"{dataset_name}: no valid runs found.\")\n        continue\n\n    # extract final-epoch metrics\n    train_loss = best_slot[\"losses\"][\"train\"][-1]\n    val_loss = best_slot[\"losses\"][\"val\"][-1]\n    train_f1 = best_slot[\"metrics\"][\"train_f1\"][-1]\n    val_f1 = best_slot[\"metrics\"][\"val_f1\"][-1]\n\n    # -------------------- print results --------------------\n    print(f\"{dataset_name}\")\n    print(f\"  Training loss: {train_loss:.4f}\")\n    print(f\"  Validation loss: {val_loss:.4f}\")\n    print(f\"  Training F1 score: {train_f1:.4f}\")\n    print(f\"  Validation F1 score: {val_f1:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'training F1 score: 0.9975', '\\n', 'validation F1 score:\n0.6980', '\\n', 'training loss: 0.007296', '\\n', 'validation loss: 2.773659',\n'\\n', 'test F1 score: 0.6999', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  learning rate: 0.002', '\\n', '  final training\nF1 score: 0.9965', '\\n', '  best validation F1 score: 0.7020', '\\n', '  final\ntraining loss: 0.0112', '\\n', '  best validation loss: 0.6548', '\\n', '  test F1\nscore: 0.6969', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best batch size: 32', '\\n', 'final training F1\nscore: 0.9930', '\\n', 'final validation F1 score: 0.6960', '\\n', 'test F1 score:\n0.6978', '\\n', 'final training loss: 0.0240', '\\n', 'final validation loss:\n2.4167', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: d_model=64', '\\n', '  Final training loss: 0.1217', '\\n', '  Final\ntraining F1 score: 0.9620', '\\n', '  Final validation loss: 1.6255', '\\n', '\nFinal validation F1 score: 0.6716', '\\n', '  Best validation F1 score: 0.6859',\n'\\n', '  Test loss: 1.6037', '\\n', '  Test F1 score: 0.6968', '\\n', 'Dataset:\nd_model=128', '\\n', '  Final training loss: 0.0273', '\\n', '  Final training F1\nscore: 0.9935', '\\n', '  Final validation loss: 2.2043', '\\n', '  Final\nvalidation F1 score: 0.6959', '\\n', '  Best validation F1 score: 0.6959', '\\n',\n'  Test loss: 1.9511', '\\n', '  Test F1 score: 0.6957', '\\n', 'Dataset:\nd_model=256', '\\n', '  Final training loss: 0.0457', '\\n', '  Final training F1\nscore: 0.9825', '\\n', '  Final validation loss: 2.4482', '\\n', '  Final\nvalidation F1 score: 0.6960', '\\n', '  Best validation F1 score: 0.6980', '\\n',\n'  Test loss: 2.6313', '\\n', '  Test F1 score: 0.6949', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training F1 score: 0.9945', '\\n', 'validation F1 score:\n0.6939', '\\n', 'test F1 score: 0.6988', '\\n', 'training loss: 0.0215', '\\n',\n'validation loss: 1.6220', '\\n', 'test loss: 2.2856', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  Training loss: 0.0200', '\\n', '  Validation loss:\n2.1754', '\\n', '  Training F1 score: 0.9925', '\\n', '  Validation F1 score:\n0.7020', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: wd_0.0', '\\n', 'Best training F1 score: 0.9960', '\\n', 'Lowest\ntraining loss: 0.0205', '\\n', 'Best validation F1 score: 0.6919', '\\n', 'Lowest\nvalidation loss: 1.3330', '\\n', 'Test F1 score: 0.6937', '\\n', 'Test loss:\n2.4181', '\\n', '\\nDataset: wd_1e-05', '\\n', 'Best training F1 score: 0.9880',\n'\\n', 'Lowest training loss: 0.0460', '\\n', 'Best validation F1 score: 0.6939',\n'\\n', 'Lowest validation loss: 0.7363', '\\n', 'Test F1 score: 0.6934', '\\n',\n'Test loss: 2.1453', '\\n', '\\nDataset: wd_0.0001', '\\n', 'Best training F1\nscore: 0.9950', '\\n', 'Lowest training loss: 0.0158', '\\n', 'Best validation F1\nscore: 0.6980', '\\n', 'Lowest validation loss: 0.7963', '\\n', 'Test F1 score:\n0.6959', '\\n', 'Test loss: 2.4127', '\\n', '\\nDataset: wd_0.001', '\\n', 'Best\ntraining F1 score: 0.9950', '\\n', 'Lowest training loss: 0.0249', '\\n', 'Best\nvalidation F1 score: 0.6980', '\\n', 'Lowest validation loss: 1.0740', '\\n',\n'Test F1 score: 0.6999', '\\n', 'Test loss: 2.1014', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final train F1 score: 0.9955', '\\n', 'final validation F1\nscore: 0.7020', '\\n', 'test F1 score: 0.6999', '\\n', 'final train loss: 0.0144',\n'\\n', 'final validation loss: 2.4256', '\\n', 'test loss: 2.4656', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  Training loss: 0.0621', '\\n', '  Validation loss:\n1.9700', '\\n', '  Training F1 score: 0.9795', '\\n', '  Validation F1 score:\n0.6940', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  Training loss: 0.0244', '\\n', '  Validation loss:\n2.6449', '\\n', '  Training F1 score: 0.9950', '\\n', '  Validation F1 score:\n0.6960', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  Training loss: 0.0285', '\\n', '  Validation loss:\n1.9642', '\\n', '  Training F1 score: 0.9925', '\\n', '  Validation F1 score:\n0.7020', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
