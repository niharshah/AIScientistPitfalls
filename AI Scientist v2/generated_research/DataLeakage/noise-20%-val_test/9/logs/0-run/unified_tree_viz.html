<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 11], [0, 9], [0, 5], [0, 4], [0, 8], [0, 10], [0, 2], [0, 12], [0, 7], [0, 1], [0, 3], [3, 6]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.1, 0.5], [0.2, 0.5], [0.3, 0.5], [0.4, 0.5], [0.2, 1.0], [0.5, 0.5], [0.6, 0.5], [0.7, 0.5], [0.8, 0.5], [0.9, 0.5], [1.0, 0.5]], "plan": ["We introduce a gated-hybrid architecture that learns two complementary experts:\na sparse linear rule head operating on character-frequency bags and a deep CNN\nexpert over token embeddings. A per-sample gate (sigmoid of the bag features)\nblends both sets of logits, allowing the network to trust explicit rules when\nsufficient and defer to the neural expert otherwise. L1 regularization shrinks\nrule weights for interpretability, while the gate regularizer encourages\ndecisive (near-binary) gating. This design typically raises accuracy and\nmaintains high Rule-Fidelity because the rule head remains an active component\nin the final prediction path. The script below loads SPR_BENCH, trains the model\nwith metric tracking, prints epoch-wise validation losses, and saves all\nexperiment data for later analysis.", "Ablation name: No-Gate Ensemble (Equal Blend). We ablate the adaptive gate by\nreplacing it with a fixed 0.5 / 0.5 blend of rule and CNN logits.  The learned\ngate layer and its regularisation term are deleted; consequently, training loss\nnow contains only cross-entropy and the optional L1 sparsity on the rule head.\nWe create a new model class NoGateHybridRuleCNN, adapt the training/evaluation\nloops to its interface, and log/save metrics under the ablation label\n\u201cNoGateEqualBlend\u201d.", "Ablation name: No Rule Sparsity (L1-\u03bb = 0). We rerun the hybrid CNN-rule model\nwhile setting the sparsity weight L1_LAMBDA to 0, leaving every other hyper-\nparameter untouched.  All training, validation and logging logic from the\nbaseline is kept, therefore any change in accuracy or rule fidelity can be\nattributed solely to the removed L1 penalty.  Results (loss curves, accuracies,\nrule fidelity, predictions, etc.) are stored under the ablation key\n\u201cNoRuleSparsity\u201d inside experiment_data.npy so they can be compared directly\nwith the baseline run.", "Ablation name: CNN-Feature Gate (No Bag-Based Gate). The gate is now derived\nfrom the CNN itself: after the convolution/max-pool blocks we obtain a flattened\nfeature vector and pass it through a tiny linear-sigmoid module to produce the\nscalar gate.  The rule head (bag-of-characters \u2192 logits) remains unchanged, but\nthe gate no longer sees the bag, letting us test whether the deep network can\nself-estimate its reliability.  The rest of the training / evaluation pipeline\nis kept identical, and all results are stored in the required experiment_data\nstructure under the ablation key \"CNNFeatureGate\".", "Ablation name: Rule-Free CNN Baseline (No Rule Head, No Gate). The ablation\nremoves the interpretable branch entirely, leaving a pure character-CNN\nclassifier. The code below keeps the data pipeline unchanged (bags are still\nproduced but ignored by the model), trains only the CNN encoder+head, logs\naccuracy/loss, sets rule-fidelity to zero by construction, and stores everything\nin the required experiment_data structure before saving it to\n\u201cexperiment_data.npy\u201d.", "Ablation name: No Gate Confidence Regularization (GATE-\u03bb = 0). We run the same\nhybrid rule-CNN architecture but set GATE_LAMBDA = 0, eliminating the quadratic\nconfidence regularizer and letting the scalar gate float freely. All other\nhyper-parameters, data handling, training, and logging remain unchanged, so any\ndifferences in accuracy, rule-fidelity, or gate distributions can be attributed\nto removing the regularizer. Results are stored under the ablation key\n\"NoGateConfidence\" and saved to experiment_data.npy for later\nplotting/comparison.", "The rule/CNN blending was reversed in the previous implementation, so the model\nalways trusted the rule head and consequently reported a constant Rule-Fidelity\nof 1.0.  Switching the mixture to logits = g\u00b7cnn + (1\u2013g)\u00b7rule restores the\nintended behaviour where g\u22481 means \u201cuse CNN\u201d and g\u22480 means \u201cuse rule\u201d, allowing\nfidelity to vary.  The outline below keeps everything else unchanged, adds the\nmandatory device handling, and logs/serialises all metrics exactly as required.", "Ablation name: Rule-Only Head (Fixed Gate = 1). The ablation is realized by\nentirely removing the CNN pathway and by fixing the gate to the constant 1, so\nthat the network degenerates to a sparse linear classifier over bag-of-\ncharacters features.  Training therefore minimizes cross-entropy plus the same\nL1 penalty on the rule weights (no gate regularizer).  Rule-Fidelity is\ntrivially 1.0 because the only head is the rule head.  All bookkeeping, logging\nand saving follow the template used in the baseline.", "Ablation name: Static\u2010Scalar Gate (Global \u03bb). We replace the per-example gating\nMLP with a single learnable logit \u03b2 whose sigmoid \u03bb=\u03c3(\u03b2) is shared by every\nsample.  The forward pass now mixes rule_logits and cnn_logits using this global\n\u03bb, and the gate regularizer is applied directly to \u03bb.  Everything else (data\nprep, training loop, evaluation, logging, saving) is unchanged.  The experiment\nresults are stored under the ablation key \u201cStaticScalarGate\u201d.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMS                                   #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY = 3e-3, 1e-4\nL1_LAMBDA = 2e-3\nEMB_DIM, CONV_CH = 48, 96\nKERNELS, MAX_LEN, PAD_IDX = [3, 4, 5], 128, 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({sp: _load(f\"{sp}.csv\") for sp in (\"train\", \"dev\", \"test\")})\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1  # plus PAD\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if seq:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab =\", VOCAB - 1, \"| Classes =\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL (no gate)                              #\n# -------------------------------------------------------------------------- #\nclass NoGateHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # bag-of-chars rule head\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        feats = [torch.amax(torch.relu(c(x)), dim=2) for c in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(feats, dim=1))  # (B,C)\n        final_logits = 0.5 * rule_logits + 0.5 * cnn_logits  # fixed blend\n        return final_logits, rule_logits  # (B,C), (B,C)\n\n\nmodel = NoGateHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"NoGateEqualBlend\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"timestamps\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot = corr = 0\n    loss_sum = fid_tot = fid_match = 0.0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss = run_corr = n_seen = 0\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, _ = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        loss = ce_loss + L1_LAMBDA * l1_loss\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    exp = experiment_data[\"NoGateEqualBlend\"][\"SPR_BENCH\"]\n    exp[\"losses\"][\"train\"].append(train_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"metrics\"][\"train_acc\"].append(train_acc)\n    exp[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp[\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    exp[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTEST: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexp[\"predictions\"] = torch.cat(preds).numpy()\nexp[\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", working_dir)\n", "# No-Rule-Sparsity Ablation (L1_LAMBDA = 0)\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using:\", device)\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY = 3e-3, 1e-4\nL1_LAMBDA = 0.0  # *** ablation: remove sparsity penalty ***\nGATE_LAMBDA = 1e-3\nEMB_DIM, CONV_CH = 48, 96\nKERNELS, MAX_LEN, PAD_IDX = [3, 4, 5], 128, 0\n\n\n# -------------------------------------------------------------------------- #\n#                               LOAD DATASET                                 #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(name: str):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict({s: _ld(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                                 VOCAB                                      #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq):\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    x = np.full(MAX_LEN, PAD_IDX, np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        x[i] = char2idx[ch]\n    return x\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, VAL_BATCH)\ntest_loader = DataLoader(test_ds, VAL_BATCH)\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                                  MODEL                                     #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab, n_cls):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)\n        gate = self.gate(bag)\n        x = self.embed(seq).transpose(1, 2)\n        feats = [torch.amax(torch.relu(c(x)), dim=2) for c in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(feats, 1))\n        return (\n            gate * rule_logits + (1 - gate) * cnn_logits,\n            rule_logits,\n            gate.squeeze(1),\n        )\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"NoRuleSparsity\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"timestamps\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               UTILITIES                                    #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    total, correct, lsum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            correct += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            lsum += loss.item() * y.size(0)\n            total += y.size(0)\n    return correct / total, lsum / total, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tr_loss, tr_corr, seen = 0.0, 0, 0\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce = criterion(logits, y)\n        l1 = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))\n        loss = ce + L1_LAMBDA * l1 + GATE_LAMBDA * gate_reg  # L1_LAMBDA==0\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * y.size(0)\n        tr_corr += (logits.argmax(1) == y).sum().item()\n        seen += y.size(0)\n    train_loss, train_acc = tr_loss / seen, tr_corr / seen\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n    ed = experiment_data[\"NoRuleSparsity\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    ed[\"timestamps\"].append(time.time())\n    print(\n        f\"Ep{epoch}: tr_loss {train_loss:.4f} tr_acc {train_acc:.3f} | \"\n        f\"val_loss {val_loss:.4f} val_acc {val_acc:.3f} RFS {val_rf:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                              FINAL TEST                                    #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss {test_loss:.4f} acc {test_acc:.3f} RuleFid {test_rf:.3f}\")\n\n# store predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        p, _, _ = model(bag.to(device), seq.to(device))\n        preds.append(p.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"NoRuleSparsity\"][\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"NoRuleSparsity\"][\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save all\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", working_dir)\n", "import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------ HOUSE-KEEPING ------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------- HYPER-PARAMETERS ----------------------------- #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY, L1_LAMBDA, GATE_LAMBDA = 3e-3, 1e-4, 2e-3, 1e-3\nEMB_DIM, CONV_CH, KERNELS = 48, 96, [3, 4, 5]\nMAX_LEN, PAD_IDX = 128, 0\n\n\n# ----------------------------- LOAD DATASET -------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):  # helper\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# --------------------------------- VOCAB ----------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 is PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq):\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.label = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.label[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = (SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\"))\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\nNUM_CLASSES = int(\n    max(train_ds.label.max(), val_ds.label.max(), test_ds.label.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"Classes\", NUM_CLASSES)\n\n\n# --------------------------------- MODEL ----------------------------------- #\nclass CNNFeatureGateHybrid(nn.Module):\n    \"\"\"\n    Hybrid model whose gate is predicted from CNN feature vector\n    instead of bag-of-chars statistics (ablation study).\n    \"\"\"\n\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # rule logits\n        # ---------- CNN part ----------\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        feat_dim = CONV_CH * len(KERNELS)\n        self.cnn_head = nn.Linear(feat_dim, n_cls)  # cnn logits\n        self.gate = nn.Sequential(nn.Linear(feat_dim, 1), nn.Sigmoid())  # NEW gate\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        # CNN forward\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        pooled = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        feats = torch.cat(pooled, dim=1)  # (B,feat_dim)\n        cnn_logits = self.cnn_head(feats)  # (B,C)\n        gate = self.gate(feats).squeeze(1)  # (B,)\n        logits = gate.unsqueeze(1) * rule_logits + (1 - gate.unsqueeze(1)) * cnn_logits\n        return logits, rule_logits, gate\n\n\nmodel = CNNFeatureGateHybrid(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# ------------------------- EXPERIMENT DATA HOLDER -------------------------- #\nexperiment_data = {\n    \"CNNFeatureGate\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"timestamps\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ------------------------------ EVALUATION --------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, correct, loss_sum = 0, 0, 0.0\n    rf_match, rf_tot = 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            correct += (preds == y).sum().item()\n            rf_match += (preds == rule_preds).sum().item()\n            rf_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return correct / tot, loss_sum / tot, rf_match / rf_tot\n\n\n# ------------------------------- TRAIN LOOP -------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    ep_loss, ep_corr, seen = 0.0, 0, 0\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce = criterion(logits, y)\n        l1 = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))\n        loss = ce + L1_LAMBDA * l1 + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        ep_loss += loss.item() * y.size(0)\n        ep_corr += (logits.argmax(1) == y).sum().item()\n        seen += y.size(0)\n\n    train_loss, train_acc = ep_loss / seen, ep_corr / seen\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n\n    ed = experiment_data[\"CNNFeatureGate\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    ed[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rf:.3f}\"\n    )\n\n# --------------------------------- TEST ------------------------------------ #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rf:.3f}\")\n\n# Save detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        plogits, _, _ = model(bag, seq)\n        preds.append(plogits.argmax(1).cpu())\n        gts.append(y)\ned = experiment_data[\"CNNFeatureGate\"][\"SPR_BENCH\"]\ned[\"predictions\"] = torch.cat(preds).numpy()\ned[\"ground_truth\"] = torch.cat(gts).numpy()\n\n# ------------------------------ SAVE RESULTS ------------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# Rule-Free CNN Baseline (No Rule Head, No Gate)\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- House-keeping -------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------- Hyper-params -------------------- #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY = 3e-3, 1e-4\nEMB_DIM, CONV_CH, MAX_LEN, PAD_IDX = 48, 96, 128, 0\nKERNELS = [3, 4, 5]\n\n\n# -------------------- Dataset -------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------- Vocab / helpers -------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 for PAD\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, VAL_BATCH)\ntest_loader = DataLoader(test_ds, VAL_BATCH)\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab:\", VOCAB - 1, \"Classes:\", NUM_CLASSES)\n\n\n# -------------------- Model -------------------- #\nclass PureCNN(nn.Module):\n    \"\"\"No rule head, no gate \u2013 just CNN encoder + linear head\"\"\"\n\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, seq):\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        feats = [torch.amax(torch.relu(cv(x)), dim=2) for cv in self.convs]\n        return self.cnn_head(torch.cat(feats, 1))  # (B,C)\n\n\nmodel = PureCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------- Experiment data container -------------------- #\nexperiment_data = {\n    \"RuleFreeCNN\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# -------------------- Evaluation -------------------- #\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    for _, seq, y in loader:\n        seq, y = seq.to(device), y.to(device)\n        logits = model(seq)\n        loss_sum += criterion(logits, y).item() * y.size(0)\n        correct += (logits.argmax(1) == y).sum().item()\n        total += y.size(0)\n    return correct / total, loss_sum / total  # acc, loss\n\n\n# -------------------- Training loop -------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tr_loss, tr_corr, seen = 0.0, 0, 0\n    for _, seq, y in train_loader:\n        seq, y = seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(seq)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * y.size(0)\n        tr_corr += (logits.argmax(1) == y).sum().item()\n        seen += y.size(0)\n    train_acc = tr_corr / seen\n    train_loss = tr_loss / seen\n    val_acc, val_loss = evaluate(val_loader)\n\n    ed = experiment_data[\"RuleFreeCNN\"][\"SPR_BENCH\"]\n    ed[\"metrics\"][\"train\"].append(train_acc)\n    ed[\"metrics\"][\"val\"].append(val_acc)\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n    )\n\n# -------------------- Final test -------------------- #\ntest_acc, test_loss = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}\")\n\n# store predictions / gts\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for _, seq, y in test_loader:\n        seq = seq.to(device)\n        preds.append(model(seq).argmax(1).cpu())\n        gts.append(y)\ned = experiment_data[\"RuleFreeCNN\"][\"SPR_BENCH\"]\ned[\"predictions\"] = torch.cat(preds).numpy()\ned[\"ground_truth\"] = torch.cat(gts).numpy()\n\n# -------------------- Save -------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# No-Gate-Confidence ablation : GATE_LAMBDA = 0\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                        HYPER-PARAMETERS / ABLATION                         #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY = 3e-3, 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 0.0  # *** ablation : no gate confidence regularizer ***\nEMB_DIM, CONV_CH = 48, 96\nKERNELS, MAX_LEN, PAD_IDX = [3, 4, 5], 128, 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rule logits\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        feats = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(feats, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"NoGateConfidence\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"timestamps\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\nexp_ref = experiment_data[\"NoGateConfidence\"][\"SPR_BENCH\"]\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot = corr = 0\n    loss_sum = 0.0\n    fid_tot = fid_match = 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss = run_corr = n_seen = 0\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # still compute for monitoring\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    exp_ref[\"losses\"][\"train\"].append(train_loss)\n    exp_ref[\"losses\"][\"val\"].append(val_loss)\n    exp_ref[\"metrics\"][\"train_acc\"].append(train_acc)\n    exp_ref[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp_ref[\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    exp_ref[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexp_ref[\"predictions\"] = torch.cat(preds).numpy()\nexp_ref[\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Experiment data saved to\", working_dir)\n", "import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------- SET-UP ------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------- HYPER-PARAMETERS ------------------------------------------- #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY, L1_LAMBDA, GATE_LAMBDA = 3e-3, 1e-4, 2e-3, 1e-3\nEMB_DIM, CONV_CH, KERNELS = 48, 96, [3, 4, 5]\nMAX_LEN, PAD_IDX = 128, 0\n\n\n# ---------------------------------------------- DATA LOADING --------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {split: _load(f\"{split}.csv\") for split in (\"train\", \"dev\", \"test\")}\n    )\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------- VOCAB ------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq):\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.label = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.label[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = (SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\"))\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.label.max(), val_ds.label.max(), test_ds.label.max()) + 1\n)\nprint(f\"Vocab size = {VOCAB-1}, #Classes = {NUM_CLASSES}\")\n\n\n# -------------------------------------------------- MODEL ------------------------------------------------- #\nclass CNNFeatureGateHybrid(nn.Module):\n    \"\"\"\n    Gate learns a confidence score from CNN features.\n    Corrected blend:   logits = g * cnn + (1-g) * rule\n    \"\"\"\n\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        # Rule branch (bag-of-chars)\n        self.rule_head = nn.Linear(vocab - 1, n_cls)\n        # CNN branch\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        feat_dim = CONV_CH * len(KERNELS)\n        self.cnn_head = nn.Linear(feat_dim, n_cls)\n        # Gate\n        self.gate = nn.Sequential(nn.Linear(feat_dim, 1), nn.Sigmoid())\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag.float())  # (B,C)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        pooled = [torch.relu(c(x)).amax(dim=2) for c in self.convs]\n        feats = torch.cat(pooled, dim=1)  # (B,feat_dim)\n        cnn_logits = self.cnn_head(feats)  # (B,C)\n        g = self.gate(feats).squeeze(1)  # (B,)\n        logits = g.unsqueeze(1) * cnn_logits + (1 - g.unsqueeze(1)) * rule_logits\n        return logits, rule_logits, g\n\n\nmodel = CNNFeatureGateHybrid(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# ------------------------------------------ EXPERIMENT LOGGING ------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ------------------------------------------------ EVALUATION --------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, correct, loss_sum = 0, 0, 0.0\n    rf_match, rf_tot = 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            correct += (preds == y).sum().item()\n            rf_match += (preds == rule_preds).sum().item()\n            rf_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return correct / tot, loss_sum / tot, rf_match / rf_tot\n\n\n# ------------------------------------------------- TRAINING ---------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    ep_loss, ep_corr, seen = 0.0, 0, 0\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce = criterion(logits, y)\n        l1 = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # encourage extremes\n        loss = ce + L1_LAMBDA * l1 + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n\n        ep_loss += loss.item() * y.size(0)\n        ep_corr += (logits.argmax(1) == y).sum().item()\n        seen += y.size(0)\n\n    train_loss, train_acc = ep_loss / seen, ep_corr / seen\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n\n    ed = experiment_data[\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    ed[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, \"\n        f\"train_acc={train_acc:.3f} | val_loss={val_loss:.4f}, \"\n        f\"val_acc={val_acc:.3f} | RuleFidelity={val_rf:.3f}\"\n    )\n\n# -------------------------------------------------- TESTING ---------------------------------------------- #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rf:.3f}\")\n\n# store predictions on test set\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        plogits, _, _ = model(bag, seq)\n        preds.append(plogits.argmax(1).cpu())\n        gts.append(y)\ned = experiment_data[\"SPR_BENCH\"]\ned[\"predictions\"] = torch.cat(preds).numpy()\ned[\"ground_truth\"] = torch.cat(gts).numpy()\n\n# --------------------------------------------- SAVE EVERYTHING ------------------------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# Rule-Only Head (Fixed Gate = 1) Ablation\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0: PAD\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq):\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack(\n            [seq_to_idx(s) for s in split[\"sequence\"]]\n        )  # kept for API compatibility\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),  # unused in ablation model\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass RuleOnly(nn.Module):\n    \"\"\"\n    Rule-Only Head (gate fixed to 1).  No CNN, no gate parameters.\n    \"\"\"\n\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)\n\n    def forward(self, bag, seq=None):  # seq ignored\n        logits = self.rule_head(bag)\n        return logits  # single output\n\n\nmodel = RuleOnly(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"rule_only\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"timestamps\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum = 0, 0, 0.0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, y = bag.to(device), y.to(device)\n            logits = model(bag)\n            loss = criterion(logits, y)\n            corr += (logits.argmax(1) == y).sum().item()\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    acc = corr / tot\n    return acc, loss_sum / tot, 1.0  # fidelity is 1 by construction\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss = run_corr = n_seen = 0\n    for bag, seq, y in train_loader:\n        bag, y = bag.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(bag)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        loss = ce_loss + L1_LAMBDA * l1_loss\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    exp = experiment_data[\"rule_only\"][\"SPR_BENCH\"]\n    exp[\"losses\"][\"train\"].append(train_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"metrics\"][\"train_acc\"].append(train_acc)\n    exp[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp[\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    exp[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag = bag.to(device)\n        logits = model(bag)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexp[\"predictions\"] = torch.cat(preds).numpy()\nexp[\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------ #\n#                         HOUSE-KEEPING                              #\n# ------------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ------------------------------------------------------------------ #\n#                       HYPER-PARAMETERS                             #\n# ------------------------------------------------------------------ #\nBATCH_SIZE, VAL_BATCH, EPOCHS = 256, 512, 10\nLR, WEIGHT_DECAY = 3e-3, 1e-4\nL1_LAMBDA, GATE_LAMBDA = 2e-3, 1e-3\nEMB_DIM, CONV_CH = 48, 96\nKERNELS, MAX_LEN, PAD_IDX = [3, 4, 5], 128, 0\n\n\n# ------------------------------------------------------------------ #\n#                           LOAD DATA                                #\n# ------------------------------------------------------------------ #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):  # helper\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------------ #\n#                           VOCAB                                    #\n# ------------------------------------------------------------------ #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1  # 0 = PAD\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = (SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\"))\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab:\", VOCAB - 1, \"Classes:\", NUM_CLASSES)\n\n\n# ------------------------------------------------------------------ #\n#                           MODEL                                    #\n# ------------------------------------------------------------------ #\nclass StaticGateHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)\n        self.beta = nn.Parameter(torch.zeros(1))  # global logit\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)\n        lam = torch.sigmoid(self.beta)  # scalar in (0,1)\n        x = self.embed(seq).transpose(1, 2)\n        feats = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(feats, dim=1))\n        logits = lam * rule_logits + (1 - lam) * cnn_logits\n        return logits, rule_logits, lam  # lam is scalar\n\n\nmodel = StaticGateHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# ------------------------------------------------------------------ #\n#                     EXPERIMENT STORAGE                             #\n# ------------------------------------------------------------------ #\nexperiment_data = {\n    \"StaticScalarGate\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"timestamps\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ------------------------------------------------------------------ #\n#                       EVALUATION                                   #\n# ------------------------------------------------------------------ #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# ------------------------------------------------------------------ #\n#                       TRAINING LOOP                                #\n# ------------------------------------------------------------------ #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss = run_corr = n_seen = 0\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, lam = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = lam * (1 - lam)  # scalar\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    ed = experiment_data[\"StaticScalarGate\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    ed[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# ------------------------------------------------------------------ #\n#                         FINAL TEST                                #\n# ------------------------------------------------------------------ #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_rfs:.3f}\")\n\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\ned = experiment_data[\"StaticScalarGate\"][\"SPR_BENCH\"]\ned[\"predictions\"] = torch.cat(preds).numpy()\ned[\"ground_truth\"] = torch.cat(gts).numpy()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.8398, train_acc=0.544 | val_loss=0.7889, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.6050, train_acc=0.600 |\nval_loss=0.5734, val_acc=0.792 | RFS=0.512', '\\n', 'Epoch 3: train_loss=0.4324,\ntrain_acc=0.849 | val_loss=0.5727, val_acc=0.720 | RFS=0.412', '\\n', 'Epoch 4:\ntrain_loss=0.3009, train_acc=0.948 | val_loss=0.5722, val_acc=0.746 |\nRFS=0.458', '\\n', 'Epoch 5: train_loss=0.2098, train_acc=0.935 |\nval_loss=0.6643, val_acc=0.766 | RFS=0.490', '\\n', 'Epoch 6: train_loss=0.1624,\ntrain_acc=0.953 | val_loss=0.7531, val_acc=0.762 | RFS=0.478', '\\n', 'Epoch 7:\ntrain_loss=0.1307, train_acc=0.965 | val_loss=0.8187, val_acc=0.764 |\nRFS=0.480', '\\n', 'Epoch 8: train_loss=0.1083, train_acc=0.969 |\nval_loss=0.8595, val_acc=0.776 | RFS=0.496', '\\n', 'Epoch 9: train_loss=0.0907,\ntrain_acc=0.974 | val_loss=0.8979, val_acc=0.778 | RFS=0.482', '\\n', 'Epoch 10:\ntrain_loss=0.0800, train_acc=0.977 | val_loss=0.9330, val_acc=0.784 |\nRFS=0.488', '\\n', '\\nTest: loss=0.9244, acc=0.784, RuleFidelity=0.490', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n12/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 129643.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 131162.17\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 193063.48\nexamples/s]', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab =', ' ', '9', ' ', '| Classes =', ' ', '2', '\\n', 'Epoch\n01: train_loss=0.7630, train_acc=0.537 | val_loss=0.6914, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 02: train_loss=0.5293, train_acc=0.764 |\nval_loss=0.5775, val_acc=0.764 | RFS=0.584', '\\n', 'Epoch 03: train_loss=0.3851,\ntrain_acc=0.921 | val_loss=0.5501, val_acc=0.768 | RFS=0.500', '\\n', 'Epoch 04:\ntrain_loss=0.2548, train_acc=0.951 | val_loss=0.6108, val_acc=0.762 |\nRFS=0.482', '\\n', 'Epoch 05: train_loss=0.1720, train_acc=0.956 |\nval_loss=0.7266, val_acc=0.768 | RFS=0.496', '\\n', 'Epoch 06: train_loss=0.1349,\ntrain_acc=0.960 | val_loss=0.8210, val_acc=0.778 | RFS=0.498', '\\n', 'Epoch 07:\ntrain_loss=0.1151, train_acc=0.968 | val_loss=0.8916, val_acc=0.762 |\nRFS=0.470', '\\n', 'Epoch 08: train_loss=0.1041, train_acc=0.971 |\nval_loss=0.9450, val_acc=0.764 | RFS=0.468', '\\n', 'Epoch 09: train_loss=0.0912,\ntrain_acc=0.976 | val_loss=0.9726, val_acc=0.772 | RFS=0.476', '\\n', 'Epoch 10:\ntrain_loss=0.0810, train_acc=0.977 | val_loss=0.9855, val_acc=0.782 |\nRFS=0.494', '\\n', '\\nTEST: loss=0.9921, acc=0.786, RuleFidelity=0.486', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n16/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 119115.76\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 131929.54\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 212649.77\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'vocab',\n' ', '9', ' ', 'classes', ' ', '2', '\\n', 'Ep1: tr_loss 0.8395 tr_acc 0.544 |\nval_loss 0.7889 val_acc 0.520 RFS 1.000', '\\n', 'Ep2: tr_loss 0.6048 tr_acc\n0.600 | val_loss 0.5734 val_acc 0.792 RFS 0.512', '\\n', 'Ep3: tr_loss 0.4321\ntr_acc 0.849 | val_loss 0.5727 val_acc 0.720 RFS 0.412', '\\n', 'Ep4: tr_loss\n0.3006 tr_acc 0.948 | val_loss 0.5722 val_acc 0.746 RFS 0.458', '\\n', 'Ep5:\ntr_loss 0.2095 tr_acc 0.935 | val_loss 0.6643 val_acc 0.766 RFS 0.490', '\\n',\n'Ep6: tr_loss 0.1621 tr_acc 0.954 | val_loss 0.7531 val_acc 0.762 RFS 0.478',\n'\\n', 'Ep7: tr_loss 0.1305 tr_acc 0.965 | val_loss 0.8187 val_acc 0.764 RFS\n0.480', '\\n', 'Ep8: tr_loss 0.1080 tr_acc 0.969 | val_loss 0.8594 val_acc 0.776\nRFS 0.496', '\\n', 'Ep9: tr_loss 0.0905 tr_acc 0.974 | val_loss 0.8976 val_acc\n0.778 RFS 0.482', '\\n', 'Ep10: tr_loss 0.0796 tr_acc 0.977 | val_loss 0.9324\nval_acc 0.784 RFS 0.488', '\\n', '\\nTest: loss 0.9247 acc 0.784 RuleFid 0.490',\n'\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n17/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n109196.81 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 119332.65\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 188949.64\nexamples/s]', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'Classes', ' ', '2', '\\n', 'Epoch 1:\ntrain_loss=0.8590, train_acc=0.510 | val_loss=0.6929, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.6938, train_acc=0.500 |\nval_loss=0.6900, val_acc=0.520 | RFS=1.000', '\\n', 'Epoch 3: train_loss=0.6893,\ntrain_acc=0.500 | val_loss=0.6882, val_acc=0.520 | RFS=1.000', '\\n', 'Epoch 4:\ntrain_loss=0.6859, train_acc=0.500 | val_loss=0.6872, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 5: train_loss=0.6839, train_acc=0.524 |\nval_loss=0.6868, val_acc=0.594 | RFS=1.000', '\\n', 'Epoch 6: train_loss=0.6821,\ntrain_acc=0.805 | val_loss=0.6864, val_acc=0.734 | RFS=1.000', '\\n', 'Epoch 7:\ntrain_loss=0.6808, train_acc=0.887 | val_loss=0.6861, val_acc=0.698 |\nRFS=1.000', '\\n', 'Epoch 8: train_loss=0.6795, train_acc=0.829 |\nval_loss=0.6855, val_acc=0.668 | RFS=1.000', '\\n', 'Epoch 9: train_loss=0.6783,\ntrain_acc=0.828 | val_loss=0.6847, val_acc=0.682 | RFS=1.000', '\\n', 'Epoch 10:\ntrain_loss=0.6770, train_acc=0.862 | val_loss=0.6839, val_acc=0.710 |\nRFS=1.000', '\\n', '\\nTest: loss=0.6830, acc=0.717, RuleFidelity=1.000', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n18/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n49448.59 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 110679.33\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 122565.21\nexamples/s]', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab:', ' ', '9', ' ', 'Classes:', ' ', '2', '\\n', 'Epoch 1:\ntrain_loss=1.1565, train_acc=0.523 | val_loss=0.8857, val_acc=0.520', '\\n',\n'Epoch 2: train_loss=0.5896, train_acc=0.665 | val_loss=0.6074, val_acc=0.646',\n'\\n', 'Epoch 3: train_loss=0.4037, train_acc=0.882 | val_loss=0.5682,\nval_acc=0.770', '\\n', 'Epoch 4: train_loss=0.2884, train_acc=0.938 |\nval_loss=0.5644, val_acc=0.784', '\\n', 'Epoch 5: train_loss=0.1978,\ntrain_acc=0.966 | val_loss=0.6262, val_acc=0.772', '\\n', 'Epoch 6:\ntrain_loss=0.1474, train_acc=0.967 | val_loss=0.7225, val_acc=0.774', '\\n',\n'Epoch 7: train_loss=0.1211, train_acc=0.968 | val_loss=0.8054, val_acc=0.778',\n'\\n', 'Epoch 8: train_loss=0.0997, train_acc=0.976 | val_loss=0.8672,\nval_acc=0.774', '\\n', 'Epoch 9: train_loss=0.0895, train_acc=0.975 |\nval_loss=0.9089, val_acc=0.788', '\\n', 'Epoch 10: train_loss=0.0799,\ntrain_acc=0.976 | val_loss=0.9449, val_acc=0.784', '\\n', '\\nTest: loss=0.9478,\nacc=0.784', '\\n', 'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist\n-v2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n19/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.8395, train_acc=0.544 | val_loss=0.7889, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.6048, train_acc=0.600 |\nval_loss=0.5734, val_acc=0.792 | RFS=0.512', '\\n', 'Epoch 3: train_loss=0.4322,\ntrain_acc=0.849 | val_loss=0.5727, val_acc=0.720 | RFS=0.412', '\\n', 'Epoch 4:\ntrain_loss=0.3006, train_acc=0.948 | val_loss=0.5722, val_acc=0.746 |\nRFS=0.458', '\\n', 'Epoch 5: train_loss=0.2095, train_acc=0.935 |\nval_loss=0.6643, val_acc=0.766 | RFS=0.490', '\\n', 'Epoch 6: train_loss=0.1622,\ntrain_acc=0.954 | val_loss=0.7531, val_acc=0.762 | RFS=0.478', '\\n', 'Epoch 7:\ntrain_loss=0.1305, train_acc=0.965 | val_loss=0.8186, val_acc=0.764 |\nRFS=0.480', '\\n', 'Epoch 8: train_loss=0.1080, train_acc=0.969 |\nval_loss=0.8594, val_acc=0.776 | RFS=0.496', '\\n', 'Epoch 9: train_loss=0.0905,\ntrain_acc=0.974 | val_loss=0.8973, val_acc=0.778 | RFS=0.482', '\\n', 'Epoch 10:\ntrain_loss=0.0797, train_acc=0.977 | val_loss=0.9324, val_acc=0.784 |\nRFS=0.488', '\\n', '\\nTest: loss=0.9240, acc=0.784, RuleFidelity=0.490', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n18/working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000, 'dev': 500,\n'test': 1000}\", '\\n', 'Vocab size = 9, #Classes = 2', '\\n', 'Epoch 1:\ntrain_loss=0.7993, train_acc=0.509 | val_loss=0.6914, val_acc=0.520 |\nRuleFidelity=1.000', '\\n', 'Epoch 2: train_loss=0.6916, train_acc=0.500 |\nval_loss=0.6891, val_acc=0.520 | RuleFidelity=1.000', '\\n', 'Epoch 3:\ntrain_loss=0.6876, train_acc=0.500 | val_loss=0.6880, val_acc=0.520 |\nRuleFidelity=1.000', '\\n', 'Epoch 4: train_loss=0.6851, train_acc=0.507 |\nval_loss=0.6875, val_acc=0.554 | RuleFidelity=1.000', '\\n', 'Epoch 5:\ntrain_loss=0.6836, train_acc=0.729 | val_loss=0.6873, val_acc=0.728 |\nRuleFidelity=1.000', '\\n', 'Epoch 6: train_loss=0.6821, train_acc=0.874 |\nval_loss=0.6868, val_acc=0.706 | RuleFidelity=1.000', '\\n', 'Epoch 7:\ntrain_loss=0.6809, train_acc=0.838 | val_loss=0.6862, val_acc=0.672 |\nRuleFidelity=1.000', '\\n', 'Epoch 8: train_loss=0.6796, train_acc=0.806 |\nval_loss=0.6855, val_acc=0.670 | RuleFidelity=1.000', '\\n', 'Epoch 9:\ntrain_loss=0.6783, train_acc=0.831 | val_loss=0.6847, val_acc=0.696 |\nRuleFidelity=1.000', '\\n', 'Epoch 10: train_loss=0.6771, train_acc=0.874 |\nval_loss=0.6838, val_acc=0.722 | RuleFidelity=1.000', '\\n', '\\nTest:\nloss=0.6830, acc=0.732, RuleFidelity=1.000', '\\n', 'Experiment data saved to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n16/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000,\n'dev': 500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2',\n'\\n', 'Epoch 1: train_loss=0.6948, train_acc=0.500 | val_loss=0.6904,\nval_acc=0.520', '\\n', 'Epoch 2: train_loss=0.6898, train_acc=0.500 |\nval_loss=0.6886, val_acc=0.520', '\\n', 'Epoch 3: train_loss=0.6864,\ntrain_acc=0.500 | val_loss=0.6880, val_acc=0.532', '\\n', 'Epoch 4:\ntrain_loss=0.6844, train_acc=0.656 | val_loss=0.6878, val_acc=0.710', '\\n',\n'Epoch 5: train_loss=0.6830, train_acc=0.852 | val_loss=0.6874, val_acc=0.664',\n'\\n', 'Epoch 6: train_loss=0.6819, train_acc=0.725 | val_loss=0.6870,\nval_acc=0.618', '\\n', 'Epoch 7: train_loss=0.6806, train_acc=0.706 |\nval_loss=0.6863, val_acc=0.634', '\\n', 'Epoch 8: train_loss=0.6793,\ntrain_acc=0.788 | val_loss=0.6852, val_acc=0.680', '\\n', 'Epoch 9:\ntrain_loss=0.6779, train_acc=0.860 | val_loss=0.6844, val_acc=0.712', '\\n',\n'Epoch 10: train_loss=0.6767, train_acc=0.899 | val_loss=0.6834, val_acc=0.752',\n'\\n', '\\nTest: loss=0.6828, acc=0.750, RuleFidelity=1.000', '\\n', 'Experiment\ndata saved to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-\n43-53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n17/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 2000,\n'dev': 500, 'test': 1000}\", '\\n', 'Vocab:', ' ', '9', ' ', 'Classes:', ' ', '2',\n'\\n', 'Epoch 1: train_loss=0.7626, train_acc=0.539 | val_loss=0.6909,\nval_acc=0.520 | RFS=1.000', '\\n', 'Epoch 2: train_loss=0.5304, train_acc=0.762 |\nval_loss=0.5767, val_acc=0.762 | RFS=0.582', '\\n', 'Epoch 3: train_loss=0.3865,\ntrain_acc=0.920 | val_loss=0.5492, val_acc=0.774 | RFS=0.494', '\\n', 'Epoch 4:\ntrain_loss=0.2545, train_acc=0.952 | val_loss=0.6123, val_acc=0.762 |\nRFS=0.482', '\\n', 'Epoch 5: train_loss=0.1707, train_acc=0.955 |\nval_loss=0.7328, val_acc=0.768 | RFS=0.496', '\\n', 'Epoch 6: train_loss=0.1330,\ntrain_acc=0.961 | val_loss=0.8325, val_acc=0.776 | RFS=0.500', '\\n', 'Epoch 7:\ntrain_loss=0.1129, train_acc=0.969 | val_loss=0.9056, val_acc=0.766 |\nRFS=0.474', '\\n', 'Epoch 8: train_loss=0.1021, train_acc=0.971 |\nval_loss=0.9633, val_acc=0.766 | RFS=0.470', '\\n', 'Epoch 9: train_loss=0.0885,\ntrain_acc=0.977 | val_loss=0.9985, val_acc=0.772 | RFS=0.476', '\\n', 'Epoch 10:\ntrain_loss=0.0799, train_acc=0.978 | val_loss=1.0120, val_acc=0.778 |\nRFS=0.490', '\\n', '\\nTest: loss=1.0154, acc=0.786, RuleFid=0.484', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_02-43-53_interpretable_neural_rule_learning_attempt_0/0-\nrun/process_ForkProcess-19/working', '\\n', 'Execution time: 3 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.7258, train_acc=0.531 | val_loss=0.6675, val_acc=0.500 |\nRFS=0.964', '\\n', 'Epoch 2: train_loss=0.5030, train_acc=0.785 |\nval_loss=0.5864, val_acc=0.718 | RFS=0.598', '\\n', 'Epoch 3: train_loss=0.3522,\ntrain_acc=0.938 | val_loss=0.5666, val_acc=0.746 | RFS=0.558', '\\n', 'Epoch 4:\ntrain_loss=0.2228, train_acc=0.950 | val_loss=0.6560, val_acc=0.754 |\nRFS=0.534', '\\n', 'Epoch 5: train_loss=0.1558, train_acc=0.958 |\nval_loss=0.7902, val_acc=0.754 | RFS=0.530', '\\n', 'Epoch 6: train_loss=0.1220,\ntrain_acc=0.966 | val_loss=0.8836, val_acc=0.766 | RFS=0.506', '\\n', 'Epoch 7:\ntrain_loss=0.1056, train_acc=0.971 | val_loss=0.9521, val_acc=0.772 |\nRFS=0.492', '\\n', 'Epoch 8: train_loss=0.0954, train_acc=0.972 |\nval_loss=0.9965, val_acc=0.774 | RFS=0.486', '\\n', 'Epoch 9: train_loss=0.0818,\ntrain_acc=0.976 | val_loss=1.0551, val_acc=0.776 | RFS=0.472', '\\n', 'Epoch 10:\ntrain_loss=0.0734, train_acc=0.978 | val_loss=1.0887, val_acc=0.780 |\nRFS=0.484', '\\n', '\\nTest: loss=1.0875, acc=0.779, RuleFidelity=0.489', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n17/working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.7679, train_acc=0.540 | val_loss=0.6982, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.5532, train_acc=0.763 |\nval_loss=0.5841, val_acc=0.748 | RFS=0.580', '\\n', 'Epoch 3: train_loss=0.4018,\ntrain_acc=0.948 | val_loss=0.5496, val_acc=0.770 | RFS=0.506', '\\n', 'Epoch 4:\ntrain_loss=0.2640, train_acc=0.957 | val_loss=0.5898, val_acc=0.772 |\nRFS=0.492', '\\n', 'Epoch 5: train_loss=0.1722, train_acc=0.959 |\nval_loss=0.7062, val_acc=0.770 | RFS=0.490', '\\n', 'Epoch 6: train_loss=0.1262,\ntrain_acc=0.966 | val_loss=0.8272, val_acc=0.776 | RFS=0.496', '\\n', 'Epoch 7:\ntrain_loss=0.1000, train_acc=0.971 | val_loss=0.9109, val_acc=0.778 |\nRFS=0.494', '\\n', 'Epoch 8: train_loss=0.0860, train_acc=0.974 |\nval_loss=0.9668, val_acc=0.784 | RFS=0.496', '\\n', 'Epoch 9: train_loss=0.0756,\ntrain_acc=0.977 | val_loss=1.0126, val_acc=0.786 | RFS=0.494', '\\n', 'Epoch 10:\ntrain_loss=0.0654, train_acc=0.983 | val_loss=1.0517, val_acc=0.786 |\nRFS=0.494', '\\n', '\\nTest: loss=1.0565, acc=0.788, RuleFidelity=0.488', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n18/working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.7722, train_acc=0.558 | val_loss=0.7278, val_acc=0.480 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.5505, train_acc=0.711 |\nval_loss=0.5932, val_acc=0.730 | RFS=0.598', '\\n', 'Epoch 3: train_loss=0.3936,\ntrain_acc=0.948 | val_loss=0.5441, val_acc=0.774 | RFS=0.530', '\\n', 'Epoch 4:\ntrain_loss=0.2504, train_acc=0.971 | val_loss=0.5870, val_acc=0.778 |\nRFS=0.518', '\\n', 'Epoch 5: train_loss=0.1568, train_acc=0.967 |\nval_loss=0.7186, val_acc=0.784 | RFS=0.504', '\\n', 'Epoch 6: train_loss=0.1123,\ntrain_acc=0.971 | val_loss=0.8522, val_acc=0.784 | RFS=0.504', '\\n', 'Epoch 7:\ntrain_loss=0.0915, train_acc=0.972 | val_loss=0.9508, val_acc=0.784 |\nRFS=0.504', '\\n', 'Epoch 8: train_loss=0.0772, train_acc=0.978 |\nval_loss=1.0128, val_acc=0.788 | RFS=0.504', '\\n', 'Epoch 9: train_loss=0.0680,\ntrain_acc=0.982 | val_loss=1.0600, val_acc=0.788 | RFS=0.504', '\\n', 'Epoch 10:\ntrain_loss=0.0604, train_acc=0.983 | val_loss=1.1089, val_acc=0.782 |\nRFS=0.490', '\\n', '\\nTest: loss=1.1226, acc=0.788, RuleFidelity=0.496', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n16/working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", ""], "analysis": ["", "The execution of the training script was successful without any bugs. The model\nwas trained and evaluated on the SPR_BENCH dataset, achieving a test accuracy of\n78.6% and a Rule Fidelity Score (RFS) of 0.486. The training process showed\nconsistent improvement in training accuracy and a slight improvement in\nvalidation accuracy over epochs, though the Rule Fidelity Score decreased after\nthe first epoch. The experiment data was saved successfully for further\nanalysis. No issues were detected in the implementation or execution.", "The training script executed successfully without any errors or bugs. The\ntraining, validation, and testing processes were completed as expected. The\noutputs, such as training loss, validation loss, accuracy, and Rule Fidelity\nscores, were logged per epoch. The experiment data was saved successfully. The\nmodel achieved a test accuracy of 78.4% with a Rule Fidelity score of 49.0%. No\nissues were detected in the implementation or execution.", "The model training script exhibits a significant issue with the dataset size.\nThe training dataset contains only 2000 rows, the validation dataset has 500\nrows, and the test dataset has 1000 rows, which are much smaller than expected\n(20000, 5000, and 10000 rows respectively as per the SPR_BENCH benchmark). This\ndiscrepancy likely affects the model's performance and the validity of the\nablation study. To fix this, ensure that the correct dataset files are loaded,\nand verify the integrity and size of the dataset files in the specified\ndirectory.", "The training script executed successfully without any errors. The Rule-Free CNN\nBaseline model was trained on the SPR_BENCH dataset, and the training and\nvalidation accuracies improved over epochs. The final validation accuracy was\n78.8%, and the test accuracy was 78.4%, which is close to the validation\nperformance. The experiment data was saved successfully. No bugs were detected\nin the execution.", "The training script executed successfully without any errors or bugs. The\ndataset was loaded correctly, and the training process proceeded as expected.\nThe model achieved a final test accuracy of 78.4% with a Rule Fidelity Score of\n0.49. The results align with the expected behavior of the No-Gate-Confidence\nablation study, where GATE_LAMBDA was set to 0. No issues were observed in the\nimplementation or execution.", "", "", "The execution was successful without any apparent bugs. The training and\nevaluation of the StaticGateHybridRuleCNN model proceeded as expected. The model\nachieved a final test accuracy of 78.6% and a Rule Fidelity Score (RFS) of 48.4%\non the test set. While the test accuracy is promising, the Rule Fidelity Score\nindicates that the model's interpretability (alignment between rule-based and\nneural predictions) could be improved. Further tuning of hyperparameters or\narchitectural adjustments may enhance interpretability and performance.", "The execution output indicates that the training script ran successfully without\nany bugs. The model was trained and evaluated on the Synthetic PolyRule\nReasoning task, achieving a test accuracy of 77.9% and rule fidelity of 48.9%.\nWhile the accuracy is promising and approaches the state-of-the-art benchmark of\n80%, the rule fidelity metric decreased over the epochs, suggesting a potential\ntrade-off between interpretability and accuracy. This behavior aligns with the\nrisk factors outlined in the research proposal. Overall, the script functioned\nas intended and produced meaningful results for further analysis.", "The execution of the training script completed successfully without any bugs.\nThe model was trained and evaluated on the SPR_BENCH dataset, achieving a test\naccuracy of 78.8% and a Rule Fidelity Score of 0.488. While the Rule Fidelity\nScore did not improve significantly after the first epoch, the validation and\ntest accuracy showed consistent improvement, nearing the state-of-the-art\nbenchmark of 80.0%. The experiment data was saved successfully for further\nanalysis.", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9765, "best_value": 0.9765}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Indicates how closely the rules align with the model's decisions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Measures the loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.08, "best_value": 0.08}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5722, "best_value": 0.5722}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.784}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.977, "best_value": 0.977}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.782, "best_value": 0.782}]}, {"metric_name": "Rule Fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.081, "best_value": 0.081}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5501, "best_value": 0.5501}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9765, "best_value": 0.9765}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.784}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.488, "best_value": 0.488}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.07962108564376831, "best_value": 0.07962108564376831}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9324386119842529, "best_value": 0.9324386119842529}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8875, "best_value": 0.8875}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.734, "best_value": 0.734}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.677, "best_value": 0.677}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6839, "best_value": 0.6839}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.717, "best_value": 0.717}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.976, "best_value": 0.976}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.784}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0799, "best_value": 0.0799}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9449, "best_value": 0.9449}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.784}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9765, "best_value": 0.9765}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0797, "best_value": 0.0797}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5722, "best_value": 0.5722}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.488, "best_value": 0.488}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly predicted instances out of the total instances.", "data": [{"dataset_name": "train", "final_value": 0.8735, "best_value": 0.8735}, {"dataset_name": "validation", "final_value": 0.728, "best_value": 0.728}, {"dataset_name": "test", "final_value": 0.732, "best_value": 0.732}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Indicates how well the rules generated by the model align with the true outcomes.", "data": [{"dataset_name": "general", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Represents the error or difference between the predicted and true values. Lower values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.6771, "best_value": 0.6771}, {"dataset_name": "validation", "final_value": 0.6838, "best_value": 0.6838}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8995, "best_value": 0.8995}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.752, "best_value": 0.752}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6767, "best_value": 0.6767}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6834, "best_value": 0.6834}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.778, "best_value": 0.778}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0799, "best_value": 0.0799}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5492, "best_value": 0.5492}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the model's rules to the data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.49, "best_value": 0.49}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.786, "best_value": 0.786}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy achieved on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9775, "best_value": 0.9775}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.78, "best_value": 0.78}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The rule fidelity score for the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.964, "best_value": 0.964}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value achieved on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0734, "best_value": 0.0734}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5666, "best_value": 0.5666}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.779, "best_value": 0.779}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9825, "best_value": 0.9825}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.786, "best_value": 0.786}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "How well the rules generated by the model align with the data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0654, "best_value": 0.0654}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5496, "best_value": 0.5496}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.788, "best_value": 0.788}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "The accuracy of the model in predicting the correct outputs.", "data": [{"dataset_name": "train", "final_value": 0.983, "best_value": 0.983}, {"dataset_name": "validation", "final_value": 0.788, "best_value": 0.788}, {"dataset_name": "test", "final_value": 0.788, "best_value": 0.788}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "train", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The loss value, representing the error during training and validation.", "data": [{"dataset_name": "train", "final_value": 0.0604, "best_value": 0.0604}, {"dataset_name": "validation", "final_value": 0.5441, "best_value": 0.5441}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_Loss_Curves.png", "../../logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_Accuracy_Curves.png", "../../logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_RuleFidelity.png", "../../logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_ConfusionMatrix.png"], ["../../logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_Loss_Curves_NoRuleSparsity.png", "../../logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_Accuracy_Curves_NoRuleSparsity.png", "../../logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_RuleFidelity_NoRuleSparsity.png", "../../logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_ConfusionMatrix_NoRuleSparsity.png"], [], ["../../logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_accuracy_curve_RuleFreeCNN.png", "../../logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_loss_curve_RuleFreeCNN.png", "../../logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_confusion_matrix_RuleFreeCNN.png"], ["../../logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_rule_fidelity_curve.png", "../../logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_067fea7a4c404675ac61f4a11dcb7b30/SPR_BENCH_agg_accuracy_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_067fea7a4c404675ac61f4a11dcb7b30/SPR_BENCH_agg_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_067fea7a4c404675ac61f4a11dcb7b30/SPR_BENCH_agg_rule_fidelity.png"]], "plot_paths": [["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_Loss_Curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_Accuracy_Curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_RuleFidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_ConfusionMatrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_Loss_Curves_NoRuleSparsity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_Accuracy_Curves_NoRuleSparsity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_RuleFidelity_NoRuleSparsity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_ConfusionMatrix_NoRuleSparsity.png"], [], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_accuracy_curve_RuleFreeCNN.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_loss_curve_RuleFreeCNN.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_confusion_matrix_RuleFreeCNN.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_rule_fidelity_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_067fea7a4c404675ac61f4a11dcb7b30/SPR_BENCH_agg_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_067fea7a4c404675ac61f4a11dcb7b30/SPR_BENCH_agg_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_067fea7a4c404675ac61f4a11dcb7b30/SPR_BENCH_agg_rule_fidelity.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation accuracy over epochs. The training accuracy increases steadily, reaching close to 100%, indicating that the model is learning effectively on the training data. However, the validation accuracy plateaus around 80% after an initial rise, suggesting potential overfitting as the gap between training and validation accuracy widens.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot displays the training and validation loss over epochs. The training loss decreases steadily, demonstrating that the model is optimizing on the training data. However, the validation loss begins to increase after an initial decrease, which is a classic sign of overfitting. The model fails to generalize well to unseen data beyond a certain point.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the rule fidelity over epochs. Rule fidelity starts at a high value, indicating that the model initially adheres well to the extracted rules. However, it drops significantly by the second epoch and remains relatively low and stable afterward. This suggests that the model's rule-based interpretability diminishes as training progresses, potentially due to overfitting to the data rather than adhering to interpretable rules.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_rule_fidelity.png"}, {"analysis": "This confusion matrix for the test set shows that the model performs well in distinguishing between the two classes, with a strong diagonal indicating correct predictions. However, the exact counts of false positives and false negatives are not visible in this heatmap, making it hard to assess specific areas of misclassification.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning effectively from the training data. However, the validation loss initially decreases but then starts to increase after around the 4th epoch, suggesting potential overfitting. This implies that the model's generalization capability to unseen data might be compromised if training continues for too long.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_Loss_Curves.png"}, {"analysis": "The training accuracy improves rapidly and reaches close to 100%, showing that the model fits the training data well. However, the validation accuracy plateaus around 80% and shows slight fluctuations, which further supports the observation of overfitting from the loss plot. The model achieves reasonable validation performance but struggles to generalize beyond this point.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_Accuracy_Curves.png"}, {"analysis": "The rule fidelity score starts at 1.0 but drops sharply within the first few epochs and then stabilizes around 0.5. This indicates that while the model initially attempts to align with the underlying rules, its ability to maintain high fidelity to the rules diminishes as training progresses. This could reflect a trade-off between optimizing for classification accuracy and adhering to interpretable rule-based reasoning.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_RuleFidelity.png"}, {"analysis": "The confusion matrix shows that the model performs well in distinguishing between the two classes, with a high number of correct predictions for both classes. However, there are some misclassifications, which might be due to the limitations in the model's ability to generalize or its interpretability trade-offs. The imbalance in misclassification rates, if any, should be further analyzed to identify specific weaknesses.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_7f137d05c10e4b88a62be45d4ddc56bb_proc_3218403/SPR_BENCH_NoGateEqualBlend_ConfusionMatrix.png"}], [{"analysis": "This plot shows the loss curves for both training and validation datasets. The training loss decreases steadily, indicating that the model is learning effectively. However, the validation loss initially decreases but starts increasing after epoch 5, suggesting overfitting. This behavior indicates that the model is memorizing the training data rather than generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_Loss_Curves_NoRuleSparsity.png"}, {"analysis": "This plot displays the accuracy curves for training and validation datasets. The training accuracy increases consistently and approaches 100%, while the validation accuracy plateaus around 75-80% after an initial increase. The gap between training and validation accuracy further supports the observation of overfitting.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_Accuracy_Curves_NoRuleSparsity.png"}, {"analysis": "This plot presents the rule fidelity metric over epochs. Initially, the fidelity starts high but drops significantly after the first epoch, stabilizing at a lower value around 0.4-0.5. This suggests that the model's ability to maintain interpretable rule-based reasoning diminishes over time, possibly due to overfitting or a lack of mechanisms to enforce rule sparsity.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_RuleFidelity_NoRuleSparsity.png"}, {"analysis": "The confusion matrix indicates that the model performs well in distinguishing between the two classes, as evident from the strong diagonal elements. However, the off-diagonal elements highlight some degree of misclassification, which could be further analyzed to understand specific failure cases.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_3f73a5f15525498abad0c0cf3e6e23de_proc_3218404/SPR_BENCH_ConfusionMatrix_NoRuleSparsity.png"}], [], [{"analysis": "This plot shows the accuracy of the model on the training and validation datasets over 10 epochs. The training accuracy increases rapidly, reaching over 95% by epoch 5, and then plateaus. Validation accuracy also improves initially but starts to plateau around 80% after epoch 4. This suggests that the model is learning effectively on the training data but may be approaching its generalization limit on the validation set. The gap between training and validation accuracy after epoch 4 indicates potential overfitting.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_accuracy_curve_RuleFreeCNN.png"}, {"analysis": "This plot depicts the cross-entropy loss for both training and validation datasets across 10 epochs. The training loss decreases steadily, approaching near-zero values by epoch 10, which indicates effective learning on the training data. However, the validation loss decreases initially but starts to increase after epoch 6, signaling overfitting. This aligns with the accuracy plot, suggesting that the model is memorizing the training data rather than generalizing well.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_loss_curve_RuleFreeCNN.png"}, {"analysis": "The confusion matrix provides a summary of the model's performance on the test set. The diagonal elements, representing correct predictions, are significantly higher compared to off-diagonal elements, indicating good classification performance. However, a slight imbalance is observed, with the model performing slightly better on one class than the other. This might suggest class imbalance in the dataset or model bias.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b992ba63c3c344d2b227416dc16c222f_proc_3218406/SPR_BENCH_confusion_matrix_RuleFreeCNN.png"}], [{"analysis": "The loss curves indicate that the training loss decreases steadily, which is expected as the model learns from the training data. However, the validation loss initially decreases but starts to increase after a few epochs, which suggests overfitting. This means the model is performing well on the training data but struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_loss_curves.png"}, {"analysis": "The accuracy curves show that the training accuracy increases rapidly and plateaus near 100%, indicating the model is learning the training data well. However, the validation accuracy improves initially but then stabilizes at a lower value, suggesting limited generalization. This pattern aligns with the overfitting observed in the loss curves.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The rule fidelity curve shows a sharp decline after the first epoch and then stabilizes at a lower value. This suggests that the scalar gate ablation negatively impacts the model's ability to maintain rule fidelity, which could be due to the removal of a critical component for interpretability in the model.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix shows a relatively balanced distribution of correct and incorrect predictions across the two classes. However, there is room for improvement in reducing the misclassification rates, as some predictions are still incorrect.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_0bce7edbb25b4e71a0f87dcd38eab683_proc_3218405/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over 10 epochs. The training loss decreases rapidly in the first few epochs and continues to decline steadily, indicating that the model is learning from the training data. The validation loss starts at a lower level than the training loss and decreases more gradually, suggesting that the model is generalizing reasonably well to the validation data. There is no significant indication of overfitting, as the validation loss does not increase at later epochs.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot displays training and validation accuracy over 10 epochs. Training accuracy improves significantly, reaching over 85%, while validation accuracy also increases but plateaus around 70%. The gap between training and validation accuracy suggests some overfitting, as the model performs better on the training data than on the validation data. However, the validation accuracy appears stable, indicating that the model is not severely overfitting.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_accuracy_curve.png"}, {"analysis": "This plot shows rule fidelity over epochs, which remains constant at 1.0 throughout the training process. This indicates that the model consistently adheres to the learned rules, ensuring that the rule representation is stable and reliable during training.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_rule_fidelity_curve.png"}, {"analysis": "This confusion matrix visualizes the model's performance in terms of true and predicted labels. The darker diagonal cells indicate correct predictions, while the off-diagonal cells represent misclassifications. The model appears to perform well, with a majority of predictions falling along the diagonal, though there is room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c54e6db6c54543bb962a5945f4ba61d3_proc_3218403/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows a steady decrease in both training and validation loss over the epochs. The validation loss is consistently higher than the training loss, indicating some generalization gap. However, the gap appears to remain stable, which suggests that the model is not overfitting significantly. The decline in loss for both sets indicates that the model is learning effectively.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot demonstrates the progression of training and validation accuracy over the epochs. While training accuracy improves steadily, validation accuracy shows fluctuations, particularly around the middle epochs. This could indicate that the model struggles to generalize optimally at certain points, possibly due to the complexity of the task or the model architecture. Nonetheless, validation accuracy trends upward overall, which is a positive sign.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_accuracy_curve.png"}, {"analysis": "This plot indicates that rule fidelity remains consistently high across all epochs, suggesting that the model is effectively adhering to the learned rules. This high fidelity implies that the interpretability aspect of the model is functioning as intended, as the rules are being consistently followed throughout training.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix for the test set reveals the distribution of correct and incorrect predictions for each class. The darker diagonal indicates that the majority of predictions are correct, but there is still some misclassification. The relative intensity of the off-diagonal elements suggests the specific areas where the model struggles, which could point to certain classes being more challenging to distinguish.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_130ce571a04940d0b1b299f74ca8032d_proc_3218404/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily and reaches a very low value, indicating that the model is learning effectively on the training set. However, the validation loss initially decreases but starts to increase after a few epochs, suggesting overfitting. The model performs well on the training data but struggles to generalize to the validation set after a certain point.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_loss_curve.png"}, {"analysis": "The training accuracy improves rapidly and reaches near 100%, showing that the model is highly effective at memorizing the training data. However, the validation accuracy plateaus and slightly fluctuates, indicating that generalization to unseen data is limited. This aligns with the overfitting observed in the loss plot.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_accuracy_curve.png"}, {"analysis": "Rule fidelity starts at a high value but decreases sharply within the first few epochs and stabilizes at a low level. This suggests that as the model optimizes for classification accuracy, it sacrifices its ability to maintain interpretable rule fidelity. This trade-off between interpretability and performance is a key challenge in the model's design.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix shows a strong diagonal dominance, indicating that the model performs well in correctly classifying the majority of instances. However, there are still some misclassifications, as evident from the off-diagonal elements. This highlights areas where the model could improve to achieve better accuracy and robustness.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f752f1a9af1448748bfac6b0aa3c82d1_proc_3218406/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation accuracy over 10 epochs. The training accuracy increases rapidly and plateaus close to 1.0, indicating that the model fits the training data well. However, the validation accuracy plateaus around 0.8, which suggests potential overfitting. The gap between training and validation accuracy indicates that the model may not generalize perfectly to unseen data.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot presents the training and validation loss over 10 epochs. The training loss decreases steadily, showing that the model is optimizing well on the training data. However, validation loss starts increasing after epoch 3, which indicates overfitting. The divergence between training and validation loss further supports this observation.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot tracks rule fidelity over epochs. Rule fidelity starts high but decreases significantly by epoch 2 and continues to decline until it stabilizes around 0.5. This suggests that while the model initially adheres to the learned rules, it increasingly diverges from them, potentially prioritizing accuracy over interpretability.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix indicates the classification performance on the test set. The diagonal elements represent correct predictions, while off-diagonal elements represent misclassifications. The results suggest a moderate level of accuracy, with more correct predictions than misclassifications, but room for improvement remains. The balance between the two classes appears reasonable.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The train accuracy rapidly increases and reaches near-perfect values by epoch 3, indicating that the model is learning the training data effectively. However, the validation accuracy plateaus at around 0.8, suggesting that the model quickly overfits to the training data. The gap between train and validation accuracy highlights the potential overfitting problem, which might require regularization techniques or additional data augmentation.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The training loss decreases steadily, showing that the model is optimizing well on the training set. In contrast, the validation loss increases after epoch 4, which is a clear indication of overfitting. This divergence between training and validation loss reinforces the need for strategies to improve generalization, such as early stopping, dropout, or reducing model complexity.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_loss_curves.png"}, {"analysis": "Rule fidelity starts at 1.0 and sharply decreases to around 0.5 by epoch 3, stabilizing at this low level for the remaining epochs. This suggests that while the model is achieving classification accuracy, it is not preserving the interpretability of the learned rules. The trade-off between performance and interpretability might need to be revisited, possibly by rebalancing the loss function or introducing constraints to enforce rule fidelity.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix shows a clear diagonal dominance, indicating that the model is making correct predictions for most of the test samples. However, there is some degree of misclassification, as evident from the off-diagonal elements. A more detailed error analysis could reveal whether these misclassifications are systematic (e.g., certain rule subsets are harder to classify) or random.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation accuracy over 10 epochs. Training accuracy increases rapidly and reaches near 100% by epoch 3, indicating that the model is fitting the training data well. However, validation accuracy plateaus around 80% after a few epochs and even slightly decreases towards the end. This suggests potential overfitting, where the model performs excellently on training data but struggles to generalize to unseen validation data.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot illustrates the cross-entropy loss for both training and validation datasets. Training loss decreases steadily throughout the epochs, indicating continuous improvement in fitting the training data. However, validation loss starts increasing after epoch 4, which is a strong indicator of overfitting. The divergence between training and validation loss further supports this observation.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_loss_curves.png"}, {"analysis": "The rule fidelity metric decreases significantly after the first epoch and stabilizes around 0.5 after epoch 4. This indicates that the model's ability to adhere to interpretable rules diminishes as training progresses, possibly due to the model prioritizing accuracy over interpretability. This trade-off might need to be addressed if interpretability is a key goal of the research.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix for the test set shows a relatively balanced performance across the two classes. However, the diagonal elements (representing correct classifications) are not maximized, indicating room for improvement in classification accuracy. This aligns with the validation performance plateau observed earlier. Further analysis might be required to understand specific areas of misclassification.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The plots indicate that while the model achieves high training accuracy, it\nsuffers from overfitting, as evidenced by the widening gap between training and\nvalidation accuracy and the increasing validation loss. Rule fidelity diminishes\nover epochs, suggesting a trade-off between interpretability and performance.\nThe confusion matrix shows strong performance but lacks detailed\nmisclassification insights.", "The plots reveal that while the model demonstrates strong learning and\nclassification capabilities on the training data, it struggles with overfitting\nand maintaining rule fidelity. Validation performance plateaus, and rule\nfidelity decreases over time, suggesting a trade-off between interpretability\nand accuracy. The confusion matrix indicates good overall performance but\nhighlights areas for further improvement in generalization and interpretability.", "The plots indicate that the model is overfitting, as evidenced by the divergence\nbetween training and validation loss, as well as the accuracy curves. Rule\nfidelity drops significantly after the first epoch, suggesting challenges in\nmaintaining interpretability. The confusion matrix shows reasonably good\nperformance but highlights areas for improvement in classification accuracy.", "[]", "The plots indicate that the model achieves high training accuracy but exhibits\nsigns of overfitting, as evidenced by the divergence between training and\nvalidation loss after epoch 6. While the confusion matrix shows good\nclassification performance, slight class imbalance or bias may need to be\naddressed for improved generalization.", "The results highlight overfitting in the model as evidenced by the divergence\nbetween training and validation performance. The scalar gate ablation negatively\nimpacts rule fidelity, suggesting its importance in maintaining\ninterpretability. The confusion matrix indicates a fairly balanced performance\nbut leaves room for improvement in classification accuracy.", "The provided plots indicate that the model is learning effectively, with stable\nrule fidelity and reasonable generalization to the validation data. However,\nthere is a noticeable gap between training and validation accuracy, suggesting\nmild overfitting. The confusion matrix highlights generally good performance,\nwith some scope for improving classification accuracy.", "The provided plots indicate that the model is learning effectively, with\ndecreasing loss and increasing accuracy over epochs. Rule fidelity remains high,\nsupporting interpretability. The confusion matrix highlights areas of\nmisclassification, which could guide further optimization efforts.", "The results show that while the model achieves high training accuracy, it\nsuffers from overfitting as evident from the increasing validation loss and\nplateauing validation accuracy. Rule fidelity declines significantly, indicating\na trade-off between interpretability and performance. The confusion matrix\ndemonstrates good classification performance but also reveals areas for\nimprovement.", "The results reveal significant overfitting in the model as indicated by the\ndivergence between training and validation metrics. While the model achieves\nhigh training accuracy, its generalization to validation data is limited. Rule\nfidelity decreases over time, highlighting a trade-off between interpretability\nand performance. The confusion matrix shows moderate test accuracy but suggests\nthere is room for improvement in classification performance.", "The plots indicate that the model performs well on the training data but suffers\nfrom overfitting, as evidenced by the divergence between training and validation\nmetrics. Rule fidelity is not well-maintained, pointing to a trade-off between\ninterpretability and performance. The confusion matrix suggests overall good\nclassification performance but highlights areas for improvement in reducing\nmisclassifications.", "The plots reveal issues of overfitting, as evidenced by the divergence between\ntraining and validation metrics. While the model achieves high training\naccuracy, validation performance plateaus and declines, and rule fidelity\ndecreases significantly over epochs. The confusion matrix indicates balanced but\nsuboptimal classification performance. Addressing the trade-off between\ninterpretability and accuracy is critical for achieving the research goals.", "[]"], "exec_time": [3.7457423210144043, 3.650692939758301, 3.621767044067383, 3.64780330657959, 3.4842166900634766, 4.261453628540039, 3.5929572582244873, 3.5562736988067627, 3.9716527462005615, 4.3295440673828125, 4.198748350143433, 4.272007465362549, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["\"\""], ["[]"], [], ["[\"SPR_BENCH\"]"], ["[]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[]"], ["[]"], ["\"\""], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------ #\n#                       LOAD EXPERIMENT DATA                         #\n# ------------------------------------------------------------------ #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Expect single experiment/dataset; guard in case of more.\nexp_name = next(iter(experiment_data.keys()), None)\nds_name = None\nif exp_name:\n    ds_name = next(iter(experiment_data[exp_name].keys()), None)\n\nif not (exp_name and ds_name):\n    print(\"No experiment data found, nothing to plot.\")\nelse:\n    exp = experiment_data[exp_name][ds_name]\n\n    epochs = np.arange(1, len(exp[\"losses\"][\"train\"]) + 1)\n\n    # --------------------------- LOSS CURVES --------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{ds_name} \u2013 {exp_name}\\nTraining vs Validation Loss\")\n        plt.legend()\n        save_path = os.path.join(working_dir, f\"{ds_name}_{exp_name}_Loss_Curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ------------------------ ACCURACY CURVES -------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, exp[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{ds_name} \u2013 {exp_name}\\nTraining vs Validation Accuracy\")\n        plt.legend()\n        save_path = os.path.join(\n            working_dir, f\"{ds_name}_{exp_name}_Accuracy_Curves.png\"\n        )\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # ----------------------- RULE FIDELITY CURVE ----------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"Rule_Fidelity\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity Score\")\n        plt.title(f\"{ds_name} \u2013 {exp_name}\\nRule Fidelity Across Epochs\")\n        save_path = os.path.join(working_dir, f\"{ds_name}_{exp_name}_RuleFidelity.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot: {e}\")\n        plt.close()\n\n    # --------------------------- CONF MATRIX --------------------------- #\n    try:\n        preds = exp.get(\"predictions\")\n        gts = exp.get(\"ground_truth\")\n        if preds is not None and gts is not None:\n            n_cls = int(max(gts.max(), preds.max()) + 1)\n            conf = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                conf[t, p] += 1\n            plt.figure()\n            im = plt.imshow(conf, interpolation=\"nearest\", cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            plt.title(f\"{ds_name} \u2013 {exp_name}\\nConfusion Matrix (Test Set)\")\n            save_path = os.path.join(\n                working_dir, f\"{ds_name}_{exp_name}_ConfusionMatrix.png\"\n            )\n            plt.savefig(save_path)\n            plt.close()\n        else:\n            print(\"Predictions/ground truth missing; skipping confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ----------------------- PRINT TEST ACCURACY ----------------------- #\n    try:\n        if preds is not None and gts is not None:\n            test_acc = (preds == gts).mean()\n            print(f\"Test Accuracy (recomputed): {test_acc:.3f}\")\n    except Exception as e:\n        print(f\"Error computing test accuracy: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------ #\n#                          LOAD EXPERIMENT                           #\n# ------------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------ #\n#                    EXTRACT SPR_BENCH RESULTS                       #\n# ------------------------------------------------------------------ #\ned = None\ntry:\n    ed = experiment_data[\"NoRuleSparsity\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error extracting dataset: {e}\")\n\nif ed:\n    train_loss = np.asarray(ed[\"losses\"][\"train\"])\n    val_loss = np.asarray(ed[\"losses\"][\"val\"])\n    train_acc = np.asarray(ed[\"metrics\"][\"train_acc\"])\n    val_acc = np.asarray(ed[\"metrics\"][\"val_acc\"])\n    rule_fid = np.asarray(ed[\"metrics\"][\"Rule_Fidelity\"])\n    preds = np.asarray(ed[\"predictions\"])\n    gts = np.asarray(ed[\"ground_truth\"])\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # -------------------- Plot 1: Loss Curves ---------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH \u2013 Loss Curves (NoRuleSparsity)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_Loss_Curves_NoRuleSparsity.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # -------------------- Plot 2: Accuracy Curves ------------------ #\n    try:\n        plt.figure()\n        plt.plot(epochs, train_acc, label=\"Train\")\n        plt.plot(epochs, val_acc, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH \u2013 Accuracy Curves (NoRuleSparsity)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_Accuracy_Curves_NoRuleSparsity.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve: {e}\")\n        plt.close()\n\n    # -------------------- Plot 3: Rule Fidelity -------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, rule_fid, color=\"purple\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity\")\n        plt.title(\"SPR_BENCH \u2013 Rule Fidelity over Epochs (NoRuleSparsity)\")\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_RuleFidelity_NoRuleSparsity.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule-fidelity plot: {e}\")\n        plt.close()\n\n    # -------------------- Plot 4: Confusion Matrix ----------------- #\n    try:\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"SPR_BENCH \u2013 Confusion Matrix (NoRuleSparsity)\")\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_ConfusionMatrix_NoRuleSparsity.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # -------------------- Print Key Metrics ------------------------ #\n    try:\n        test_acc = (preds == gts).mean()\n        print(\n            f\"Final Val Acc: {val_acc[-1]:.3f} | \"\n            f\"Test Acc: {test_acc:.3f} | \"\n            f\"Final Rule Fidelity: {rule_fid[-1]:.3f}\"\n        )\n    except Exception as e:\n        print(f\"Error computing metrics: {e}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to fetch nested dict safely\ndef get(path, default=None):\n    obj = experiment_data\n    for key in path:\n        if key not in obj:\n            return default\n        obj = obj[key]\n    return obj\n\n\n# Extract data for plotting\ntrain_acc = get([\"RuleFreeCNN\", \"SPR_BENCH\", \"metrics\", \"train\"], [])\nval_acc = get([\"RuleFreeCNN\", \"SPR_BENCH\", \"metrics\", \"val\"], [])\ntrain_loss = get([\"RuleFreeCNN\", \"SPR_BENCH\", \"losses\", \"train\"], [])\nval_loss = get([\"RuleFreeCNN\", \"SPR_BENCH\", \"losses\", \"val\"], [])\npreds = get([\"RuleFreeCNN\", \"SPR_BENCH\", \"predictions\"], np.array([]))\ngts = get([\"RuleFreeCNN\", \"SPR_BENCH\", \"ground_truth\"], np.array([]))\n\n# 1) Accuracy curve\ntry:\n    if len(train_acc) and len(val_acc):\n        epochs = np.arange(1, len(train_acc) + 1)\n        plt.figure()\n        plt.plot(epochs, train_acc, label=\"Train\")\n        plt.plot(epochs, val_acc, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"RuleFreeCNN on SPR_BENCH - Accuracy vs Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curve_RuleFreeCNN.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# 2) Loss curve\ntry:\n    if len(train_loss) and len(val_loss):\n        epochs = np.arange(1, len(train_loss) + 1)\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"RuleFreeCNN on SPR_BENCH - Loss vs Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve_RuleFreeCNN.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 3) Confusion matrix\ntry:\n    if preds.size and gts.size:\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\"RuleFreeCNN on SPR_BENCH - Confusion Matrix (Test Set)\")\n        plt.xticks(np.arange(num_classes))\n        plt.yticks(np.arange(num_classes))\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix_RuleFreeCNN.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\n\n        # Print overall accuracy\n        accuracy = np.trace(cm) / cm.sum() if cm.sum() else 0.0\n        print(f\"Test accuracy (recomputed): {accuracy:.3f}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------- load experiment data ------------------------- #\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    exp = experiment_data[\"NoGateConfidence\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None:\n    epochs = np.arange(1, len(exp[\"losses\"][\"train\"]) + 1)\n    # ----------------------------- plot 1 -------------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n    # ----------------------------- plot 2 -------------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"train_acc\"], label=\"Train\")\n        plt.plot(epochs, exp[\"metrics\"][\"val_acc\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n    # ----------------------------- plot 3 -------------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"Rule_Fidelity\"], color=\"purple\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity\")\n        plt.title(\"SPR_BENCH Rule Fidelity Over Epochs\\nScalar Gate Ablation\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_rule_fidelity.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating fidelity plot: {e}\")\n        plt.close()\n    # ----------------------------- plot 4 -------------------------------- #\n    try:\n        preds = np.array(exp[\"predictions\"])\n        gts = np.array(exp[\"ground_truth\"])\n        n_cls = int(max(preds.max(), gts.max())) + 1\n        conf = np.zeros((n_cls, n_cls), dtype=int)\n        for p, t in zip(preds, gts):\n            conf[t, p] += 1\n        plt.figure()\n        plt.imshow(conf, cmap=\"Blues\", interpolation=\"nearest\")\n        plt.colorbar(label=\"Count\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\"SPR_BENCH Confusion Matrix\\nTest Set Predictions\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n    # ------------------------- evaluation metric ------------------------- #\n    try:\n        test_acc = (preds == gts).mean()\n        print(f\"Test Accuracy (recomputed): {test_acc:.4f}\")\n    except Exception as e:\n        print(f\"Error computing test accuracy: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------- load experiment data ----------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed is not None:\n    epochs = range(1, len(ed[\"losses\"][\"train\"]) + 1)\n\n    # ---------------------------- plot 1 ----------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, ed[\"losses\"][\"val\"], label=\"Val Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ---------------------------- plot 2 ----------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.title(\"SPR_BENCH: Training vs Validation Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve: {e}\")\n        plt.close()\n\n    # ---------------------------- plot 3 ----------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"metrics\"][\"Rule_Fidelity\"], color=\"purple\")\n        plt.title(\"SPR_BENCH: Rule Fidelity Over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_rule_fidelity_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity curve: {e}\")\n        plt.close()\n\n    # ---------------------------- plot 4 ----------------------------- #\n    try:\n        preds = ed[\"predictions\"].astype(int)\n        gts = ed[\"ground_truth\"].astype(int)\n        n_cls = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft: Ground Truth (rows)  Right: Predictions (cols)\"\n        )\n        plt.xlabel(\"Predicted label\")\n        plt.ylabel(\"True label\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ---------------------- print evaluation metric ------------------ #\n    test_acc = (ed[\"predictions\"] == ed[\"ground_truth\"]).mean()\n    print(f\"Final Test Accuracy: {test_acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ensure working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"rule_only\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None:\n    epochs = np.arange(1, len(exp[\"losses\"][\"train\"]) + 1)\n\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Val Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2) Accuracy curves\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, exp[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.title(\"SPR_BENCH: Training vs Validation Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # 3) Rule fidelity curve\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"Rule_Fidelity\"], marker=\"o\")\n        plt.title(\"SPR_BENCH: Rule Fidelity Over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity\")\n        plt.ylim(0, 1.05)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_rule_fidelity.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating fidelity plot: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix\n    try:\n        preds = exp[\"predictions\"]\n        gts = exp[\"ground_truth\"]\n        n_cls = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH: Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted Class\")\n        plt.ylabel(\"True Class\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # print final test accuracy\n    try:\n        test_acc = (preds == gts).mean()\n        print(f\"Test Accuracy: {test_acc:.4f}\")\n    except Exception as e:\n        print(f\"Error computing test accuracy: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------ #\n#                      SETUP & LOAD DATA                             #\n# ------------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------ #\n#                        VISUALISATIONS                              #\n# ------------------------------------------------------------------ #\nfor model_name, dsets in experiment_data.items():\n    for dset_name, d in dsets.items():\n        t_loss = d[\"losses\"].get(\"train\", [])\n        v_loss = d[\"losses\"].get(\"val\", [])\n        t_acc = d[\"metrics\"].get(\"train_acc\", [])\n        v_acc = d[\"metrics\"].get(\"val_acc\", [])\n        fidelity = d[\"metrics\"].get(\"Rule_Fidelity\", [])\n        preds = np.asarray(d.get(\"predictions\", []))\n        gts = np.asarray(d.get(\"ground_truth\", []))\n\n        # -------- Loss curves -------- #\n        try:\n            plt.figure()\n            plt.plot(t_loss, label=\"train\")\n            plt.plot(v_loss, label=\"val\")\n            plt.xlabel(\"epoch\")\n            plt.ylabel(\"cross-entropy loss\")\n            plt.title(f\"{dset_name} - Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_loss_curve.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve: {e}\")\n            plt.close()\n\n        # -------- Accuracy curves -------- #\n        try:\n            plt.figure()\n            plt.plot(t_acc, label=\"train\")\n            plt.plot(v_acc, label=\"val\")\n            plt.xlabel(\"epoch\")\n            plt.ylabel(\"accuracy\")\n            plt.title(f\"{dset_name} - Training vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_accuracy_curve.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating accuracy curve: {e}\")\n            plt.close()\n\n        # -------- Rule fidelity -------- #\n        try:\n            plt.figure()\n            plt.plot(fidelity, color=\"purple\")\n            plt.xlabel(\"epoch\")\n            plt.ylabel(\"rule fidelity\")\n            plt.title(f\"{dset_name} - Rule Fidelity Over Epochs\")\n            fname = os.path.join(working_dir, f\"{dset_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating fidelity plot: {e}\")\n            plt.close()\n\n        # -------- Confusion matrix -------- #\n        try:\n            if preds.size and gts.size:\n                n_cls = int(max(preds.max(), gts.max()) + 1)\n                cm = np.zeros((n_cls, n_cls), dtype=int)\n                for p, g in zip(preds, gts):\n                    cm[g, p] += 1\n                plt.figure()\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar()\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.title(f\"{dset_name} - Confusion Matrix\")\n                fname = os.path.join(working_dir, f\"{dset_name}_confusion_matrix.png\")\n                plt.savefig(fname)\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n\n        # ------------------------------------------------------------------ #\n        #                    PRINT EVALUATION METRICS                        #\n        # ------------------------------------------------------------------ #\n        if v_acc:  # last validation accuracy\n            print(f\"{model_name}/{dset_name} - Final Val Acc: {v_acc[-1]:.4f}\")\n        if preds.size and gts.size:\n            test_acc = (preds == gts).mean()\n            print(f\"{model_name}/{dset_name} - Test Acc: {test_acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom collections import defaultdict\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- list of result files (relative to AI_SCIENTIST_ROOT) --------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f15fbc114c7540f7a174e39f5472eec9_proc_3218404/experiment_data.npy\",\n    \"experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d5256260cf344c2f9e837e7a2f913293_proc_3218405/experiment_data.npy\",\n    \"experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_5f3e12ea62624591a4a63d4ba703cf78_proc_3218403/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_p = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        blob = np.load(full_p, allow_pickle=True).item()\n        all_experiment_data.append(blob)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# -------- aggregate runs per-dataset --------\nagg = defaultdict(lambda: defaultdict(list))  # dataset \u2192 metric \u2192 list[runs]\n\nfor run_blob in all_experiment_data:\n    for ds_name, ds_blob in run_blob.items():\n        metrics = ds_blob.get(\"metrics\", {})\n        losses = ds_blob.get(\"losses\", {})\n        # accuracy curves\n        for tag in [\"train_acc\", \"val_acc\"]:\n            arr = np.asarray(metrics.get(tag, []))\n            if arr.size:\n                agg[ds_name][tag].append(arr)\n        # loss curves\n        for tag_raw, tag_store in [(\"train\", \"train_loss\"), (\"val\", \"val_loss\")]:\n            arr = np.asarray(losses.get(tag_raw, []))\n            if arr.size:\n                agg[ds_name][tag_store].append(arr)\n        # rule fidelity\n        rf = np.asarray(metrics.get(\"Rule_Fidelity\", []))\n        if rf.size:\n            agg[ds_name][\"Rule_Fidelity\"].append(rf)\n        # test accuracy (single number)\n        preds = np.asarray(ds_blob.get(\"predictions\", []))\n        gts = np.asarray(ds_blob.get(\"ground_truth\", []))\n        if preds.size and gts.size and preds.shape == gts.shape:\n            agg[ds_name][\"test_acc\"].append(float((preds == gts).mean()))\n\n\ndef plot_mean_sem(ax, series_list, label, color):\n    \"\"\"Plot mean with shaded SEM.\"\"\"\n    if not series_list:\n        return\n    min_len = min(len(s) for s in series_list)\n    data = np.stack([s[:min_len] for s in series_list], axis=0)\n    mean = data.mean(0)\n    sem = (\n        data.std(0, ddof=1) / np.sqrt(data.shape[0])\n        if data.shape[0] > 1\n        else np.zeros_like(mean)\n    )\n    epochs = np.arange(1, min_len + 1)\n    ax.plot(epochs, mean, label=label, color=color)\n    ax.fill_between(epochs, mean - sem, mean + sem, color=color, alpha=0.3)\n\n\n# -------- iterate over aggregated datasets --------\nfor ds_name, metrics in agg.items():\n    # 1) Accuracy curves (aggregate)\n    try:\n        if metrics.get(\"train_acc\") and metrics.get(\"val_acc\"):\n            plt.figure()\n            ax = plt.gca()\n            plot_mean_sem(ax, metrics[\"train_acc\"], \"Train (mean \u00b1 SEM)\", \"tab:blue\")\n            plot_mean_sem(\n                ax, metrics[\"val_acc\"], \"Validation (mean \u00b1 SEM)\", \"tab:orange\"\n            )\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Accuracy\")\n            ax.set_title(f\"{ds_name}: Aggregated Train vs Validation Accuracy\")\n            ax.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) Loss curves (aggregate)\n    try:\n        if metrics.get(\"train_loss\") and metrics.get(\"val_loss\"):\n            plt.figure()\n            ax = plt.gca()\n            plot_mean_sem(ax, metrics[\"train_loss\"], \"Train (mean \u00b1 SEM)\", \"tab:green\")\n            plot_mean_sem(ax, metrics[\"val_loss\"], \"Validation (mean \u00b1 SEM)\", \"tab:red\")\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Cross-Entropy Loss\")\n            ax.set_title(f\"{ds_name}: Aggregated Train vs Validation Loss\")\n            ax.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) Rule Fidelity\n    try:\n        if metrics.get(\"Rule_Fidelity\"):\n            plt.figure()\n            ax = plt.gca()\n            plot_mean_sem(\n                ax, metrics[\"Rule_Fidelity\"], \"Rule Fidelity (mean \u00b1 SEM)\", \"tab:purple\"\n            )\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Rule Fidelity\")\n            ax.set_title(f\"{ds_name}: Aggregated Rule Fidelity over Epochs\")\n            ax.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated rule fidelity plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ---- print aggregated test accuracy ----\n    test_accs = metrics.get(\"test_acc\", [])\n    if test_accs:\n        mean_acc = np.mean(test_accs)\n        std_acc = np.std(test_accs, ddof=1) if len(test_accs) > 1 else 0.0\n        print(\n            f\"{ds_name} Test Accuracy: {mean_acc:.3f} \u00b1 {std_acc:.3f} (n={len(test_accs)})\"\n        )\n"], "plot_plan": ["We will load experiment_data.npy from the working directory, loop over every dataset present, and for each dataset we will: (1) extract metric/loss arrays safely, (2) generate up-to-four plots\u2014accuracy, loss, rule fidelity, and confusion matrix\u2014each wrapped in its own try-except block, (3) save each figure with a descriptive filename containing the dataset name and plot type, and (4) always close figures.  For datasets that have predictions and ground-truth available, we will compute and print the final test accuracy (or other metrics) after plotting.  Helper functions ensure missing keys gracefully return empty arrays so plots are only attempted when data exist; this keeps the script robust across arbitrary experiment dumps.  All plots are created with basic matplotlib, titled clearly, and saved into the pre-made \u201cworking\u201d folder.  The number of generated figures is bounded (max five per dataset).  Finally, the code is concise and self-contained, starting with the required import statements and printing the evaluation metric(s).", "We load experiment_data.npy, pull out the sole experiment (NoGateEqualBlend on SPR_BENCH), and recover per-epoch lists: train/val loss, train/val accuracy and rule-fidelity.  \nFor visual analytics we create four independent figures: (1) loss curves, (2) accuracy curves, (3) rule-fidelity curve, and (4) a confusion-matrix heat-map for the saved test predictions.  \nEach figure is wrapped in its own try-except guard, titled clearly, saved into working_dir with descriptive filenames, and then closed.  \nWe honour the plotting template by starting with the prescribed imports, by never inventing data, and by limiting ourselves to at most four figures.  \nThe code first ensures working_dir exists, then loads the numpy dictionary with allow_pickle=True.  \nAfter plotting, we compute and print the held-out test accuracy from the stored predictions and ground-truth, giving an easy sanity check of model quality.  \nAll logic is kept minimal and self-contained so it can be dropped into any environment that already produced experiment_data.npy.", "We will load the serialized experiment_data, pull out the arrays for losses, accuracies, rule-fidelity, and the test predictions/ground truth.  \nFour separate plots are generated inside individual try-except blocks: (1) training vs. validation loss over epochs, (2) training vs. validation accuracy over epochs, (3) rule fidelity over epochs, and (4) a confusion-matrix heat-map of final test predictions.  \nEach figure is given an informative title containing the dataset and plot type and saved into working_dir, then closed to free memory.  \nWe also compute and print the final validation accuracy, test accuracy, and test rule-fidelity so the script is self-contained.  \nAll plotting strictly uses values already stored in experiment_data.npy; no synthetic data are created.  \nAt most four figures are saved, satisfying the \u22645-figure guideline.  \nThe code is concise, uses only matplotlib/numpy/os, and guards against missing files or other runtime issues with try-except blocks.", null, "We will load experiment_data.npy, extract the stored train/val accuracy and loss arrays, and plot two learning curves (accuracy and loss) over epochs. We will also compute a confusion matrix from the saved test\u2010set predictions and ground truths, visualise it, and print overall test accuracy. Each plot is wrapped in its own try-except to satisfy the robustness requirement, figures are titled clearly, saved with descriptive names into working_dir, and closed afterward. At most three figures are produced, all derived strictly from the contents of experiment_data.npy.", "We first load the stored experiment_data.npy, extract the \u201cNoGateConfidence / SPR_BENCH\u201d dictionary, and recover arrays for train/val loss, train/val accuracy, rule fidelity, predictions, and ground-truth labels.  Using these, we prepare up to four standard plots: (1) train vs validation loss over epochs, (2) train vs validation accuracy over epochs, (3) rule-fidelity over epochs, and (4) a confusion-matrix heat-map of test predictions.  Each figure is wrapped in its own try-except block, saved into working_dir with descriptive names, titled properly, then closed to free memory.  We plot at most one point per epoch, well under the five-figure limit.  After plotting, we compute overall test accuracy from the saved predictions/labels and print it as our evaluation metric.  All plotting relies solely on the values present in experiment_data.npy\u2014no fabricated data.  The code begins with the mandated imports/working_dir definition and follows the given style constraints, ensuring robustness and clarity.  Finally, every figure is closed even when an error occurs, and we print any caught exceptions for easier debugging.", "We will begin by loading the saved experiment_data.npy, extract the \u2018SPR_BENCH\u2019 entry, and infer the number of epochs from the length of the stored metrics. Using this information we will plot, in separate try-except blocks, (1) training vs. validation loss, (2) training vs. validation accuracy, (3) rule-fidelity over epochs, and (4) a confusion matrix of test predictions vs. ground-truth labels. All plots will be saved in the working directory with descriptive names containing the dataset and plot type. After plotting, we will compute and print the final test accuracy obtained from the stored predictions and labels to verify performance. Each figure will be closed after saving to avoid memory leaks, and only data present in experiment_data.npy will be used\u2014no synthetic values. At most four figures are generated, staying within the required limit of five. The code follows the specified import order and directory handling, ensuring robustness through the mandated error handling pattern.", "The script loads the stored experiment dictionary, isolates the \u201crule_only/SPR_BENCH\u201d results, and produces four standard visualizations. First, it plots training vs. validation losses across epochs; next, it shows training vs. validation accuracies; third, it visualizes the (constant-in-this-setup) rule-fidelity curve; finally, it constructs a confusion-matrix heat-map from saved test predictions and labels. Each figure is generated inside its own try-except block, titled clearly, saved into working_dir with descriptive filenames, and then closed. After plotting, the code computes and prints the final test accuracy to give an immediate quantitative summary. Only data already present in experiment_data.npy are used\u2014no synthetic values are introduced. No more than four figures are created, satisfying the maximum-of-five guideline, and only basic matplotlib functionality is employed.", "We will load experiment_data.npy from the working directory and iterate through every stored model/dataset pair.  For each dataset we first extract time-ordered vectors for training/validation loss, training/validation accuracy, and rule fidelity, then create three simple line plots (losses, accuracies, fidelity) and one confusion-matrix heat-map computed from the saved test predictions vs. ground-truth.  Each plot is wrapped in its own try-except block, carries a descriptive title that includes the dataset name, and is saved to working_dir with an informative filename.  To respect the \u201c\u22645 figures\u201d rule we cap ourselves at exactly these four plots.  After plotting, we compute the final validation accuracy (last epoch) and the overall test accuracy from the prediction array, printing them to stdout so the user sees the evaluation metrics immediately.  All figures are explicitly closed after saving to avoid memory leaks, and we never fabricate data\u2014everything comes directly from experiment_data.npy.  Basic matplotlib defaults are used and no extra stylistic packages are imported.  The whole procedure is implemented concisely in the following code.", null, null, null, null], "ablation_name": [null, "No-Gate Ensemble (Equal Blend)", "No Rule Sparsity (L1-\u03bb = 0)", "CNN-Feature Gate (No Bag-Based Gate)", "Rule-Free CNN Baseline (No Rule Head, No Gate)", "No Gate Confidence Regularization (GATE-\u03bb = 0)", null, "Rule-Only Head (Fixed Gate = 1)", "Static\u2010Scalar Gate (Global \u03bb)", null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", "The script will locate the working directory, load the saved NumPy file, and\nwalk through the nested dictionaries (experiment \u2192 dataset \u2192 metrics / losses).\nFor every dataset it prints a header with the dataset\u2019s name, then prints one\nline per metric or loss.   \u201cBest\u201d values are selected: minima for losses (since\nlower is better) and maxima for accuracies or any metric whose name contains\n\u201cacc\u201d or \u201cfidelity\u201d.   If a list is empty, the script skips that metric\ngracefully.", "The script will locate the saved experiment_data.npy inside the \u201cworking\u201d\ndirectory, load it into a Python dictionary, and iterate through each experiment\nand dataset entry. For every dataset (e.g., \u201cSPR_BENCH\u201d) it will print the\ndataset name first, then output the final recorded values of explicit\nmetrics\u2014training accuracy, validation accuracy, rule fidelity, training loss,\nand validation loss\u2014each preceded by a clear, descriptive label. The code runs\nimmediately on execution with no need for a special entry point.", "The script loads the saved NumPy dictionary, traverses its nested structure to\naccess metrics for each dataset, determines the \u201cbest\u201d value for every metric\n(maximum for accuracies / fidelities, minimum for losses), calculates test\naccuracy from the stored predictions, and prints everything in a clear, labelled\nformat. All code runs immediately at import time and follows the directory and\nprinting rules stated in the instructions.", "The script will locate the working directory, load the serialized\nexperiment_data dictionary, and iterate through every stored (model, dataset)\npair. For each dataset it will pull the last recorded value in the\ntraining/validation accuracy and loss lists\u2014interpreted as the \u201cfinal\u201d\nmetrics\u2014and compute the test accuracy directly from the saved predictions and\nground-truth labels. All results will be printed with explicit metric names\nexactly once per dataset, with no plotting or entry-point guard.", "The script will load the saved NumPy dictionary from the working directory,\niterate through every experiment and dataset it contains, and pull out the\nstored lists for accuracy, loss, and rule-fidelity. For each metric it prints\neither the \u201cbest\u201d value (maximum accuracy, minimum loss) or the final epoch\nvalue, using explicit labels such as \u201ctrain accuracy\u201d and \u201cvalidation loss.\u201d It\nrespects the required folder path, runs immediately at import time, and produces\nplain-text output without any plots.", "The script will load the stored NumPy dictionary from the \u201cworking\u201d directory,\npull out the single dataset (\u201cSPR_BENCH\u201d), and then inspect every metric list it\ncontains.  For accuracy-style metrics (larger is better) it selects the maximum\nvalue, while for loss values (smaller is better) it picks the minimum.  It also\nrecomputes the final test accuracy directly from the saved prediction and\nground-truth arrays.  After determining these \u201cbest\u201d figures, the script prints\nthem with explicit, descriptive names so there is no ambiguity about what each\nnumber represents.", "The script will locate the working directory, load experiment_data.npy, and\niterate through every model and dataset stored inside. For each dataset it will\ncompute the best (maximum) train/validation accuracy, the best (minimum)\ntrain/validation loss, and report the final recorded rule fidelity. Each dataset\nname is printed first, followed by clearly labelled metric/value pairs. All code\nis kept at the global scope so that it runs immediately when the file is\nexecuted.", "The script will load the experiment file from the \u201cworking\u201d directory, walk\nthrough every model and dataset contained inside, and then compute the required\nsummary statistics.   For each dataset it prints: (1) the final training\naccuracy, (2) the best validation accuracy, (3) the final training loss, (4) the\nbest (lowest) validation loss, (5) the final rule-fidelity score, and (6) the\ntest accuracy computed from the stored predictions and ground-truth labels.\nMetric names are written out explicitly to satisfy the formatting requirements,\nand the code is kept at the top level so it runs immediately when executed.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                             #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------------------------- #\n#                         HELPER: PICK BEST VALUE                            #\n# -------------------------------------------------------------------------- #\ndef best_value(name, values):\n    \"\"\"\n    Select the best (min or max) value depending on the metric type.\n    Returns None if the list is empty.\n    \"\"\"\n    if not values:\n        return None\n    lower_is_better = any(k in name.lower() for k in (\"loss\",))\n    return min(values) if lower_is_better else max(values)\n\n\n# -------------------------------------------------------------------------- #\n#                        PRINT METRICS FOR EACH DATASET                      #\n# -------------------------------------------------------------------------- #\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(dataset_name)  # Dataset header\n\n        # Handle metrics such as accuracies and fidelity scores\n        for metric_name, metric_values in data.get(\"metrics\", {}).items():\n            value = best_value(metric_name, metric_values)\n            if value is not None:\n                # Convert readable names\n                pretty_name = (\n                    metric_name.replace(\"train_\", \"train \")\n                    .replace(\"val_\", \"validation \")\n                    .replace(\"_\", \" \")\n                )\n                print(f\"{pretty_name}: {value:.4f}\")\n\n        # Handle losses separately\n        for loss_split, loss_values in data.get(\"losses\", {}).items():\n            metric_name = f\"{loss_split} loss\"\n            value = best_value(metric_name, loss_values)\n            if value is not None:\n                print(f\"{metric_name}: {value:.4f}\")\n\n        # Blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                             #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -------------------------------------------------------------------------- #\n#                         EXTRACT & PRINT METRICS                            #\n# -------------------------------------------------------------------------- #\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, contents in datasets.items():\n        print(f\"{dataset_name}\")  # Dataset header\n\n        metrics = contents.get(\"metrics\", {})\n        losses = contents.get(\"losses\", {})\n\n        # Fetch the last (final) recorded value for each metric/loss\n        if metrics.get(\"train_acc\"):\n            print(\"train accuracy:\", metrics[\"train_acc\"][-1])\n        if metrics.get(\"val_acc\"):\n            print(\"validation accuracy:\", metrics[\"val_acc\"][-1])\n        if metrics.get(\"Rule_Fidelity\"):\n            print(\"rule fidelity:\", metrics[\"Rule_Fidelity\"][-1])\n\n        if losses.get(\"train\"):\n            print(\"training loss:\", losses[\"train\"][-1])\n        if losses.get(\"val\"):\n            print(\"validation loss:\", losses[\"val\"][-1])\n", "import os\nimport numpy as np\n\n# --------------------------- LOAD EXPERIMENT DATA --------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# --------------------------- HELPER: PRETTY NAMES --------------------------- #\npretty = {\n    \"train_acc\": \"train accuracy\",\n    \"val_acc\": \"validation accuracy\",\n    \"Rule_Fidelity\": \"rule fidelity\",\n    \"train_loss\": \"train loss\",\n    \"val_loss\": \"validation loss\",\n    \"test_acc\": \"test accuracy\",\n}\n\n# --------------------------- EXTRACT & PRINT METRICS ------------------------ #\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # Collect best (or final) metric values\n        results = {}\n\n        # 1. Accuracy / fidelity style metrics\n        for key, series in data.get(\"metrics\", {}).items():\n            if not series:\n                continue\n            best_val = (\n                max(series)\n                if (\"acc\" in key.lower() or \"fidelity\" in key.lower())\n                else series[-1]\n            )\n            results[key] = best_val\n\n        # 2. Loss metrics (take minimum)\n        for key, series in data.get(\"losses\", {}).items():\n            if not series:\n                continue\n            best_val = min(series)\n            results[f\"{key}_loss\"] = best_val\n\n        # 3. Test accuracy from stored predictions / ground truth\n        preds = data.get(\"predictions\")\n        gts = data.get(\"ground_truth\")\n        if preds is not None and gts is not None and len(gts):\n            test_acc = (preds == gts).mean()\n            results[\"test_acc\"] = float(test_acc)\n\n        # 4. Print in a readable, labelled form\n        for key, value in results.items():\n            label = pretty.get(key, key.replace(\"_\", \" \"))\n            print(f\"{label}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------- Locate and load data -------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# -------------------- Helper for safe extraction -------------------- #\ndef last_or_none(lst):\n    return lst[-1] if isinstance(lst, (list, tuple)) and len(lst) else None\n\n\n# -------------------- Metric extraction & printing -------------------- #\nfor model_name, model_block in experiment_data.items():\n    for dataset_name, ds in model_block.items():\n        print(dataset_name)  # dataset header\n\n        # Final / best stored values\n        final_train_acc = last_or_none(ds[\"metrics\"][\"train\"])\n        final_val_acc = last_or_none(ds[\"metrics\"][\"val\"])\n        final_train_loss = last_or_none(ds[\"losses\"][\"train\"])\n        final_val_loss = last_or_none(ds[\"losses\"][\"val\"])\n\n        if final_train_acc is not None:\n            print(f\"final train accuracy: {final_train_acc:.4f}\")\n        if final_val_acc is not None:\n            print(f\"final validation accuracy: {final_val_acc:.4f}\")\n        if final_train_loss is not None:\n            print(f\"final train loss: {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"final validation loss: {final_val_loss:.4f}\")\n\n        # Compute test accuracy from saved predictions/labels if available\n        preds = ds.get(\"predictions\")\n        gts = ds.get(\"ground_truth\")\n        if isinstance(preds, np.ndarray) and isinstance(gts, np.ndarray) and preds.size:\n            test_accuracy = (preds == gts).mean()\n            print(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n#                       LOAD SAVED EXPERIMENT DICTIONARY                       #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n#                          PRINT BEST / FINAL METRICS                          #\n# -----------------------------------------------------------------------------#\nfor exp_name, datasets in experiment_data.items():  # e.g. \"NoGateConfidence\"\n    for dataset_name, data_dict in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(dataset_name)\n\n        metrics = data_dict.get(\"metrics\", {})\n        losses = data_dict.get(\"losses\", {})\n\n        # Accuracy metrics\n        train_acc_history = metrics.get(\"train_acc\", [])\n        val_acc_history = metrics.get(\"val_acc\", [])\n        if train_acc_history:\n            print(f\"train accuracy: {train_acc_history[-1]:.4f}\")  # final epoch\n        if val_acc_history:\n            print(f\"validation accuracy: {max(val_acc_history):.4f}\")  # best epoch\n\n        # Loss metrics\n        train_loss_history = losses.get(\"train\", [])\n        val_loss_history = losses.get(\"val\", [])\n        if train_loss_history:\n            print(f\"train loss: {train_loss_history[-1]:.4f}\")  # final epoch\n        if val_loss_history:\n            print(f\"validation loss: {min(val_loss_history):.4f}\")  # best epoch\n\n        # Rule-fidelity metric\n        rule_fid_history = metrics.get(\"Rule_Fidelity\", [])\n        if rule_fid_history:\n            print(f\"rule fidelity: {rule_fid_history[-1]:.4f}\")  # final epoch\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------- LOAD DATA ------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------- HELPER FUNCS ---------------------------------------------- #\ndef best_value(values, bigger_is_better=True):\n    \"\"\"Return best value according to direction.\"\"\"\n    return max(values) if bigger_is_better else min(values)\n\n\ndef print_metric(name, value):\n    \"\"\"Uniform printing function.\"\"\"\n    print(f\"  {name}: {value:.4f}\")\n\n\n# ------------------------------------------- EXTRACT & PRINT METRICS --------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # Requirement 3\n\n    # Accuracy-like metrics\n    train_acc_best = best_value(data[\"metrics\"][\"train_acc\"], True)\n    val_acc_best = best_value(data[\"metrics\"][\"val_acc\"], True)\n    rule_fid_best = best_value(data[\"metrics\"][\"Rule_Fidelity\"], True)\n\n    # Loss metrics\n    train_loss_best = best_value(data[\"losses\"][\"train\"], False)\n    val_loss_best = best_value(data[\"losses\"][\"val\"], False)\n\n    # Test accuracy from saved predictions\n    if len(data.get(\"predictions\", [])) and len(data.get(\"ground_truth\", [])):\n        test_accuracy = (data[\"predictions\"] == data[\"ground_truth\"]).mean()\n    else:\n        test_accuracy = float(\"nan\")\n\n    # Printing (Requirement 4 & 5)\n    print_metric(\"best train accuracy\", train_acc_best)\n    print_metric(\"best validation accuracy\", val_acc_best)\n    print_metric(\"best rule fidelity\", rule_fid_best)\n    print_metric(\"minimum training loss\", train_loss_best)\n    print_metric(\"minimum validation loss\", val_loss_best)\n    print_metric(\"test accuracy\", test_accuracy)\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATA                                      #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Cannot find experiment data at {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# -------------------------------------------------------------------------- #\n#                         METRIC EXTRACTION & PRINT                          #\n# -------------------------------------------------------------------------- #\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(dataset_name)  # Dataset headline\n\n        # Retrieve metric series\n        metrics = content.get(\"metrics\", {})\n        losses = content.get(\"losses\", {})\n        train_acc = metrics.get(\"train_acc\", [])\n        val_acc = metrics.get(\"val_acc\", [])\n        fidelity = metrics.get(\"Rule_Fidelity\", [])\n        train_loss = losses.get(\"train\", [])\n        val_loss = losses.get(\"val\", [])\n\n        # Best / final values\n        if train_acc:\n            print(f\"Best train accuracy: {max(train_acc):.4f}\")\n        if val_acc:\n            print(f\"Best validation accuracy: {max(val_acc):.4f}\")\n        if train_loss:\n            print(f\"Best train loss: {min(train_loss):.4f}\")\n        if val_loss:\n            print(f\"Best validation loss: {min(val_loss):.4f}\")\n        if fidelity:\n            print(f\"Final rule fidelity: {fidelity[-1]:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ #\n#                    LOAD STORED EXPERIMENT DATA                     #\n# ------------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------ #\n#               EXTRACT AND PRINT SUMMARY STATISTICS                 #\n# ------------------------------------------------------------------ #\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, result_dict in datasets.items():\n        metrics = result_dict.get(\"metrics\", {})\n        losses = result_dict.get(\"losses\", {})\n        preds = result_dict.get(\"predictions\", None)\n        gts = result_dict.get(\"ground_truth\", None)\n\n        # Derive statistics --------------------------------------------------\n        training_accuracy_final = metrics.get(\"train_acc\", [np.nan])[-1]\n        validation_accuracy_best = (\n            max(metrics.get(\"val_acc\", [np.nan])) if metrics.get(\"val_acc\") else np.nan\n        )\n        training_loss_final = losses.get(\"train\", [np.nan])[-1]\n        validation_loss_best = (\n            min(losses.get(\"val\", [np.nan])) if losses.get(\"val\") else np.nan\n        )\n        rule_fidelity_final = metrics.get(\"Rule_Fidelity\", [np.nan])[-1]\n\n        if preds is not None and gts is not None and len(preds) == len(gts):\n            test_accuracy = (preds == gts).mean()\n        else:\n            test_accuracy = np.nan\n\n        # Print results -------------------------------------------------------\n        print(f\"\\nDataset: {dataset_name}\")\n        print(f\"training accuracy: {training_accuracy_final:.4f}\")\n        print(f\"best validation accuracy: {validation_accuracy_best:.4f}\")\n        print(f\"training loss: {training_loss_final:.4f}\")\n        print(f\"best validation loss: {validation_loss_best:.4f}\")\n        print(f\"rule fidelity: {rule_fidelity_final:.4f}\")\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9765', '\\n', '  best\nvalidation accuracy accuracy: 0.7920', '\\n', '  best rule fidelity: 1.0000',\n'\\n', '  best train loss: 0.0800', '\\n', '  best validation loss: 0.5722', '\\n',\n'  test accuracy: 0.7840', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', 'train acc: 0.9770', '\\n', 'validation acc: 0.7820', '\\n',\n'Rule Fidelity: 1.0000', '\\n', 'train loss: 0.0810', '\\n', 'val loss: 0.5501',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy:', ' ', '0.9765', '\\n', 'validation\naccuracy:', ' ', '0.784', '\\n', 'rule fidelity:', ' ', '0.488', '\\n', 'training\nloss:', ' ', '0.07962108564376831', '\\n', 'validation loss:', ' ',\n'0.9324386119842529', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'train accuracy: 0.8875', '\\n', 'validation\naccuracy: 0.7340', '\\n', 'rule fidelity: 1.0000', '\\n', 'train loss: 0.6770',\n'\\n', 'validation loss: 0.6839', '\\n', 'test accuracy: 0.7170', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final train accuracy: 0.9760', '\\n', 'final validation\naccuracy: 0.7840', '\\n', 'final train loss: 0.0799', '\\n', 'final validation\nloss: 0.9449', '\\n', 'test accuracy: 0.7840', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.9765', '\\n', 'validation accuracy:\n0.7920', '\\n', 'train loss: 0.0797', '\\n', 'validation loss: 0.5722', '\\n',\n'rule fidelity: 0.4880', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy: 0.8735', '\\n', '  best validation\naccuracy: 0.7280', '\\n', '  best rule fidelity: 1.0000', '\\n', '  minimum\ntraining loss: 0.6771', '\\n', '  minimum validation loss: 0.6838', '\\n', '  test\naccuracy: 0.7320', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'Best train accuracy: 0.8995', '\\n', 'Best validation\naccuracy: 0.7520', '\\n', 'Best train loss: 0.6767', '\\n', 'Best validation loss:\n0.6834', '\\n', 'Final rule fidelity: 1.0000', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'training accuracy: 0.9780', '\\n', 'best\nvalidation accuracy: 0.7780', '\\n', 'training loss: 0.0799', '\\n', 'best\nvalidation loss: 0.5492', '\\n', 'rule fidelity: 0.4900', '\\n', 'test accuracy:\n0.7860', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9775', '\\n', '  best\nvalidation accuracy accuracy: 0.7800', '\\n', '  best rule fidelity: 0.9640',\n'\\n', '  best train loss: 0.0734', '\\n', '  best validation loss: 0.5666', '\\n',\n'  test accuracy: 0.7790', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9825', '\\n', '  best\nvalidation accuracy accuracy: 0.7860', '\\n', '  best rule fidelity: 1.0000',\n'\\n', '  best train loss: 0.0654', '\\n', '  best validation loss: 0.5496', '\\n',\n'  test accuracy: 0.7880', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9830', '\\n', '  best\nvalidation accuracy accuracy: 0.7880', '\\n', '  best rule fidelity: 1.0000',\n'\\n', '  best train loss: 0.0604', '\\n', '  best validation loss: 0.5441', '\\n',\n'  test accuracy: 0.7880', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"], "current_stage": "Stage_4"};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
