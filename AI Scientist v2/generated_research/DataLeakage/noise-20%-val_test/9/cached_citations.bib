
% The paper 'Neural Logic Machines' (2019) is a foundational work in neural-symbolic reasoning, introducing a framework that combines neural networks and logic programming for inductive learning and reasoning tasks. This paper is relevant to the discussion of related work on neural rule learning, providing a basis for comparing the interpretability and performance of the proposed model. It will be cited in the 'Related Work' section to summarize existing literature in neural rule learning.
@article{dong2019neurallm,
 author = {Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Neural Logic Machines},
 volume = {abs/1904.11694},
 year = {2019}
}

% The paper 'Interpretable Neural-Symbolic Concept Reasoning' introduces the Deep Concept Reasoner (DCR), which builds syntactic rule structures using concept embeddings to enhance interpretability in concept-based reasoning. This work is relevant for discussing symbolic reasoning models in the 'Related Work' section, highlighting the limitations of current approaches reliant on high-dimensional embeddings and their interpretability challenges.
@article{barbiero2023interpretablenc,
 author = {Pietro Barbiero and Gabriele Ciravegna and Francesco Giannini and Mateo Espinosa Zarlenga and Lucie Charlotte Magister and A. Tonda and Pietro Lio' and F. Precioso and M. Jamnik and G. Marra},
 booktitle = {International Workshop on Neural-Symbolic Learning and Reasoning},
 journal = {ArXiv},
 title = {Interpretable Neural-Symbolic Concept Reasoning},
 volume = {abs/2304.14068},
 year = {2023}
}

% The foundational work on Local Interpretable Model-agnostic Explanations (LIME) by P. Biecek and T. Burzykowski is highly relevant for discussing post-hoc interpretability methods in the 'Related Work' section. This citation will provide context on existing methods that offer explanations for model decisions, serving as a basis to contrast with the proposed approach that inherently learns interpretable rules.
@article{biecek2021localim,
 author = {P. Biecek and T. Burzykowski},
 booktitle = {Explanatory Model Analysis},
 journal = {Explanatory Model Analysis},
 title = {Local Interpretable Model-agnostic Explanations (LIME)},
 year = {2021}
}

% The paper 'A Novel Approach for Balancing Accuracy and Interpretability in Neural Networks' presents the GNAM framework to balance performance and interpretability using feature clustering in neural networks. This work is relevant for discussing existing methods addressing the accuracy-interpretability trade-off in neural network design and will be cited in the 'Related Work' section to provide context and contrast with the proposed approach that inherently learns interpretable poly-factor rules.
@conference{li2023ana,
 author = {Qianchu Li},
 booktitle = {2023 3rd International Conference on Electronic Information Engineering and Computer (EIECT)},
 journal = {2023 3rd International Conference on Electronic Information Engineering and Computer (EIECT)},
 pages = {464-470},
 title = {A Novel Approach for Balancing Accuracy and Interpretability in Neural Networks},
 year = {2023}
}

% The paper 'Explainable Deep RDFS Reasoner' explores explainable reasoning models using synthetic datasets such as the LUBM benchmark, which is relevant to the Synthetic PolyRule Reasoning (SPR) task. It provides context for discussing symbolic datasets and reasoning tasks, making it suitable for the 'Related Work' section to highlight prior work on reasoning benchmarks and explainability in AI.
@article{makni2020explainabledr,
 author = {B. Makni and I. Abdelaziz and J. Hendler},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Explainable Deep RDFS Reasoner},
 volume = {abs/2002.03514},
 year = {2020}
}
