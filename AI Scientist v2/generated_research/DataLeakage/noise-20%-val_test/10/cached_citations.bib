
% The paper 'Neural Logic Machines' by Dong et al., 2019, introduces a neural-symbolic architecture that combines the strengths of neural networks and logic programming for inductive learning and logic reasoning. This work is foundational to neural rule learning and serves as a key reference for the development of interpretable neural rule-based systems. It will be cited in the related work section to provide context and highlight advancements in neural-symbolic reasoning approaches.
@article{dong2019neurallm,
 author = {Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Neural Logic Machines},
 volume = {abs/1904.11694},
 year = {2019}
}

% The paper 'Interpretable Neural-Symbolic Concept Reasoning' introduces the Deep Concept Reasoner (DCR), an interpretable concept-based model that utilizes syntactic rule structures and concept embeddings for semantically consistent predictions. It is relevant for discussing symbolic reasoning models in the related work section, as it addresses the interpretability challenges of state-of-the-art concept-based models by proposing a model that builds meaningful logic rules. This work helps contextualize the gaps in interpretability addressed by the proposed approach.
@article{barbiero2023interpretablenc,
 author = {Pietro Barbiero and Gabriele Ciravegna and Francesco Giannini and Mateo Espinosa Zarlenga and Lucie Charlotte Magister and A. Tonda and Pietro Lio' and F. Precioso and M. Jamnik and G. Marra},
 booktitle = {International Workshop on Neural-Symbolic Learning and Reasoning},
 journal = {ArXiv},
 title = {Interpretable Neural-Symbolic Concept Reasoning},
 volume = {abs/2304.14068},
 year = {2023}
}

% The paper 'Problems With SHAP and LIME in Interpretable AI for Education' provides a critical analysis of the limitations of post-hoc explanation methods like SHAP and LIME, particularly their variability and misalignment with structured knowledge. It contrasts these methods with neural-symbolic rule extraction approaches, demonstrating the latter's advantages in interpretability and computational efficiency. This citation is relevant for the related work section to highlight the challenges of post-hoc interpretability methods and support the motivation for developing interpretable models that inherently learn rules.
@article{hooshyar2024problemsws,
 author = {Danial Hooshyar and Yeongwook Yang},
 booktitle = {IEEE Access},
 journal = {IEEE Access},
 pages = {137472-137490},
 title = {Problems With SHAP and LIME in Interpretable AI for Education: A Comparative Study of Post-Hoc Explanations and Neural-Symbolic Rule Extraction},
 volume = {12},
 year = {2024}
}

% The paper 'KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks' introduces a benchmark designed to evaluate reasoning abilities in rule-driven and out-of-distribution settings. It is relevant for discussing the importance of benchmarking in rule-based reasoning tasks and can be cited in the related work section to contextualize the significance of SPR_BENCH as a dataset for testing models on reasoning with explicit rules.
@article{ma2024korbenchbl,
 author = {Kaijing Ma and Xinrun Du and Yunran Wang and Haoran Zhang and Zhoufutu Wen and Xingwei Qu and Jian Yang and Jiaheng Liu and Minghao Liu and Xiang Yue and Wenhao Huang and Ge Zhang},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks},
 volume = {abs/2410.06526},
 year = {2024}
}

% The paper 'Adam: A Method for Stochastic Optimization' by Kingma and Ba (2014) introduces the Adam optimizer, a widely used algorithm for gradient-based optimization in deep learning. This work is foundational and will be cited in the experiments section to acknowledge the optimizer used in training the baseline model.
@article{kingma2014adamam,
 author = {Diederik P. Kingma and Jimmy Ba},
 booktitle = {International Conference on Learning Representations},
 journal = {CoRR},
 title = {Adam: A Method for Stochastic Optimization},
 volume = {abs/1412.6980},
 year = {2014}
}

% The paper 'Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective' provides a thorough review of neural-symbolic reasoning systems, particularly focusing on challenges like interpretability and integrating symbolic and neural methodologies. This work is relevant for highlighting research gaps in neural-symbolic reasoning and will be cited in the related work section to contextualize the motivation for developing interpretable models that explicitly learn rules.
@article{liu2024neuralsymbolicro,
 author = {Lihui Liu and Zihao Wang and Hanghang Tong},
 booktitle = {SIGKDD Explorations},
 journal = {ArXiv},
 title = {Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective},
 volume = {abs/2412.10390},
 year = {2024}
}

% The paper 'SupRB: A Supervised Rule-based Learning System for Continuous Problems' introduces a Pittsburgh-style learning classifier system (LCS) that generates human-readable rules for decision-making in continuous decision problems. It emphasizes interpretability and trust by providing clear explanations behind its recommendations. This work is relevant for discussing methodologies that combine rule-based learning with interpretability and can be cited in the related work section to contextualize efforts in interpretable rule-learning systems.
@article{heider2020suprbas,
 author = {Michael Heider and David Pätzel and J. Hähner},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {SupRB: A Supervised Rule-based Learning System for Continuous Problems},
 volume = {abs/2002.10295},
 year = {2020}
}

% The paper 'Ablation Studies in Artificial Neural Networks' by Meyes et al., 2019, discusses the use of ablation studies to investigate knowledge representations in artificial neural networks (ANNs) and their robustness to structural changes. This work is relevant for the experiments section to contextualize the methodology used for analyzing the impact of the rule-based layer and rule complexity on the model's performance. It supports the exploration of ablation studies and their role in understanding neural network behaviors.
@article{meyes2019ablationsi,
 author = {R. Meyes and Melanie Lu and C. W. D. Puiseau and Tobias Meisen},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Ablation Studies in Artificial Neural Networks},
 volume = {abs/1901.08644},
 year = {2019}
}
