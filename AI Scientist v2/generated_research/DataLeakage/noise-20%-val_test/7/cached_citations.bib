
% This paper provides a comparative analysis of LIME and SHAP, two widely used model-agnostic interpretability techniques. It discusses their frameworks, behavior, and effectiveness in explaining machine learning models, making it highly relevant for the related work section to compare these methods with the proposed interpretable neural rule-learning model.
@article{korade2024unlockingml,
 author = {Deepak Mane Anand Magar Om Khode Sarvesh Koli Komal Bhat Prajwal Korade},
 booktitle = {Journal of Electrical Systems},
 journal = {Journal of Electrical Systems},
 title = {Unlocking Machine Learning Model Decisions: A Comparative Analysis of LIME and SHAP for Enhanced Interpretability},
 year = {2024}
}

% The Neural Logic Machines paper introduces a neural-symbolic architecture that combines inductive learning and logic reasoning, making it a foundational work in the domain of neural rule learning. This citation is relevant for summarizing existing research and providing context for the proposed interpretable neural rule-learning model in the related work section.
@article{dong2019neurallm,
 author = {Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Neural Logic Machines},
 volume = {abs/1904.11694},
 year = {2019}
}
