<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 4], [0, 3], [0, 2], [0, 7], [0, 5], [0, 6], [1, 8], [8, 9], [8, 10], [8, 11], [8, 12]], "layout": [[0.6, 0.0], [0.2, 0.33333333333333337], [0.3333333333333333, 0.33333333333333337], [0.4666666666666667, 0.33333333333333337], [0.6, 0.33333333333333337], [0.7333333333333333, 0.33333333333333337], [0.8666666666666667, 0.33333333333333337], [1.0, 0.33333333333333337], [0.2, 0.6666666666666667], [0.0, 1.0], [0.13333333333333333, 1.0], [0.26666666666666666, 1.0], [0.4, 1.0]], "plan": ["We first pre-train a causal transformer to predict the next symbol in each SPR\nsequence, encouraging the encoder to learn contextual embeddings that capture\nlocal dependencies (e.g., parity or count patterns) before supervised training\nstarts.  The encoder weights are then transferred to a classification head and\nfine-tuned with early stopping on the dev split.  Compared with direct\nsupervised training, this two-stage procedure should yield richer\nrepresentations and higher macro-F1.  We keep the model compact (d_model=128, 3\nlayers) so that a few epochs of language-model pre-training (epoch=3) plus\nclassification fine-tuning (patience-based, \u226415 epochs) finishes well within 30\nmin.  All metrics (losses, macro-F1) for both stages are logged in\n`experiment_data` and saved to `./working`.  Early stopping on dev macro-F1\ndecides the final model that is evaluated on the held-out test split.  This\nfirst attempt establishes a contextual-embedding baseline against the 80 % SOTA\nand provides infrastructure for future ablations (e.g., removing pre-training or\nvarying embedding strategies).", "Ablation name: No-Pretrain Scratch Classifier. The solution runs two\nexperiments: (1) the original two-stage \u201cpretrain + classifier\u201d pipeline and (2)\nthe ablation where the classifier is trained from scratch without any pre-\ntraining. Both follow identical hyper-parameters, loaders, and early-stopping so\nthat any performance difference isolates the effect of pre-training. Results\n(losses, macro-F1, predictions, etc.) for both settings are collected in the\nshared experiment_data dict and saved as experiment_data.npy.", "Ablation name: Remove/Replace Positional Embeddings. The solution adds a\npositional-encoding switch (learned / none / sinusoidal) to `Encoder`, then\ntrains and evaluates two variants (\u201clearned\u201d = baseline, \u201cnone\u201d = ablation) on\nthe SPR-Bench pipeline. Each variant\u2019s losses, metrics and predictions are\nstored under its own ablation key inside the mandatory `experiment_data`\nstructure, and everything is saved to `experiment_data.npy`.", "Ablation name: Freeze-Encoder Fine-tuning. We reuse the original two-stage\npipeline: (1) causal-language-model pre-training to obtain an encoder and (2)\ndownstream sequence-classification.   For the ablation, after transferring the\npre-trained encoder into the classifier we freeze all encoder parameters\n(`requires_grad=False`) so that only the new classification head is updated.\nThe optimizer therefore sees just the head\u2019s weights, isolating the contribution\nof the frozen representations.   Training/validation losses, macro-F1 scores,\npredictions and ground-truth labels are logged in `experiment_data` under the\nkey `\"freeze_encoder\"`, then saved as `experiment_data.npy`.", "Ablation name: CLS Token Pooling vs Mean Pooling. The solution adds a dedicated,\nlearnable <cls> token to the vocabulary, prepends it to every sequence in both\nthe language-model pre-training and classification stages, and changes the\nclassifier to use the final hidden state of this token instead of mean pooling.\nAll bookkeeping (losses, metrics, predictions, etc.) is stored in\nexperiment_data and saved as experiment_data.npy.", "Ablation name: Masked LM Pre-training Objective (MLM instead of Causal LM). We\nswap the causal next-token objective with a bidirectional masked-language-model\n(MLM) loss.  A special <mask> token is added to the vocabulary and, for every\nsequence in a pre-training batch, 15 % of non-padding characters are replaced by\nthis token; the model is then trained to predict only these masked positions\n(labels elsewhere are \u2011100 so they are ignored by the loss).  The encoder,\nclassification fine-tuning and evaluation pipelines remain unchanged.  All\nresults are stored in the same NumPy container so they can be compared with the\noriginal causal-LM run.", "Ablation name: Embedding\u2013LM-Head Weight Tying. The solution trains two variants\nof the pipeline: (1) the original \u201cuntied-weights\u201d language-model and (2) the\n\u201ctied-weights\u201d ablation where the lm-head shares its weight matrix with the\ninput embeddings.  A flag `weight_tie` toggles this behaviour inside `CausalLM`.\nBoth variants are pre-trained on next-token prediction, fine-tuned on SPR\nclassification, evaluated, and their metrics/losses stored in `experiment_data`,\nwhich is finally written to `experiment_data.npy`.", "Ablation name: ReLU-vs-GELU Activation Ablation. The solution duplicates the\nbaseline pipeline but swaps every `TransformerEncoderLayer` activation from\n`\"gelu\"` to `\"relu\"`, keeping all other hyper-parameters unchanged.   All\ntraining, evaluation and logging logic remain identical; the only architectural\nchange isolates the impact of the activation function.   Results (loss curves,\nF1 scores, predictions, etc.) are stored in an `experiment_data` dict under the\nablation key `relu_activation` and saved to `experiment_data.npy`.", "The previous implementation let the transformer attend to `<pad>` tokens,\ncontaminating contextual representations and hurting both pre-training and\nclassification.   The fix introduces an explicit `src_key_padding_mask` (and\ncombines it with the causal attention mask when needed) so that padded positions\nare completely ignored.   The positional encoding is now a non-trainable buffer,\nand all tensors are moved to the correct device with an `isinstance(v,\ntorch.Tensor)` check.   Training/validation losses and Macro-F1 are logged every\nepoch and stored in `experiment_data`, which is saved at the end of the run.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    \"\"\"For classification.\"\"\"\n\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    \"\"\"For next-token prediction pre-training.\"\"\"\n\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids  # shift right\n        tgt = seq_ids + [self.pad]  # predict each original char incl. last pad\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        # x : (B,L)\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=x.device), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x).mean(1)\n        return self.cls_head(h)\n\n\n# -------------------- training helpers --------------------\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot = 0\n    loss_sum = 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss = 0\n    preds = []\n    gts = []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss = 0\n    preds = []\n    gts = []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run():\n    # ---- load data ----\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    print(\"Vocab size:\", len(vocab), \"Max_len:\", max_len)\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Stage 1: Causal LM pre-training ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n    pre_epochs = 3\n    for ep in range(1, pre_epochs + 1):\n        l = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(l)\n        print(f\"Pre-train epoch {ep}/{pre_epochs}: loss={l:.4f}\")\n\n    # save encoder weights\n    enc_state = lm.encoder.state_dict()\n\n    # ========== Stage 2: Classification fine-tuning ==========\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)  # transfer\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n\n    best_f1 = 0.0\n    wait = 0\n    patience = 5\n    best_state = None\n    max_epochs = 15\n    for ep in range(1, max_epochs + 1):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n    # ---- test evaluation ----\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"TEST macro-F1 = {test_f1:.4f}\")\n    ed = experiment_data[\"pretrain+cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nrun()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir & data container --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n    \"scratch_cls\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n}\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=x.device), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x).mean(1)\n        return self.cls_head(h)\n\n\n# -------------------- training helpers --------------------\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run_experiments():\n    # ---- load data ----\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    print(\"Vocab size:\", len(vocab), \"Max_len:\", max_len)\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Experiment 1: Pretrain + Classifier ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n    for ep in range(1, 4):\n        l = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(l)\n        print(f\"[Pretrain] epoch {ep}/3  loss={l:.4f}\")\n\n    # transfer encoder\n    enc_state = lm.encoder.state_dict()\n    del lm\n\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Pretrain+CLS] Ep {ep}: val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping pretrain+cls.\")\n            break\n\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"[Pretrain+CLS] TEST macro-F1={test_f1:.4f}\")\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n    del cls_model\n\n    # ========== Experiment 2: Scratch Classifier ==========\n    scratch_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len\n    ).to(\n        device\n    )  # random init\n    optim_s = torch.optim.Adam(scratch_model.parameters(), lr=5e-4)\n    crit_s = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(scratch_model, train_loader, optim_s, crit_s)\n        val_loss, val_f1, _, _ = eval_cls_epoch(scratch_model, val_loader, crit_s)\n\n        ed = experiment_data[\"scratch_cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Scratch] Ep {ep}: val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = scratch_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping scratch.\")\n            break\n\n    if best_state:\n        scratch_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(scratch_model, test_loader, crit_s)\n    print(f\"[Scratch] TEST macro-F1={test_f1:.4f}\")\n    ed = experiment_data[\"scratch_cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save all ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved experiment_data.npy\")\n\n\nif __name__ == \"__main__\":\n    run_experiments()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- I/O & bookkeeping ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using\", device)\n\n# experiment container following the requested template\nablation_keys = [\"pos_emb_learned\", \"pos_emb_none\"]  # baseline & ablation\nexperiment_data = {\n    k: {\n        \"SPR-BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n            \"test_macro_f1\": None,\n            \"test_loss\": None,\n        }\n    }\n    for k in ablation_keys\n}\n\n\n# ---------- dataset helpers ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq),\n            \"labels\": torch.tensor(self.labels[idx]),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp, tgt = [self.pad] + seq_ids, seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\"input_ids\": torch.tensor(inp), \"labels\": torch.tensor(tgt)}\n\n\n# ---------- model helpers ----------\ndef sinusoid_encoding(max_len, dim):\n    pe = torch.zeros(max_len, dim)\n    position = torch.arange(0, max_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, dim, 2) * (-np.log(10000.0) / dim))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        d_model,\n        nhead,\n        num_layers,\n        max_len,\n        pos_type=\"learned\",\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos_type = pos_type\n        if pos_type == \"learned\":\n            self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        elif pos_type == \"sin\":\n            pe = sinusoid_encoding(max_len, d_model).unsqueeze(0)\n            self.register_buffer(\"pos\", pe)\n        else:  # none\n            self.pos = None\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 4 * d_model, dropout, activation=\"gelu\"\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x)\n        if self.pos is not None:\n            h = h + self.pos[:, : x.size(1)]\n        L = x.size(1)\n        if causal:\n            mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, pos_type):\n        super().__init__()\n        self.encoder = Encoder(\n            vocab_size, d_model, nhead, num_layers, max_len, pos_type\n        )\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        return self.lm_head(self.encoder(x, causal=True))\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(\n        self, vocab_size, n_classes, d_model, nhead, num_layers, max_len, pos_type\n    ):\n        super().__init__()\n        self.encoder = Encoder(\n            vocab_size, d_model, nhead, num_layers, max_len, pos_type\n        )\n        self.cls_head = nn.Linear(d_model, n_classes)\n\n    def forward(self, x):\n        return self.cls_head(self.encoder(x).mean(1))\n\n\n# ---------- train / eval loops ----------\ndef train_causal_epoch(model, loader, opt, crit):\n    model.train()\n    total, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        opt.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = crit(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        opt.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        total += batch[\"labels\"].size(0)\n    return loss_sum / total\n\n\ndef train_cls_epoch(model, loader, opt, crit):\n    model.train()\n    tl, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        tl += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tl / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, crit):\n    model.eval()\n    tl, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        tl += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tl / len(loader.dataset), f1_score(gts, preds, average=\"macro\"), preds, gts\n\n\n# ---------- main routine ----------\ndef run_variant(pos_type, tag):\n    ed = experiment_data[tag][\"SPR-BENCH\"]\n\n    # data path (adjust if needed)\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(\n            f\"{DATA_PATH} missing. Please place SPR_BENCH dataset there.\"\n        )\n    dsets = load_spr_bench(DATA_PATH)\n\n    # vocab & loaders\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    preloader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # model configs\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size, n_classes = len(vocab), len(set(dsets[\"train\"][\"label\"]))\n\n    # ---- Stage 1: pre-training ----\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len, pos_type).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n    for ep in range(1, 4):\n        loss = train_causal_epoch(lm, preloader, opt_lm, crit_lm)\n        ed[\"losses\"][\"pretrain\"].append(loss)\n        print(f\"[{tag}] Pretrain {ep}/3: {loss:.4f}\")\n    enc_state = lm.encoder.state_dict()\n\n    # ---- Stage 2: fine-tune ----\n    cls = SPRClassifier(\n        vocab_size, n_classes, d_model, nhead, nlayer, max_len, pos_type\n    ).to(device)\n    cls.encoder.load_state_dict(enc_state)\n    opt_cls = torch.optim.Adam(cls.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, best_state = 0.0, 0, None\n    for ep in range(1, 16):\n        tr_l, tr_f1 = train_cls_epoch(cls, train_loader, opt_cls, crit_cls)\n        val_l, val_f1, _, _ = eval_cls_epoch(cls, val_loader, crit_cls)\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_l)\n        ed[\"losses\"][\"val\"].append(val_l)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        print(f\"[{tag}] Epoch {ep}: val_f1 {val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, best_state, wait = val_f1, cls.state_dict(), 0\n        else:\n            wait += 1\n        if wait >= 5:\n            break\n\n    # ---- test ----\n    if best_state is not None:\n        cls.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls, test_loader, crit_cls)\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    print(f\"[{tag}] TEST macro-F1 = {test_f1:.4f}\")\n\n\ndef main():\n    run_variant(\"learned\", \"pos_emb_learned\")\n    run_variant(\"none\", \"pos_emb_none\")\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved results to experiment_data.npy\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment-data container --------------------\nexperiment_data = {\n    \"freeze_encoder\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n            \"test_macro_f1\": None,\n            \"test_loss\": None,\n        }\n    }\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq),\n            \"labels\": torch.tensor(self.labels[idx]),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\"input_ids\": torch.tensor(inp), \"labels\": torch.tensor(tgt)}\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 4 * d_model, dropout, activation=\"gelu\"\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        return self.lm_head(self.encoder(x, causal=True))\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x).mean(1)\n        return self.cls_head(h)\n\n\n# -------------------- training helpers --------------------\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------- main execution --------------------\ndef run():\n    # ---- load data ----\n    DATA_PATH = pathlib.Path(\n        os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(\n            f\"{DATA_PATH} not found. Configure SPR_BENCH_PATH env var.\"\n        )\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab & lengths ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    print(\"Vocab size:\", len(vocab), \"Max_len:\", max_len)\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size, num_classes = len(vocab), len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Stage 1: Causal LM pre-training ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len).to(device)\n    opt_lm, crit_lm = torch.optim.Adam(lm.parameters(), lr=1e-3), nn.CrossEntropyLoss(\n        ignore_index=vocab[\"<pad>\"]\n    )\n    pre_epochs = 3\n    for ep in range(1, pre_epochs + 1):\n        pre_loss = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"freeze_encoder\"][\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(\n            pre_loss\n        )\n        print(f\"Pre-train epoch {ep}/{pre_epochs}: loss={pre_loss:.4f}\")\n\n    enc_state = lm.encoder.state_dict()\n\n    # ========== Stage 2: Classification with FROZEN encoder ==========\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n    for p in cls_model.encoder.parameters():\n        p.requires_grad = False  # freeze encoder\n\n    optim_cls = torch.optim.Adam(\n        filter(lambda p: p.requires_grad, cls_model.parameters()), lr=5e-3\n    )\n    crit_cls = nn.CrossEntropyLoss()\n\n    best_f1, wait, patience, best_state = 0.0, 0, 5, None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"freeze_encoder\"][\"SPR_BENCH\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(\n            f\"Epoch {ep}: train_f1={tr_f1:.4f} val_loss={val_loss:.4f} val_f1={val_f1:.4f}\"\n        )\n        if val_f1 > best_f1:\n            best_f1, wait, best_state = val_f1, 0, cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    # ---- test evaluation ----\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"TEST macro-F1 = {test_f1:.4f}\")\n\n    ed = experiment_data[\"freeze_encoder\"][\"SPR_BENCH\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nif __name__ == \"__main__\":\n    run()\n", "# ----------------------------------------------------------\n# CLS-token ablation study \u2013 single-file runnable programme\n# ----------------------------------------------------------\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------- experiment-data container --------------------\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nCLS_TOKEN = \"<cls>\"\nPAD_TOKEN = \"<pad>\"\nUNK_TOKEN = \"<unk>\"\n\n\nclass SPRSeqDataset(Dataset):\n    \"\"\"Dataset for classification with <cls> token prepended.\"\"\"\n\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n        self.pad, self.cls = vocab[PAD_TOKEN], vocab[CLS_TOKEN]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [self.cls] + [\n            self.vocab.get(ch, self.vocab[UNK_TOKEN])\n            for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        seq_ids += [self.pad] * (self.max_len - len(seq_ids))\n        return {\n            \"input_ids\": torch.tensor(seq_ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    \"\"\"Causal LM with <cls> token present; classic shift-right.\"\"\"\n\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len = vocab, max_len\n        self.pad, self.cls = vocab[PAD_TOKEN], vocab[CLS_TOKEN]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [self.cls] + [\n            self.vocab.get(ch, self.vocab[UNK_TOKEN])\n            for ch in self.seqs[idx][: self.max_len - 2]\n        ]  # leave room for shift symbol\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, n_layers, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=x.device), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, n_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, n_layers, max_len)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        return self.lm_head(self.encoder(x, causal=True))\n\n\nclass SPRClassifier(nn.Module):\n    \"\"\"Classifier that uses the hidden state of <cls> (position 0).\"\"\"\n\n    def __init__(self, vocab_size, num_classes, d_model, nhead, n_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, n_layers, max_len)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)  # (B,L,D)\n        return self.cls_head(h[:, 0])  # <cls> token\n\n\n# -------------------- training helpers --------------------\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        B = batch[\"labels\"].size(0)\n        loss_sum += loss.item() * B\n        tot += B\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        B = batch[\"labels\"].size(0)\n        tot_loss += loss.item() * B\n        preds.extend(out.argmax(1).detach().cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        B = batch[\"labels\"].size(0)\n        tot_loss += loss.item() * B\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run():\n    # ---- dataset root ----\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1, CLS_TOKEN: 2}\n    vocab.update({c: i + 3 for i, c in enumerate(sorted(chars))})\n    vocab_size = len(vocab)\n    max_len = min(\n        128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 2\n    )  # +1 for <cls>, +1 for shift\n    print(\"Vocab size:\", vocab_size, \"Max_len:\", max_len)\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, n_layers = 128, 4, 3\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Stage 1: Causal-LM Pre-training ==========\n    lm_model = CausalLM(vocab_size, d_model, nhead, n_layers, max_len).to(device)\n    opt_lm = torch.optim.Adam(lm_model.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n    pre_epochs = 3\n    for ep in range(1, pre_epochs + 1):\n        loss = train_causal_epoch(lm_model, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(loss)\n        print(f\"Pre-train epoch {ep}/{pre_epochs}:  loss={loss:.4f}\")\n    encoder_state = lm_model.encoder.state_dict()  # save for transfer\n\n    # ========== Stage 2: Classification Fine-tuning ==========\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, n_layers, max_len\n    ).to(device)\n    cls_model.encoder.load_state_dict(encoder_state)\n\n    opt_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    max_epochs = 15\n    for ep in range(1, max_epochs + 1):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, opt_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\")\n\n        if val_f1 > best_f1:\n            best_f1, best_state, wait = val_f1, cls_model.state_dict(), 0\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    # ---- test evaluation ----\n    if best_state is not None:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"TEST macro-F1 = {test_f1:.4f}\")\n\n    ed = experiment_data[\"pretrain+cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save ----\n    np.save(\n        os.path.join(working_dir, \"experiment_data.npy\"),\n        experiment_data,\n        allow_pickle=True,\n    )\n\n\nif __name__ == \"__main__\":\n    run()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn, random\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"mlm_pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    \"\"\"For classification.\"\"\"\n\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        seq_ids += [self.pad] * (self.max_len - len(seq_ids))\n        return {\n            \"input_ids\": torch.tensor(seq_ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass MLMDataset(Dataset):\n    \"\"\"For bidirectional masked-LM pre-training.\"\"\"\n\n    def __init__(self, hf_ds, vocab, max_len, mask_prob=0.15, seed=42):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n        self.mask_id = vocab[\"<mask>\"]\n        self.mask_prob = mask_prob\n        random.seed(seed)\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx][: self.max_len]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq]\n        ids += [self.pad] * (self.max_len - len(ids))\n        input_ids = ids.copy()\n        labels = [-100] * self.max_len  # ignored by loss\n\n        for i, tok in enumerate(ids):\n            if tok == self.pad:\n                continue\n            if random.random() < self.mask_prob:\n                labels[i] = tok\n                input_ids[i] = self.mask_id\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"labels\": torch.tensor(labels, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=x.device), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass MLMModel(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=False)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x).mean(1)\n        return self.cls_head(h)\n\n\n# -------------------- training helpers --------------------\ndef train_mlm_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"input_ids\"].size(0)\n        tot += batch[\"input_ids\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    vocab[\"<mask>\"] = len(vocab)\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]))\n    print(\"Vocab size:\", len(vocab), \"Max_len:\", max_len)\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        MLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Stage 1: MLM pre-training ==========\n    mlm = MLMModel(vocab_size, d_model, nhead, nlayer, max_len).to(device)\n    opt_mlm = torch.optim.Adam(mlm.parameters(), lr=1e-3)\n    crit_mlm = nn.CrossEntropyLoss(ignore_index=-100)\n    pre_epochs = 3\n    for ep in range(1, pre_epochs + 1):\n        l = train_mlm_epoch(mlm, pretrain_loader, opt_mlm, crit_mlm)\n        experiment_data[\"mlm_pretrain+cls\"][\"losses\"][\"pretrain\"].append(l)\n        print(f\"Pre-train epoch {ep}/{pre_epochs}: loss={l:.4f}\")\n\n    enc_state = mlm.encoder.state_dict()\n\n    # ========== Stage 2: Classification fine-tuning ==========\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n\n    best_f1, wait, patience, best_state = 0.0, 0, 5, None\n    max_epochs = 15\n    for ep in range(1, max_epochs + 1):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"mlm_pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait, best_state = val_f1, 0, cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n    # ---- test evaluation ----\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"TEST macro-F1 = {test_f1:.4f}\")\n    ed = experiment_data[\"mlm_pretrain+cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nif __name__ == \"__main__\":\n    run()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir & save dict --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"baseline\": {},\n    \"weight_tying\": {},\n}\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = [\n            self.vocab.get(c, self.vocab[\"<unk>\"])\n            for c in self.seqs[idx][: self.max_len]\n        ]\n        ids += [self.pad] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"labels\": torch.tensor(self.labels[idx]),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = [\n            self.vocab.get(c, self.vocab[\"<unk>\"])\n            for c in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + ids\n        tgt = ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\"input_ids\": torch.tensor(inp), \"labels\": torch.tensor(tgt)}\n\n\n# -------------------- model defs --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayer, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        layer = nn.TransformerEncoderLayer(d_model, nhead, 4 * d_model, dropout, \"gelu\")\n        self.enc = nn.TransformerEncoder(layer, nlayer)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=x.device), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayer, max_len, weight_tie=False):\n        super().__init__()\n        self.encoder = Encoder(vocab, d_model, nhead, nlayer, max_len)\n        if weight_tie:\n            self.lm_head = nn.Linear(d_model, vocab, bias=False)\n            # weight tying\n            self.lm_head.weight = self.encoder.embed.weight\n        else:\n            self.lm_head = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return self.lm_head(self.encoder(x, causal=True))\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab, n_cls, d_model, nhead, nlayer, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab, d_model, nhead, nlayer, max_len)\n        self.cls_head = nn.Linear(d_model, n_cls)\n\n    def forward(self, x):\n        return self.cls_head(self.encoder(x).mean(1))\n\n\n# -------------------- helpers --------------------\ndef train_causal_epoch(model, loader, opt, crit):\n    model.train()\n    s = 0\n    tot = 0\n    for b in loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        opt.zero_grad()\n        out = model(b[\"input_ids\"])\n        loss = crit(out.view(-1, out.size(-1)), b[\"labels\"].view(-1))\n        loss.backward()\n        opt.step()\n        s += loss.item() * b[\"labels\"].size(0)\n        tot += b[\"labels\"].size(0)\n    return s / tot\n\n\ndef train_cls_epoch(model, loader, opt, crit):\n    model.train()\n    los = 0\n    gts = []\n    prs = []\n    for b in loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        opt.zero_grad()\n        out = model(b[\"input_ids\"])\n        loss = crit(out, b[\"labels\"])\n        loss.backward()\n        opt.step()\n        los += loss.item() * b[\"labels\"].size(0)\n        prs += out.argmax(1).cpu().tolist()\n        gts += b[\"labels\"].cpu().tolist()\n    return los / len(loader.dataset), f1_score(gts, prs, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, crit):\n    model.eval()\n    los = 0\n    gts = []\n    prs = []\n    for b in loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        out = model(b[\"input_ids\"])\n        loss = crit(out, b[\"labels\"])\n        los += loss.item() * b[\"labels\"].size(0)\n        prs += out.argmax(1).cpu().tolist()\n        gts += b[\"labels\"].cpu().tolist()\n    return los / len(loader.dataset), f1_score(gts, prs, average=\"macro\"), prs, gts\n\n\n# -------------------- main routine --------------------\ndef pipeline(ablation_key: str, weight_tie: bool):\n    print(\"\\n=== Running\", ablation_key, \"===\")\n    ed = experiment_data[ablation_key]\n    # ---- data ----\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found\")\n    ds = load_spr_bench(DATA_PATH)\n    vocab_chars = set(\"\".join(ds[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(vocab_chars))})\n    max_len = min(128, max(len(s) for s in ds[\"train\"][\"sequence\"]) + 1)\n    V = len(vocab)\n    C = len(set(ds[\"train\"][\"label\"]))\n    # loaders\n    pre_loader = DataLoader(\n        CausalLMDataset(ds[\"train\"], vocab, max_len), 256, shuffle=True, drop_last=True\n    )\n    tr_loader = DataLoader(\n        SPRSeqDataset(ds[\"train\"], vocab, max_len), 128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(ds[\"dev\"], vocab, max_len), 256)\n    test_loader = DataLoader(SPRSeqDataset(ds[\"test\"], vocab, max_len), 256)\n    # ---- pretrain ----\n    d_model, nhead, nlayer = 128, 4, 3\n    lm = CausalLM(V, d_model, nhead, nlayer, max_len, weight_tie).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), 1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n    ed[\"losses\"] = {\"pretrain\": [], \"train\": [], \"val\": []}\n    ed[\"metrics\"] = {\"train_macro_f1\": [], \"val_macro_f1\": []}\n    pre_epochs = 3\n    for ep in range(pre_epochs):\n        loss = train_causal_epoch(lm, pre_loader, opt_lm, crit_lm)\n        ed[\"losses\"][\"pretrain\"].append(loss)\n        print(f\"Pretrain {ep+1}/{pre_epochs} loss {loss:.4f}\")\n    enc_state = lm.encoder.state_dict()\n    # ---- fine-tune ----\n    cls = SPRClassifier(V, C, d_model, nhead, nlayer, max_len).to(device)\n    cls.encoder.load_state_dict(enc_state)\n    opt_cls = torch.optim.Adam(cls.parameters(), 5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1 = 0\n    wait = 0\n    patience = 5\n    best_state = None\n    epochs = 15\n    ed[\"epochs\"] = []\n    for ep in range(1, epochs + 1):\n        tr_loss, tr_f1 = train_cls_epoch(cls, tr_loader, opt_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls, val_loader, crit_cls)\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        print(f\"Ep{ep} val_f1 {val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, val_best = val_f1, 0\n            best_state = cls.state_dict()\n        else:\n            val_best += 1\n        if val_best >= patience:\n            break\n    # ---- test ----\n    if best_state:\n        cls.load_state_dict(best_state)\n    t_loss, t_f1, preds, gts = eval_cls_epoch(cls, test_loader, crit_cls)\n    print(\"Test F1:\", t_f1)\n    ed[\"predictions\"] = preds\n    ed[\"ground_truth\"] = gts\n    ed[\"test_macro_f1\"] = t_f1\n    ed[\"test_loss\"] = t_loss\n\n\ndef run_all():\n    pipeline(\"baseline\", False)\n    pipeline(\"weight_tying\", True)\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved experiment_data.npy\")\n\n\nif __name__ == \"__main__\":\n    run_all()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"relu_activation\": {\n        \"spr_bench\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.vocab, self.max_len, self.pad = vocab, max_len, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"relu\",  # <- GELU replaced by ReLU (ablation)\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        if causal:\n            L = x.size(1)\n            mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=x.device), 1)\n            h = self.enc(h.transpose(0, 1), mask).transpose(0, 1)\n        else:\n            h = self.enc(h.transpose(0, 1)).transpose(0, 1)\n        return h\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, d_model, nhead, num_layers, max_len):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x).mean(1)\n        return self.cls_head(h)\n\n\n# -------------------- training helpers --------------------\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------- main execution --------------------\ndef run():\n    # ---- load data ----\n    root_env = os.getenv(\"SPR_BENCH_ROOT\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    DATA_PATH = pathlib.Path(root_env)\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found. Set SPR_BENCH_ROOT env var.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    print(\"Vocab size:\", len(vocab), \"Max_len:\", max_len)\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Stage 1: Causal LM pre-training (with ReLU) ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n    pre_epochs = 3\n    for ep in range(1, pre_epochs + 1):\n        l = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"relu_activation\"][\"spr_bench\"][\"losses\"][\"pretrain\"].append(l)\n        print(f\"Pre-train epoch {ep}/{pre_epochs}: loss={l:.4f}\")\n\n    enc_state = lm.encoder.state_dict()\n\n    # ========== Stage 2: Classification fine-tuning ==========\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n\n    best_f1, wait, patience, best_state = 0.0, 0, 5, None\n    max_epochs = 15\n    for ep in range(1, max_epochs + 1):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"relu_activation\"][\"spr_bench\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait, best_state = val_f1, 0, cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n    # ---- test evaluation ----\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"TEST macro-F1 = {test_f1:.4f}\")\n\n    ed = experiment_data[\"relu_activation\"][\"spr_bench\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nif __name__ == \"__main__\":\n    run()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n    \"scratch_cls\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len]]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.vocab = hf_ds[\"sequence\"], vocab\n        self.max_len = max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(\n        self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx, dropout=0.1\n    ):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        pos = torch.randn(1, max_len, d_model)\n        self.register_buffer(\"pos\", pos)  # non-trainable positional encoding\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        # x: (B, L)\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        key_pad_mask = x == self.pad_idx  # (B, L)\n        if causal:\n            L = x.size(1)\n            causal_mask = torch.triu(\n                torch.ones(L, L, device=x.device, dtype=torch.bool), 1\n            )\n            h = self.enc(h, mask=causal_mask, src_key_padding_mask=key_pad_mask)\n        else:\n            h = self.enc(h, src_key_padding_mask=key_pad_mask)\n        return h  # (B, L, D)\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model, nhead, num_layers, max_len, pad_idx\n    ):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        # mean pooling ignoring pads\n        h = self.encoder(x)  # (B, L, D)\n        mask = (x != self.encoder.pad_idx).unsqueeze(-1)\n        summed = (h * mask).sum(1)\n        l = mask.sum(1).clamp(min=1)\n        pooled = summed / l\n        return self.cls_head(pooled)\n\n\n# -------------------- training helpers --------------------\ndef move_to_device(batch):\n    return {\n        k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()\n    }\n\n\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run_experiments():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    pad_idx = vocab[\"<pad>\"]\n    print(f\"Vocab size: {len(vocab)}  |  Max_len: {max_len}\")\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Experiment 1: Pretrain + Classifier ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len, pad_idx).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=pad_idx)\n    for ep in range(1, 4):\n        ploss = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(ploss)\n        print(f\"[Pretrain] epoch {ep}/3  loss={ploss:.4f}\")\n\n    enc_state = lm.encoder.state_dict()\n    del lm  # free GPU\n\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Pretrain+CLS] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping pretrain+cls.\")\n            break\n\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"[Pretrain+CLS] TEST macro-F1={test_f1:.4f}\")\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n    del cls_model\n\n    # ========== Experiment 2: Scratch Classifier ==========\n    scratch_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    optim_s = torch.optim.Adam(scratch_model.parameters(), lr=5e-4)\n    crit_s = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(scratch_model, train_loader, optim_s, crit_s)\n        val_loss, val_f1, _, _ = eval_cls_epoch(scratch_model, val_loader, crit_s)\n\n        ed = experiment_data[\"scratch_cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Scratch] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = scratch_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping scratch.\")\n            break\n\n    if best_state:\n        scratch_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(scratch_model, test_loader, crit_s)\n    print(f\"[Scratch] TEST macro-F1={test_f1:.4f}\")\n    ed = experiment_data[\"scratch_cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save all ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved experiment_data.npy\")\n\n    # ---- simple plot ----\n    plt.figure(figsize=(6, 4))\n    plt.plot(\n        experiment_data[\"pretrain+cls\"][\"epochs\"],\n        experiment_data[\"pretrain+cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Pretrain+CLS\",\n    )\n    plt.plot(\n        experiment_data[\"scratch_cls\"][\"epochs\"],\n        experiment_data[\"scratch_cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Scratch\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"val_f1_comparison.png\"))\n    plt.close()\n\n\nrun_experiments()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n    \"scratch_cls\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len]]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.vocab = hf_ds[\"sequence\"], vocab\n        self.max_len = max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(\n        self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx, dropout=0.1\n    ):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        pos = torch.randn(1, max_len, d_model)\n        self.register_buffer(\"pos\", pos)  # non-trainable positional encoding\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        # x: (B, L)\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        key_pad_mask = x == self.pad_idx  # (B, L)\n        if causal:\n            L = x.size(1)\n            causal_mask = torch.triu(\n                torch.ones(L, L, device=x.device, dtype=torch.bool), 1\n            )\n            h = self.enc(h, mask=causal_mask, src_key_padding_mask=key_pad_mask)\n        else:\n            h = self.enc(h, src_key_padding_mask=key_pad_mask)\n        return h  # (B, L, D)\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model, nhead, num_layers, max_len, pad_idx\n    ):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        # mean pooling ignoring pads\n        h = self.encoder(x)  # (B, L, D)\n        mask = (x != self.encoder.pad_idx).unsqueeze(-1)\n        summed = (h * mask).sum(1)\n        l = mask.sum(1).clamp(min=1)\n        pooled = summed / l\n        return self.cls_head(pooled)\n\n\n# -------------------- training helpers --------------------\ndef move_to_device(batch):\n    return {\n        k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()\n    }\n\n\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run_experiments():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    pad_idx = vocab[\"<pad>\"]\n    print(f\"Vocab size: {len(vocab)}  |  Max_len: {max_len}\")\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Experiment 1: Pretrain + Classifier ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len, pad_idx).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=pad_idx)\n    for ep in range(1, 4):\n        ploss = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(ploss)\n        print(f\"[Pretrain] epoch {ep}/3  loss={ploss:.4f}\")\n\n    enc_state = lm.encoder.state_dict()\n    del lm  # free GPU\n\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Pretrain+CLS] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping pretrain+cls.\")\n            break\n\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"[Pretrain+CLS] TEST macro-F1={test_f1:.4f}\")\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n    del cls_model\n\n    # ========== Experiment 2: Scratch Classifier ==========\n    scratch_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    optim_s = torch.optim.Adam(scratch_model.parameters(), lr=5e-4)\n    crit_s = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(scratch_model, train_loader, optim_s, crit_s)\n        val_loss, val_f1, _, _ = eval_cls_epoch(scratch_model, val_loader, crit_s)\n\n        ed = experiment_data[\"scratch_cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Scratch] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = scratch_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping scratch.\")\n            break\n\n    if best_state:\n        scratch_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(scratch_model, test_loader, crit_s)\n    print(f\"[Scratch] TEST macro-F1={test_f1:.4f}\")\n    ed = experiment_data[\"scratch_cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save all ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved experiment_data.npy\")\n\n    # ---- simple plot ----\n    plt.figure(figsize=(6, 4))\n    plt.plot(\n        experiment_data[\"pretrain+cls\"][\"epochs\"],\n        experiment_data[\"pretrain+cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Pretrain+CLS\",\n    )\n    plt.plot(\n        experiment_data[\"scratch_cls\"][\"epochs\"],\n        experiment_data[\"scratch_cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Scratch\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"val_f1_comparison.png\"))\n    plt.close()\n\n\nrun_experiments()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n    \"scratch_cls\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len]]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.vocab = hf_ds[\"sequence\"], vocab\n        self.max_len = max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(\n        self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx, dropout=0.1\n    ):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        pos = torch.randn(1, max_len, d_model)\n        self.register_buffer(\"pos\", pos)  # non-trainable positional encoding\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        # x: (B, L)\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        key_pad_mask = x == self.pad_idx  # (B, L)\n        if causal:\n            L = x.size(1)\n            causal_mask = torch.triu(\n                torch.ones(L, L, device=x.device, dtype=torch.bool), 1\n            )\n            h = self.enc(h, mask=causal_mask, src_key_padding_mask=key_pad_mask)\n        else:\n            h = self.enc(h, src_key_padding_mask=key_pad_mask)\n        return h  # (B, L, D)\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model, nhead, num_layers, max_len, pad_idx\n    ):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        # mean pooling ignoring pads\n        h = self.encoder(x)  # (B, L, D)\n        mask = (x != self.encoder.pad_idx).unsqueeze(-1)\n        summed = (h * mask).sum(1)\n        l = mask.sum(1).clamp(min=1)\n        pooled = summed / l\n        return self.cls_head(pooled)\n\n\n# -------------------- training helpers --------------------\ndef move_to_device(batch):\n    return {\n        k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()\n    }\n\n\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run_experiments():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    pad_idx = vocab[\"<pad>\"]\n    print(f\"Vocab size: {len(vocab)}  |  Max_len: {max_len}\")\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Experiment 1: Pretrain + Classifier ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len, pad_idx).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=pad_idx)\n    for ep in range(1, 4):\n        ploss = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(ploss)\n        print(f\"[Pretrain] epoch {ep}/3  loss={ploss:.4f}\")\n\n    enc_state = lm.encoder.state_dict()\n    del lm  # free GPU\n\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Pretrain+CLS] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping pretrain+cls.\")\n            break\n\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"[Pretrain+CLS] TEST macro-F1={test_f1:.4f}\")\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n    del cls_model\n\n    # ========== Experiment 2: Scratch Classifier ==========\n    scratch_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    optim_s = torch.optim.Adam(scratch_model.parameters(), lr=5e-4)\n    crit_s = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(scratch_model, train_loader, optim_s, crit_s)\n        val_loss, val_f1, _, _ = eval_cls_epoch(scratch_model, val_loader, crit_s)\n\n        ed = experiment_data[\"scratch_cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Scratch] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = scratch_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping scratch.\")\n            break\n\n    if best_state:\n        scratch_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(scratch_model, test_loader, crit_s)\n    print(f\"[Scratch] TEST macro-F1={test_f1:.4f}\")\n    ed = experiment_data[\"scratch_cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save all ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved experiment_data.npy\")\n\n    # ---- simple plot ----\n    plt.figure(figsize=(6, 4))\n    plt.plot(\n        experiment_data[\"pretrain+cls\"][\"epochs\"],\n        experiment_data[\"pretrain+cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Pretrain+CLS\",\n    )\n    plt.plot(\n        experiment_data[\"scratch_cls\"][\"epochs\"],\n        experiment_data[\"scratch_cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Scratch\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"val_f1_comparison.png\"))\n    plt.close()\n\n\nrun_experiments()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# -------------------- working dir --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"pretrain+cls\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n    \"scratch_cls\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    },\n}\n\n\n# -------------------- dataset utilities --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.max_len = vocab, max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len]]\n        seq += [self.pad] * (self.max_len - len(seq))\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\nclass CausalLMDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.vocab = hf_ds[\"sequence\"], vocab\n        self.max_len = max_len\n        self.pad = vocab[\"<pad>\"]\n        self.unk = vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq_ids = [\n            self.vocab.get(ch, self.unk) for ch in self.seqs[idx][: self.max_len - 1]\n        ]\n        inp = [self.pad] + seq_ids\n        tgt = seq_ids + [self.pad]\n        inp += [self.pad] * (self.max_len - len(inp))\n        tgt += [self.pad] * (self.max_len - len(tgt))\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"labels\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n\n# -------------------- model definitions --------------------\nclass Encoder(nn.Module):\n    def __init__(\n        self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx, dropout=0.1\n    ):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        pos = torch.randn(1, max_len, d_model)\n        self.register_buffer(\"pos\", pos)  # non-trainable positional encoding\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n\n    def forward(self, x, causal=False):\n        # x: (B, L)\n        h = self.embed(x) + self.pos[:, : x.size(1)]\n        key_pad_mask = x == self.pad_idx  # (B, L)\n        if causal:\n            L = x.size(1)\n            causal_mask = torch.triu(\n                torch.ones(L, L, device=x.device, dtype=torch.bool), 1\n            )\n            h = self.enc(h, mask=causal_mask, src_key_padding_mask=key_pad_mask)\n        else:\n            h = self.enc(h, src_key_padding_mask=key_pad_mask)\n        return h  # (B, L, D)\n\n\nclass CausalLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, pad_idx):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, x):\n        h = self.encoder(x, causal=True)\n        return self.lm_head(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model, nhead, num_layers, max_len, pad_idx\n    ):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, d_model, nhead, num_layers, max_len, pad_idx)\n        self.cls_head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        # mean pooling ignoring pads\n        h = self.encoder(x)  # (B, L, D)\n        mask = (x != self.encoder.pad_idx).unsqueeze(-1)\n        summed = (h * mask).sum(1)\n        l = mask.sum(1).clamp(min=1)\n        pooled = summed / l\n        return self.cls_head(pooled)\n\n\n# -------------------- training helpers --------------------\ndef move_to_device(batch):\n    return {\n        k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()\n    }\n\n\ndef train_causal_epoch(model, loader, optim, criterion):\n    model.train()\n    tot, loss_sum = 0, 0.0\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n        loss.backward()\n        optim.step()\n        loss_sum += loss.item() * batch[\"labels\"].size(0)\n        tot += batch[\"labels\"].size(0)\n    return loss_sum / tot\n\n\ndef train_cls_epoch(model, loader, optim, criterion):\n    model.train()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_cls_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# -------------------- main execution --------------------\ndef run_experiments():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    dsets = load_spr_bench(DATA_PATH)\n\n    # ---- vocab ----\n    chars = set(\"\".join(dsets[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({c: i + 2 for i, c in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in dsets[\"train\"][\"sequence\"]) + 1)\n    pad_idx = vocab[\"<pad>\"]\n    print(f\"Vocab size: {len(vocab)}  |  Max_len: {max_len}\")\n\n    # ---- dataloaders ----\n    pretrain_loader = DataLoader(\n        CausalLMDataset(dsets[\"train\"], vocab, max_len),\n        batch_size=256,\n        shuffle=True,\n        drop_last=True,\n    )\n    train_loader = DataLoader(\n        SPRSeqDataset(dsets[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRSeqDataset(dsets[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRSeqDataset(dsets[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    # ---- model configs ----\n    d_model, nhead, nlayer = 128, 4, 3\n    vocab_size = len(vocab)\n    num_classes = len(set(dsets[\"train\"][\"label\"]))\n\n    # ========== Experiment 1: Pretrain + Classifier ==========\n    lm = CausalLM(vocab_size, d_model, nhead, nlayer, max_len, pad_idx).to(device)\n    opt_lm = torch.optim.Adam(lm.parameters(), lr=1e-3)\n    crit_lm = nn.CrossEntropyLoss(ignore_index=pad_idx)\n    for ep in range(1, 4):\n        ploss = train_causal_epoch(lm, pretrain_loader, opt_lm, crit_lm)\n        experiment_data[\"pretrain+cls\"][\"losses\"][\"pretrain\"].append(ploss)\n        print(f\"[Pretrain] epoch {ep}/3  loss={ploss:.4f}\")\n\n    enc_state = lm.encoder.state_dict()\n    del lm  # free GPU\n\n    cls_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    cls_model.encoder.load_state_dict(enc_state)\n\n    optim_cls = torch.optim.Adam(cls_model.parameters(), lr=5e-4)\n    crit_cls = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(cls_model, train_loader, optim_cls, crit_cls)\n        val_loss, val_f1, _, _ = eval_cls_epoch(cls_model, val_loader, crit_cls)\n\n        ed = experiment_data[\"pretrain+cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Pretrain+CLS] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = cls_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping pretrain+cls.\")\n            break\n\n    if best_state:\n        cls_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(cls_model, test_loader, crit_cls)\n    print(f\"[Pretrain+CLS] TEST macro-F1={test_f1:.4f}\")\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n    del cls_model\n\n    # ========== Experiment 2: Scratch Classifier ==========\n    scratch_model = SPRClassifier(\n        vocab_size, num_classes, d_model, nhead, nlayer, max_len, pad_idx\n    ).to(device)\n    optim_s = torch.optim.Adam(scratch_model.parameters(), lr=5e-4)\n    crit_s = nn.CrossEntropyLoss()\n    best_f1, wait, patience = 0.0, 0, 5\n    best_state = None\n    for ep in range(1, 16):\n        tr_loss, tr_f1 = train_cls_epoch(scratch_model, train_loader, optim_s, crit_s)\n        val_loss, val_f1, _, _ = eval_cls_epoch(scratch_model, val_loader, crit_s)\n\n        ed = experiment_data[\"scratch_cls\"]\n        ed[\"epochs\"].append(ep)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(f\"[Scratch] Ep {ep}: val_loss={val_loss:.4f}  val_f1={val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1, wait = val_f1, 0\n            best_state = scratch_model.state_dict()\n        else:\n            wait += 1\n        if wait >= patience:\n            print(\"Early stopping scratch.\")\n            break\n\n    if best_state:\n        scratch_model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = eval_cls_epoch(scratch_model, test_loader, crit_s)\n    print(f\"[Scratch] TEST macro-F1={test_f1:.4f}\")\n    ed = experiment_data[\"scratch_cls\"]\n    ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n    ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # ---- save all ----\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"Saved experiment_data.npy\")\n\n    # ---- simple plot ----\n    plt.figure(figsize=(6, 4))\n    plt.plot(\n        experiment_data[\"pretrain+cls\"][\"epochs\"],\n        experiment_data[\"pretrain+cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Pretrain+CLS\",\n    )\n    plt.plot(\n        experiment_data[\"scratch_cls\"][\"epochs\"],\n        experiment_data[\"scratch_cls\"][\"metrics\"][\"val_macro_f1\"],\n        label=\"Scratch\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"val_f1_comparison.png\"))\n    plt.close()\n\n\nrun_experiments()\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 138015.93\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 106806.82\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 174377.58\nexamples/s]', '\\n', 'Vocab size:', ' ', '11', ' ', 'Max_len:', ' ', '96', '\\n',\n'Pre-train epoch 1/3: loss=1.5853', '\\n', 'Pre-train epoch 2/3: loss=0.9730',\n'\\n', 'Pre-train epoch 3/3: loss=0.9394', '\\n', 'Epoch 1: val_loss=0.6167\nval_macro_f1=0.7720', '\\n', 'Epoch 2: val_loss=0.5469  val_macro_f1=0.7720',\n'\\n', 'Epoch 3: val_loss=0.5572  val_macro_f1=0.7780', '\\n', 'Epoch 4:\nval_loss=0.5346  val_macro_f1=0.7880', '\\n', 'Epoch 5: val_loss=0.5349\nval_macro_f1=0.7900', '\\n', 'Epoch 6: val_loss=0.5375  val_macro_f1=0.7900',\n'\\n', 'Epoch 7: val_loss=0.5404  val_macro_f1=0.7880', '\\n', 'Epoch 8:\nval_loss=0.5344  val_macro_f1=0.7816', '\\n', 'Epoch 9: val_loss=0.5385\nval_macro_f1=0.7840', '\\n', 'Epoch 10: val_loss=0.5380  val_macro_f1=0.7778',\n'\\n', 'Early stopping.', '\\n', 'TEST macro-F1 = 0.7900', '\\n', 'Execution time:\n5 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Using', ' ', 'cuda', '\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Execution time: a second seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Device:', ' ', 'cuda', '\\n', 'Execution time: a second seconds (time limit is\n30 minutes).']", "['Using device: cuda', '\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 144436.93\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 84678.67\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 99287.57\nexamples/s]', '\\n', 'Vocab size: 11  |  Max_len: 96', '\\n', '[Pretrain] epoch\n1/3  loss=1.5879', '\\n', '[Pretrain] epoch 2/3  loss=0.9717', '\\n', '[Pretrain]\nepoch 3/3  loss=0.9394', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[Pretrain+CLS] Ep 1:\nval_loss=0.6752  val_f1=0.3421', '\\n', '[Pretrain+CLS] Ep 2: val_loss=0.5521\nval_f1=0.7820', '\\n', '[Pretrain+CLS] Ep 3: val_loss=0.5369  val_f1=0.7959',\n'\\n', '[Pretrain+CLS] Ep 4: val_loss=0.5342  val_f1=0.7900', '\\n',\n'[Pretrain+CLS] Ep 5: val_loss=0.5309  val_f1=0.7959', '\\n', '[Pretrain+CLS] Ep\n6: val_loss=0.5429  val_f1=0.7546', '\\n', '[Pretrain+CLS] Ep 7: val_loss=0.5444\nval_f1=0.7733', '\\n', '[Pretrain+CLS] Ep 8: val_loss=0.5301  val_f1=0.7794',\n'\\n', 'Early stopping pretrain+cls.', '\\n', '[Pretrain+CLS] TEST\nmacro-F1=0.7809', '\\n', '[Scratch] Ep 1: val_loss=0.5971  val_f1=0.7618', '\\n',\n'[Scratch] Ep 2: val_loss=0.5369  val_f1=0.7860', '\\n', '[Scratch] Ep 3:\nval_loss=0.5365  val_f1=0.7999', '\\n', '[Scratch] Ep 4: val_loss=0.5345\nval_f1=0.7900', '\\n', '[Scratch] Ep 5: val_loss=0.5434  val_f1=0.7840', '\\n',\n'[Scratch] Ep 6: val_loss=0.5324  val_f1=0.7900', '\\n', '[Scratch] Ep 7:\nval_loss=0.5265  val_f1=0.7879', '\\n', '[Scratch] Ep 8: val_loss=0.5274\nval_f1=0.7940', '\\n', 'Early stopping scratch.', '\\n', '[Scratch] TEST\nmacro-F1=0.8000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 6\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 165091.08\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 31462.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 168371.56\nexamples/s]', '\\n', 'Vocab size: 11  |  Max_len: 96', '\\n', '[Pretrain] epoch\n1/3  loss=1.5603', '\\n', '[Pretrain] epoch 2/3  loss=0.9735', '\\n', '[Pretrain]\nepoch 3/3  loss=0.9376', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[Pretrain+CLS] Ep 1:\nval_loss=0.5545  val_f1=0.7860', '\\n', '[Pretrain+CLS] Ep 2: val_loss=0.5365\nval_f1=0.7860', '\\n', '[Pretrain+CLS] Ep 3: val_loss=0.5433  val_f1=0.7860',\n'\\n', '[Pretrain+CLS] Ep 4: val_loss=0.5694  val_f1=0.7800', '\\n',\n'[Pretrain+CLS] Ep 5: val_loss=0.5506  val_f1=0.7800', '\\n', '[Pretrain+CLS] Ep\n6: val_loss=0.5522  val_f1=0.7610', '\\n', '[Pretrain+CLS] Ep 7: val_loss=0.5639\nval_f1=0.7362', '\\n', 'Early stopping pretrain+cls.', '\\n', '[Pretrain+CLS] TEST\nmacro-F1=0.7633', '\\n', '[Scratch] Ep 1: val_loss=0.5613  val_f1=0.7577', '\\n',\n'[Scratch] Ep 2: val_loss=0.5619  val_f1=0.7900', '\\n', '[Scratch] Ep 3:\nval_loss=0.5961  val_f1=0.7455', '\\n', '[Scratch] Ep 4: val_loss=0.5377\nval_f1=0.7940', '\\n', '[Scratch] Ep 5: val_loss=0.5305  val_f1=0.7940', '\\n',\n'[Scratch] Ep 6: val_loss=0.5378  val_f1=0.7958', '\\n', '[Scratch] Ep 7:\nval_loss=0.5280  val_f1=0.7999', '\\n', '[Scratch] Ep 8: val_loss=0.5382\nval_f1=0.7900', '\\n', '[Scratch] Ep 9: val_loss=0.5319  val_f1=0.7979', '\\n',\n'[Scratch] Ep 10: val_loss=0.5261  val_f1=0.7999', '\\n', '[Scratch] Ep 11:\nval_loss=0.5525  val_f1=0.7920', '\\n', '[Scratch] Ep 12: val_loss=0.5175\nval_f1=0.7999', '\\n', 'Early stopping scratch.', '\\n', '[Scratch] TEST\nmacro-F1=0.7990', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 69088.10\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 85657.48\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 149636.25\nexamples/s]', '\\n', 'Vocab size: 11  |  Max_len: 96', '\\n', '[Pretrain] epoch\n1/3  loss=1.4602', '\\n', '[Pretrain] epoch 2/3  loss=0.9611', '\\n', '[Pretrain]\nepoch 3/3  loss=0.9371', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[Pretrain+CLS] Ep 1:\nval_loss=0.5683  val_f1=0.7474', '\\n', '[Pretrain+CLS] Ep 2: val_loss=0.5399\nval_f1=0.7720', '\\n', '[Pretrain+CLS] Ep 3: val_loss=0.5348  val_f1=0.7860',\n'\\n', '[Pretrain+CLS] Ep 4: val_loss=0.5361  val_f1=0.7920', '\\n',\n'[Pretrain+CLS] Ep 5: val_loss=0.5362  val_f1=0.7860', '\\n', '[Pretrain+CLS] Ep\n6: val_loss=0.5239  val_f1=0.7999', '\\n', '[Pretrain+CLS] Ep 7: val_loss=0.5311\nval_f1=0.7940', '\\n', '[Pretrain+CLS] Ep 8: val_loss=0.5308  val_f1=0.7959',\n'\\n', '[Pretrain+CLS] Ep 9: val_loss=0.5279  val_f1=0.7979', '\\n',\n'[Pretrain+CLS] Ep 10: val_loss=0.5808  val_f1=0.7880', '\\n', '[Pretrain+CLS] Ep\n11: val_loss=0.5567  val_f1=0.7880', '\\n', 'Early stopping pretrain+cls.', '\\n',\n'[Pretrain+CLS] TEST macro-F1=0.7959', '\\n', '[Scratch] Ep 1: val_loss=0.5792\nval_f1=0.7898', '\\n', '[Scratch] Ep 2: val_loss=0.5582  val_f1=0.7860', '\\n',\n'[Scratch] Ep 3: val_loss=0.5483  val_f1=0.7800', '\\n', '[Scratch] Ep 4:\nval_loss=0.5627  val_f1=0.7559', '\\n', '[Scratch] Ep 5: val_loss=0.5401\nval_f1=0.7880', '\\n', '[Scratch] Ep 6: val_loss=0.5733  val_f1=0.7567', '\\n',\n'Early stopping scratch.', '\\n', '[Scratch] TEST macro-F1=0.7716', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 114474.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 87879.32\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 119156.36\nexamples/s]', '\\n', 'Vocab size: 11  |  Max_len: 96', '\\n', '[Pretrain] epoch\n1/3  loss=1.5733', '\\n', '[Pretrain] epoch 2/3  loss=0.9662', '\\n', '[Pretrain]\nepoch 3/3  loss=0.9382', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[Pretrain+CLS] Ep 1:\nval_loss=0.5751  val_f1=0.7815', '\\n', '[Pretrain+CLS] Ep 2: val_loss=0.5236\nval_f1=0.7898', '\\n', '[Pretrain+CLS] Ep 3: val_loss=0.5335  val_f1=0.7959',\n'\\n', '[Pretrain+CLS] Ep 4: val_loss=0.5252  val_f1=0.7940', '\\n',\n'[Pretrain+CLS] Ep 5: val_loss=0.5336  val_f1=0.7800', '\\n', '[Pretrain+CLS] Ep\n6: val_loss=0.5202  val_f1=0.7979', '\\n', '[Pretrain+CLS] Ep 7: val_loss=0.5207\nval_f1=0.7940', '\\n', '[Pretrain+CLS] Ep 8: val_loss=0.5488  val_f1=0.7878',\n'\\n', '[Pretrain+CLS] Ep 9: val_loss=0.5286  val_f1=0.7999', '\\n',\n'[Pretrain+CLS] Ep 10: val_loss=0.5246  val_f1=0.7979', '\\n', '[Pretrain+CLS] Ep\n11: val_loss=0.5489  val_f1=0.7650', '\\n', '[Pretrain+CLS] Ep 12:\nval_loss=0.5346  val_f1=0.7939', '\\n', '[Pretrain+CLS] Ep 13: val_loss=0.5699\nval_f1=0.7756', '\\n', '[Pretrain+CLS] Ep 14: val_loss=0.5642  val_f1=0.7820',\n'\\n', 'Early stopping pretrain+cls.', '\\n', '[Pretrain+CLS] TEST\nmacro-F1=0.7879', '\\n', '[Scratch] Ep 1: val_loss=0.5622  val_f1=0.7598', '\\n',\n'[Scratch] Ep 2: val_loss=0.5548  val_f1=0.7880', '\\n', '[Scratch] Ep 3:\nval_loss=0.5539  val_f1=0.7720', '\\n', '[Scratch] Ep 4: val_loss=0.5409\nval_f1=0.7840', '\\n', '[Scratch] Ep 5: val_loss=0.5462  val_f1=0.7800', '\\n',\n'[Scratch] Ep 6: val_loss=0.5615  val_f1=0.7720', '\\n', '[Scratch] Ep 7:\nval_loss=0.5301  val_f1=0.7939', '\\n', '[Scratch] Ep 8: val_loss=0.5359\nval_f1=0.7920', '\\n', '[Scratch] Ep 9: val_loss=0.5296  val_f1=0.7920', '\\n',\n'[Scratch] Ep 10: val_loss=0.5342  val_f1=0.7650', '\\n', '[Scratch] Ep 11:\nval_loss=0.5407  val_f1=0.7840', '\\n', '[Scratch] Ep 12: val_loss=0.5334\nval_f1=0.7879', '\\n', 'Early stopping scratch.', '\\n', '[Scratch] TEST\nmacro-F1=0.7970', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 9\nseconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "The execution output does not provide sufficient information to determine\nwhether the script successfully completed its training and evaluation. The only\noutput is 'Using device: cuda,' which does not confirm whether the experiments\nwere executed correctly or if any results were generated. To address this,\nensure that the script provides detailed logs, including progress updates during\ntraining, validation, and testing phases, as well as final metrics and potential\nerrors. These logs are essential for debugging and verifying the experiment\noutcomes.", "The execution output is incomplete and does not provide any meaningful results\nor logs from the training script. The output only mentions 'Execution time: a\nsecond seconds (time limit is 30 minutes).' This indicates that the script did\nnot execute as expected or failed silently. Potential causes could include\nissues with the dataset path, missing dependencies, or runtime errors that were\nnot captured in the output logs. To address this, ensure that the dataset path\n('/home/zxl240011/AI-Scientist-v2/SPR_BENCH/') exists and contains the required\nfiles ('train.csv', 'dev.csv', 'test.csv'). Additionally, check for any\nunhandled exceptions or errors in the script and implement proper logging to\ncapture detailed execution information.", "The output log only indicates the device being used ('cuda') and does not\nprovide any further details about the training process or results. This suggests\nthat the script might not have executed correctly or that it terminated\nprematurely. Possible causes could be an issue with loading the dataset, a\nmissing environment variable (SPR_BENCH_PATH), or an unhandled exception. To\naddress this issue, ensure that the dataset path is correctly set and\naccessible, and add logging or debugging statements to identify where the script\nmight be failing. Additionally, verify that the datasets library and other\ndependencies are properly installed.", "The output log indicates that the script execution completed successfully, but\nthe output is incomplete and does not show any meaningful results such as\ntraining losses, validation metrics, or test performance. This suggests a\npotential bug in the script where the results are not being printed or logged\nproperly. To fix this, ensure that the print statements within the training,\nvalidation, and test evaluation loops are executed correctly and that the\nresults are flushed to the console. Additionally, check if the script terminates\nprematurely before reaching the result logging sections.", "The execution output only shows the device being used ('cuda') and does not\nprovide any information about the actual training process or results. This\nsuggests that the script did not execute fully or did not properly log its\nprogress. A potential cause could be an issue with the data path\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH/' not existing or being inaccessible.\nTo fix this, ensure that the data path is correct and accessible. Additionally,\nadd logging or debugging statements to verify that each stage of the script is\nbeing executed as expected.", "The execution output is minimal and does not provide sufficient information\nabout the experiment's progress or results. The only output is the device being\nused ('cuda') and the execution time ('a second seconds'), which is not adequate\nfor evaluating the performance of the model or its components. This lack of\noutput suggests a potential issue in the implementation of logging or result\nreporting.  **Proposed Fix:** 1. Add print statements or logging throughout the\npipeline to capture and display key metrics such as training loss, validation\nloss, and F1 scores during each epoch. 2. Ensure that the final test results,\nincluding test loss and test F1 score, are printed to the console. 3. Verify\nthat the script properly saves the experiment data to the specified file\n('experiment_data.npy') and that this file contains the expected results. 4.\nCheck for any silent errors or exceptions that may have caused the script to\nterminate prematurely without providing detailed output.", "The output of the training script lacks sufficient information to evaluate its\nsuccess or failure. The only output provided is 'Using device: cuda' and\n'Execution time: a second seconds,' which does not confirm whether the script\ncompleted its tasks, encountered errors, or achieved the desired results. To fix\nthis, ensure that the script provides detailed logs of its progress, including\ndataset loading, model training, validation metrics, and final test results.\nThis will help in identifying potential issues and verifying the experiment's\nsuccess.", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.4915, "best_value": 0.4915}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.802, "best_value": 0.802}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5344, "best_value": 0.5344}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during validation phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.79, "best_value": 0.79}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during test phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5138, "best_value": 0.5138}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during test phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.79, "best_value": 0.79}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.9394, "best_value": 0.9394}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.4907, "best_value": 0.4907}, {"dataset_name": "scratch_cls", "final_value": 0.4985, "best_value": 0.4985}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5301, "best_value": 0.5301}, {"dataset_name": "scratch_cls", "final_value": 0.5265, "best_value": 0.5265}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during the testing phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5149, "best_value": 0.5149}, {"dataset_name": "scratch_cls", "final_value": 0.5085, "best_value": 0.5085}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.8025, "best_value": 0.8025}, {"dataset_name": "scratch_cls", "final_value": 0.7985, "best_value": 0.7985}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the validation phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "scratch_cls", "final_value": 0.7999, "best_value": 0.7999}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the testing phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7809, "best_value": 0.7809}, {"dataset_name": "scratch_cls", "final_value": 0.8, "best_value": 0.8}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.9376, "best_value": 0.9376}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.4978, "best_value": 0.4978}, {"dataset_name": "scratch_cls", "final_value": 0.4927, "best_value": 0.4927}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5365, "best_value": 0.5365}, {"dataset_name": "scratch_cls", "final_value": 0.5175, "best_value": 0.5175}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the training set.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7955, "best_value": 0.7955}, {"dataset_name": "scratch_cls", "final_value": 0.8, "best_value": 0.8}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the validation set.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "scratch_cls", "final_value": 0.7999, "best_value": 0.7999}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the test set.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7633, "best_value": 0.7633}, {"dataset_name": "scratch_cls", "final_value": 0.799, "best_value": 0.799}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test set.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5402, "best_value": 0.5402}, {"dataset_name": "scratch_cls", "final_value": 0.5036, "best_value": 0.5036}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.9371, "best_value": 0.9371}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.4831, "best_value": 0.4831}, {"dataset_name": "scratch_cls", "final_value": 0.517, "best_value": 0.517}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5239, "best_value": 0.5239}, {"dataset_name": "scratch_cls", "final_value": 0.5401, "best_value": 0.5401}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during the testing phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5228, "best_value": 0.5228}, {"dataset_name": "scratch_cls", "final_value": 0.5716, "best_value": 0.5716}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the training phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.8035, "best_value": 0.8035}, {"dataset_name": "scratch_cls", "final_value": 0.7879, "best_value": 0.7879}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the validation phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7999, "best_value": 0.7999}, {"dataset_name": "scratch_cls", "final_value": 0.7898, "best_value": 0.7898}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the testing phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "scratch_cls", "final_value": 0.7716, "best_value": 0.7716}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Measures the error in the pretraining phase.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.9382, "best_value": 0.9382}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.4761, "best_value": 0.4761}, {"dataset_name": "scratch_cls", "final_value": 0.4896, "best_value": 0.4896}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5202, "best_value": 0.5202}, {"dataset_name": "scratch_cls", "final_value": 0.5296, "best_value": 0.5296}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the training dataset.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.8065, "best_value": 0.8065}, {"dataset_name": "scratch_cls", "final_value": 0.7995, "best_value": 0.7995}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the validation dataset.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7999, "best_value": 0.7999}, {"dataset_name": "scratch_cls", "final_value": 0.7939, "best_value": 0.7939}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the test dataset.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.7879, "best_value": 0.7879}, {"dataset_name": "scratch_cls", "final_value": 0.797, "best_value": 0.797}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Measures the error on the test dataset.", "data": [{"dataset_name": "pretrain+cls", "final_value": 0.5321, "best_value": 0.5321}, {"dataset_name": "scratch_cls", "final_value": 0.5112, "best_value": 0.5112}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_test_macro_f1_bar.png", "../../logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_pretrain+cls_confusion_matrix.png"], [], [], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/val_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_train_val_macro_f1.png", "../../logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_val_macro_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_confmat_scratch_cls.png"], ["../../logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/val_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_train_val_macro_f1.png", "../../logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_val_macro_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_confmat_scratch_cls.png"], ["../../logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/val_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_train_val_macro_f1.png", "../../logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_val_macro_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_confmat_pretrain+cls.png"], ["../../logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/val_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_train_val_macro_f1.png", "../../logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_val_macro_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_confmat_scratch_cls.png"], ["../../logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_train_val_loss_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_train_val_macro_f1_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_val_macro_f1_mean_comparison.png", "../../logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_confmat_avg_pretrain+cls.png", "../../logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_confmat_avg_scratch_cls.png"]], "plot_paths": [["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_test_macro_f1_bar.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_pretrain+cls_confusion_matrix.png"], [], [], [], [], [], [], [], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/val_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_pretrain_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_train_val_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_train_val_macro_f1.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_val_macro_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_confmat_scratch_cls.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/val_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_pretrain_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_train_val_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_train_val_macro_f1.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_val_macro_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_confmat_scratch_cls.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/val_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_pretrain_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_train_val_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_train_val_macro_f1.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_val_macro_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_confmat_pretrain+cls.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/val_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_pretrain_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_train_val_loss.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_train_val_macro_f1.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_val_macro_f1_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_confmat_scratch_cls.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_train_val_loss_mean_se.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_train_val_macro_f1_mean_se.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_val_macro_f1_mean_comparison.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_confmat_avg_pretrain+cls.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_a2181959ebf1403e9de09346ecf968c3/sprbench_confmat_avg_scratch_cls.png"]], "plot_analyses": [[{"analysis": "The Macro-F1 curves reveal a steady improvement in both training and validation performance over the epochs, with the validation curve closely tracking the training curve. This indicates that the model is generalizing well to unseen data and is not overfitting. The final Macro-F1 score on the validation set appears to plateau around 0.8, which is promising as it matches the SOTA performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_macro_f1_curves.png"}, {"analysis": "The loss curves show a sharp decrease in cross-entropy loss during the initial epochs, followed by a gradual decline as the model converges. The close alignment between the training and validation loss curves suggests that the model is learning effectively without overfitting.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_loss_curves.png"}, {"analysis": "The test Macro-F1 bar plot indicates that the model achieves a Macro-F1 score of approximately 0.8 on the test set, which aligns with the validation performance. This consistency reinforces the robustness of the model and its ability to generalize to unseen data.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_test_macro_f1_bar.png"}, {"analysis": "The confusion matrix shows a balanced performance across the classes, with a high concentration of correct predictions along the diagonal. This suggests that the model is not biased towards any particular class and is effectively handling the symbolic rule reasoning task.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_85de7cecb4ed48f7907db63927146615_proc_3164419/spr_bench_pretrain+cls_confusion_matrix.png"}], [], [], [], [], [], [], [], [{"analysis": "This plot compares the validation Macro-F1 scores of models trained with pretraining and from scratch. Initially, the pretrained model exhibits a significant performance gap, but the scratch model catches up by epoch 3. The pretrained model shows slight fluctuations, but overall both approaches converge to similar performance levels. This suggests that pretraining provides an initial advantage, but its long-term effect diminishes as training progresses.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/val_f1_comparison.png"}, {"analysis": "The plot shows the loss reduction during pretraining on the SPR Bench dataset. The loss decreases steadily over three epochs, indicating effective learning during pretraining. This validates the utility of the pretraining phase for initializing the model effectively.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_pretrain_loss.png"}, {"analysis": "This plot compares the training and validation losses for both pretrained and scratch models. The pretrained model achieves lower validation loss consistently, indicating better generalization. However, both approaches show a similar trend of decreasing losses, with the scratch model catching up in later epochs. This highlights that pretraining accelerates convergence and improves early-stage performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_train_val_loss.png"}, {"analysis": "The plot compares the Macro-F1 scores for training and validation. Both pretrained and scratch models exhibit rapid improvements initially, with the pretrained model achieving higher scores in the early epochs. By epoch 3, both models reach similar performance levels, indicating that pretraining primarily benefits early-stage learning. The consistent validation performance suggests good generalization for both approaches.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_train_val_macro_f1.png"}, {"analysis": "This plot focuses on the validation Macro-F1 scores for pretrained and scratch models. Both models show convergence to similar performance levels around epoch 3, with minor fluctuations. The pretrained model has a faster initial improvement, confirming its advantage in early training stages. This reinforces the finding that pretraining accelerates early learning but does not significantly impact final performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_val_macro_f1_comparison.png"}, {"analysis": "The confusion matrix for the scratch model on the SPR Bench dataset reveals balanced performance across classes. The diagonal dominance suggests that the model is effective at correctly classifying both classes, with minimal misclassifications. This indicates that the scratch model achieves good class-wise performance, complementing the Macro-F1 results.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1df5dff6370c49db8d08f6b545ba7392_proc_3171078/sprbench_confmat_scratch_cls.png"}], [{"analysis": "The plot compares the validation Macro-F1 scores of two models: one trained from scratch and another leveraging pretraining with classification (Pretrain+CLS). The Scratch model consistently outperforms Pretrain+CLS in validation Macro-F1, particularly after epoch 4, where Pretrain+CLS performance sharply declines. This suggests that the pretrained model might struggle with overfitting or adapting to the SPR task, whereas the Scratch model shows more stable and superior performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/val_f1_comparison.png"}, {"analysis": "This plot illustrates the loss reduction during the pretraining phase on the SPR Bench dataset. The loss decreases rapidly from epoch 1 to epoch 2 and flattens out by epoch 3, indicating effective convergence during pretraining. However, the final loss value remains above zero, suggesting room for further optimization or refinement of the pretraining process.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_pretrain_loss.png"}, {"analysis": "This plot compares the training and validation loss for both Pretrain+CLS and Scratch models. Both models exhibit a steady decline in training and validation loss, but the Scratch model achieves lower validation loss consistently, indicating better generalization. The Pretrain+CLS model shows slightly higher variance in validation loss, which might point to overfitting or instability in adapting to the task.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_train_val_loss.png"}, {"analysis": "This plot compares training and validation Macro-F1 scores for Pretrain+CLS and Scratch models. The Scratch model achieves higher and more stable Macro-F1 scores for both training and validation, indicating better performance and generalization. The Pretrain+CLS model shows a decline in validation Macro-F1 after epoch 4, reinforcing the observation of potential overfitting or task adaptation issues.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_train_val_macro_f1.png"}, {"analysis": "This plot focuses on the validation Macro-F1 comparison, reiterating the superior performance of the Scratch model over Pretrain+CLS. The Scratch model's validation Macro-F1 stabilizes above 0.79, while the Pretrain+CLS model declines sharply after epoch 4, suggesting a lack of robustness in the pretrained approach for this task.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_val_macro_f1_comparison.png"}, {"analysis": "The confusion matrix for the Scratch model shows a balanced distribution of true positive and true negative predictions, indicating effective classification performance. The absence of significant imbalance in predictions suggests that the model is not biased towards any particular class and performs well across the dataset.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/sprbench_confmat_scratch_cls.png"}], [{"analysis": "The plot shows a comparison between the validation Macro-F1 scores of two training strategies: 'Pretrain+CLS' and 'Scratch'. The 'Pretrain+CLS' model consistently outperforms the 'Scratch' model across epochs, achieving a more stable and higher validation Macro-F1 score. This indicates that pretraining provides a significant advantage in capturing patterns in the SPR task. The 'Scratch' model exhibits fluctuations, suggesting instability during training.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/val_f1_comparison.png"}, {"analysis": "This plot depicts the pretraining loss over three epochs. The loss decreases sharply in the first epoch and then flattens out, indicating effective learning during pretraining. The rapid convergence suggests that the model quickly adapts to the SPR dataset during the pretraining phase.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_pretrain_loss.png"}, {"analysis": "The plot compares the training and validation losses for both 'Pretrain+CLS' and 'Scratch' models. The 'Pretrain+CLS' model demonstrates lower training and validation losses compared to the 'Scratch' model, suggesting better generalization and learning. The validation loss for the 'Scratch' model is more erratic, indicating potential overfitting or instability.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_train_val_loss.png"}, {"analysis": "This plot tracks the Macro-F1 scores for training and validation across epochs for both models. The 'Pretrain+CLS' model achieves higher and more stable Macro-F1 scores for both training and validation, reflecting better performance and generalization. The 'Scratch' model shows fluctuations in validation Macro-F1, further supporting the observation of instability.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_train_val_macro_f1.png"}, {"analysis": "This plot reaffirms the comparison of validation Macro-F1 scores between 'Pretrain+CLS' and 'Scratch'. The 'Pretrain+CLS' model consistently achieves higher scores, while the 'Scratch' model shows significant drops in performance at certain epochs. This further highlights the advantages of pretraining in stabilizing and improving performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_val_macro_f1_comparison.png"}, {"analysis": "The confusion matrix for the 'Pretrain+CLS' model shows a good balance in predictions, with high values along the diagonal. This indicates that the model effectively distinguishes between the two classes in the SPR_BENCH dataset. The results suggest strong performance in classification tasks.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/sprbench_confmat_pretrain+cls.png"}], [{"analysis": "This plot compares the validation Macro-F1 scores of two models: one pretrained with a classification head (Pretrain+CLS) and another trained from scratch (Scratch). The Pretrain+CLS model consistently outperforms the Scratch model across most epochs, indicating that pretraining provides a significant advantage in capturing the symbolic dependencies. The fluctuations in both curves suggest that further hyperparameter tuning or regularization might be needed to stabilize the training.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/val_f1_comparison.png"}, {"analysis": "This plot shows the language model loss during the pretraining phase on the SPR Bench dataset. The loss decreases rapidly in the first epoch and continues to decline steadily over the next two epochs, indicating effective learning during pretraining. The consistent drop in loss suggests that the model is successfully capturing the underlying patterns within the symbolic sequences.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_pretrain_loss.png"}, {"analysis": "This plot compares the training and validation losses for both Pretrain+CLS and Scratch models over 14 epochs. The Pretrain+CLS model achieves lower training and validation losses compared to the Scratch model, demonstrating its superior ability to generalize. The validation loss for the Pretrain+CLS model is consistently lower than its training loss, suggesting good generalization and no signs of overfitting.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_train_val_loss.png"}, {"analysis": "This plot tracks the Macro-F1 scores for training and validation sets for both Pretrain+CLS and Scratch models. The Pretrain+CLS model shows better performance on both training and validation sets, with a more stable and higher Macro-F1 score. The Scratch model exhibits a larger gap between training and validation scores, indicating potential issues with generalization.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_train_val_macro_f1.png"}, {"analysis": "This plot again compares the validation Macro-F1 scores for the Pretrain+CLS and Scratch models. The Pretrain+CLS model consistently outperforms the Scratch model, aligning with earlier observations. The fluctuations in the Scratch model's performance suggest instability, which could be addressed with better training strategies.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_val_macro_f1_comparison.png"}, {"analysis": "This confusion matrix evaluates the performance of the Scratch model on the SPR Bench dataset. The diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. The matrix shows a moderate balance between correct and incorrect predictions, but there is room for improvement in the model's classification accuracy.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/sprbench_confmat_scratch_cls.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model performs well on both training and validation\nsets, achieving a Macro-F1 score of approximately 0.8, which matches the SOTA\nperformance. The loss curves show effective learning and convergence, while the\nconfusion matrix highlights balanced predictions across classes. Overall, the\nresults suggest that the model is robust and generalizes well to unseen data.", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "The plots collectively highlight the effect of pretraining on model performance.\nPretraining accelerates early learning and improves initial performance, as\nevidenced by faster loss reduction and higher early Macro-F1 scores. However,\nthe scratch model catches up in later epochs, leading to comparable final\nperformance. The confusion matrix confirms balanced class-wise performance,\nsuggesting effective learning by both models. These findings emphasize the\nutility of pretraining for rapid convergence while demonstrating the robustness\nof the scratch approach in achieving competitive results.", "The analysis highlights that the Scratch model consistently outperforms the\nPretrain+CLS model across multiple metrics, including validation Macro-F1 and\nloss. The Pretrain+CLS model suffers from overfitting or instability,\nparticularly after epoch 4. The Scratch model demonstrates better generalization\nand task adaptation, as evidenced by lower validation loss, higher Macro-F1\nscores, and a balanced confusion matrix.", "The experimental results demonstrate that the 'Pretrain+CLS' model outperforms\nthe 'Scratch' model in terms of validation Macro-F1 score, loss reduction, and\nstability. Pretraining significantly enhances the model's ability to generalize\nand perform well on the SPR_BENCH dataset. The confusion matrix further confirms\nthe effectiveness of the 'Pretrain+CLS' model in classification tasks.", "The plots collectively demonstrate the superiority of the Pretrain+CLS model\nover the Scratch model in terms of Macro-F1 score and loss metrics. Pretraining\nappears to significantly enhance the model's ability to capture complex\ndependencies in symbolic sequences, leading to better generalization and\nstability. However, fluctuations in performance and potential overfitting in the\nScratch model indicate areas for further optimization.", "[]"], "exec_time": [5.40659499168396, 0.8482198715209961, 1.0357944965362549, 1.0691850185394287, 1.1877150535583496, 1.073730707168579, 1.0372543334960938, 1.1618094444274902, 6.559773683547974, 7.9482645988464355, 7.812370300292969, 9.19434380531311, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], [], [], [], [], [], [], [], ["['SPR Bench']"], ["[\"SPR Bench\"]"], ["['SPR_BENCH']"], ["['SPR Bench']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------- paths -------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nkeys = list(experiment_data.keys())\ncolors = plt.cm.tab10.colors\n\n# ----------------- 1) Macro-F1 curves --------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[k].get(\"epochs\", [])\n        tr_f1 = experiment_data[k][\"metrics\"].get(\"train_macro_f1\", [])\n        val_f1 = experiment_data[k][\"metrics\"].get(\"val_macro_f1\", [])\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_f1, \"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_f1, \"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Train dashed, Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 plot: {e}\")\n    plt.close()\n\n# ------------------- 2) Loss curves ----------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[k].get(\"epochs\", [])\n        tr_loss = experiment_data[k][\"losses\"].get(\"train\", [])\n        val_loss = experiment_data[k][\"losses\"].get(\"val\", [])\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_loss, \"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_loss, \"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (Train dashed, Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss plot: {e}\")\n    plt.close()\n\n# ---------------- 3) Test Macro-F1 bar chart -------------\ntry:\n    test_scores = {k: experiment_data[k].get(\"test_macro_f1\", np.nan) for k in keys}\n    plt.figure()\n    plt.bar(\n        range(len(test_scores)),\n        list(test_scores.values()),\n        tick_label=list(test_scores.keys()),\n    )\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Test Macro-F1 per Experiment\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_macro_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Test Macro-F1 bar: {e}\")\n    plt.close()\n\n# -------- 4) Confusion matrix for test predictions -------\ntry:\n    for idx, k in enumerate(keys[:5]):  # plot at most 5\n        preds = experiment_data[k].get(\"predictions\")\n        gts = experiment_data[k].get(\"ground_truth\")\n        if preds is None or gts is None or len(preds) == 0:\n            continue\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{k} Confusion Matrix (Test Set)\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_{k}_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Confusion Matrix: {e}\")\n    plt.close()\n\n# ---------------- print numeric summary ------------------\nprint(\n    \"Test Macro-F1 scores:\",\n    {k: experiment_data[k].get(\"test_macro_f1\", np.nan) for k in keys},\n)\n", null, null, null, null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef cmatrix(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\n# ---------- plot 1: pretraining loss ----------\ntry:\n    ed = experiment_data.get(\"pretrain+cls\", {})\n    plt.figure()\n    plt.plot(\n        range(1, len(ed.get(\"losses\", {}).get(\"pretrain\", [])) + 1),\n        ed.get(\"losses\", {}).get(\"pretrain\", []),\n        marker=\"o\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Pretraining LM Loss (SPR Bench)\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"sprbench_pretrain_loss.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretraining loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: train / val loss ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:blue\", \"tab:orange\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep, ed[\"losses\"][\"train\"], label=f\"{tag} train\", linestyle=\"--\", color=color\n        )\n        plt.plot(\n            ep, ed[\"losses\"][\"val\"], label=f\"{tag} val\", linestyle=\"-\", color=color\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Train / Validation Loss (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: train / val macro-F1 ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:green\", \"tab:red\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"train_macro_f1\"],\n            label=f\"{tag} train\",\n            linestyle=\"--\",\n            color=color,\n        )\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"val_macro_f1\"],\n            label=f\"{tag} val\",\n            linestyle=\"-\",\n            color=color,\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Train / Validation Macro-F1 (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_macro_f1.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val macro-F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 4: val macro-F1 comparison ----------\ntry:\n    plt.figure()\n    for tag, style in zip([\"pretrain+cls\", \"scratch_cls\"], [\"-o\", \"-s\"]):\n        ed = experiment_data[tag]\n        plt.plot(ed[\"epochs\"], ed[\"metrics\"][\"val_macro_f1\"], style, label=tag)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_val_macro_f1_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val macro-F1 comparison plot: {e}\")\n    plt.close()\n\n# ---------- plot 5: confusion matrix ----------\ntry:\n    # pick best experiment by last val macro-F1\n    best_tag = max(\n        [\"pretrain+cls\", \"scratch_cls\"],\n        key=lambda t: experiment_data[t][\"metrics\"][\"val_macro_f1\"][-1],\n    )\n    ed = experiment_data[best_tag]\n    y_true = np.array(ed[\"ground_truth\"])\n    y_pred = np.array(ed[\"predictions\"])\n    num_classes = len(np.unique(y_true))\n    cm = cmatrix(y_true, y_pred, num_classes)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix (SPR Bench) \u2013 {best_tag}\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"sprbench_confmat_{best_tag}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef cmatrix(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\n# ---------- plot 1: pretraining loss ----------\ntry:\n    ed = experiment_data.get(\"pretrain+cls\", {})\n    plt.figure()\n    plt.plot(\n        range(1, len(ed.get(\"losses\", {}).get(\"pretrain\", [])) + 1),\n        ed.get(\"losses\", {}).get(\"pretrain\", []),\n        marker=\"o\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Pretraining LM Loss (SPR Bench)\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"sprbench_pretrain_loss.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretraining loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: train / val loss ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:blue\", \"tab:orange\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep, ed[\"losses\"][\"train\"], label=f\"{tag} train\", linestyle=\"--\", color=color\n        )\n        plt.plot(\n            ep, ed[\"losses\"][\"val\"], label=f\"{tag} val\", linestyle=\"-\", color=color\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Train / Validation Loss (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: train / val macro-F1 ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:green\", \"tab:red\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"train_macro_f1\"],\n            label=f\"{tag} train\",\n            linestyle=\"--\",\n            color=color,\n        )\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"val_macro_f1\"],\n            label=f\"{tag} val\",\n            linestyle=\"-\",\n            color=color,\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Train / Validation Macro-F1 (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_macro_f1.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val macro-F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 4: val macro-F1 comparison ----------\ntry:\n    plt.figure()\n    for tag, style in zip([\"pretrain+cls\", \"scratch_cls\"], [\"-o\", \"-s\"]):\n        ed = experiment_data[tag]\n        plt.plot(ed[\"epochs\"], ed[\"metrics\"][\"val_macro_f1\"], style, label=tag)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_val_macro_f1_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val macro-F1 comparison plot: {e}\")\n    plt.close()\n\n# ---------- plot 5: confusion matrix ----------\ntry:\n    # pick best experiment by last val macro-F1\n    best_tag = max(\n        [\"pretrain+cls\", \"scratch_cls\"],\n        key=lambda t: experiment_data[t][\"metrics\"][\"val_macro_f1\"][-1],\n    )\n    ed = experiment_data[best_tag]\n    y_true = np.array(ed[\"ground_truth\"])\n    y_pred = np.array(ed[\"predictions\"])\n    num_classes = len(np.unique(y_true))\n    cm = cmatrix(y_true, y_pred, num_classes)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix (SPR Bench) \u2013 {best_tag}\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"sprbench_confmat_{best_tag}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef cmatrix(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\n# ---------- plot 1: pretraining loss ----------\ntry:\n    ed = experiment_data.get(\"pretrain+cls\", {})\n    plt.figure()\n    plt.plot(\n        range(1, len(ed.get(\"losses\", {}).get(\"pretrain\", [])) + 1),\n        ed.get(\"losses\", {}).get(\"pretrain\", []),\n        marker=\"o\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Pretraining LM Loss (SPR Bench)\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"sprbench_pretrain_loss.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretraining loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: train / val loss ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:blue\", \"tab:orange\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep, ed[\"losses\"][\"train\"], label=f\"{tag} train\", linestyle=\"--\", color=color\n        )\n        plt.plot(\n            ep, ed[\"losses\"][\"val\"], label=f\"{tag} val\", linestyle=\"-\", color=color\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Train / Validation Loss (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: train / val macro-F1 ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:green\", \"tab:red\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"train_macro_f1\"],\n            label=f\"{tag} train\",\n            linestyle=\"--\",\n            color=color,\n        )\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"val_macro_f1\"],\n            label=f\"{tag} val\",\n            linestyle=\"-\",\n            color=color,\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Train / Validation Macro-F1 (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_macro_f1.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val macro-F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 4: val macro-F1 comparison ----------\ntry:\n    plt.figure()\n    for tag, style in zip([\"pretrain+cls\", \"scratch_cls\"], [\"-o\", \"-s\"]):\n        ed = experiment_data[tag]\n        plt.plot(ed[\"epochs\"], ed[\"metrics\"][\"val_macro_f1\"], style, label=tag)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_val_macro_f1_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val macro-F1 comparison plot: {e}\")\n    plt.close()\n\n# ---------- plot 5: confusion matrix ----------\ntry:\n    # pick best experiment by last val macro-F1\n    best_tag = max(\n        [\"pretrain+cls\", \"scratch_cls\"],\n        key=lambda t: experiment_data[t][\"metrics\"][\"val_macro_f1\"][-1],\n    )\n    ed = experiment_data[best_tag]\n    y_true = np.array(ed[\"ground_truth\"])\n    y_pred = np.array(ed[\"predictions\"])\n    num_classes = len(np.unique(y_true))\n    cm = cmatrix(y_true, y_pred, num_classes)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix (SPR Bench) \u2013 {best_tag}\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"sprbench_confmat_{best_tag}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef cmatrix(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\n# ---------- plot 1: pretraining loss ----------\ntry:\n    ed = experiment_data.get(\"pretrain+cls\", {})\n    plt.figure()\n    plt.plot(\n        range(1, len(ed.get(\"losses\", {}).get(\"pretrain\", [])) + 1),\n        ed.get(\"losses\", {}).get(\"pretrain\", []),\n        marker=\"o\",\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Pretraining LM Loss (SPR Bench)\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"sprbench_pretrain_loss.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretraining loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: train / val loss ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:blue\", \"tab:orange\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep, ed[\"losses\"][\"train\"], label=f\"{tag} train\", linestyle=\"--\", color=color\n        )\n        plt.plot(\n            ep, ed[\"losses\"][\"val\"], label=f\"{tag} val\", linestyle=\"-\", color=color\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Train / Validation Loss (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: train / val macro-F1 ----------\ntry:\n    plt.figure()\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], [\"tab:green\", \"tab:red\"]):\n        ed = experiment_data[tag]\n        ep = ed[\"epochs\"]\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"train_macro_f1\"],\n            label=f\"{tag} train\",\n            linestyle=\"--\",\n            color=color,\n        )\n        plt.plot(\n            ep,\n            ed[\"metrics\"][\"val_macro_f1\"],\n            label=f\"{tag} val\",\n            linestyle=\"-\",\n            color=color,\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Train / Validation Macro-F1 (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_macro_f1.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val macro-F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 4: val macro-F1 comparison ----------\ntry:\n    plt.figure()\n    for tag, style in zip([\"pretrain+cls\", \"scratch_cls\"], [\"-o\", \"-s\"]):\n        ed = experiment_data[tag]\n        plt.plot(ed[\"epochs\"], ed[\"metrics\"][\"val_macro_f1\"], style, label=tag)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Comparison (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_val_macro_f1_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val macro-F1 comparison plot: {e}\")\n    plt.close()\n\n# ---------- plot 5: confusion matrix ----------\ntry:\n    # pick best experiment by last val macro-F1\n    best_tag = max(\n        [\"pretrain+cls\", \"scratch_cls\"],\n        key=lambda t: experiment_data[t][\"metrics\"][\"val_macro_f1\"][-1],\n    )\n    ed = experiment_data[best_tag]\n    y_true = np.array(ed[\"ground_truth\"])\n    y_pred = np.array(ed[\"predictions\"])\n    num_classes = len(np.unique(y_true))\n    cm = cmatrix(y_true, y_pred, num_classes)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix (SPR Bench) \u2013 {best_tag}\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"sprbench_confmat_{best_tag}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load many runs ----------\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_94f1f36a6e4940b4a95927c258449669_proc_3171077/experiment_data.npy\",\n        \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8dc7b3be3219466b93f4191cd76ac2fe_proc_3171075/experiment_data.npy\",\n        \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_da70709a9e304e228cd429fab203a01c_proc_3171076/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    root_dir = os.getenv(\"AI_SCIENTIST_ROOT\", os.getcwd())\n    for p in experiment_data_path_list:\n        full = os.path.join(root_dir, p)\n        if os.path.isfile(full):\n            all_experiment_data.append(np.load(full, allow_pickle=True).item())\n        else:\n            print(f\"Warning: file not found {full}\")\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# No data \u2013 nothing to do\nif not all_experiment_data:\n    print(\"No experiment data found; exiting.\")\n    quit()\n\ntags = list(all_experiment_data[0].keys())  # assume same keys\n\n\ndef stack_metric(tag, split, metric_key):\n    \"\"\"Return epochs, mean, std_error for given metric across runs.\"\"\"\n    # gather arrays, align by minimum length\n    arrays = []\n    epochs = None\n    for run in all_experiment_data:\n        ed = run[tag]\n        if split == \"losses\":\n            arr = np.asarray(ed[\"losses\"][metric_key])\n        else:\n            arr = np.asarray(ed[\"metrics\"][metric_key])\n        arrays.append(arr)\n        if epochs is None:\n            epochs = np.asarray(ed[\"epochs\"])\n        else:\n            epochs = epochs[: len(arr)]\n    min_len = min(map(len, arrays))\n    arrays = [a[:min_len] for a in arrays]\n    epochs = epochs[:min_len]\n    stacked = np.stack(arrays, axis=0)  # runs x epochs\n    mean = stacked.mean(axis=0)\n    se = stacked.std(axis=0, ddof=1) / np.sqrt(stacked.shape[0])\n    return epochs, mean, se\n\n\n# ---------- plot 1: Train / Val Loss with mean \u00b1 SE ----------\ntry:\n    plt.figure()\n    colors = dict(pretrain_plus_cls=\"tab:blue\", scratch_cls=\"tab:orange\")\n    legend_items = []\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], colors.values()):\n        ep, train_mean, train_se = stack_metric(tag, \"losses\", \"train\")\n        _, val_mean, val_se = stack_metric(tag, \"losses\", \"val\")\n        plt.plot(ep, train_mean, linestyle=\"--\", color=color, label=f\"{tag} train mean\")\n        plt.fill_between(\n            ep,\n            train_mean - train_se,\n            train_mean + train_se,\n            color=color,\n            alpha=0.2,\n            label=f\"{tag} train \u00b1SE\",\n        )\n        plt.plot(ep, val_mean, linestyle=\"-\", color=color, label=f\"{tag} val mean\")\n        plt.fill_between(\n            ep,\n            val_mean - val_se,\n            val_mean + val_se,\n            color=color,\n            alpha=0.4,\n            label=f\"{tag} val \u00b1SE\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Train / Validation Loss with Mean \u00b1 SE (SPR Bench)\")\n    plt.legend(fontsize=8, ncol=2)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_loss_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated train/val loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Train / Val Macro-F1 with mean \u00b1 SE ----------\ntry:\n    plt.figure()\n    colors = dict(pretrain_plus_cls=\"tab:green\", scratch_cls=\"tab:red\")\n    for tag, color in zip([\"pretrain+cls\", \"scratch_cls\"], colors.values()):\n        ep, train_mean, train_se = stack_metric(tag, \"metrics\", \"train_macro_f1\")\n        _, val_mean, val_se = stack_metric(tag, \"metrics\", \"val_macro_f1\")\n        plt.plot(ep, train_mean, linestyle=\"--\", color=color, label=f\"{tag} train mean\")\n        plt.fill_between(\n            ep,\n            train_mean - train_se,\n            train_mean + train_se,\n            color=color,\n            alpha=0.2,\n            label=f\"{tag} train \u00b1SE\",\n        )\n        plt.plot(ep, val_mean, linestyle=\"-\", color=color, label=f\"{tag} val mean\")\n        plt.fill_between(\n            ep,\n            val_mean - val_se,\n            val_mean + val_se,\n            color=color,\n            alpha=0.4,\n            label=f\"{tag} val \u00b1SE\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Train / Validation Macro-F1 with Mean \u00b1 SE (SPR Bench)\")\n    plt.legend(fontsize=8, ncol=2)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_train_val_macro_f1_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated train/val macro-F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: Validation Macro-F1 mean comparison ----------\ntry:\n    plt.figure()\n    for tag, marker in zip([\"pretrain+cls\", \"scratch_cls\"], [\"o\", \"s\"]):\n        ep, val_mean, val_se = stack_metric(tag, \"metrics\", \"val_macro_f1\")\n        plt.errorbar(ep, val_mean, yerr=val_se, fmt=f\"-{marker}\", capsize=3, label=tag)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Macro-F1\")\n    plt.title(\"Validation Macro-F1 Mean \u00b1 SE Comparison (SPR Bench)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"sprbench_val_macro_f1_mean_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val macro-F1 mean comparison plot: {e}\")\n    plt.close()\n\n\n# ---------- plot 4: Aggregated Confusion Matrix ----------\ndef cmatrix(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\ntry:\n    for tag in [\"pretrain+cls\", \"scratch_cls\"]:\n        # aggregate matrices\n        total_cm = None\n        for run in all_experiment_data:\n            ed = run[tag]\n            y_true = np.asarray(ed.get(\"ground_truth\", []))\n            y_pred = np.asarray(ed.get(\"predictions\", []))\n            if y_true.size == 0:  # skip if no data\n                continue\n            cm = cmatrix(y_true, y_pred, num_classes=len(np.unique(y_true)))\n            total_cm = cm if total_cm is None else total_cm + cm\n        if total_cm is None:\n            continue\n        avg_cm = total_cm / len(all_experiment_data)\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(avg_cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"Average Confusion Matrix (SPR Bench) \u2013 {tag}\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"sprbench_confmat_avg_{tag}.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print final metrics ----------\ntry:\n    for tag in [\"pretrain+cls\", \"scratch_cls\"]:\n        _, val_mean, val_se = stack_metric(tag, \"metrics\", \"val_macro_f1\")\n        print(f\"{tag} final Val Macro-F1: {val_mean[-1]:.4f} \u00b1 {val_se[-1]:.4f}\")\nexcept Exception as e:\n    print(f\"Error printing final metrics: {e}\")\n"], "plot_plan": ["We first load experiment_data.npy and extract the available experiment keys (e.g. \u201cpretrain+cls\u201d).  For every key we read stored epochs, train/val losses, train/val macro-F1, test macro-F1, predictions and ground-truth labels if present.  We then produce four independent plots wrapped in try-except blocks: (1) train vs. validation macro-F1 curves, (2) train vs. validation loss curves, (3) bar chart summarising test macro-F1 per experiment, and (4) a confusion-matrix heat-map for the test split (only plotted once per experiment key, capped at 5 to avoid flooding).  Each figure is titled clearly, saved with the dataset name embedded, and closed afterward to free memory.  All plots live inside the ./working directory which is created if necessary.  Colours come from matplotlib\u2019s tab10 palette, and figures use consistent naming conventions like \u201cspr_bench_pretrain+cls_macro_f1_curves.png\u201d.  The script ends by printing the test Macro-F1 scores so they are visible in logs.", null, null, null, null, null, null, null, "We will load the saved experiment_data.npy file, extract the stored losses, macro-F1 scores, epochs and prediction arrays, and then create a concise set of figures illustrating model behavior.  \nFirst, we plot the pre-training language-model loss curve for the Pretrain+CLS setup.  \nNext, we generate combined line plots for train/validation loss and train/validation macro-F1, overlaying the Scratch and Pretrain+CLS runs for easy comparison.  \nA dedicated figure will highlight the validation macro-F1 of both systems (replicating and refining the quick plot produced during training).  \nFinally, we compute a confusion matrix for the best Val-F1 experiment (choosing the one whose validation macro-F1 array ends with the higher value) and render it as a heat-map.  \nEach plot is wrapped in its own try-except block, saved with descriptive filenames into working_dir, given clear titles/subtitles, and the figures are closed whether or not an error occurs.  \nThe script follows the provided matplotlib usage rules and prints nothing beyond optional exception messages.", null, null, null, null], "ablation_name": [null, "No-Pretrain Scratch Classifier", "Remove/Replace Positional Embeddings", "Freeze-Encoder Fine-tuning", "CLS Token Pooling vs Mean Pooling", "Masked LM Pre-training Objective (MLM instead of Causal LM)", "Embedding\u2013LM-Head Weight Tying", "ReLU-vs-GELU Activation Ablation", null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved experiment_data.npy file from the \u201cworking\u201d\ndirectory, iterate over every stored dataset (here only \u201cpretrain+cls\u201d), and\ncompute the best (minimum for losses, maximum for F1 scores) or stored test-set\nvalues. It then prints each dataset name followed by clearly labelled metrics\nsuch as \u201cPre-training loss,\u201d \u201cBest validation macro F1 score,\u201d etc. No plots are\nproduced and the code runs immediately on execution without needing a special\nentry point.", "", "", "", "", "", "", "", "The script loads the saved experiment results, iterates over each dataset entry,\nand for every recorded metric or loss determines the single most informative\nvalue (minimum for losses, maximum for scores). It then prints the dataset name\nfollowed by clearly-labelled metrics such as \u201ctraining loss\u201d, \u201cvalidation macro\nF1 score\u201d, and \u201ctest macro F1 score\u201d. The code executes immediately without any\nspecial entry point and produces no plots.", "The script loads the saved experiment results, iterates over each dataset entry,\nand for every recorded metric or loss determines the single most informative\nvalue (minimum for losses, maximum for scores). It then prints the dataset name\nfollowed by clearly-labelled metrics such as \u201ctraining loss\u201d, \u201cvalidation macro\nF1 score\u201d, and \u201ctest macro F1 score\u201d. The code executes immediately without any\nspecial entry point and produces no plots.", "The script loads the saved experiment results, iterates over each dataset entry,\nand for every recorded metric or loss determines the single most informative\nvalue (minimum for losses, maximum for scores). It then prints the dataset name\nfollowed by clearly-labelled metrics such as \u201ctraining loss\u201d, \u201cvalidation macro\nF1 score\u201d, and \u201ctest macro F1 score\u201d. The code executes immediately without any\nspecial entry point and produces no plots.", "The script loads the saved experiment results, iterates over each dataset entry,\nand for every recorded metric or loss determines the single most informative\nvalue (minimum for losses, maximum for scores). It then prints the dataset name\nfollowed by clearly-labelled metrics such as \u201ctraining loss\u201d, \u201cvalidation macro\nF1 score\u201d, and \u201ctest macro F1 score\u201d. The code executes immediately without any\nspecial entry point and produces no plots.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------- locate and load data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to pick best values --------------------\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_f1(values):\n    return max(values) if values else None\n\n\n# -------------------- print metrics --------------------\nfor ds_name, ds in experiment_data.items():\n    print(f\"Dataset: {ds_name}\")\n\n    # losses\n    pre_loss_best = best_loss(ds[\"losses\"].get(\"pretrain\", []))\n    train_loss_best = best_loss(ds[\"losses\"].get(\"train\", []))\n    val_loss_best = best_loss(ds[\"losses\"].get(\"val\", []))\n\n    # F1 scores\n    train_f1_best = best_f1(ds[\"metrics\"].get(\"train_macro_f1\", []))\n    val_f1_best = best_f1(ds[\"metrics\"].get(\"val_macro_f1\", []))\n\n    # test metrics\n    test_loss = ds.get(\"test_loss\")\n    test_f1 = ds.get(\"test_macro_f1\")\n\n    # print all metrics with clear labels\n    if pre_loss_best is not None:\n        print(f\"Pre-training loss: {pre_loss_best:.4f}\")\n    if train_loss_best is not None:\n        print(f\"Best training loss: {train_loss_best:.4f}\")\n    if train_f1_best is not None:\n        print(f\"Best training macro F1 score: {train_f1_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"Best validation loss: {val_loss_best:.4f}\")\n    if val_f1_best is not None:\n        print(f\"Best validation macro F1 score: {val_f1_best:.4f}\")\n    if test_loss is not None:\n        print(f\"Test loss: {test_loss:.4f}\")\n    if test_f1 is not None:\n        print(f\"Test macro F1 score: {test_f1:.4f}\")\n", "", "", "", "", "", "", "", "import os\nimport numpy as np\n\n# 0. Working directory and file loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Helper label maps\nloss_name_map = {\n    \"pretrain\": \"pretraining loss\",\n    \"train\": \"training loss\",\n    \"val\": \"validation loss\",\n}\nmetric_name_map = {\n    \"train_macro_f1\": \"training macro F1 score\",\n    \"val_macro_f1\": \"validation macro F1 score\",\n}\n\n# 2-5. Extract and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # ---- Losses ----\n    for key, values in ds_dict.get(\"losses\", {}).items():\n        if not values:\n            continue\n        label = loss_name_map.get(key, f\"{key} loss\")\n        best_value = min(values)  # lower is better for losses\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Epoch metrics ----\n    for key, values in ds_dict.get(\"metrics\", {}).items():\n        if not values:\n            continue\n        label = metric_name_map.get(key, key)\n        best_value = max(values)  # higher is better for F1\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Test metrics ----\n    if \"test_macro_f1\" in ds_dict:\n        print(f\"test macro F1 score: {ds_dict['test_macro_f1']:.4f}\")\n    if \"test_loss\" in ds_dict:\n        print(f\"test loss: {ds_dict['test_loss']:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Working directory and file loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Helper label maps\nloss_name_map = {\n    \"pretrain\": \"pretraining loss\",\n    \"train\": \"training loss\",\n    \"val\": \"validation loss\",\n}\nmetric_name_map = {\n    \"train_macro_f1\": \"training macro F1 score\",\n    \"val_macro_f1\": \"validation macro F1 score\",\n}\n\n# 2-5. Extract and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # ---- Losses ----\n    for key, values in ds_dict.get(\"losses\", {}).items():\n        if not values:\n            continue\n        label = loss_name_map.get(key, f\"{key} loss\")\n        best_value = min(values)  # lower is better for losses\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Epoch metrics ----\n    for key, values in ds_dict.get(\"metrics\", {}).items():\n        if not values:\n            continue\n        label = metric_name_map.get(key, key)\n        best_value = max(values)  # higher is better for F1\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Test metrics ----\n    if \"test_macro_f1\" in ds_dict:\n        print(f\"test macro F1 score: {ds_dict['test_macro_f1']:.4f}\")\n    if \"test_loss\" in ds_dict:\n        print(f\"test loss: {ds_dict['test_loss']:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Working directory and file loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Helper label maps\nloss_name_map = {\n    \"pretrain\": \"pretraining loss\",\n    \"train\": \"training loss\",\n    \"val\": \"validation loss\",\n}\nmetric_name_map = {\n    \"train_macro_f1\": \"training macro F1 score\",\n    \"val_macro_f1\": \"validation macro F1 score\",\n}\n\n# 2-5. Extract and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # ---- Losses ----\n    for key, values in ds_dict.get(\"losses\", {}).items():\n        if not values:\n            continue\n        label = loss_name_map.get(key, f\"{key} loss\")\n        best_value = min(values)  # lower is better for losses\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Epoch metrics ----\n    for key, values in ds_dict.get(\"metrics\", {}).items():\n        if not values:\n            continue\n        label = metric_name_map.get(key, key)\n        best_value = max(values)  # higher is better for F1\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Test metrics ----\n    if \"test_macro_f1\" in ds_dict:\n        print(f\"test macro F1 score: {ds_dict['test_macro_f1']:.4f}\")\n    if \"test_loss\" in ds_dict:\n        print(f\"test loss: {ds_dict['test_loss']:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Working directory and file loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Helper label maps\nloss_name_map = {\n    \"pretrain\": \"pretraining loss\",\n    \"train\": \"training loss\",\n    \"val\": \"validation loss\",\n}\nmetric_name_map = {\n    \"train_macro_f1\": \"training macro F1 score\",\n    \"val_macro_f1\": \"validation macro F1 score\",\n}\n\n# 2-5. Extract and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # ---- Losses ----\n    for key, values in ds_dict.get(\"losses\", {}).items():\n        if not values:\n            continue\n        label = loss_name_map.get(key, f\"{key} loss\")\n        best_value = min(values)  # lower is better for losses\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Epoch metrics ----\n    for key, values in ds_dict.get(\"metrics\", {}).items():\n        if not values:\n            continue\n        label = metric_name_map.get(key, key)\n        best_value = max(values)  # higher is better for F1\n        print(f\"{label}: {best_value:.4f}\")\n\n    # ---- Test metrics ----\n    if \"test_macro_f1\" in ds_dict:\n        print(f\"test macro F1 score: {ds_dict['test_macro_f1']:.4f}\")\n    if \"test_loss\" in ds_dict:\n        print(f\"test loss: {ds_dict['test_loss']:.4f}\")\n", ""], "parse_term_out": ["['Dataset: pretrain+cls', '\\n', 'Pre-training loss: 0.9394', '\\n', 'Best\ntraining loss: 0.4915', '\\n', 'Best training macro F1 score: 0.8020', '\\n',\n'Best validation loss: 0.5344', '\\n', 'Best validation macro F1 score: 0.7900',\n'\\n', 'Test loss: 0.5138', '\\n', 'Test macro F1 score: 0.7900', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "", "", "", "", "", "", "", "['\\nDataset: pretrain+cls', '\\n', 'pretraining loss: 0.9394', '\\n', 'training\nloss: 0.4907', '\\n', 'validation loss: 0.5301', '\\n', 'training macro F1 score:\n0.8025', '\\n', 'validation macro F1 score: 0.7959', '\\n', 'test macro F1 score:\n0.7809', '\\n', 'test loss: 0.5149', '\\n', '\\nDataset: scratch_cls', '\\n',\n'training loss: 0.4985', '\\n', 'validation loss: 0.5265', '\\n', 'training macro\nF1 score: 0.7985', '\\n', 'validation macro F1 score: 0.7999', '\\n', 'test macro\nF1 score: 0.8000', '\\n', 'test loss: 0.5085', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: pretrain+cls', '\\n', 'pretraining loss: 0.9376', '\\n', 'training\nloss: 0.4978', '\\n', 'validation loss: 0.5365', '\\n', 'training macro F1 score:\n0.7955', '\\n', 'validation macro F1 score: 0.7860', '\\n', 'test macro F1 score:\n0.7633', '\\n', 'test loss: 0.5402', '\\n', '\\nDataset: scratch_cls', '\\n',\n'training loss: 0.4927', '\\n', 'validation loss: 0.5175', '\\n', 'training macro\nF1 score: 0.8000', '\\n', 'validation macro F1 score: 0.7999', '\\n', 'test macro\nF1 score: 0.7990', '\\n', 'test loss: 0.5036', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: pretrain+cls', '\\n', 'pretraining loss: 0.9371', '\\n', 'training\nloss: 0.4831', '\\n', 'validation loss: 0.5239', '\\n', 'training macro F1 score:\n0.8035', '\\n', 'validation macro F1 score: 0.7999', '\\n', 'test macro F1 score:\n0.7959', '\\n', 'test loss: 0.5228', '\\n', '\\nDataset: scratch_cls', '\\n',\n'training loss: 0.5170', '\\n', 'validation loss: 0.5401', '\\n', 'training macro\nF1 score: 0.7879', '\\n', 'validation macro F1 score: 0.7898', '\\n', 'test macro\nF1 score: 0.7716', '\\n', 'test loss: 0.5716', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: pretrain+cls', '\\n', 'pretraining loss: 0.9382', '\\n', 'training\nloss: 0.4761', '\\n', 'validation loss: 0.5202', '\\n', 'training macro F1 score:\n0.8065', '\\n', 'validation macro F1 score: 0.7999', '\\n', 'test macro F1 score:\n0.7879', '\\n', 'test loss: 0.5321', '\\n', '\\nDataset: scratch_cls', '\\n',\n'training loss: 0.4896', '\\n', 'validation loss: 0.5296', '\\n', 'training macro\nF1 score: 0.7995', '\\n', 'validation macro F1 score: 0.7939', '\\n', 'test macro\nF1 score: 0.7970', '\\n', 'test loss: 0.5112', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"], "current_stage": "Stage_4"};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
