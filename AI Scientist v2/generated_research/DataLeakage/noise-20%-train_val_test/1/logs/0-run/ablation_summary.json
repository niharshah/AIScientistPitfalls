[
  {
    "overall_plan": "The overall plan focuses on enhancing transformer models for sequence processing tasks by both expanding model architecture and employing multitask learning. Initially, the plan involved enlarging the model, incorporating label-smoothing, and introducing an auxiliary length-parity task to improve understanding of sequence-level statistics, resulting in better Macro-F1 scores compared to the baseline. Currently, the focus is on an ablation study to assess the impact of removing positional encoding, aiming to understand how the model infers syntactic relations without explicit order cues. This iterative approach balances performance improvements with a deep investigation into model component functionalities.",
    "analysis": "",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "accuracy",
            "lower_is_better": false,
            "description": "The proportion of correctly predicted instances out of the total instances.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.793,
                "best_value": 0.793
              }
            ]
          },
          {
            "metric_name": "F1 score",
            "lower_is_better": false,
            "description": "The harmonic mean of precision and recall, providing a balance between the two.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.793,
                "best_value": 0.793
              }
            ]
          },
          {
            "metric_name": "loss",
            "lower_is_better": true,
            "description": "A measure of the error between predicted and actual values; lower values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.5965,
                "best_value": 0.558
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "# ------------------------------ Remove-PE Ablation -----------------------------\nimport os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# reproducibility --------------------------------------------------------------\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# --------------------- working dir & device -----------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment data dict -----------------------------------\nexperiment_data = {\n    \"no_positional_encoding\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor((len(ids) - 1) % 2, dtype=torch.long)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model (WITHOUT positional encoding) --------------------\nclass SPRTransformerNoPE(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)  # (B, L, d_model)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformerNoPE(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metric --------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train / val epoch --------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training loop ------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    ed = experiment_data[\"no_positional_encoding\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(t_loss)\n    ed[\"losses\"][\"val\"].append(v_loss)\n    ed[\"metrics\"][\"train_acc\"].append(t_acc)\n    ed[\"metrics\"][\"val_acc\"].append(v_acc)\n    ed[\"metrics\"][\"train_f1\"].append(t_f1)\n    ed[\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: val_loss={v_loss:.4f} | val_acc={v_acc*100:.2f}% | val_macroF1={v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\npreds_all, labels_all = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\npreds_all = torch.cat(preds_all)\nlabels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\n# store predictions & gt -------------------------------------------------------\ned[\"predictions\"] = preds_all.numpy()\ned[\"ground_truth\"] = labels_all.numpy()\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nsaved_files = []\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to get nested dict safely\ndef _get(dic, *keys, default=None):\n    for k in keys:\n        if dic is None:\n            return default\n        dic = dic.get(k, None)\n    return dic if dic is not None else default\n\n\ned = _get(experiment_data, \"no_positional_encoding\", \"SPR_BENCH\", default={})\nmetrics = ed.get(\"metrics\", {})\nlosses = ed.get(\"losses\", {})\n\n# Plot 1: Loss curves ---------------------------------------------------------\ntry:\n    train_loss = losses.get(\"train\", [])\n    val_loss = losses.get(\"val\", [])\n    if train_loss and val_loss:\n        plt.figure()\n        plt.plot(train_loss, label=\"Train\")\n        plt.plot(val_loss, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot 2: Accuracy curves ------------------------------------------------------\ntry:\n    tr_acc = metrics.get(\"train_acc\", [])\n    va_acc = metrics.get(\"val_acc\", [])\n    if tr_acc and va_acc:\n        plt.figure()\n        plt.plot(tr_acc, label=\"Train\")\n        plt.plot(va_acc, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot 3: Macro-F1 curves ------------------------------------------------------\ntry:\n    tr_f1 = metrics.get(\"train_f1\", [])\n    va_f1 = metrics.get(\"val_f1\", [])\n    if tr_f1 and va_f1:\n        plt.figure()\n        plt.plot(tr_f1, label=\"Train\")\n        plt.plot(va_f1, label=\"Validation\")\n        plt.title(\"SPR_BENCH Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# Plot 4: Confusion matrix -----------------------------------------------------\ntry:\n    preds = ed.get(\"predictions\", [])\n    gts = ed.get(\"ground_truth\", [])\n    if len(preds) and len(gts):\n        preds = np.asarray(preds, dtype=int)\n        gts = np.asarray(gts, dtype=int)\n        num_cls = int(preds.max() + 1)\n        cm = np.zeros((num_cls, num_cls), dtype=int)\n        for p, g in zip(preds, gts):\n            cm[g, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\nprint(\"Saved figures:\", saved_files)\n",
    "plot_plan": "We first load the saved experiment_data.npy dictionary, gracefully handling cases where it is missing or corrupted. After locating the \u201cSPR_BENCH\u201d split inside \u201cno_positional_encoding\u201d, we extract time-series arrays for train/val loss, accuracy and macro-F1, plus the prediction and ground-truth vectors when present. For each type of curve (loss, accuracy, F1) we open a new figure in a try-except block, plot the training and validation trajectories, add a clear title and axis labels, save the figure to working_dir with descriptive file names, and close it. If both predictions and ground truth exist, we compute a confusion matrix (using numpy only) and render it as an image with imshow, again wrapped in its own try-except section. We limit ourselves to at most four figures, well within the five-figure cap, and make sure every figure is closed even on error. All paths use the provided working_dir variable, and we rely exclusively on data found in experiment_data.npy\u2014no fabrication or simulation. The script ends by printing the list of files it saved so users can quickly verify the outputs.",
    "plot_analyses": [
      {
        "analysis": "The loss curves indicate a sharp decrease in training loss initially, followed by stabilization. However, the validation loss begins to increase after the second epoch, suggesting overfitting. This could be mitigated by introducing regularization techniques or early stopping.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_loss_curves.png"
      },
      {
        "analysis": "The accuracy curves show an increase in both training and validation accuracy up to the second epoch, after which the validation accuracy starts to decline while training accuracy continues to improve. This divergence further supports the observation of overfitting.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_accuracy_curves.png"
      },
      {
        "analysis": "The macro-F1 score curves closely mirror the accuracy trends, with the validation macro-F1 score peaking at the second epoch and declining thereafter. This suggests that the model's ability to maintain balanced performance across classes diminishes due to overfitting.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_f1_curves.png"
      },
      {
        "analysis": "The confusion matrix reveals that the model performs well in distinguishing between the two classes, with a high number of correct predictions for both. However, there is a slight imbalance in misclassification, which could be addressed by fine-tuning the model or using a class-balanced loss function.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_confusion_matrix.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_loss_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_accuracy_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_f1_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/SPR_BENCH_confusion_matrix.png"
    ],
    "vlm_feedback_summary": "The plots reveal a clear trend of overfitting in the model after the second epoch, as evidenced by the divergence between training and validation metrics. While the model shows strong initial learning, the decline in validation performance highlights the need for regularization or early stopping. The confusion matrix indicates good overall classification performance but suggests room for improvement in handling class imbalance.",
    "exp_results_dir": "experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675",
    "ablation_name": "Remove Positional Encoding",
    "exp_results_npy_files": [
      "experiment_results/experiment_d3ac1e5060024140bd05e1c41e4086a6_proc_3173675/experiment_data.npy"
    ]
  },
  {
    "overall_plan": "The overall plan involves a progressive enhancement of transformer models for improved sequence representation and task performance. Initially, the strategy focused on enlarging the model, incorporating label-smoothing, and utilizing a multitask learning approach with an auxiliary length-parity task to encode useful sequence statistics for SPR rules. This setup consistently improved performance while remaining computationally efficient. The current plan continues this trajectory by conducting an ablation study to explore alternative sequence representation methods. By eliminating the '<cls>' token and employing masked mean-pooling over true tokens, the study aims to understand the impact of these architectural changes on sequence encoding. Despite these modifications, the core training loop, metrics, and data saving remain consistent, allowing for direct performance comparisons. This iterative process reflects a rigorous empirical approach to refining model architectures and optimizing their efficacy.",
    "analysis": "",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "training loss",
            "lower_is_better": true,
            "description": "Quantifies the error during training.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.548732,
                "best_value": 0.548732
              }
            ]
          },
          {
            "metric_name": "validation loss",
            "lower_is_better": true,
            "description": "Quantifies the error during validation.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.557228,
                "best_value": 0.557228
              }
            ]
          },
          {
            "metric_name": "training accuracy",
            "lower_is_better": false,
            "description": "Measures the percentage of correct predictions during training.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 79.5,
                "best_value": 79.5
              }
            ]
          },
          {
            "metric_name": "validation accuracy",
            "lower_is_better": false,
            "description": "Measures the percentage of correct predictions during validation.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 79.6,
                "best_value": 79.6
              }
            ]
          },
          {
            "metric_name": "training macro-F1",
            "lower_is_better": false,
            "description": "Evaluates the macro-averaged F1 score during training.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.795,
                "best_value": 0.795
              }
            ]
          },
          {
            "metric_name": "validation macro-F1",
            "lower_is_better": false,
            "description": "Evaluates the macro-averaged F1 score during validation.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.7959,
                "best_value": 0.7959
              }
            ]
          },
          {
            "metric_name": "test accuracy",
            "lower_is_better": false,
            "description": "Measures the percentage of correct predictions on the test set.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 79.7,
                "best_value": 79.7
              }
            ]
          },
          {
            "metric_name": "test macro-F1",
            "lower_is_better": false,
            "description": "Evaluates the macro-averaged F1 score on the test set.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.797,
                "best_value": 0.797
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "import os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- house-keeping & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment container -----------------------------------\nexperiment_data = {\n    \"NoCLS_MeanPool\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    need = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and need.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf, self.vocab = hf_ds, vocab\n        self.pad = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        return [self.vocab[t] for t in s.strip().split()]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor(len(ids) % 2, dtype=torch.long)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformerMeanPool(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def mean_pool(self, x, mask):\n        mask = mask.unsqueeze(-1).type_as(x)\n        summed = (x * mask).sum(1)\n        denom = mask.sum(1).clamp(min=1e-6)\n        return summed / denom\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        pooled = self.norm(self.mean_pool(x, attention_mask))\n        return self.main_head(pooled), self.parity_head(pooled)\n\n\nmodel = SPRTransformerMeanPool(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, par_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_par = criterion_parity(par_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_par\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds, all_lbls = torch.cat(all_preds), torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor ep in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    ed = experiment_data[\"NoCLS_MeanPool\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(t_loss)\n    ed[\"losses\"][\"val\"].append(v_loss)\n    ed[\"metrics\"][\"train_acc\"].append(t_acc)\n    ed[\"metrics\"][\"val_acc\"].append(v_acc)\n    ed[\"metrics\"][\"train_f1\"].append(t_f1)\n    ed[\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {ep}: val_loss={v_loss:.4f} | val_acc={v_acc*100:.2f}% | val_f1={v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\ned = experiment_data[\"NoCLS_MeanPool\"][\"SPR_BENCH\"]\ned[\"predictions\"] = preds_all.numpy()\ned[\"ground_truth\"] = labels_all.numpy()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef macro_f1(preds, labels, num_cls):\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# ------------------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    run = experiment_data[\"NoCLS_MeanPool\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    run = None\n\nif run:\n    epochs = np.arange(1, len(run[\"losses\"][\"train\"]) + 1)\n\n    # ------------------------- loss curves ------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, run[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, run[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH Loss Curves (NoCLS_MeanPool)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_loss_curves_NoCLS_MeanPool.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ------------------------- accuracy curves -------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, run[\"metrics\"][\"train_acc\"], label=\"Train\")\n        plt.plot(epochs, run[\"metrics\"][\"val_acc\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH Accuracy Curves (NoCLS_MeanPool)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_accuracy_curves_NoCLS_MeanPool.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # ------------------------- F1 curves -------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, run[\"metrics\"][\"train_f1\"], label=\"Train\")\n        plt.plot(epochs, run[\"metrics\"][\"val_f1\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 Curves (NoCLS_MeanPool)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves_NoCLS_MeanPool.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ------------------------- confusion matrix ------------------------------\n    try:\n        preds = run[\"predictions\"]\n        gts = run[\"ground_truth\"]\n        if len(preds) and len(gts):\n            classes = np.unique(np.concatenate([preds, gts]))\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\"SPR_BENCH Confusion Matrix (Test set)\")\n            plt.tight_layout()\n            plt.savefig(\n                os.path.join(\n                    working_dir, \"SPR_BENCH_confusion_matrix_NoCLS_MeanPool.png\"\n                )\n            )\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ------------------------- print metrics ----------------------------------\n    try:\n        preds = run[\"predictions\"]\n        gts = run[\"ground_truth\"]\n        if len(preds) and len(gts):\n            acc = (preds == gts).mean()\n            f1 = macro_f1(preds, gts, len(np.unique(gts)))\n            print(f\"Test accuracy: {acc*100:.2f}% | Test macroF1: {f1:.4f}\")\n    except Exception as e:\n        print(f\"Error computing metrics: {e}\")\n",
    "plot_plan": "We will first load experiment_data.npy, identify the single run (NoCLS_MeanPool on SPR_BENCH), and extract training/validation losses, accuracies, and macro-F1 scores along with test-set predictions and labels. Using these arrays we will generate up to four standard plots\u2014loss, accuracy, macro-F1 learning curves, and a test-set confusion matrix\u2014placing each in its own try-except block to guarantee graceful failure and figure closure. Every plot will carry an informative title and subtitle, be saved inside the working directory with filenames that embed the dataset name and plot type, and the figure will be closed immediately after saving. We respect the \u201cat most five plots\u201d rule by plotting exactly four figures. After plotting, we compute and print the final test accuracy and macro-F1 using the same macro_f1 routine as in the training script to keep consistency. All matplotlib calls are plain (no custom styles), and only data present in experiment_data.npy are used\u2014no fabricated values. Finally, the code is concise, self-contained, and starts with the mandated imports, allowing quick execution inside the provided environment.",
    "plot_analyses": [
      {
        "analysis": "The loss curves indicate a steady decline in both training and validation loss over the epochs, suggesting that the model is learning effectively. The validation loss stabilizes after epoch 4, indicating that the model's performance on unseen data is not deteriorating due to overfitting. However, the gap between training and validation loss remains small, which is a positive sign of good generalization.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_loss_curves_NoCLS_MeanPool.png"
      },
      {
        "analysis": "The accuracy curves show a consistent improvement for both training and validation sets, with the validation accuracy peaking and stabilizing after epoch 4. This indicates that the model achieves high classification performance and does not suffer from overfitting. The model appears to be approaching the benchmark SOTA performance of 80% accuracy.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_accuracy_curves_NoCLS_MeanPool.png"
      },
      {
        "analysis": "The Macro-F1 curves demonstrate a similar trend to accuracy, with steady improvement and stabilization after epoch 4 for both training and validation sets. This suggests that the model performs well across all classes, not favoring any specific class, which is critical for balanced datasets like SPR_BENCH.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_f1_curves_NoCLS_MeanPool.png"
      },
      {
        "analysis": "The confusion matrix for the test set shows a clear diagonal dominance, indicating that the model is accurately predicting the majority of the test samples. However, there is still room for improvement, as some misclassifications are evident. These errors could be further analyzed to identify specific patterns or challenging cases for the model.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_confusion_matrix_NoCLS_MeanPool.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_loss_curves_NoCLS_MeanPool.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_accuracy_curves_NoCLS_MeanPool.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_f1_curves_NoCLS_MeanPool.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/SPR_BENCH_confusion_matrix_NoCLS_MeanPool.png"
    ],
    "vlm_feedback_summary": "The provided plots suggest that the model is learning effectively and generalizing well to the validation set. Loss, accuracy, and Macro-F1 curves all show consistent improvement and stabilization, indicating that the model is approaching optimal performance. The confusion matrix confirms good classification performance on the test set, with some room for further improvements.",
    "exp_results_dir": "experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676",
    "ablation_name": "No-CLS Mean Pooling",
    "exp_results_npy_files": [
      "experiment_results/experiment_b6e6287e6c0e458da22bf4c0fcb3f2d3_proc_3173676/experiment_data.npy"
    ]
  },
  {
    "overall_plan": "The project involves creating a more robust transformer model by expanding its size, utilizing label-smoothing for improved regularization, and integrating an auxiliary task to enhance sequence-level statistic encoding for SPR rules. This model employs multitask learning with two heads\u2014one for SPR classification and another for sequence parity. The training minimizes the combined loss from both tasks, optimizing for Macro-F1 on the primary task. Currently, an ablation study is conducted to examine the effects of label-smoothing by setting it to zero while keeping all other parameters constant, aiming to understand its specific impact on model performance. This approach allows for both the development of a more effective baseline model and a detailed analysis of individual enhancements through systematic ablation studies.",
    "analysis": "",
    "metric": {
      "value": null,
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "import os, pathlib, math, time, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- experiment container -----------------------------------\nexperiment_data = {\n    \"no_label_smoothing\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\nexp_ref = experiment_data[\"no_label_smoothing\"][\"SPR_BENCH\"]\n\n# --------------------- reproducibility & device -------------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# --------------------- locate & load SPR-BENCH --------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    cands = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    need = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in cands:\n        if c and c.exists() and need.issubset({p.name for p in c.iterdir()}):\n            print(\"Found SPR_BENCH at\", c)\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(p) -> DatasetDict:\n    def _ld(n):\n        return load_dataset(\n            \"csv\", data_files=str(p / n), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocab & datasets ---------------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor((len(ids) - 1) % 2, dtype=torch.long)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(split):\n    v = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in v:\n                v[tok] = len(v)\n    return v\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id = vocab[\"<pad>\"]\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds, dev_ds, test_ds = [\n    SPRTokenDataset(spr[s], vocab) for s in [\"train\", \"dev\", \"test\"]\n]\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & losses (NO label smoothing) ----------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.0)  # <- ablation: hard labels\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    p, l = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((p == c) & (l == c)).sum()\n        fp = ((p == c) & (l != c)).sum()\n        fn = ((p != c) & (l == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val ----------------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training loop ------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    tr_loss, tr_acc, tr_f1 = run_epoch(train_loader, True)\n    va_loss, va_acc, va_f1 = run_epoch(dev_loader, False)\n    exp_ref[\"losses\"][\"train\"].append(tr_loss)\n    exp_ref[\"losses\"][\"val\"].append(va_loss)\n    exp_ref[\"metrics\"][\"train_acc\"].append(tr_acc)\n    exp_ref[\"metrics\"][\"val_acc\"].append(va_acc)\n    exp_ref[\"metrics\"][\"train_f1\"].append(tr_f1)\n    exp_ref[\"metrics\"][\"val_f1\"].append(va_f1)\n    print(\n        f\"Epoch {epoch}: val_loss {va_loss:.4f} | val_acc {va_acc*100:.2f}% | val_F1 {va_f1:.4f}\"\n    )\n    if va_f1 > best_f1:\n        best_f1 = va_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexp_ref[\"predictions\"] = preds_all.numpy()\nexp_ref[\"ground_truth\"] = labels_all.numpy()\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"Saved experiment data to experiment_data.npy\")\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\nexp_file_cands = [\n    \"experiment_data.npy\",\n    os.path.join(working_dir, \"experiment_data.npy\"),\n]\nexp_data = None\nfor f in exp_file_cands:\n    if os.path.exists(f):\n        try:\n            exp_data = np.load(f, allow_pickle=True).item()\n            break\n        except Exception as e:\n            print(f\"Error loading {f}: {e}\")\nif exp_data is None:\n    print(\"experiment_data.npy not found, aborting plotting.\")\n    quit()\n\n# Safely navigate the expected keys\ntry:\n    run = exp_data[\"no_label_smoothing\"][\"SPR_BENCH\"]\nexcept KeyError as e:\n    print(f\"Expected keys missing: {e}\")\n    quit()\n\nepochs = range(1, len(run[\"losses\"][\"train\"]) + 1)\n\n# 1) Loss curve\ntry:\n    plt.figure()\n    plt.plot(epochs, run[\"losses\"][\"train\"], label=\"Train\")\n    plt.plot(epochs, run[\"losses\"][\"val\"], label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH Loss Curve\\nTrain vs Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) Accuracy curve\ntry:\n    plt.figure()\n    plt.plot(epochs, run[\"metrics\"][\"train_acc\"], label=\"Train\")\n    plt.plot(epochs, run[\"metrics\"][\"val_acc\"], label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH Accuracy Curve\\nTrain vs Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# 3) Macro-F1 curve\ntry:\n    plt.figure()\n    plt.plot(epochs, run[\"metrics\"][\"train_f1\"], label=\"Train\")\n    plt.plot(epochs, run[\"metrics\"][\"val_f1\"], label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curve\\nTrain vs Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 4) Confusion matrix (test set)\ntry:\n    preds = np.array(run[\"predictions\"])\n    gts = np.array(run[\"ground_truth\"])\n    num_cls = int(max(preds.max(), gts.max())) + 1\n    cm = np.zeros((num_cls, num_cls), dtype=int)\n    for p, t in zip(preds, gts):\n        cm[t, p] += 1\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(\"SPR_BENCH Confusion Matrix\\nLeft: GT rows, Right: Pred cols\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# Print final evaluation metrics\ntry:\n    test_acc = (preds == gts).mean()\n    # macro-F1\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (gts == c)).sum()\n        fp = ((preds == c) & (gts != c)).sum()\n        fn = ((preds != c) & (gts == c)).sum()\n        prec = tp / (tp + fp) if tp + fp else 0\n        rec = tp / (tp + fn) if tp + fn else 0\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    test_f1 = float(np.mean(f1s))\n    print(f\"Test Accuracy: {test_acc*100:.2f}%  |  Test Macro-F1: {test_f1:.4f}\")\nexcept Exception as e:\n    print(f\"Error computing final metrics: {e}\")\n",
    "plot_plan": "We will load the saved experiment_data.npy, locate the run \u2018no_label_smoothing / SPR_BENCH\u2019, and pull out the per-epoch lists for train/val loss, accuracy and macro-F1 together with the test predictions/ground-truth arrays. Four separate try-except blocks will create: (1) the train vs. validation loss curve, (2) train vs. validation accuracy, (3) train vs. validation macro-F1, and (4) a confusion-matrix heat-map comparing test predictions to ground truth. Each figure will be titled clearly, closed after saving, and written to working_dir with descriptive names (e.g. \u201cSPR_BENCH_loss_curve.png\u201d). If experiment_data.npy is missing or any key is absent the code will report the error and skip that plot. At the end we compute and print overall test accuracy and macro-F1 so the user can verify the final numbers. No data are fabricated\u2014only the arrays present in the file are used, and we plot at most these four figures, satisfying the \u22645-figure rule.",
    "plot_analyses": [
      {
        "analysis": "The loss curve shows a steady decrease in both training and validation loss over the epochs, indicating that the model is learning effectively. However, the gap between training and validation loss remains relatively small, suggesting minimal overfitting at this stage. The slight fluctuations in validation loss after epoch 5 may indicate some instability or sensitivity to the learning rate or batch size.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_loss_curve.png"
      },
      {
        "analysis": "The accuracy curve demonstrates consistent improvement in both training and validation accuracy over the epochs, plateauing around epoch 5. The training accuracy slightly exceeds the validation accuracy, which is expected but should be monitored to ensure no overfitting occurs. The validation accuracy stabilizes near 0.78, which is close to the SOTA benchmark of 80%.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_accuracy_curve.png"
      },
      {
        "analysis": "The macro F1 curve follows a similar trend to the accuracy curve, with both training and validation F1 scores improving and stabilizing around epoch 5. This indicates balanced performance across different classes and suggests that the model is not favoring one class over another. The validation F1 score stabilizes near 0.78, aligning with the accuracy trends.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_macroF1_curve.png"
      },
      {
        "analysis": "The confusion matrix indicates that the model performs well on both classes, with the majority of predictions falling on the diagonal, representing correct classifications. However, there is some misclassification, particularly in one of the classes, which could be addressed by further tuning of the model or exploring class-specific loss adjustments.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_confusion_matrix.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_loss_curve.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_accuracy_curve.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_macroF1_curve.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677/SPR_BENCH_confusion_matrix.png"
    ],
    "vlm_feedback_summary": "The provided plots indicate that the model is learning effectively, with steady improvement in loss, accuracy, and F1 scores. While the results are promising, with validation metrics nearing the SOTA benchmark, there are minor signs of instability and misclassification that may require further investigation and fine-tuning.",
    "exp_results_dir": "experiment_results/experiment_b727e5194713456d8650222aec9848e9_proc_3173677",
    "ablation_name": "No Label Smoothing",
    "exp_results_npy_files": []
  },
  {
    "overall_plan": "The overall plan involves enhancing a transformer model by enlarging its architecture, applying label-smoothing, and introducing a multitask learning setup with an auxiliary length-parity task to improve sequence-level statistics encoding for SPR rules. The transformer is a 6-layer model with 256-dimensional embeddings, aiming for better regularization and improved Macro-F1 scores within a 30-minute runtime. Additionally, the plan includes an ablation study to test the model's robustness to token order by shuffling input tokens into a bag-of-words format, while maintaining all other components identical. This aims to evaluate the model's reliance on word order and guide future architectural or methodological improvements.",
    "analysis": "",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "train loss",
            "lower_is_better": true,
            "description": "Measures the error during training. Lower values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.557621,
                "best_value": 0.557621
              }
            ]
          },
          {
            "metric_name": "train accuracy",
            "lower_is_better": false,
            "description": "Measures the accuracy during training. Higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.7885,
                "best_value": 0.7885
              }
            ]
          },
          {
            "metric_name": "train F1 score",
            "lower_is_better": false,
            "description": "Measures the F1 score during training. Higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.788402,
                "best_value": 0.788402
              }
            ]
          },
          {
            "metric_name": "validation loss",
            "lower_is_better": true,
            "description": "Measures the error on the validation set. Lower values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.572714,
                "best_value": 0.572714
              }
            ]
          },
          {
            "metric_name": "validation accuracy",
            "lower_is_better": false,
            "description": "Measures the accuracy on the validation set. Higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.8,
                "best_value": 0.8
              }
            ]
          },
          {
            "metric_name": "validation F1 score",
            "lower_is_better": false,
            "description": "Measures the F1 score on the validation set. Higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.79992,
                "best_value": 0.79992
              }
            ]
          },
          {
            "metric_name": "test accuracy",
            "lower_is_better": false,
            "description": "Measures the accuracy on the test set. Higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.791,
                "best_value": 0.791
              }
            ]
          },
          {
            "metric_name": "test F1 score",
            "lower_is_better": false,
            "description": "Measures the F1 score on the test set. Higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.790995,
                "best_value": 0.790995
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "import os, pathlib, math, time, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------ house-keeping & device ------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------- experiment data container (new naming) -------------------------\nexperiment_data = {\n    \"token_shuffle_bow\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    \"\"\"Returns token-shuffled sentences (bag-of-words) each call.\"\"\"\n\n    def __init__(self, hf_ds, vocab, shuffle_bow: bool = True):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n        self.shuffle_bow = shuffle_bow\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        toks = s.strip().split()\n        if self.shuffle_bow and len(toks) > 1:  # leave <cls> untouched\n            idx = torch.randperm(len(toks))\n            toks = [toks[i] for i in idx]\n        return [self.cls] + [self.vocab[t] for t in toks if t in self.vocab]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor((len(ids) - 1) % 2, dtype=torch.long)  # exclude CLS\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab, shuffle_bow=True)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab, shuffle_bow=True)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab, shuffle_bow=True)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n\n    experiment_data[\"token_shuffle_bow\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"token_shuffle_bow\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"token_shuffle_bow\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": t_acc, \"f1\": t_f1}\n    )\n    experiment_data[\"token_shuffle_bow\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": v_acc, \"f1\": v_f1}\n    )\n\n    print(\n        f\"Epoch {epoch}: val_loss={v_loss:.4f} | val_acc={v_acc*100:.2f}% | val_macroF1={v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"token_shuffle_bow\"][\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"token_shuffle_bow\"][\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef macro_f1(preds, labels, num_cls):\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec, rec = tp / (tp + fp), tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# -------------------- load experiment data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Only proceed if data exists\nfor exp_name, ds_dict in experiment_data.items():\n    for ds_name, record in ds_dict.items():\n        losses = record.get(\"losses\", {})\n        metrics = record.get(\"metrics\", {})\n        preds = np.asarray(record.get(\"predictions\", []))\n        gts = np.asarray(record.get(\"ground_truth\", []))\n        epochs = range(1, 1 + len(losses.get(\"train\", [])))\n\n        # --------------- plot 1: loss curves -------------------\n        try:\n            if losses:\n                plt.figure()\n                plt.plot(epochs, losses[\"train\"], label=\"Train\")\n                plt.plot(epochs, losses[\"val\"], label=\"Validation\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.title(f\"{ds_name}: Training vs Validation Loss\")\n                plt.legend()\n                fname = f\"{ds_name}_loss_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot: {e}\")\n            plt.close()\n\n        # --------------- plot 2: accuracy & F1 -----------------\n        try:\n            if metrics:\n                train_acc = [m[\"acc\"] for m in metrics[\"train\"]]\n                val_acc = [m[\"acc\"] for m in metrics[\"val\"]]\n                train_f1 = [m[\"f1\"] for m in metrics[\"train\"]]\n                val_f1 = [m[\"f1\"] for m in metrics[\"val\"]]\n\n                fig, ax1 = plt.subplots()\n                ax1.set_xlabel(\"Epoch\")\n                ax1.set_ylabel(\"Accuracy\")\n                ax1.plot(epochs, train_acc, \"b-\", label=\"Train Acc\")\n                ax1.plot(epochs, val_acc, \"b--\", label=\"Val Acc\")\n                ax2 = ax1.twinx()\n                ax2.set_ylabel(\"Macro-F1\")\n                ax2.plot(epochs, train_f1, \"r-\", label=\"Train F1\")\n                ax2.plot(epochs, val_f1, \"r--\", label=\"Val F1\")\n\n                lines, labs = ax1.get_legend_handles_labels()\n                lines2, labs2 = ax2.get_legend_handles_labels()\n                ax1.legend(lines + lines2, labs + labs2, loc=\"lower center\")\n                plt.title(f\"{ds_name}: Accuracy & Macro-F1\")\n                plt.tight_layout()\n                fname = f\"{ds_name}_metric_curves.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating metric plot: {e}\")\n            plt.close()\n\n        # --------------- plot 3: confusion matrix --------------\n        try:\n            if preds.size and gts.size:\n                num_cls = int(max(gts.max(), preds.max()) + 1)\n                cm = np.zeros((num_cls, num_cls), int)\n                for p, t in zip(preds, gts):\n                    cm[t, p] += 1\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im)\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"True\")\n                plt.title(f\"{ds_name}: Confusion Matrix\")\n                plt.tight_layout()\n                fname = f\"{ds_name}_confusion_matrix.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n\n                acc = (preds == gts).mean()\n                f1 = macro_f1(preds, gts, num_cls)\n                print(f\"{ds_name} Test Accuracy: {acc*100:.2f}% | Macro-F1: {f1:.4f}\")\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n",
    "plot_plan": "We first load the saved experiment_data.npy and unpack the single run (token_shuffle_bow \u2192 SPR_BENCH).  \nFrom this dict we extract the epoch-wise lists of training / validation losses and metrics.  \nWe build three visualizations, each protected by its own try-except so one failure will not stop the rest.  \n(1) Line plot of training vs. validation loss across epochs.  \n(2) Two-axis line plot showing accuracy and macro-F1 for both training and validation splits.  \n(3) Confusion-matrix heat-map of the best checkpoint\u2019s test predictions vs. ground-truth; we recompute it only if those arrays are present and non-empty.  \nAll plots are saved inside working_dir with descriptive names that include the dataset and plot type, and every figure is closed afterward to free memory.  \nWe also print out the recalculated test accuracy and macro-F1 so the notebook logs contain the final numbers.  \nThe code obeys the given plotting guidelines (basic matplotlib, titles/sub-titles, \u22645 figures, close figures, no fake data).",
    "plot_analyses": [
      {
        "analysis": "This plot shows the training and validation loss over 5 epochs. The training loss consistently decreases, indicating that the model is learning effectively from the data. The validation loss also decreases but at a slower rate, suggesting that the model generalizes reasonably well without significant overfitting. However, the gap between training and validation loss narrows towards the end, which is a positive sign of convergence. The final validation loss stabilizes, indicating that additional epochs might not yield significant improvements.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/SPR_BENCH_loss_curve.png"
      },
      {
        "analysis": "This plot illustrates the accuracy and macro-F1 scores for both training and validation sets over 5 epochs. Both metrics improve steadily, with the validation accuracy and F1 scores closely tracking the training metrics. This indicates good generalization and balanced performance across classes. The slight plateauing of the validation metrics around the final epoch suggests that the model is nearing its performance ceiling under the current configuration. The macro-F1 score being close to the accuracy implies that the model is handling class imbalances effectively.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/SPR_BENCH_metric_curves.png"
      },
      {
        "analysis": "This confusion matrix provides a summary of the model's predictions on the test set. The darker diagonal cells indicate that the model is correctly classifying a majority of instances for each class. The lighter off-diagonal cells suggest a relatively low rate of misclassifications. This indicates that the model has strong predictive capabilities and is effectively distinguishing between the symbolic rule-based classes.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/SPR_BENCH_confusion_matrix.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/SPR_BENCH_loss_curve.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/SPR_BENCH_metric_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/SPR_BENCH_confusion_matrix.png"
    ],
    "vlm_feedback_summary": "The experimental plots provide clear insights into the model's performance. The loss curves indicate effective learning and good generalization, while the accuracy and macro-F1 trends highlight balanced and robust performance. The confusion matrix confirms strong predictive capabilities with minimal misclassifications. Overall, the results suggest that the proposed contextual embedding-based model is performing well on the SPR_BENCH dataset.",
    "exp_results_dir": "experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674",
    "ablation_name": "Token Order Shuffling (Bag-of-Words)",
    "exp_results_npy_files": [
      "experiment_results/experiment_4827a969888d455faf2d60e8f91aa7b5_proc_3173674/experiment_data.npy"
    ]
  },
  {
    "overall_plan": "The overarching plan revolves around enhancing a transformer model for a Sequence Pattern Recognition (SPR) task. Initially, the model's architecture was expanded with a larger transformer and improvements such as label-smoothing and a multitask setup with an auxiliary parity task, aimed at better capturing sequence-level statistics and improving the Macro-F1 score. The current plan builds on this by conducting an ablation study to replace the static sinusoidal positional encoding with a learnable embedding layer, isolating this change to investigate its impact on model performance. This iterative approach, maintaining consistency in all other components, seeks to systematically refine the model's architecture and components for optimal efficacy on the SPR task.",
    "analysis": "",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "train accuracy",
            "lower_is_better": false,
            "description": "Accuracy during the training phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.7645,
                "best_value": 0.7645
              }
            ]
          },
          {
            "metric_name": "validation accuracy",
            "lower_is_better": false,
            "description": "Accuracy during the validation phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.772,
                "best_value": 0.772
              }
            ]
          },
          {
            "metric_name": "train macro F1 score",
            "lower_is_better": false,
            "description": "Macro F1 score during the training phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.7645,
                "best_value": 0.7645
              }
            ]
          },
          {
            "metric_name": "validation macro F1 score",
            "lower_is_better": false,
            "description": "Macro F1 score during the validation phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.772,
                "best_value": 0.772
              }
            ]
          },
          {
            "metric_name": "train loss",
            "lower_is_better": true,
            "description": "Loss during the training phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.5725,
                "best_value": 0.5725
              }
            ]
          },
          {
            "metric_name": "validation loss",
            "lower_is_better": true,
            "description": "Loss during the validation phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.576,
                "best_value": 0.576
              }
            ]
          },
          {
            "metric_name": "test accuracy",
            "lower_is_better": false,
            "description": "Accuracy during the testing phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.784,
                "best_value": 0.784
              }
            ]
          },
          {
            "metric_name": "test macro F1 score",
            "lower_is_better": false,
            "description": "Macro F1 score during the testing phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.7838,
                "best_value": 0.7838
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "# Learned-positional-embedding ablation for SPR-BENCH\nimport os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- house-keeping & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment data container ------------------------------\nexperiment_data = {\n    \"learned_positional_embeddings\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\n                \"train_acc\": [],\n                \"val_acc\": [],\n                \"train_f1\": [],\n                \"val_f1\": [],\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\nexp_key = (\"learned_positional_embeddings\", \"SPR_BENCH\")\n# helpers to shorten later code\nmetrics = experiment_data[exp_key[0]][exp_key[1]][\"metrics\"]\nlosses = experiment_data[exp_key[0]][exp_key[1]][\"losses\"]\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor((len(ids) - 1) % 2, dtype=torch.long)  # even/odd length\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass LearnedPositionalEncoding(nn.Module):\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        # x: (batch, seq, d)\n        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n        return x + self.pos_emb(positions)\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = LearnedPositionalEncoding(max_len=max_len, d_model=d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    losses[\"train\"].append(t_loss)\n    losses[\"val\"].append(v_loss)\n    metrics[\"train_acc\"].append(t_acc)\n    metrics[\"val_acc\"].append(v_acc)\n    metrics[\"train_f1\"].append(t_f1)\n    metrics[\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: val_loss={v_loss:.4f} | val_acc={v_acc*100:.2f}% | val_macroF1={v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexp_dict = experiment_data[exp_key[0]][exp_key[1]]\nexp_dict[\"predictions\"] = preds_all.numpy()\nexp_dict[\"ground_truth\"] = labels_all.numpy()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load experiment data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"learned_positional_embeddings\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    raise RuntimeError(f\"Error loading experiment data: {e}\")\n\nloss_train, loss_val = np.asarray(exp[\"losses\"][\"train\"]), np.asarray(\n    exp[\"losses\"][\"val\"]\n)\nacc_train, acc_val = np.asarray(exp[\"metrics\"][\"train_acc\"]), np.asarray(\n    exp[\"metrics\"][\"val_acc\"]\n)\nf1_train, f1_val = np.asarray(exp[\"metrics\"][\"train_f1\"]), np.asarray(\n    exp[\"metrics\"][\"val_f1\"]\n)\npreds, labels = np.asarray(exp[\"predictions\"]), np.asarray(exp[\"ground_truth\"])\n\n\n# ---------------- utility for macro-F1 ----------------\ndef macro_f1(preds, labels, num_cls):\n    f1s = []\n    for c in range(num_cls):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\nnum_cls = int(labels.max()) + 1\ntest_acc = (preds == labels).mean()\ntest_f1 = macro_f1(preds, labels, num_cls)\nprint(f\"SPR_BENCH Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\n# ---------------- plotting ----------------\n# 1) Loss curves\ntry:\n    plt.figure()\n    epochs = np.arange(1, len(loss_train) + 1)\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 2) Accuracy curves\ntry:\n    plt.figure()\n    plt.plot(epochs, acc_train, label=\"Train\")\n    plt.plot(epochs, acc_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH Training vs Validation Accuracy\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# 3) F1 curves\ntry:\n    plt.figure()\n    plt.plot(epochs, f1_train, label=\"Train\")\n    plt.plot(epochs, f1_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix\ntry:\n    cm = np.zeros((num_cls, num_cls), dtype=int)\n    for t, p in zip(labels, preds):\n        cm[t, p] += 1\n    cm_norm = cm / cm.sum(axis=1, keepdims=True).clip(min=1)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm_norm, cmap=\"Blues\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(\"SPR_BENCH Normalized Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n",
    "plot_plan": "We first load the stored experiment_data.npy, extract the SPR_BENCH block and unpack losses, accuracy, F1 lists, together with the test\u2010set predictions and ground-truth labels. Using these arrays we compute final test accuracy and macro-F1 to report in the console. We then create up to four plots: (1) training vs validation loss, (2) training vs validation accuracy, (3) training vs validation macro-F1, and (4) a normalized confusion matrix of predictions vs ground truth on the test set. Each figure is generated inside its own try-except block, titled clearly, saved to working_dir with descriptive filenames, and closed afterwards to free memory. All plotting relies only on data present in experiment_data.npy; no synthetic values are introduced. We compute macro-F1 locally to avoid external dependencies, keeping the code self-contained. Finally, the script prints the computed test metrics so users can verify quantitative performance alongside the visual summaries.",
    "plot_analyses": [
      {
        "analysis": "The training and validation loss curves show a consistent decrease in loss over the epochs, indicating that the model is learning effectively. However, the validation loss starts higher than the training loss and decreases at a slower rate initially, suggesting potential overfitting in the early stages. By the third epoch, both losses align closely, which could imply improved generalization.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_loss_curves.png"
      },
      {
        "analysis": "The training and validation accuracy curves reveal an initial divergence, with validation accuracy decreasing sharply in the second epoch. However, it recovers and surpasses training accuracy by the third epoch. This pattern suggests that the model initially struggles to generalize but eventually adapts well to the validation set, possibly due to improved regularization or learning rate adjustments.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_accuracy_curves.png"
      },
      {
        "analysis": "The macro-F1 score trends for training and validation follow a similar pattern to accuracy, with validation performance dipping significantly in the second epoch before recovering strongly. The final alignment of training and validation macro-F1 scores indicates that the model achieves balanced performance across classes by the end of training.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_f1_curves.png"
      },
      {
        "analysis": "The normalized confusion matrix illustrates the model's classification performance across classes. The darker diagonal cells indicate strong class-wise prediction accuracy, while lighter off-diagonal cells suggest areas where misclassifications occur. The matrix shows relatively balanced performance, with no single class dominating errors, which is a positive indicator for the model's generalization capabilities.",
        "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_confusion_matrix.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_loss_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_accuracy_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_f1_curves.png",
      "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/SPR_BENCH_confusion_matrix.png"
    ],
    "vlm_feedback_summary": "The plots indicate that the model experiences initial challenges with generalization, as seen in the divergence of training and validation metrics in the early epochs. However, it demonstrates strong recovery and alignment by the end of training, achieving balanced performance across metrics. The confusion matrix further supports this, showing relatively even classification performance across classes.",
    "exp_results_dir": "experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677",
    "ablation_name": "Learned Positional Embeddings",
    "exp_results_npy_files": [
      "experiment_results/experiment_3c56282f781b4aefbeec99f7a5fc85fb_proc_3173677/experiment_data.npy"
    ]
  }
]