<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 3], [0, 4], [0, 2]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.3333333333333333, 1.0], [0.6666666666666666, 1.0], [1.0, 1.0]], "plan": ["We first load the SPR_BENCH splits with the provided helper, build a character-\nlevel vocabulary from the training sequences, and convert every sequence into an\ninteger tensor. A lightweight Transformer encoder (4 layers, 4 heads, 128 hidden\nsize) embeds these tokens, applies mean-pooling over the valid positions and\nfeeds the result to a linear classifier. Batches are padded on-the-fly by a\ncustom collate function; all tensors and the model are moved to GPU when\navailable. We train with cross-entropy for at most ten epochs, reporting\nvalidation loss and Macro-F1 after every epoch while keeping the best checkpoint\nby dev F1. At the end we evaluate on the withheld test split, store predictions,\nlosses and metrics in the required experiment_data structure, and persist it\nwith np.save. Basic plots are also saved to ./working for later inspection. This\ngives a solid, fast baseline that already exploits contextual embeddings and\npositions us for more sophisticated symbolic-reasoning extensions in subsequent\nexperiments.", "Hyperparam tuning name: num_epochs. We keep every setting identical to the\nbaseline and simply loop over several candidate epoch budgets (10, 20 and 30).\nFor each budget we (1) re-initialise the model/optimizer, (2) train for the\nrequested number of epochs while tracking train/validation loss & macro-F1, (3)\nkeep the best validation checkpoint, (4) evaluate it on the test split and (5)\nstore all histories plus the final test predictions in `experiment_data`.\nResults are nested under the top-level key `'num_epochs'` so that later analyses\ncan easily compare the three runs.", "Hyperparam tuning name: learning_rate. We loop over the learning-rate grid\n{1e-4, 5e-4, 1e-3}.   For every value we: (1) re-initialise the model,\ncriterion, optimizer and a cosine-decay LR scheduler, (2) train for the fixed\nnumber of epochs while tracking losses/F1 on train & dev, (3) keep the epoch\ncheckpoint that achieves the best dev macro-F1, and (4) evaluate that checkpoint\non the test set.   All results are stored in the required `experiment_data`\ndictionary under the key `learning_rate`, and finally dumped to\n`experiment_data.npy`.", "Hyperparam tuning name: batch_size. We extend the baseline by looping over\nseveral mini-batch sizes (32, 64, 128, 256).   For each value we rebuild the\nDataLoader, re-init the model, train for the same number of epochs, track\nmetrics/losses, and keep the best validation checkpoint.   The results (per-\nepoch train/val losses & F1 plus final test predictions) are stored under\nexperiment_data['batch_size'][bs] and the whole dict is saved to\nexperiment_data.npy.   Nothing else in the pipeline is modified, so\narchitecture, optimizer and other hyper-parameters remain identical.", "Hyperparam tuning name: dropout. We sweep several dropout probabilities,\ntraining a fresh model for each value while keeping all other hyper-parameters\nunchanged. For every run we track per-epoch train/val losses and macro-F1, save\nthe best model on dev, evaluate it on test, and store all plottable data in the\nmandatory experiment_data structure (lists of lists keyed by dropout). Finally\nthe whole dictionary is persisted to experiment_data.npy."], "code": ["import os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id = vocab[\"<PAD>\"]\nunk_id = vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)  # max len 5k\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]  # (B,L,D)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )  # mean pooling\n        return self.classifier(x)\n\n\nmodel = CharTransformer(vocab_size, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n\n# ---------- training loop ----------\ndef run_epoch(split):\n    is_train = split == \"train\"\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    loader = loaders[split]\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\nbest_f1 = 0.0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(\"train\")\n    val_loss, val_f1, _, _ = run_epoch(\"dev\")\n\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_MacroF1={val_f1:.4f}\"\n    )\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n\n# ---------- test evaluation ----------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(\"test\")\nprint(f\"Test MacroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, json, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"num_epochs\": {  # <-- hyper-parameter we sweep\n        \"SPR_BENCH\": {}  # every epoch budget gets its own sub-dict below\n    }\n}\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(file):  # small helper\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# you can change SPR_PATH with an env variable if needed\ndata_root = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nspr = load_spr_bench(data_root)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id, vocab_size = vocab[\"<PAD>\"], vocab[\"<UNK>\"], len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):  # char to id\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- helpers ----------\ndef run_epoch(model, loader, is_train, optimizer=None):\n    model.train() if is_train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyper-parameter sweep ----------\nepoch_candidates = [10, 20, 30]\n\nfor epochs in epoch_candidates:\n    print(f\"\\n=== Training with {epochs} epochs ===\")\n    run_key = str(epochs)  # use as dict key\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][run_key] = {\n        \"epochs\": [],\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    best_f1, best_path = 0.0, os.path.join(working_dir, f\"best_{epochs}.pt\")\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(\n            model, loaders[\"train\"], is_train=True, optimizer=optimizer\n        )\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], is_train=False)\n        exp = experiment_data[\"num_epochs\"][\"SPR_BENCH\"][run_key]\n        exp[\"epochs\"].append(epoch)\n        exp[\"losses\"][\"train\"].append(tr_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp[\"metrics\"][\"val_f1\"].append(val_f1)\n\n        print(\n            f\"Epoch {epoch}/{epochs} | \"\n            f\"train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), best_path)\n\n    # ---------- test on best checkpoint ----------\n    model.load_state_dict(torch.load(best_path))\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], is_train=False\n    )\n\n    print(\n        f\"Finished {epochs} epochs: Best Val F1={best_f1:.4f} | \"\n        f\"Test F1={test_f1:.4f}\"\n    )\n\n    exp = experiment_data[\"num_epochs\"][\"SPR_BENCH\"][run_key]\n    exp[\"test_f1\"] = test_f1\n    exp[\"predictions\"] = test_preds\n    exp[\"ground_truth\"] = test_gts\n\n# ---------- save everything ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ------------- reproducibility -------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# ------------- experiment bookkeeping -------------\nexperiment_data = {\"learning_rate\": {\"SPR_BENCH\": {}}}\n\n# ------------- device & dirs -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nprint(f\"Using device: {device}\")\n\n\n# ------------- dataset loader -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ------------- build vocab -------------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id = vocab[\"<PAD>\"]\nunk_id = vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ------------- encode sequences -------------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ------------- collate -------------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ------------- model -------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ------------- helper : run epoch -------------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ------------- training & tuning -------------\nlr_grid = [1e-4, 5e-4, 1e-3]\nepochs = 10\nbest_global_f1 = 0.0\nbest_global_info = {\"lr\": None, \"state_dict\": None, \"preds\": None, \"gts\": None}\n\nfor lr in lr_grid:\n    key = f\"lr_{lr:.0e}\"\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    best_val_f1 = 0.0\n    best_state = None\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n        scheduler.step()\n\n        exp_rec = experiment_data[\"learning_rate\"][\"SPR_BENCH\"][key]\n        exp_rec[\"epochs\"].append(epoch)\n        exp_rec[\"losses\"][\"train\"].append(tr_loss)\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n        exp_rec[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp_rec[\"metrics\"][\"val_f1\"].append(val_f1)\n\n        print(\n            f\"[{key}] Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_state = model.state_dict()\n\n    # ----- test with best checkpoint of this LR -----\n    model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][key][\"predictions\"] = test_preds\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][key][\"ground_truth\"] = test_gts\n    print(f\"[{key}] Best Val F1={best_val_f1:.4f} | Test F1={test_f1:.4f}\")\n\n    if best_val_f1 > best_global_f1:\n        best_global_f1 = best_val_f1\n        best_global_info = {\n            \"lr\": lr,\n            \"state_dict\": best_state,\n            \"preds\": test_preds,\n            \"gts\": test_gts,\n        }\n\nprint(f\"Best LR: {best_global_info['lr']} with Dev F1={best_global_f1:.4f}\")\n\n# ------------- save experiment data -------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ---------- reproducibility ----------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Loaded SPR_BENCH with {len(spr['train'])} train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id, vocab_size = vocab[\"<PAD>\"], vocab[\"<UNK>\"], len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences once ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)  # max len 5k\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- hyperparameter tuning over batch size ----------\nbatch_sizes = [32, 64, 128, 256]\nepochs = 10\nexperiment_data = {\"batch_size\": {}}\n\n\ndef run_epoch(model, loader, criterion, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\nfor bsz in batch_sizes:\n    print(f\"\\n========== Training with batch_size={bsz} ==========\")\n    # DataLoaders\n    loaders = {\n        split: DataLoader(\n            spr[split],\n            batch_size=bsz,\n            shuffle=(split == \"train\"),\n            collate_fn=collate_fn,\n        )\n        for split in [\"train\", \"dev\", \"test\"]\n    }\n\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    exp_entry = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    best_f1 = 0.0\n    best_path = os.path.join(working_dir, f\"best_model_bsz{bsz}.pt\")\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n\n        exp_entry[\"epochs\"].append(epoch)\n        exp_entry[\"losses\"][\"train\"].append(tr_loss)\n        exp_entry[\"losses\"][\"val\"].append(val_loss)\n        exp_entry[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp_entry[\"metrics\"][\"val_f1\"].append(val_f1)\n\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), best_path)\n\n    # ---------- test evaluation ----------\n    model.load_state_dict(torch.load(best_path))\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    print(f\"Batch {bsz} -> Test MacroF1: {test_f1:.4f}\")\n\n    exp_entry[\"predictions\"] = test_preds\n    exp_entry[\"ground_truth\"] = test_gts\n    exp_entry[\"test_f1\"] = test_f1\n    experiment_data[\"batch_size\"][bsz] = exp_entry\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ---------- saving dict ----------\nexperiment_data = {\n    \"dropout_sweep\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"dropouts\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- dataset ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Loaded SPR_BENCH with {len(spr['train'])} train examples.\")\n\n\n# ---------- vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id = vocab[\"<PAD>\"]\nunk_id = vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.in_drop = nn.Dropout(dropout)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.in_drop(x)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- training helpers ----------\ndef run_epoch(model, loader, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- sweep ----------\ndropout_values = [0.0, 0.1, 0.15, 0.25, 0.35]\nepochs = 15\n\nfor p_drop in dropout_values:\n    print(f\"\\n=== Training with dropout={p_drop} ===\")\n    model = CharTransformer(vocab_size, num_labels, dropout=p_drop).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    best_f1, best_state = 0.0, None\n    tr_losses, val_losses, tr_f1s, val_f1s = [], [], [], []\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"])\n        tr_losses.append(tr_loss)\n        val_losses.append(val_loss)\n        tr_f1s.append(tr_f1)\n        val_f1s.append(val_f1)\n        print(\n            f\"  Epoch {epoch:02d} | train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n\n    # reload best and test\n    model.load_state_dict(best_state)\n    test_loss, test_f1, preds, gts = run_epoch(model, loaders[\"test\"])\n\n    # save run data\n    ed = experiment_data[\"dropout_sweep\"][\"SPR_BENCH\"]\n    ed[\"dropouts\"].append(p_drop)\n    ed[\"losses\"][\"train\"].append(tr_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1s)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1s)\n    ed[\"predictions\"].append(preds)\n    ed[\"ground_truth\"].append(gts)\n    ed[\"epochs\"].append(list(range(1, epochs + 1)))\n\n    print(f\"Best val_F1={best_f1:.4f} | Test_F1={test_f1:.4f}\")\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 164003.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 116044.27\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 151129.75\nexamples/s]', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 24820.57 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n23990.76 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 24867.07\nexamples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6648\nval_loss=0.5937 val_MacroF1=0.6981', '\\n', 'Epoch 2: train_loss=0.5566\nval_loss=0.5504 val_MacroF1=0.7860', '\\n', 'Epoch 3: train_loss=0.5348\nval_loss=0.5389 val_MacroF1=0.7740', '\\n', 'Epoch 4: train_loss=0.5197\nval_loss=0.5389 val_MacroF1=0.7860', '\\n', 'Epoch 5: train_loss=0.5195\nval_loss=0.5226 val_MacroF1=0.7959', '\\n', 'Epoch 6: train_loss=0.5153\nval_loss=0.5195 val_MacroF1=0.7920', '\\n', 'Epoch 7: train_loss=0.5120\nval_loss=0.5155 val_MacroF1=0.7959', '\\n', 'Epoch 8: train_loss=0.5089\nval_loss=0.5125 val_MacroF1=0.7959', '\\n', 'Epoch 9: train_loss=0.5090\nval_loss=0.5131 val_MacroF1=0.7959', '\\n', 'Epoch 10: train_loss=0.5090\nval_loss=0.5115 val_MacroF1=0.7959', '\\n', 'Test MacroF1: 0.7950', '\\n',\n'Execution time: 5 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 38, in <module>\\n    spr = load_spr_bench(data_root)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 32, in load_spr_bench\\n\ntrain=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\\n\n^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 27, in _load\\n    return\nload_dataset(\\n           ^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n46_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n6/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 91997.50\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 66771.27\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 94476.95\nexamples/s]', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 19830.10 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n23799.90 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 25151.59\nexamples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[lr_1e-04] Epoch 1:\ntrain_loss=0.6468 val_loss=0.5939 val_F1=0.7619', '\\n', '[lr_1e-04] Epoch 2:\ntrain_loss=0.5615 val_loss=0.5492 val_F1=0.7619', '\\n', '[lr_1e-04] Epoch 3:\ntrain_loss=0.5347 val_loss=0.5418 val_F1=0.7820', '\\n', '[lr_1e-04] Epoch 4:\ntrain_loss=0.5272 val_loss=0.5372 val_F1=0.7860', '\\n', '[lr_1e-04] Epoch 5:\ntrain_loss=0.5249 val_loss=0.5375 val_F1=0.7860', '\\n', '[lr_1e-04] Epoch 6:\ntrain_loss=0.5231 val_loss=0.5341 val_F1=0.7860', '\\n', '[lr_1e-04] Epoch 7:\ntrain_loss=0.5211 val_loss=0.5359 val_F1=0.7860', '\\n', '[lr_1e-04] Epoch 8:\ntrain_loss=0.5199 val_loss=0.5325 val_F1=0.7860', '\\n', '[lr_1e-04] Epoch 9:\ntrain_loss=0.5193 val_loss=0.5323 val_F1=0.7860', '\\n', '[lr_1e-04] Epoch 10:\ntrain_loss=0.5178 val_loss=0.5322 val_F1=0.7860', '\\n', '[lr_1e-04] Best Val\nF1=0.7860 | Test F1=0.7889', '\\n', '[lr_5e-04] Epoch 1: train_loss=0.6949\nval_loss=0.5716 val_F1=0.7414', '\\n', '[lr_5e-04] Epoch 2: train_loss=0.5411\nval_loss=0.5365 val_F1=0.7860', '\\n', '[lr_5e-04] Epoch 3: train_loss=0.5212\nval_loss=0.5341 val_F1=0.7860', '\\n', '[lr_5e-04] Epoch 4: train_loss=0.5123\nval_loss=0.5151 val_F1=0.7959', '\\n', '[lr_5e-04] Epoch 5: train_loss=0.5120\nval_loss=0.5235 val_F1=0.7860', '\\n', '[lr_5e-04] Epoch 6: train_loss=0.5108\nval_loss=0.5093 val_F1=0.7959', '\\n', '[lr_5e-04] Epoch 7: train_loss=0.5086\nval_loss=0.5182 val_F1=0.7920', '\\n', '[lr_5e-04] Epoch 8: train_loss=0.5082\nval_loss=0.5094 val_F1=0.7959', '\\n', '[lr_5e-04] Epoch 9: train_loss=0.5055\nval_loss=0.5094 val_F1=0.7959', '\\n', '[lr_5e-04] Epoch 10: train_loss=0.5047\nval_loss=0.5102 val_F1=0.7959', '\\n', '[lr_5e-04] Best Val F1=0.7959 | Test\nF1=0.7950', '\\n', '[lr_1e-03] Epoch 1: train_loss=0.7809 val_loss=0.6494\nval_F1=0.3421', '\\n', '[lr_1e-03] Epoch 2: train_loss=0.5578 val_loss=0.5405\nval_F1=0.7860', '\\n', '[lr_1e-03] Epoch 3: train_loss=0.5410 val_loss=0.5245\nval_F1=0.7860', '\\n', '[lr_1e-03] Epoch 4: train_loss=0.5187 val_loss=0.5589\nval_F1=0.7599', '\\n', '[lr_1e-03] Epoch 5: train_loss=0.5156 val_loss=0.5255\nval_F1=0.7860', '\\n', '[lr_1e-03] Epoch 6: train_loss=0.5108 val_loss=0.5248\nval_F1=0.7860', '\\n', '[lr_1e-03] Epoch 7: train_loss=0.5078 val_loss=0.5075\nval_F1=0.7959', '\\n', '[lr_1e-03] Epoch 8: train_loss=0.5060 val_loss=0.5067\nval_F1=0.7959', '\\n', '[lr_1e-03] Epoch 9: train_loss=0.5053 val_loss=0.5079\nval_F1=0.7959', '\\n', '[lr_1e-03] Epoch 10: train_loss=0.5042 val_loss=0.5090\nval_F1=0.7959', '\\n', '[lr_1e-03] Best Val F1=0.7959 | Test F1=0.7950', '\\n',\n'Best LR: 0.0005 with Dev F1=0.7959', '\\n', 'Execution time: 12 seconds seconds\n(time limit is 30 minutes).']", "['\\rGenerating train split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating\ntrain split: 2000 examples [00:00, 101971.80 examples/s]', '\\n', '\\rGenerating\ntrain split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split:\n500 examples [00:00, 112027.35 examples/s]', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 1000 examples\n[00:00, 179075.40 examples/s]', '\\n', 'Loaded SPR_BENCH with 2000 train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 24483.35 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n24097.44 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 25546.83\nexamples/s]', '\\n', 'Using device: cuda', '\\n', '\\n========== Training with\nbatch_size=32 ==========', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.5754\nval_loss=0.5382 val_F1=0.7959', '\\n', 'Epoch 2: train_loss=0.5369\nval_loss=0.5280 val_F1=0.7959', '\\n', 'Epoch 3: train_loss=0.5275\nval_loss=0.5485 val_F1=0.7599', '\\n', 'Epoch 4: train_loss=0.5152\nval_loss=0.5225 val_F1=0.7860', '\\n', 'Epoch 5: train_loss=0.5181\nval_loss=0.5155 val_F1=0.7860', '\\n', 'Epoch 6: train_loss=0.5137\nval_loss=0.5336 val_F1=0.7860', '\\n', 'Epoch 7: train_loss=0.5160\nval_loss=0.5107 val_F1=0.7959', '\\n', 'Epoch 8: train_loss=0.5129\nval_loss=0.5077 val_F1=0.7959', '\\n', 'Epoch 9: train_loss=0.5107\nval_loss=0.5184 val_F1=0.7860', '\\n', 'Epoch 10: train_loss=0.5114\nval_loss=0.5059 val_F1=0.7959', '\\n', 'Batch 32 -> Test MacroF1: 0.7960', '\\n',\n'\\n========== Training with batch_size=64 ==========', '\\n', 'Epoch 1:\ntrain_loss=0.5874 val_loss=0.5422 val_F1=0.7680', '\\n', 'Epoch 2:\ntrain_loss=0.5507 val_loss=0.5294 val_F1=0.7860', '\\n', 'Epoch 3:\ntrain_loss=0.5244 val_loss=0.5196 val_F1=0.7860', '\\n', 'Epoch 4:\ntrain_loss=0.5161 val_loss=0.5171 val_F1=0.7860', '\\n', 'Epoch 5:\ntrain_loss=0.5140 val_loss=0.5293 val_F1=0.7860', '\\n', 'Epoch 6:\ntrain_loss=0.5227 val_loss=0.5135 val_F1=0.7959', '\\n', 'Epoch 7:\ntrain_loss=0.5118 val_loss=0.5177 val_F1=0.7860', '\\n', 'Epoch 8:\ntrain_loss=0.5101 val_loss=0.5072 val_F1=0.7959', '\\n', 'Epoch 9:\ntrain_loss=0.5095 val_loss=0.5501 val_F1=0.7680', '\\n', 'Epoch 10:\ntrain_loss=0.5151 val_loss=0.5106 val_F1=0.7959', '\\n', 'Batch 64 -> Test\nMacroF1: 0.7950', '\\n', '\\n========== Training with batch_size=128 ==========',\n'\\n', 'Epoch 1: train_loss=0.6420 val_loss=0.5495 val_F1=0.7880', '\\n', 'Epoch\n2: train_loss=0.5366 val_loss=0.5772 val_F1=0.7558', '\\n', 'Epoch 3:\ntrain_loss=0.5433 val_loss=0.5485 val_F1=0.7959', '\\n', 'Epoch 4:\ntrain_loss=0.5320 val_loss=0.5522 val_F1=0.7599', '\\n', 'Epoch 5:\ntrain_loss=0.5233 val_loss=0.5325 val_F1=0.7860', '\\n', 'Epoch 6:\ntrain_loss=0.5158 val_loss=0.5196 val_F1=0.7860', '\\n', 'Epoch 7:\ntrain_loss=0.5107 val_loss=0.5159 val_F1=0.7959', '\\n', 'Epoch 8:\ntrain_loss=0.5103 val_loss=0.5152 val_F1=0.7959', '\\n', 'Epoch 9:\ntrain_loss=0.5131 val_loss=0.5278 val_F1=0.7860', '\\n', 'Epoch 10:\ntrain_loss=0.5113 val_loss=0.5311 val_F1=0.7959', '\\n', 'Batch 128 -> Test\nMacroF1: 0.7950', '\\n', '\\n========== Training with batch_size=256 ==========',\n'\\n', 'Epoch 1: train_loss=0.7120 val_loss=0.6381 val_F1=0.7719', '\\n', 'Epoch\n2: train_loss=0.6090 val_loss=0.5588 val_F1=0.7900', '\\n', 'Epoch 3:\ntrain_loss=0.5444 val_loss=0.5553 val_F1=0.7959', '\\n', 'Epoch 4:\ntrain_loss=0.5293 val_loss=0.5371 val_F1=0.7860', '\\n', 'Epoch 5:\ntrain_loss=0.5245 val_loss=0.5338 val_F1=0.7860', '\\n', 'Epoch 6:\ntrain_loss=0.5199 val_loss=0.5314 val_F1=0.7860', '\\n', 'Epoch 7:\ntrain_loss=0.5174 val_loss=0.5276 val_F1=0.7860', '\\n', 'Epoch 8:\ntrain_loss=0.5174 val_loss=0.5247 val_F1=0.7900', '\\n', 'Epoch 9:\ntrain_loss=0.5170 val_loss=0.5241 val_F1=0.7959', '\\n', 'Epoch 10:\ntrain_loss=0.5157 val_loss=0.5248 val_F1=0.7959', '\\n', 'Batch 256 -> Test\nMacroF1: 0.7950', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Sci\nentist-v2/experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n22 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n104622.20 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 74997.39\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 138737.23\nexamples/s]', '\\n', 'Loaded SPR_BENCH with 2000 train examples.', '\\n', 'Vocab\nsize:', ' ', '11', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 21428.81\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 23706.81 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 1000/1000 [00:00<00:00, 25346.29 examples/s]', '\\n', '\\n===\nTraining with dropout=0.0 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Epoch 01 | train_loss=0.6681\nval_loss=0.5894 val_F1=0.7227', '\\n', '  Epoch 02 | train_loss=0.5548\nval_loss=0.5469 val_F1=0.7860', '\\n', '  Epoch 03 | train_loss=0.5345\nval_loss=0.5433 val_F1=0.7599', '\\n', '  Epoch 04 | train_loss=0.5194\nval_loss=0.5392 val_F1=0.7860', '\\n', '  Epoch 05 | train_loss=0.5190\nval_loss=0.5240 val_F1=0.7920', '\\n', '  Epoch 06 | train_loss=0.5156\nval_loss=0.5221 val_F1=0.7880', '\\n', '  Epoch 07 | train_loss=0.5113\nval_loss=0.5161 val_F1=0.7959', '\\n', '  Epoch 08 | train_loss=0.5087\nval_loss=0.5144 val_F1=0.7940', '\\n', '  Epoch 09 | train_loss=0.5095\nval_loss=0.5141 val_F1=0.7959', '\\n', '  Epoch 10 | train_loss=0.5090\nval_loss=0.5128 val_F1=0.7959', '\\n', '  Epoch 11 | train_loss=0.5072\nval_loss=0.5134 val_F1=0.7959', '\\n', '  Epoch 12 | train_loss=0.5052\nval_loss=0.5149 val_F1=0.7959', '\\n', '  Epoch 13 | train_loss=0.5117\nval_loss=0.5092 val_F1=0.7959', '\\n', '  Epoch 14 | train_loss=0.5140\nval_loss=0.5189 val_F1=0.7860', '\\n', '  Epoch 15 | train_loss=0.5104\nval_loss=0.5158 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', '\\n=== Training with dropout=0.1 ===', '\\n', '  Epoch 01 |\ntrain_loss=0.6438 val_loss=0.5662 val_F1=0.7394', '\\n', '  Epoch 02 |\ntrain_loss=0.5404 val_loss=0.5411 val_F1=0.7860', '\\n', '  Epoch 03 |\ntrain_loss=0.5241 val_loss=0.5527 val_F1=0.7800', '\\n', '  Epoch 04 |\ntrain_loss=0.5268 val_loss=0.5317 val_F1=0.7880', '\\n', '  Epoch 05 |\ntrain_loss=0.5215 val_loss=0.5243 val_F1=0.7860', '\\n', '  Epoch 06 |\ntrain_loss=0.5142 val_loss=0.5216 val_F1=0.7959', '\\n', '  Epoch 07 |\ntrain_loss=0.5159 val_loss=0.5177 val_F1=0.7880', '\\n', '  Epoch 08 |\ntrain_loss=0.5220 val_loss=0.5193 val_F1=0.7860', '\\n', '  Epoch 09 |\ntrain_loss=0.5174 val_loss=0.5294 val_F1=0.7959', '\\n', '  Epoch 10 |\ntrain_loss=0.5199 val_loss=0.5237 val_F1=0.7860', '\\n', '  Epoch 11 |\ntrain_loss=0.5127 val_loss=0.5148 val_F1=0.7959', '\\n', '  Epoch 12 |\ntrain_loss=0.5086 val_loss=0.5134 val_F1=0.7959', '\\n', '  Epoch 13 |\ntrain_loss=0.5099 val_loss=0.5096 val_F1=0.7959', '\\n', '  Epoch 14 |\ntrain_loss=0.5049 val_loss=0.5135 val_F1=0.7959', '\\n', '  Epoch 15 |\ntrain_loss=0.5119 val_loss=0.5104 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 |\nTest_F1=0.7950', '\\n', '\\n=== Training with dropout=0.15 ===', '\\n', '  Epoch 01\n| train_loss=0.6487 val_loss=0.5689 val_F1=0.7393', '\\n', '  Epoch 02 |\ntrain_loss=0.5431 val_loss=0.5480 val_F1=0.7639', '\\n', '  Epoch 03 |\ntrain_loss=0.5315 val_loss=0.5419 val_F1=0.7959', '\\n', '  Epoch 04 |\ntrain_loss=0.5339 val_loss=0.5412 val_F1=0.7959', '\\n', '  Epoch 05 |\ntrain_loss=0.5221 val_loss=0.5215 val_F1=0.7940', '\\n', '  Epoch 06 |\ntrain_loss=0.5203 val_loss=0.5162 val_F1=0.7920', '\\n', '  Epoch 07 |\ntrain_loss=0.5159 val_loss=0.5130 val_F1=0.7959', '\\n', '  Epoch 08 |\ntrain_loss=0.5206 val_loss=0.5187 val_F1=0.7959', '\\n', '  Epoch 09 |\ntrain_loss=0.5128 val_loss=0.5128 val_F1=0.7959', '\\n', '  Epoch 10 |\ntrain_loss=0.5157 val_loss=0.5231 val_F1=0.7860', '\\n', '  Epoch 11 |\ntrain_loss=0.5206 val_loss=0.5148 val_F1=0.7959', '\\n', '  Epoch 12 |\ntrain_loss=0.5133 val_loss=0.5090 val_F1=0.7959', '\\n', '  Epoch 13 |\ntrain_loss=0.5121 val_loss=0.5261 val_F1=0.7860', '\\n', '  Epoch 14 |\ntrain_loss=0.5190 val_loss=0.5173 val_F1=0.7920', '\\n', '  Epoch 15 |\ntrain_loss=0.5120 val_loss=0.5067 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 |\nTest_F1=0.7950', '\\n', '\\n=== Training with dropout=0.25 ===', '\\n', '  Epoch 01\n| train_loss=0.6707 val_loss=0.5710 val_F1=0.7517', '\\n', '  Epoch 02 |\ntrain_loss=0.5437 val_loss=0.5776 val_F1=0.7860', '\\n', '  Epoch 03 |\ntrain_loss=0.5382 val_loss=0.5420 val_F1=0.7860', '\\n', '  Epoch 04 |\ntrain_loss=0.5322 val_loss=0.5475 val_F1=0.7959', '\\n', '  Epoch 05 |\ntrain_loss=0.5300 val_loss=0.5916 val_F1=0.7488', '\\n', '  Epoch 06 |\ntrain_loss=0.5261 val_loss=0.5351 val_F1=0.7959', '\\n', '  Epoch 07 |\ntrain_loss=0.5275 val_loss=0.5484 val_F1=0.7959', '\\n', '  Epoch 08 |\ntrain_loss=0.5247 val_loss=0.5210 val_F1=0.7959', '\\n', '  Epoch 09 |\ntrain_loss=0.5153 val_loss=0.5184 val_F1=0.7959', '\\n', '  Epoch 10 |\ntrain_loss=0.5135 val_loss=0.5100 val_F1=0.7959', '\\n', '  Epoch 11 |\ntrain_loss=0.5205 val_loss=0.5373 val_F1=0.7940', '\\n', '  Epoch 12 |\ntrain_loss=0.5166 val_loss=0.5135 val_F1=0.7959', '\\n', '  Epoch 13 |\ntrain_loss=0.5178 val_loss=0.5385 val_F1=0.7959', '\\n', '  Epoch 14 |\ntrain_loss=0.5152 val_loss=0.5424 val_F1=0.7777', '\\n', '  Epoch 15 |\ntrain_loss=0.5089 val_loss=0.5354 val_F1=0.7899', '\\n', 'Best val_F1=0.7959 |\nTest_F1=0.7910', '\\n', '\\n=== Training with dropout=0.35 ===', '\\n', '  Epoch 01\n| train_loss=0.6501 val_loss=0.5473 val_F1=0.7720', '\\n', '  Epoch 02 |\ntrain_loss=0.5659 val_loss=0.5638 val_F1=0.7639', '\\n', '  Epoch 03 |\ntrain_loss=0.5454 val_loss=0.5780 val_F1=0.7959', '\\n', '  Epoch 04 |\ntrain_loss=0.5300 val_loss=0.5982 val_F1=0.7430', '\\n', '  Epoch 05 |\ntrain_loss=0.5319 val_loss=0.5727 val_F1=0.7834', '\\n', '  Epoch 06 |\ntrain_loss=0.5291 val_loss=0.5449 val_F1=0.7959', '\\n', '  Epoch 07 |\ntrain_loss=0.5256 val_loss=0.5622 val_F1=0.7812', '\\n', '  Epoch 08 |\ntrain_loss=0.5265 val_loss=0.5744 val_F1=0.7265', '\\n', '  Epoch 09 |\ntrain_loss=0.5150 val_loss=0.6335 val_F1=0.4115', '\\n', '  Epoch 10 |\ntrain_loss=0.5125 val_loss=0.5137 val_F1=0.7939', '\\n', '  Epoch 11 |\ntrain_loss=0.5122 val_loss=0.5236 val_F1=0.7877', '\\n', '  Epoch 12 |\ntrain_loss=0.5122 val_loss=0.5086 val_F1=0.7959', '\\n', '  Epoch 13 |\ntrain_loss=0.5105 val_loss=0.7386 val_F1=0.3458', '\\n', '  Epoch 14 |\ntrain_loss=0.5097 val_loss=0.5346 val_F1=0.7728', '\\n', '  Epoch 15 |\ntrain_loss=0.5119 val_loss=0.5124 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 |\nTest_F1=0.7950', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 25\nseconds seconds (time limit is 30 minutes).']"], "analysis": ["The training script executed successfully without any errors or bugs. The model\nwas trained on the SPR_BENCH dataset, and the training, validation, and test\nlosses and macro F1 scores were recorded. The best validation macro F1 score\nachieved was 0.7959, and the test macro F1 score was 0.7950. Overall, the\nimplementation appears to be functioning correctly for this preliminary stage.", "The execution failed due to a FileNotFoundError. The code was unable to locate\nthe dataset file 'train.csv' at the specified path '/home/zxl240011/AI-Scientist\n-v2/experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-6/SPR_BENCH/train.csv'.  Proposed Fix: 1. Verify that\nthe dataset files ('train.csv', 'dev.csv', 'test.csv') are present in the\nspecified directory. 2. If the files are located in a different directory,\nupdate the 'data_root' path in the code to point to the correct location. 3.\nEnsure that the environment variable 'SPR_PATH' is set correctly if it is being\nused to specify the dataset path. 4. Add a check in the code to validate the\nexistence of the dataset files before attempting to load them, and provide a\nclear error message if they are missing.", "", "The training script executed successfully without any bugs. It loaded the\ndataset, built the vocabulary, encoded sequences, and trained the model using\ndifferent batch sizes. The Macro F1 scores for the test set were consistent\nacross batch sizes, indicating stable performance. The experiment data was saved\nas expected. No issues were observed.", ""], "exc_type": [null, "FileNotFoundError", null, null, null], "exc_info": [null, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-6/SPR_BENCH/train.csv'"]}, null, null, null], "exc_stack": [null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 38, "<module>", "spr = load_spr_bench(data_root)"], ["runfile.py", 32, "load_spr_bench", "train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")"], ["runfile.py", 27, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "Measures the harmonic mean of precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.7959}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Represents the error in the model's predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5115, "best_value": 0.509}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.504736, "best_value": 0.504736}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796993, "best_value": 0.796993}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.510158, "best_value": 0.510158}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795948, "best_value": 0.795948}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79499, "best_value": 0.79499}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score for the training dataset.", "data": [{"dataset_name": "batch_size=32", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "batch_size=64", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "batch_size=128", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "batch_size=256", "final_value": 0.7945, "best_value": 0.7945}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score for the validation dataset.", "data": [{"dataset_name": "batch_size=32", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "batch_size=64", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "batch_size=128", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "batch_size=256", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss for the training dataset.", "data": [{"dataset_name": "batch_size=32", "final_value": 0.5114, "best_value": 0.5114}, {"dataset_name": "batch_size=64", "final_value": 0.5151, "best_value": 0.5151}, {"dataset_name": "batch_size=128", "final_value": 0.5113, "best_value": 0.5113}, {"dataset_name": "batch_size=256", "final_value": 0.5157, "best_value": 0.5157}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset.", "data": [{"dataset_name": "batch_size=32", "final_value": 0.5059, "best_value": 0.5059}, {"dataset_name": "batch_size=64", "final_value": 0.5106, "best_value": 0.5106}, {"dataset_name": "batch_size=128", "final_value": 0.5311, "best_value": 0.5311}, {"dataset_name": "batch_size=256", "final_value": 0.5248, "best_value": 0.5248}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score for the test dataset.", "data": [{"dataset_name": "batch_size=32", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "batch_size=64", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "batch_size=128", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "batch_size=256", "final_value": 0.795, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Lowest training loss achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5089, "best_value": 0.5049}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Lowest validation loss achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.51, "best_value": 0.5067}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "Best F1 score achieved on training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.797}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Best F1 score achieved on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score achieved on test data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}]}], "is_best_node": [false, false, true, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_loss_curve.png", "../../logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_f1_curve.png", "../../logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_confusion_matrix_lr_5e-04.png"], ["../../logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_test_macroF1_vs_batchsize.png"], ["../../logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_train_F1_curves.png", "../../logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_val_F1_curves.png", "../../logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_train_loss_curves.png", "../../logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_val_loss_curves.png", "../../logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_best_val_F1_vs_dropout.png"]], "plot_paths": [["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_loss_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_f1_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_confusion_matrix.png"], [], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_confusion_matrix_lr_5e-04.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_test_macroF1_vs_batchsize.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_train_F1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_val_F1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_train_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_val_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_best_val_F1_vs_dropout.png"]], "plot_analyses": [[{"analysis": "The loss curve indicates a steady decrease in both training and validation loss over the epochs, showing convergence of the model. The validation loss closely follows the training loss, suggesting that the model is not overfitting and is generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_loss_curve.png"}, {"analysis": "The Macro-F1 score curve shows a rapid improvement in both training and validation scores within the first few epochs, stabilizing around a similar value. The close alignment of the training and validation curves indicates a well-trained model with minimal overfitting.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_f1_curve.png"}, {"analysis": "The confusion matrix shows a clear division between the predicted labels and true labels, indicating good classification performance. The diagonal dominance suggests that the model accurately predicts the majority of instances, with relatively few misclassifications.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_a1aac18ba84f46a1a4f9dc3e7612a135_proc_3154742/spr_bench_confusion_matrix.png"}], [], [{"analysis": "The loss curves show that all learning rates converge to a similar loss value of approximately 0.5 after 10 epochs. However, the learning rate of 1e-03 shows instability in the validation loss, indicating potential overfitting or difficulty in generalizing. The learning rates 1e-04 and 5e-04 show smoother convergence with better alignment between training and validation losses, suggesting they are more suitable for this task.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_loss_curves.png"}, {"analysis": "The F1 curves indicate that all learning rates reach a macro F1 score of approximately 0.8 by the end of 10 epochs. The learning rate of 1e-04 has the most stable performance, with consistent F1 scores for both training and validation sets. The learning rate of 5e-04 also performs well, but the validation scores show slight fluctuations, particularly in the initial epochs. The learning rate of 1e-03 shows more pronounced instability, especially in the first few epochs, further reinforcing its unsuitability for this task.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix for the best-performing learning rate (5e-04) shows good classification performance, with 394 true negatives and 401 true positives. However, there are still 104 false positives and 101 false negatives, indicating room for improvement in reducing misclassifications. This performance aligns with the macro F1 score of 0.8 observed in the F1 curves.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6246d4622fb9422c8f5a990e12eb5cc2_proc_3158571/SPR_BENCH_confusion_matrix_lr_5e-04.png"}], [{"analysis": "The loss curves indicate that the model's training loss decreases steadily across all batch sizes, converging to a similar value by epoch 10. Validation loss generally follows a similar trend but shows more fluctuations, particularly for larger batch sizes like 256. This suggests that smaller batch sizes might provide more stable validation performance, potentially due to better generalization. The convergence of training and validation loss values across batch sizes suggests that the model's capacity is sufficient to fit the data.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_loss_curves.png"}, {"analysis": "The macro-F1 curves show that performance improves rapidly within the first few epochs and stabilizes around epoch 5 for both training and validation sets. For batch sizes 32, 64, and 128, the validation macro-F1 scores closely track the training scores, indicating minimal overfitting. However, for batch size 256, there is a slight lag in validation performance compared to training, hinting at potential generalization challenges with larger batch sizes.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_f1_curves.png"}, {"analysis": "The bar chart shows that the best test macro-F1 scores are nearly identical across all batch sizes, ranging from 0.795 to 0.796. This suggests that batch size has minimal impact on the model's ultimate test performance. However, considering computational efficiency and stability, smaller batch sizes like 32 may be preferable for this task.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_2599e7b4dc40479788a18e8597c7422a_proc_3158572/SPR_BENCH_test_macroF1_vs_batchsize.png"}], [{"analysis": "This plot shows the training F1 scores over epochs for different dropout rates. All dropout configurations converge to a similar training F1 score of approximately 0.80 by the end of training. The model with no dropout (dropout=0.0) starts slightly lower but catches up quickly, suggesting that dropout does not significantly affect the final training performance. However, the convergence speed slightly varies across configurations, with dropout=0.1 and dropout=0.15 showing slightly faster stabilization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_train_F1_curves.png"}, {"analysis": "This plot illustrates validation F1 scores over epochs for various dropout rates. While most dropout configurations maintain stable validation F1 scores near 0.80, dropout=0.35 exhibits instability, with significant drops at certain epochs. This indicates that higher dropout rates might lead to over-regularization, causing fluctuations in performance. Dropout values between 0.1 and 0.25 appear to provide more consistent validation performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_val_F1_curves.png"}, {"analysis": "The training loss decreases steadily across epochs for all dropout configurations, converging to a similar value around 0.50. Dropout rates of 0.1 and 0.15 show slightly faster loss reduction, indicating efficient learning. Higher dropout rates (e.g., 0.35) do not appear to hinder convergence but may slow down the initial learning rate slightly.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_train_loss_curves.png"}, {"analysis": "Validation loss trends for different dropout rates indicate that lower dropout values (0.1 and 0.15) achieve more stable and lower loss values. Dropout=0.35 shows significant spikes in loss at certain epochs, suggesting that excessive dropout may harm generalization. Dropout values in the range of 0.1 to 0.25 strike a balance between regularization and stability, leading to a smoother loss curve.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_val_loss_curves.png"}, {"analysis": "The summary scatter plot confirms that all dropout configurations achieve the same best validation F1 score of 0.796. This suggests that the choice of dropout rate does not significantly impact the peak validation performance. However, stability and convergence behavior during training, as observed in previous plots, may still inform the optimal dropout selection.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_494cc2470c9744ba88c898d8b9761e31_proc_3158573/SPR_BENCH_best_val_F1_vs_dropout.png"}]], "vlm_feedback_summary": ["The experimental results show promising performance. The loss and Macro-F1\ncurves indicate robust training and validation processes, with minimal\noverfitting. The confusion matrix further validates the model's classification\naccuracy, demonstrating effective symbolic reasoning capabilities on the\nSPR_BENCH dataset.", "[]", "The analysis highlights that learning rates 1e-04 and 5e-04 perform well in\nterms of loss and F1 score stability, with 5e-04 being the best-performing\nlearning rate overall. The confusion matrix confirms good classification\nperformance but suggests areas for reducing misclassifications. The learning\nrate of 1e-03 is less suitable due to instability in both loss and F1 metrics.", "The plots reveal consistent performance across batch sizes, with smaller batch\nsizes showing slightly more stable validation trends. The model achieves a near-\noptimal macro-F1 score of approximately 0.796, indicating strong performance on\nthe SPR_BENCH task.", "The analysis highlights that while all dropout configurations achieve the same\npeak validation F1 score, lower dropout rates (0.1 to 0.25) offer more stable\nand consistent performance during training and validation. Higher dropout rates\n(e.g., 0.35) introduce instability, as seen in fluctuations in validation F1 and\nspikes in validation loss. Training loss trends suggest efficient learning with\ndropout values of 0.1 and 0.15. Overall, dropout values in the range of 0.1 to\n0.25 are recommended for achieving a balance between regularization and\nperformance stability."], "exec_time": [5.797231197357178, 1.098698377609253, 12.299885034561157, 22.251184940338135, 25.304816722869873], "exec_time_feedback": ["", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], [], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nds_name = \"SPR_BENCH\"\ndata = experiment_data.get(ds_name, {})\n\nepochs = data.get(\"epochs\", [])\ntrain_l = data.get(\"losses\", {}).get(\"train\", [])\nval_l = data.get(\"losses\", {}).get(\"val\", [])\ntrain_f1 = data.get(\"metrics\", {}).get(\"train_f1\", [])\nval_f1 = data.get(\"metrics\", {}).get(\"val_f1\", [])\npreds = data.get(\"predictions\", [])\ngts = data.get(\"ground_truth\", [])\n\n# Plot 1: Loss curves\ntry:\n    if epochs and train_l and val_l:\n        plt.figure()\n        plt.plot(epochs, train_l, label=\"Train Loss\")\n        plt.plot(epochs, val_l, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{ds_name} Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name.lower()}_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# Plot 2: F1 curves\ntry:\n    if epochs and train_f1 and val_f1:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name} Macro-F1 Curve\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name.lower()}_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# Plot 3: Confusion matrix\ntry:\n    if preds and gts:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(f\"{ds_name} Confusion Matrix\\nDataset: Test Split\")\n        fname = os.path.join(working_dir, f\"{ds_name.lower()}_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# Print overall Macro-F1 on test set if available\nif preds and gts:\n    print(\"Test Macro-F1:\", f1_score(gts, preds, average=\"macro\"))\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_runs = experiment_data.get(\"learning_rate\", {}).get(\"SPR_BENCH\", {})\n\n# ----------------- gather per-run data -----------------\nloss_curves, f1_curves = {}, {}\nbest_run_key, best_val_f1 = None, -1\nfor key, rec in spr_runs.items():\n    epochs = np.array(rec.get(\"epochs\", []))\n    tr_loss = np.array(rec.get(\"losses\", {}).get(\"train\", []))\n    val_loss = np.array(rec.get(\"losses\", {}).get(\"val\", []))\n    tr_f1 = np.array(rec.get(\"metrics\", {}).get(\"train_f1\", []))\n    val_f1 = np.array(rec.get(\"metrics\", {}).get(\"val_f1\", []))\n    if len(epochs):\n        loss_curves[key] = (epochs, tr_loss, val_loss)\n        f1_curves[key] = (epochs, tr_f1, val_f1)\n        if val_f1.max() > best_val_f1:\n            best_val_f1 = val_f1.max()\n            best_run_key = key\n\nprint(f\"Best run: {best_run_key} with Dev F1={best_val_f1:.4f}\")\n\n# ------------- plot 1: loss curves -------------\ntry:\n    plt.figure()\n    for key, (ep, tr, val) in loss_curves.items():\n        plt.plot(ep, tr, label=f\"{key} train\")\n        plt.plot(ep, val, \"--\", label=f\"{key} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curves (all learning rates)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------- plot 2: F1 curves -------------\ntry:\n    plt.figure()\n    for key, (ep, tr, val) in f1_curves.items():\n        plt.plot(ep, tr, label=f\"{key} train\")\n        plt.plot(ep, val, \"--\", label=f\"{key} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH \u2013 F1 Curves (all learning rates)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# ------------- plot 3: confusion matrix for best run -------------\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    if best_run_key:\n        preds = np.array(spr_runs[best_run_key][\"predictions\"])\n        gts = np.array(spr_runs[best_run_key][\"ground_truth\"])\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH \u2013 Confusion Matrix (best LR: {best_run_key})\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_{best_run_key}.png\")\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = exp.get(\"batch_size\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n\n# helper to gather lists aligned by epoch\ndef gather(metric_key, bsz):\n    return exp[bsz][\"metrics\" if \"f1\" in metric_key else \"losses\"][metric_key]\n\n\n# ---------- Figure 1: loss curves ----------\ntry:\n    plt.figure()\n    for bsz in sorted(exp):\n        epochs = exp[bsz][\"epochs\"]\n        plt.plot(epochs, gather(\"train\", bsz), label=f\"train bsz{bsz}\")\n        plt.plot(epochs, gather(\"val\", bsz), label=f\"val   bsz{bsz}\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- Figure 2: F1 curves ----------\ntry:\n    plt.figure()\n    for bsz in sorted(exp):\n        epochs = exp[bsz][\"epochs\"]\n        plt.plot(epochs, gather(\"train_f1\", bsz), label=f\"train bsz{bsz}\")\n        plt.plot(epochs, gather(\"val_f1\", bsz), label=f\"val   bsz{bsz}\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- Figure 3: Test F1 by batch size ----------\ntry:\n    plt.figure()\n    bsizes = sorted(exp)\n    test_f1 = [exp[b][\"test_f1\"] for b in bsizes]\n    plt.bar([str(b) for b in bsizes], test_f1, color=\"skyblue\")\n    for i, v in enumerate(test_f1):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.xlabel(\"Batch Size\")\n    plt.ylabel(\"Best Test Macro-F1\")\n    plt.title(\"SPR_BENCH Test Performance vs. Batch Size\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_macroF1_vs_batchsize.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Test F1 plot: {e}\")\n    plt.close()\n\n# ---------- quick console summary ----------\nif exp:\n    for bsz in sorted(exp):\n        print(\n            f\"Batch {bsz}: best dev F1={max(exp[bsz]['metrics']['val_f1']):.3f}, \"\n            f\"test F1={exp[bsz]['test_f1']:.3f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"dropout_sweep\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed:\n    dropouts = ed[\"dropouts\"]\n    epochs_list = ed[\"epochs\"]\n    train_f1 = ed[\"metrics\"][\"train_f1\"]\n    val_f1 = ed[\"metrics\"][\"val_f1\"]\n    train_loss = ed[\"losses\"][\"train\"]\n    val_loss = ed[\"losses\"][\"val\"]\n\n    # helper to compute best val f1 per run\n    best_val_f1 = [max(v) for v in val_f1]\n\n    # 1. Train F1 curves\n    try:\n        plt.figure()\n        for d, ep, f1s in zip(dropouts, epochs_list, train_f1):\n            plt.plot(ep, f1s, label=f\"dropout={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Train F1\")\n        plt.title(\"SPR_BENCH: Training F1 vs Epoch\\nLeft: Different Dropouts\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_train_F1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train F1 plot: {e}\")\n        plt.close()\n\n    # 2. Val F1 curves\n    try:\n        plt.figure()\n        for d, ep, f1s in zip(dropouts, epochs_list, val_f1):\n            plt.plot(ep, f1s, label=f\"dropout={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation F1\")\n        plt.title(\"SPR_BENCH: Validation F1 vs Epoch\\nLeft: Different Dropouts\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_F1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val F1 plot: {e}\")\n        plt.close()\n\n    # 3. Train loss curves\n    try:\n        plt.figure()\n        for d, ep, losses in zip(dropouts, epochs_list, train_loss):\n            plt.plot(ep, losses, label=f\"dropout={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Train Loss\")\n        plt.title(\"SPR_BENCH: Training Loss vs Epoch\\nLeft: Different Dropouts\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_train_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train loss plot: {e}\")\n        plt.close()\n\n    # 4. Val loss curves\n    try:\n        plt.figure()\n        for d, ep, losses in zip(dropouts, epochs_list, val_loss):\n            plt.plot(ep, losses, label=f\"dropout={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Loss\")\n        plt.title(\"SPR_BENCH: Validation Loss vs Epoch\\nLeft: Different Dropouts\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val loss plot: {e}\")\n        plt.close()\n\n    # 5. Best val F1 vs dropout\n    try:\n        plt.figure()\n        plt.scatter(dropouts, best_val_f1, c=\"red\")\n        for d, f in zip(dropouts, best_val_f1):\n            plt.text(d, f, f\"{f:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n        plt.xlabel(\"Dropout Probability\")\n        plt.ylabel(\"Best Validation F1\")\n        plt.title(\"SPR_BENCH: Best Validation F1 per Dropout\\nRight: Summary Scatter\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_best_val_F1_vs_dropout.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating summary scatter: {e}\")\n        plt.close()\n"], "plot_plan": ["We load the stored experiment_data.npy, extract the SPR_BENCH entry, and pull out epochs, training/validation losses, macro-F1 scores, predictions, and ground-truth labels. We then create up to three standard visualizations: (1) a loss curve comparing train vs. validation; (2) a macro-F1 curve comparing train vs. validation; and (3) a confusion-matrix heat-map of test predictions vs. labels. Each figure is wrapped in its own try-except block, titled clearly, saved to working_dir with descriptive names, and closed immediately afterwards. If any array is missing or empty the corresponding plot is skipped gracefully. After plotting, we recompute and print the overall test macro-F1 so the notebook cell gives quick feedback. Only data present in experiment_data.npy are used\u2014no values are fabricated, and no more than three figures are produced. All requirements for imports, directory handling, interval control, and figure management are strictly followed.", null, "We will load experiment_data.npy and first determine the SPR-BENCH learning-rate runs that were logged.  From each run we will extract epoch lists, training/validation losses and F1 scores, and overlay them so that a single line-plot compares the different learning rates\u2014one figure for loss, one for F1 (\u2264 5 total figures).  Next we will identify the run whose validation F1 is highest (this was already stored in the array) and build a confusion matrix from its test predictions vs. ground-truth labels; this will be plotted as a heatmap in a third figure.  Every plotting step is wrapped in try-except, figures are titled clearly (e.g. \u201cSPR_BENCH \u2013 Loss Curves (3 LRs)\u201d), saved in working_dir with descriptive names, and then closed.  We avoid generating any synthetic data: everything plotted is read directly from the numpy file.  We import only matplotlib, numpy and os, use plain matplotlib defaults, and respect the five-figure maximum.  Finally, the script prints the best validation F1 found so users can verify the model selection.", "We will load experiment_data.npy, extract the \u201cbatch_size\u201d dictionary, and for each batch size gather epoch lists, training/validation losses, and training/validation macro-F1 values.  \nFigure 1 will overlay training and validation loss curves for all four batch sizes in one panel, giving a quick view of convergence behaviour.  \nFigure 2 will do the same for macro-F1, letting us compare generalisation progress across batch sizes.  \nFigure 3 will be a bar chart of the best (saved-model) test macro-F1 achieved at each batch size, highlighting the optimal configuration.  \nAll plots are wrapped in separate try-except blocks, saved to working_dir with clear names that include the dataset and plot type, and closed afterwards.  \nWe respect the 5-figure limit, use only data present in the file, and never fabricate values.  \nThe code starts with the required imports, creates working_dir if needed, prints a small summary of the plotted metrics, and is concise.", "We will load experiment_data.npy, extract the SPR_BENCH subsection, and collect epochs, dropout values, training/validation F1, and losses. Five concise figures will be produced, each in its own try-except block: (1) train-F1 curves vs. epoch for all dropout settings, (2) val-F1 curves vs. epoch, (3) train-loss curves vs. epoch, (4) val-loss curves vs. epoch, and (5) a summary scatter of best val-F1 achieved for each dropout. All plots will include informative titles and subtitles, specify the dataset, and be saved inside working_dir with descriptive names. To stay under the five-figure limit, no per-epoch sample visualizations are generated. Every figure is closed after saving to ensure clean memory management. The code relies solely on the contents of experiment_data.npy; if the file is missing or malformed, a graceful error is printed. Finally, the script prints nothing but generates the requested PNGs in working_dir."], "ablation_name": [null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "batch_size", "dropout"], "is_seed_node": [false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false], "parse_metrics_plan": ["We will load the experiment_data.npy file from the \u201cworking\u201d directory, unwrap\nit into a regular Python dictionary, and iterate over every dataset it contains.\nFor each dataset we fetch the final (last-epoch) training/validation F1 scores\nand losses; then, using the saved test predictions and ground-truth labels, we\ncompute the test macro-F1 score on-the-fly.   Finally, we print the dataset name\nfollowed by clearly labeled metric values such as \u201ctraining F1 score,\u201d\n\u201cvalidation loss,\u201d and \u201ctest F1 score.\u201d", "", "The script will load the serialized NumPy dictionary from the working directory,\ninspect the learning-rate sub-records for the SPR_BENCH dataset, and pick the\nrun whose final validation F1 score is highest. It will then extract the last-\nepoch values for training loss, validation loss, training F1, validation F1, and\ncompute the test F1 score from the stored predictions/ground-truth pairs.\nFinally, it prints the dataset name once, followed by each metric name and its\ncorresponding best value in a clear, human-readable format.", "Below is a small script that immediately loads the saved NumPy file, iterates\nover the batch-size experiments, and prints the requested \u201cbest\u201d or \u201cfinal\u201d\nmetrics with explicit, unambiguous labels.", "The script will locate the \u201cworking\u201d directory, load the NumPy file that stores\nthe experiment dictionary, and iterate through every dataset present.   For each\ndropout configuration it will compute: the lowest training loss, the lowest\nvalidation loss, the highest training F1 score, the highest validation F1 score,\nand the test-set macro-averaged F1 score (re-computed from the saved predictions\nand ground-truth labels).   Every result is printed with an explicit,\ndescriptive metric name so the output is immediately clear and unambiguous."], "parse_metrics_code": ["import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef print_dataset_metrics(name, data_dict):\n    # Fetch lists\n    train_f1_list = data_dict[\"metrics\"][\"train_f1\"]\n    val_f1_list = data_dict[\"metrics\"][\"val_f1\"]\n    train_loss_list = data_dict[\"losses\"][\"train\"]\n    val_loss_list = data_dict[\"losses\"][\"val\"]\n\n    # Final (last epoch) values\n    final_train_f1 = train_f1_list[-1] if train_f1_list else None\n    final_val_f1 = val_f1_list[-1] if val_f1_list else None\n    final_train_loss = train_loss_list[-1] if train_loss_list else None\n    final_val_loss = val_loss_list[-1] if val_loss_list else None\n\n    # Test F1 score from stored predictions / ground truth\n    preds = data_dict.get(\"predictions\", [])\n    gts = data_dict.get(\"ground_truth\", [])\n    test_f1 = f1_score(gts, preds, average=\"macro\") if preds and gts else None\n\n    # ---------- printing ----------\n    print(f\"\\n{name}\")  # dataset name\n    if final_train_f1 is not None:\n        print(f\"training F1 score: {final_train_f1:.4f}\")\n    if final_val_f1 is not None:\n        print(f\"validation F1 score: {final_val_f1:.4f}\")\n    if final_train_loss is not None:\n        print(f\"training loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"validation loss: {final_val_loss:.4f}\")\n    if test_f1 is not None:\n        print(f\"test F1 score: {test_f1:.4f}\")\n\n\n# ---------- iterate through datasets ----------\nfor dataset_name, dataset_info in experiment_data.items():\n    print_dataset_metrics(dataset_name, dataset_info)\n", "", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ------------------------------------------------------------------\n# 0. Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Extract metrics for SPR_BENCH\n# ------------------------------------------------------------------\nspr_dict = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\n\nbest_record = None\nbest_val_f1 = -1.0\n\n# Loop over every learning-rate configuration and keep the best\nfor lr_key, rec in spr_dict.items():\n    # Final (last-epoch) metrics\n    final_train_f1 = rec[\"metrics\"][\"train_f1\"][-1]\n    final_val_f1 = rec[\"metrics\"][\"val_f1\"][-1]\n    final_train_loss = rec[\"losses\"][\"train\"][-1]\n    final_val_loss = rec[\"losses\"][\"val\"][-1]\n\n    # Compute test F1 from saved predictions\n    preds = rec[\"predictions\"]\n    gts = rec[\"ground_truth\"]\n    test_f1 = f1_score(gts, preds, average=\"macro\") if preds and gts else float(\"nan\")\n\n    # Keep the configuration with the highest final validation F1\n    if final_val_f1 > best_val_f1:\n        best_val_f1 = final_val_f1\n        best_record = {\n            \"learning_rate\": lr_key,\n            \"train_f1\": final_train_f1,\n            \"val_f1\": final_val_f1,\n            \"train_loss\": final_train_loss,\n            \"val_loss\": final_val_loss,\n            \"test_f1\": test_f1,\n        }\n\n# ------------------------------------------------------------------\n# 2. Print the best metrics in the required format\n# ------------------------------------------------------------------\nprint(\"SPR_BENCH\")  # dataset name\nprint(f\"learning rate: {best_record['learning_rate']}\")\n\nprint(f\"training loss: {best_record['train_loss']:.6f}\")\nprint(f\"training F1 score: {best_record['train_f1']:.6f}\")\n\nprint(f\"validation loss: {best_record['val_loss']:.6f}\")\nprint(f\"validation F1 score: {best_record['val_f1']:.6f}\")\n\nprint(f\"test F1 score: {best_record['test_f1']:.6f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate the working directory and load the experiment data file\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Iterate through each batch-size experiment and report metrics\n# ------------------------------------------------------------------\nfor batch_size, exp in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: batch_size={batch_size}\")  # dataset name\n\n    # Extract metric/loss histories\n    train_f1_history = exp[\"metrics\"][\"train_f1\"]\n    val_f1_history = exp[\"metrics\"][\"val_f1\"]\n    train_loss_hist = exp[\"losses\"][\"train\"]\n    val_loss_hist = exp[\"losses\"][\"val\"]\n\n    # Compute required values\n    best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n    best_validation_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n    final_training_loss = train_loss_hist[-1] if train_loss_hist else float(\"nan\")\n    final_validation_loss = val_loss_hist[-1] if val_loss_hist else float(\"nan\")\n    test_f1_score = exp.get(\"test_f1\", float(\"nan\"))\n\n    # ------------------------------------------------------------------\n    # 2. Print metrics with explicit labels\n    # ------------------------------------------------------------------\n    print(f\"Best train F1 score: {best_train_f1:.4f}\")\n    print(f\"Best validation F1 score: {best_validation_f1:.4f}\")\n    print(f\"Final training loss: {final_training_loss:.4f}\")\n    print(f\"Final validation loss: {final_validation_loss:.4f}\")\n    print(f\"Test F1 score: {test_f1_score:.4f}\")\n    print(\"-\" * 60)\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor sweep_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        dropouts = data.get(\"dropouts\", [])\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        train_f1s = data[\"metrics\"][\"train_f1\"]\n        val_f1s = data[\"metrics\"][\"val_f1\"]\n        preds_list = data[\"predictions\"]\n        gts_list = data[\"ground_truth\"]\n\n        for idx, p_drop in enumerate(dropouts):\n            best_train_loss = (\n                float(min(train_losses[idx])) if train_losses else float(\"nan\")\n            )\n            best_val_loss = float(min(val_losses[idx])) if val_losses else float(\"nan\")\n            best_train_f1 = float(max(train_f1s[idx])) if train_f1s else float(\"nan\")\n            best_val_f1 = float(max(val_f1s[idx])) if val_f1s else float(\"nan\")\n            test_f1 = f1_score(gts_list[idx], preds_list[idx], average=\"macro\")\n\n            print(f\"- Dropout rate: {p_drop}\")\n            print(f\"  training loss (lowest): {best_train_loss:.4f}\")\n            print(f\"  validation loss (lowest): {best_val_loss:.4f}\")\n            print(f\"  training F1 score (best): {best_train_f1:.4f}\")\n            print(f\"  validation F1 score (best): {best_val_f1:.4f}\")\n            print(f\"  test F1 score: {test_f1:.4f}\")\n"], "parse_term_out": ["['\\nSPR_BENCH', '\\n', 'training F1 score: 0.7940', '\\n', 'validation F1 score:\n0.7959', '\\n', 'training loss: 0.5090', '\\n', 'validation loss: 0.5115', '\\n',\n'test F1 score: 0.7950', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "", "['SPR_BENCH', '\\n', 'learning rate: lr_5e-04', '\\n', 'training loss: 0.504736',\n'\\n', 'training F1 score: 0.796993', '\\n', 'validation loss: 0.510158', '\\n',\n'validation F1 score: 0.795948', '\\n', 'test F1 score: 0.794990', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: batch_size=32', '\\n', 'Best train F1 score: 0.7960', '\\n', 'Best\nvalidation F1 score: 0.7959', '\\n', 'Final training loss: 0.5114', '\\n', 'Final\nvalidation loss: 0.5059', '\\n', 'Test F1 score: 0.7960', '\\n',\n'------------------------------------------------------------', '\\n', 'Dataset:\nbatch_size=64', '\\n', 'Best train F1 score: 0.7960', '\\n', 'Best validation F1\nscore: 0.7959', '\\n', 'Final training loss: 0.5151', '\\n', 'Final validation\nloss: 0.5106', '\\n', 'Test F1 score: 0.7950', '\\n',\n'------------------------------------------------------------', '\\n', 'Dataset:\nbatch_size=128', '\\n', 'Best train F1 score: 0.7965', '\\n', 'Best validation F1\nscore: 0.7959', '\\n', 'Final training loss: 0.5113', '\\n', 'Final validation\nloss: 0.5311', '\\n', 'Test F1 score: 0.7950', '\\n',\n'------------------------------------------------------------', '\\n', 'Dataset:\nbatch_size=256', '\\n', 'Best train F1 score: 0.7945', '\\n', 'Best validation F1\nscore: 0.7959', '\\n', 'Final training loss: 0.5157', '\\n', 'Final validation\nloss: 0.5248', '\\n', 'Test F1 score: 0.7950', '\\n',\n'------------------------------------------------------------', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '- Dropout rate: 0.0', '\\n', '  training loss\n(lowest): 0.5052', '\\n', '  validation loss (lowest): 0.5092', '\\n', '  training\nF1 score (best): 0.7970', '\\n', '  validation F1 score (best): 0.7959', '\\n', '\ntest F1 score: 0.7950', '\\n', '- Dropout rate: 0.1', '\\n', '  training loss\n(lowest): 0.5049', '\\n', '  validation loss (lowest): 0.5096', '\\n', '  training\nF1 score (best): 0.7965', '\\n', '  validation F1 score (best): 0.7959', '\\n', '\ntest F1 score: 0.7950', '\\n', '- Dropout rate: 0.15', '\\n', '  training loss\n(lowest): 0.5120', '\\n', '  validation loss (lowest): 0.5067', '\\n', '  training\nF1 score (best): 0.7965', '\\n', '  validation F1 score (best): 0.7959', '\\n', '\ntest F1 score: 0.7950', '\\n', '- Dropout rate: 0.25', '\\n', '  training loss\n(lowest): 0.5089', '\\n', '  validation loss (lowest): 0.5100', '\\n', '  training\nF1 score (best): 0.7950', '\\n', '  validation F1 score (best): 0.7959', '\\n', '\ntest F1 score: 0.7910', '\\n', '- Dropout rate: 0.35', '\\n', '  training loss\n(lowest): 0.5097', '\\n', '  validation loss (lowest): 0.5086', '\\n', '  training\nF1 score (best): 0.7960', '\\n', '  validation F1 score (best): 0.7959', '\\n', '\ntest F1 score: 0.7950', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']"], "parse_exc_type": [null, null, null, null, null], "parse_exc_info": [null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
