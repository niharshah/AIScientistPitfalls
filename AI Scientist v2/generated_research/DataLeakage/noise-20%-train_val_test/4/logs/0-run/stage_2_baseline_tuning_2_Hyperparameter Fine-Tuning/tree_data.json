{"edges": [[3, 8], [3, 9], [3, 11], [3, 10]], "layout": [[0.0, 0.0], [0.14285714285714285, 0.0], [0.2857142857142857, 0.0], [0.42857142857142855, 0.0], [0.5714285714285714, 0.0], [0.7142857142857143, 0.0], [0.8571428571428571, 0.0], [1.0, 0.0], [0.21428571428571427, 1.0], [0.35714285714285715, 1.0], [0.5, 1.0], [0.6428571428571429, 1.0]], "plan": ["Hyperparam tuning name: num_epochs. We keep the original preprocessing,\ndataloaders and model; the only change is to sweep over different upper-bounds\nfor training epochs (num_epochs \u2208 {10, 20, 30, 40, 50}).   For every value we\n(re)initialise a fresh model/optimizer, train up to that many epochs while\nemploying early-stopping on the best dev F1, save the best checkpoint, and\nfinally evaluate on the test set.   All per-run statistics (epoch-wise losses/F1\nas well as final test results and predictions) are collected in the\nexperiment_data structure under the hyperparameter key \"num_epochs\" and dumped\nto experiment_data.npy for later plotting.", "Hyperparam tuning name: learning_rate. The solution performs a simple grid-\nsearch over five learning-rates.   For each rate it re-initialises the model,\ntrains for a small fixed number of epochs, records losses/F1, keeps the best\nmodel for that rate, evaluates on the test split and stores everything inside\nthe required `experiment_data` structure before saving it with `np.save`. The\nrest of the pipeline (data loading, vocabulary, model, training loop) is\nunchanged.", "Hyperparam tuning name: batch_size. We sweep batch sizes [32, 64, 128, 256],\nrescale the learning-rate linearly (lr \u221d batch) relative to the baseline (128 \u2192\n3e-4), train a fresh model for each setting, track losses/F1 per epoch, keep the\nbest checkpoint, evaluate on the test set, and store every result in a\nstructured experiment_data dictionary before saving it as experiment_data.npy.", "Hyperparam tuning name: dropout. The script below loops over a list of dropout\nprobabilities, instantiates a fresh Transformer for each value, trains for a few\nepochs, evaluates on the dev set, and stores per-epoch losses/F1 together with\nfinal test predictions.  All information is collected in the required\nexperiment_data dictionary under the key 'dropout_tuning' and saved to\nexperiment_data.npy.  The best\u2010performing dropout on the dev set is reported and\nits model is used for the final test evaluation.", "Hyperparam tuning name: weight_decay. We sweep over five weight-decay values (0,\n1e-5, 1e-4, 5e-4, 1e-3).   For every value we:   1. re-initialize model and\nAdamW with that weight-decay,   2. train for ten epochs while logging losses and\nmacro-F1,   3. keep the best epoch\u2019s state-dict per run.   After the sweep we\npick the globally best dev-F1 model, evaluate it on the test set, and store all\nper-run logs plus the final predictions/ground-truth in experiment_data (saved\nto experiment_data.npy).", "Hyperparam tuning name: d_model. We will explore the representational-capacity\ntrade-off by looping over several d_model values (96, 128, 192, 256, 384).   For\neach value we rebuild the CharTransformer with that embedding size, train for\nthe same 10 epochs, track per-epoch macro-F1 and loss, keep the best validation\ncheckpoint, and finally evaluate on the test set.   All results are stored in a\nnested experiment_data dictionary under the key 'd_model_tuning', then saved to\nexperiment_data.npy.   Other hyperparameters (optimizer, lr, epochs, batch_size,\netc.) remain unchanged.", "Hyperparam tuning name: nlayers. The solution performs a small grid-search over\nTransformer depth by looping through nlayers \u2208 {2, 4, 6, 8}.   For each depth it\nrebuilds the model, trains for 10 epochs with all other settings unchanged,\ntracks losses/F1, saves the best checkpoint, and stores every run\u2019s statistics\nin the required experiment_data structure before finally writing everything to\nexperiment_data.npy.", "Hyperparam tuning name: nhead. We loop over three candidate values for nhead (2,\n4, 8), create a fresh Transformer each time, train/validate for a fixed number\nof epochs, keep the best-validation checkpoint, evaluate on the test set, and\nstore every metric, loss curve, and test prediction inside experiment_data under\nthe key 'nhead_tuning'. After finishing all three runs, the script dumps\nexperiment_data to experiment_data.npy.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ---------------- experiment data container ----------------\nexperiment_data = {\n    \"num_epochs\": {  # hyperparameter tuning type\n        \"SPR_BENCH\": {\n            \"tried_epochs\": [],\n            \"epoch_curves\": {  # epoch-wise lists per variant\n                \"train_loss\": [],\n                \"val_loss\": [],\n                \"train_f1\": [],\n                \"val_f1\": [],\n            },\n            \"best_val_f1\": [],\n            \"test_f1\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],  # filled once\n        }\n    }\n}\n\n# ---------------- reproducibility ----------------\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------- dataset loader ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------------- build vocab ----------------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id = vocab[\"<PAD>\"]\nunk_id = vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------------- encode sequences ----------------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------------- collate fn ----------------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------------- model ----------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------------- helper: one epoch ----------------\ndef run_epoch(model, split, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    loader = loaders[split]\n    criterion = nn.CrossEntropyLoss()\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------------- hyperparameter sweep ----------------\nmax_epochs_grid = [10, 20, 30, 40, 50]\npatience = 5  # early stopping patience\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nfor max_epochs in max_epochs_grid:\n    print(f\"\\n==== Training with max_epochs = {max_epochs} ====\")\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"tried_epochs\"].append(max_epochs)\n\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    best_val_f1, best_state, waited = 0.0, None, 0\n    curve_train_loss, curve_val_loss, curve_train_f1, curve_val_f1 = [], [], [], []\n\n    for epoch in range(1, max_epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, \"train\", optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, \"dev\")\n        curve_train_loss.append(tr_loss)\n        curve_val_loss.append(val_loss)\n        curve_train_f1.append(tr_f1)\n        curve_val_f1.append(val_f1)\n        print(\n            f\"Epoch {epoch}/{max_epochs} | train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_state = model.state_dict()\n            waited = 0\n        else:\n            waited += 1\n            if waited >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # save epoch curves\n    for k, lst in zip(\n        [\"train_loss\", \"val_loss\", \"train_f1\", \"val_f1\"],\n        [curve_train_loss, curve_val_loss, curve_train_f1, curve_val_f1],\n    ):\n        experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"epoch_curves\"][k].append(lst)\n\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"best_val_f1\"].append(best_val_f1)\n\n    # ---------------- test evaluation with best ckpt ----------------\n    model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_gts = run_epoch(model, \"test\")\n    print(f\"Test MacroF1 (best ckpt): {test_f1:.4f}\")\n\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"test_f1\"].append(test_f1)\n    experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"predictions\"].append(test_preds)\n    if not experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"ground_truth\"]:\n        experiment_data[\"num_epochs\"][\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\n# ---------------- save all results ----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ------------- reproducibility -------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ------------- experiment dict -------------\nexperiment_data = {\n    \"learning_rate\": {\n        \"SPR_BENCH\": {\n            \"lrs\": [],\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],  # test predictions of best model per LR\n            \"ground_truth\": [],  # test labels\n            \"best_val_f1\": [],\n        }\n    }\n}\n\n# ------------- device & working dir -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nprint(f\"Using device: {device}\")\n\n\n# ------------- load dataset -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):  # split==train for csv loader\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ------------- build vocab -------------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ------------- encode sequences -------------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ------------- collate -------------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labs = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labs}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ------------- model -------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        L = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:L]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\n# ------------- helpers -------------\ndef run_epoch(model, loader, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ------------- learning-rate sweep -------------\nlr_grid = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3]\nnum_epochs = 8  # modest budget\n\nfor lr in lr_grid:\n    print(f\"\\n=== Learning rate {lr:.0e} ===\")\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    best_val, best_state = 0.0, None\n    tr_losses, tr_f1s, val_losses, val_f1s = [], [], [], []\n    for epoch in range(1, num_epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"])\n        tr_losses.append(tr_loss)\n        val_losses.append(val_loss)\n        tr_f1s.append(tr_f1)\n        val_f1s.append(val_f1)\n        print(\n            f\"  Epoch {epoch:02d}: tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_val:\n            best_val = val_f1\n            best_state = model.state_dict()\n    # ----- test with best model -----\n    model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_gts = run_epoch(model, loaders[\"test\"])\n    print(f\"  >> Best val_F1={best_val:.4f} | Test_F1={test_f1:.4f}\")\n\n    # ----- store results -----\n    ed = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\n    ed[\"lrs\"].append(lr)\n    ed[\"epochs\"].append(list(range(1, num_epochs + 1)))\n    ed[\"losses\"][\"train\"].append(tr_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1s)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1s)\n    ed[\"best_val_f1\"].append(best_val)\n    ed[\"predictions\"].append(test_preds)\n    ed[\"ground_truth\"] = test_gts  # same for every LR\n\n# ------------- save everything -------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ---------- reproducibility ----------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------- experiment dict ----------\nexperiment_data = {\n    \"batch_size\": {\"SPR_BENCH\": {}}  # will be filled with bs-specific results\n}\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(fname):\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- vocab & encoding ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training helpers ----------\ndef build_loaders(batch_size):\n    return {\n        split: DataLoader(\n            spr[split],\n            batch_size=batch_size,\n            shuffle=(split == \"train\"),\n            collate_fn=collate_fn,\n        )\n        for split in [\"train\", \"dev\", \"test\"]\n    }\n\n\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\nbatch_sizes = [32, 64, 128, 256]\nbase_lr = 3e-4  # at bs=128\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size={bs} ===\")\n    loaders = build_loaders(bs)\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    lr = base_lr * bs / 128\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    hist = {\n        \"epochs\": [],\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    best_f1, best_state = 0.0, None\n    epochs = 10\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion, None)\n        hist[\"epochs\"].append(epoch)\n        hist[\"losses\"][\"train\"].append(tr_loss)\n        hist[\"losses\"][\"val\"].append(val_loss)\n        hist[\"metrics\"][\"train_f1\"].append(tr_f1)\n        hist[\"metrics\"][\"val_f1\"].append(val_f1)\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss:.4f} \"\n            f\"val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n\n    # ---------- test ----------\n    model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion, None\n    )\n    print(f\"Best val_F1={best_f1:.4f} | Test_F1={test_f1:.4f}\")\n\n    hist[\"predictions\"] = test_preds\n    hist[\"ground_truth\"] = test_gts\n    hist[\"test_f1\"] = test_f1\n    hist[\"best_val_f1\"] = best_f1\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][f\"bs_{bs}\"] = hist\n\n    torch.cuda.empty_cache()\n\n# ---------- save ----------\nnp.save(\"experiment_data.npy\", experiment_data, allow_pickle=True)\nprint(\"Saved results to experiment_data.npy\")\n", "import os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ---------- experiment data ----------\nexperiment_data = {\"dropout_tuning\": {}}\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout=dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.shape[1]\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training helpers ----------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\ndropout_values = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\nepochs = 10\nbest_global_f1, best_dropout = 0.0, None\nbest_state_path = os.path.join(working_dir, \"best_model_overall.pt\")\n\nfor dp in dropout_values:\n    tag = f\"SPR_BENCH_dropout_{dp}\"\n    experiment_data[\"dropout_tuning\"][tag] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    model = CharTransformer(vocab_size, num_labels, dropout=dp).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    best_val_f1 = 0.0\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n        ed = experiment_data[\"dropout_tuning\"][tag]\n        ed[\"epochs\"].append(epoch)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_f1\"].append(val_f1)\n        print(\n            f\"[dropout={dp}] Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(\n                model.state_dict(), os.path.join(working_dir, f\"best_model_dp_{dp}.pt\")\n            )\n    # evaluate on test set with best model for this dropout\n    model.load_state_dict(\n        torch.load(os.path.join(working_dir, f\"best_model_dp_{dp}.pt\"))\n    )\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    experiment_data[\"dropout_tuning\"][tag][\"losses\"][\"test\"] = test_loss\n    experiment_data[\"dropout_tuning\"][tag][\"metrics\"][\"test_f1\"] = test_f1\n    experiment_data[\"dropout_tuning\"][tag][\"predictions\"] = test_preds\n    experiment_data[\"dropout_tuning\"][tag][\"ground_truth\"] = test_gts\n    print(f\"[dropout={dp}] Test MacroF1: {test_f1:.4f}\\n\")\n    if best_val_f1 > best_global_f1:\n        best_global_f1 = best_val_f1\n        best_dropout = dp\n        torch.save(model.state_dict(), best_state_path)\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n\nprint(f\"Best dev MacroF1={best_global_f1:.4f} obtained with dropout={best_dropout}\")\n\n# ---------- final best model on test set ----------\nbest_model = CharTransformer(vocab_size, num_labels, dropout=best_dropout).to(device)\nbest_model.load_state_dict(torch.load(best_state_path))\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_f1, test_preds, test_gts = run_epoch(\n    best_model, loaders[\"test\"], criterion\n)\nprint(f\"Final Test MacroF1 with best dropout ({best_dropout}): {test_f1:.4f}\")\n\n# save final best predictions separately\nexperiment_data[\"best_overall\"] = {\n    \"dropout\": best_dropout,\n    \"test_f1\": test_f1,\n    \"predictions\": test_preds,\n    \"ground_truth\": test_gts,\n}\n\n# ---------- save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ---------- reproducibility ----------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"weight_decay\": {\"SPR_BENCH\": {}}  # will hold one sub-dict per wd value (as str)\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id, vocab_size = vocab[\"<PAD>\"], vocab[\"<UNK>\"], len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders_full = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)  # max len 5k\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training / evaluation helpers ----------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, optimizer, loaders, split):\n    is_train = split == \"train\"\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loaders[split]:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loaders[split].dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\nweight_decays = [0.0, 1e-5, 1e-4, 5e-4, 1e-3]\nepochs = 10\nglobal_best_f1, global_best_state = -1.0, None\nglobal_best_wd = None\n\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    # init containers\n    exp_run = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": list(range(1, epochs + 1)),\n    }\n    # init model/optimizer\n    model = CharTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=wd)\n    best_val_f1_this_run, best_state_this_run = -1.0, None\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, optimizer, loaders_full, \"train\")\n        val_loss, val_f1, _, _ = run_epoch(model, optimizer, loaders_full, \"dev\")\n        exp_run[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp_run[\"metrics\"][\"val_f1\"].append(val_f1)\n        exp_run[\"losses\"][\"train\"].append(tr_loss)\n        exp_run[\"losses\"][\"val\"].append(val_loss)\n        print(\n            f\"  Epoch {epoch:02d}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_val_f1_this_run:\n            best_val_f1_this_run = val_f1\n            best_state_this_run = {k: v.cpu() for k, v in model.state_dict().items()}\n\n    # save per-weight-decay run data\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][str(wd)] = exp_run\n\n    # update global best\n    if best_val_f1_this_run > global_best_f1:\n        global_best_f1 = best_val_f1_this_run\n        global_best_state = best_state_this_run\n        global_best_wd = wd\n\n# ---------- test evaluation with best model ----------\nprint(f\"\\nBest dev F1={global_best_f1:.4f} achieved with weight_decay={global_best_wd}\")\nbest_model = CharTransformer(vocab_size, num_labels).to(device)\nbest_model.load_state_dict(global_best_state)\nbest_model.to(device)\ntest_loss, test_f1, test_preds, test_gts = run_epoch(\n    best_model, None, loaders_full, \"test\"\n)\nprint(f\"Test MacroF1: {test_f1:.4f}\")\n\n# store predictions & ground truth of best run\nexperiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"best_wd\"] = global_best_wd\nexperiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"test_f1\"] = test_f1\nexperiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\n# ---------- save all experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n# ---------- experiment bookkeeping ----------\nexperiment_data = {\"d_model_tuning\": {\"SPR_BENCH\": {}}}\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id, vocab_size = vocab[\"<PAD>\"], vocab[\"<UNK>\"], len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=4, dropout=0.1\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=max(256, d_model * 2),  # scale feed-forward with size\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]  # (B,L,D)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, split, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\nd_model_values = [96, 128, 192, 256, 384]\nepochs = 10\n\nfor dm in d_model_values:\n    print(f\"\\n=== Training with d_model={dm} ===\")\n    model = CharTransformer(vocab_size, num_labels, d_model=dm).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    # prepare storage\n    experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][dm] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": list(range(1, epochs + 1)),\n    }\n\n    best_val_f1, best_state = 0.0, None\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], \"train\", optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], \"dev\")\n\n        # log\n        edata = experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][dm]\n        edata[\"losses\"][\"train\"].append(tr_loss)\n        edata[\"losses\"][\"val\"].append(val_loss)\n        edata[\"metrics\"][\"train\"].append(tr_f1)\n        edata[\"metrics\"][\"val\"].append(val_f1)\n\n        print(\n            f\"d_model {dm} | Epoch {epoch:2d}: \"\n            f\"train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"val_F1={val_f1:.4f}\"\n        )\n\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_state = model.state_dict()\n\n    # -------- test with best checkpoint --------\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_gts = run_epoch(model, loaders[\"test\"], \"test\")\n    print(f\"d_model {dm} | Test MacroF1: {test_f1:.4f}\")\n\n    # save predictions\n    edata = experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][dm]\n    edata[\"predictions\"] = test_preds\n    edata[\"ground_truth\"] = test_gts\n    edata[\"best_val_f1\"] = best_val_f1\n    edata[\"test_f1\"] = test_f1\n\n    # free GPU memory before next run\n    del model\n    torch.cuda.empty_cache()\n\n# ---------- persist results ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nAll experiments completed and saved to experiment_data.npy\")\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n\n# ---------- reproducibility ----------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed()\n\n# ---------- working / logging ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"nlayers\": {}}  # hyper-parameter tuning container\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------- dataset ----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(fn):\n        return load_dataset(\n            \"csv\", data_files=str(root / fn), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id = vocab[\"<PAD>\"]\nunk_id = vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq):\n    return [vocab.get(c, unk_id) for c in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(t.size(0) for t in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, t in enumerate(input_ids):\n        padded[i, : t.size(0)] = t\n        attn[i, : t.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)  # max len 5000\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training & evaluation helpers ----------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, optimizer, split):\n    is_train = split == \"train\"\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loaders[split]:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        total_loss / len(loaders[split].dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------- hyper-parameter sweep ----------\nnl_values = [2, 4, 6, 8]\nepochs = 10\nfor nl in nl_values:\n    print(f\"\\n===== Training with nlayers={nl} =====\")\n    key = f\"SPR_BENCH_n{nl}\"\n    experiment_data[\"nlayers\"][key] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n\n    model = CharTransformer(vocab_size, num_labels, nlayers=nl).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    best_f1 = 0.0\n    best_path = os.path.join(working_dir, f\"best_model_n{nl}.pt\")\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, optimizer, \"train\")\n        val_loss, val_f1, _, _ = run_epoch(model, optimizer, \"dev\")\n\n        exp = experiment_data[\"nlayers\"][key]\n        exp[\"epochs\"].append(epoch)\n        exp[\"losses\"][\"train\"].append(tr_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"train_f1\"].append(tr_f1)\n        exp[\"metrics\"][\"val_f1\"].append(val_f1)\n\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), best_path)\n\n    # test evaluation\n    model.load_state_dict(torch.load(best_path))\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, optimizer=None, split=\"test\"\n    )\n    experiment_data[\"nlayers\"][key][\"predictions\"] = test_preds\n    experiment_data[\"nlayers\"][key][\"ground_truth\"] = test_gts\n    print(f\"best val_F1={best_f1:.4f} | Test_F1={test_f1:.4f}\")\n\n    # cleanup to free GPU memory\n    del model\n    del optimizer\n    torch.cuda.empty_cache()\n\n# ---------- save experiments ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict\n\n\n# ------------- reproducibility -------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed()\n\n# ------------- experiment log -------------\nexperiment_data = {\n    \"nhead_tuning\": {\n        \"SPR_BENCH\": {\n            \"results\": {},  # will hold dict per nhead value\n        }\n    }\n}\n\n# ------------- working dir -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------- dataset -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ------------- vocab -------------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ------------- encode -------------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ------------- collate -------------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(x) for x in input_ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(input_ids):\n        padded[i, : len(seq)] = seq\n        attn[i, : len(seq)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ------------- model -------------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ------------- train / eval helpers -------------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------- hyperparameter tuning -------------\nnhead_values = [2, 4, 8]\nepochs = 10\n\nfor nhead in nhead_values:\n    print(f\"\\n==== Training with nhead={nhead} ====\")\n    model = CharTransformer(vocab_size, num_labels, nhead=nhead).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    best_f1, best_state = 0.0, None\n    history = {\n        \"epochs\": [],\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n    }\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n        history[\"epochs\"].append(epoch)\n        history[\"losses\"][\"train\"].append(tr_loss)\n        history[\"losses\"][\"val\"].append(val_loss)\n        history[\"metrics\"][\"train_f1\"].append(tr_f1)\n        history[\"metrics\"][\"val_f1\"].append(val_f1)\n        print(\n            f\"nhead={nhead} | epoch {epoch:02d} | train_loss {tr_loss:.4f} | val_loss {val_loss:.4f} | val_f1 {val_f1:.4f}\"\n        )\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n\n    # --- test with best checkpoint ---\n    model.load_state_dict(best_state)\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    print(f\"nhead={nhead} | BEST val_f1 {best_f1:.4f} | Test f1 {test_f1:.4f}\")\n\n    # --- save results ---\n    experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][\"results\"][nhead] = {\n        \"history\": history,\n        \"test_f1\": test_f1,\n        \"test_loss\": test_loss,\n        \"predictions\": test_preds,\n        \"ground_truth\": test_gts,\n    }\n    # free memory\n    del model\n    torch.cuda.empty_cache()\n\n# ------------- persist -------------\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"Saved experiment data to experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ---------- experiment data ----------\nexperiment_data = {\"dropout_tuning\": {}}\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout=dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.shape[1]\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training helpers ----------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\ndropout_values = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\nepochs = 10\nbest_global_f1, best_dropout = 0.0, None\nbest_state_path = os.path.join(working_dir, \"best_model_overall.pt\")\n\nfor dp in dropout_values:\n    tag = f\"SPR_BENCH_dropout_{dp}\"\n    experiment_data[\"dropout_tuning\"][tag] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    model = CharTransformer(vocab_size, num_labels, dropout=dp).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    best_val_f1 = 0.0\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n        ed = experiment_data[\"dropout_tuning\"][tag]\n        ed[\"epochs\"].append(epoch)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_f1\"].append(val_f1)\n        print(\n            f\"[dropout={dp}] Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(\n                model.state_dict(), os.path.join(working_dir, f\"best_model_dp_{dp}.pt\")\n            )\n    # evaluate on test set with best model for this dropout\n    model.load_state_dict(\n        torch.load(os.path.join(working_dir, f\"best_model_dp_{dp}.pt\"))\n    )\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    experiment_data[\"dropout_tuning\"][tag][\"losses\"][\"test\"] = test_loss\n    experiment_data[\"dropout_tuning\"][tag][\"metrics\"][\"test_f1\"] = test_f1\n    experiment_data[\"dropout_tuning\"][tag][\"predictions\"] = test_preds\n    experiment_data[\"dropout_tuning\"][tag][\"ground_truth\"] = test_gts\n    print(f\"[dropout={dp}] Test MacroF1: {test_f1:.4f}\\n\")\n    if best_val_f1 > best_global_f1:\n        best_global_f1 = best_val_f1\n        best_dropout = dp\n        torch.save(model.state_dict(), best_state_path)\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n\nprint(f\"Best dev MacroF1={best_global_f1:.4f} obtained with dropout={best_dropout}\")\n\n# ---------- final best model on test set ----------\nbest_model = CharTransformer(vocab_size, num_labels, dropout=best_dropout).to(device)\nbest_model.load_state_dict(torch.load(best_state_path))\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_f1, test_preds, test_gts = run_epoch(\n    best_model, loaders[\"test\"], criterion\n)\nprint(f\"Final Test MacroF1 with best dropout ({best_dropout}): {test_f1:.4f}\")\n\n# save final best predictions separately\nexperiment_data[\"best_overall\"] = {\n    \"dropout\": best_dropout,\n    \"test_f1\": test_f1,\n    \"predictions\": test_preds,\n    \"ground_truth\": test_gts,\n}\n\n# ---------- save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ---------- experiment data ----------\nexperiment_data = {\"dropout_tuning\": {}}\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout=dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.shape[1]\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training helpers ----------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\ndropout_values = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\nepochs = 10\nbest_global_f1, best_dropout = 0.0, None\nbest_state_path = os.path.join(working_dir, \"best_model_overall.pt\")\n\nfor dp in dropout_values:\n    tag = f\"SPR_BENCH_dropout_{dp}\"\n    experiment_data[\"dropout_tuning\"][tag] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    model = CharTransformer(vocab_size, num_labels, dropout=dp).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    best_val_f1 = 0.0\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n        ed = experiment_data[\"dropout_tuning\"][tag]\n        ed[\"epochs\"].append(epoch)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_f1\"].append(val_f1)\n        print(\n            f\"[dropout={dp}] Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(\n                model.state_dict(), os.path.join(working_dir, f\"best_model_dp_{dp}.pt\")\n            )\n    # evaluate on test set with best model for this dropout\n    model.load_state_dict(\n        torch.load(os.path.join(working_dir, f\"best_model_dp_{dp}.pt\"))\n    )\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    experiment_data[\"dropout_tuning\"][tag][\"losses\"][\"test\"] = test_loss\n    experiment_data[\"dropout_tuning\"][tag][\"metrics\"][\"test_f1\"] = test_f1\n    experiment_data[\"dropout_tuning\"][tag][\"predictions\"] = test_preds\n    experiment_data[\"dropout_tuning\"][tag][\"ground_truth\"] = test_gts\n    print(f\"[dropout={dp}] Test MacroF1: {test_f1:.4f}\\n\")\n    if best_val_f1 > best_global_f1:\n        best_global_f1 = best_val_f1\n        best_dropout = dp\n        torch.save(model.state_dict(), best_state_path)\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n\nprint(f\"Best dev MacroF1={best_global_f1:.4f} obtained with dropout={best_dropout}\")\n\n# ---------- final best model on test set ----------\nbest_model = CharTransformer(vocab_size, num_labels, dropout=best_dropout).to(device)\nbest_model.load_state_dict(torch.load(best_state_path))\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_f1, test_preds, test_gts = run_epoch(\n    best_model, loaders[\"test\"], criterion\n)\nprint(f\"Final Test MacroF1 with best dropout ({best_dropout}): {test_f1:.4f}\")\n\n# save final best predictions separately\nexperiment_data[\"best_overall\"] = {\n    \"dropout\": best_dropout,\n    \"test_f1\": test_f1,\n    \"predictions\": test_preds,\n    \"ground_truth\": test_gts,\n}\n\n# ---------- save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# ---------- experiment data ----------\nexperiment_data = {\"dropout_tuning\": {}}\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Loaded SPR_BENCH with\", len(spr[\"train\"]), \"train examples.\")\n\n\n# ---------- build vocab ----------\ndef build_vocab(dataset):\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for ch in seq:\n            if ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\npad_id, unk_id = vocab[\"<PAD>\"], vocab[\"<UNK>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ---------- encode sequences ----------\ndef encode(seq):\n    return [vocab.get(ch, unk_id) for ch in seq]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda x: {\"input_ids\": encode(x[\"sequence\"])}, remove_columns=[]\n    )\n\n\n# ---------- collate ----------\ndef collate_fn(batch):\n    ids = [torch.tensor(b[\"input_ids\"], dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(x.size(0) for x in ids)\n    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    attn = torch.zeros_like(padded, dtype=torch.bool)\n    for i, seq in enumerate(ids):\n        padded[i, : seq.size(0)] = seq\n        attn[i, : seq.size(0)] = 1\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate_fn,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass CharTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        nlayers=4,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(5000, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout=dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.shape[1]\n        x = self.emb(input_ids) + self.pos_emb[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(x)\n\n\n# ---------- training helpers ----------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(is_train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if is_train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------- hyperparameter sweep ----------\ndropout_values = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\nepochs = 10\nbest_global_f1, best_dropout = 0.0, None\nbest_state_path = os.path.join(working_dir, \"best_model_overall.pt\")\n\nfor dp in dropout_values:\n    tag = f\"SPR_BENCH_dropout_{dp}\"\n    experiment_data[\"dropout_tuning\"][tag] = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    model = CharTransformer(vocab_size, num_labels, dropout=dp).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    best_val_f1 = 0.0\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1, _, _ = run_epoch(model, loaders[\"train\"], criterion, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, loaders[\"dev\"], criterion)\n        ed = experiment_data[\"dropout_tuning\"][tag]\n        ed[\"epochs\"].append(epoch)\n        ed[\"losses\"][\"train\"].append(tr_loss)\n        ed[\"losses\"][\"val\"].append(val_loss)\n        ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n        ed[\"metrics\"][\"val_f1\"].append(val_f1)\n        print(\n            f\"[dropout={dp}] Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_F1={val_f1:.4f}\"\n        )\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(\n                model.state_dict(), os.path.join(working_dir, f\"best_model_dp_{dp}.pt\")\n            )\n    # evaluate on test set with best model for this dropout\n    model.load_state_dict(\n        torch.load(os.path.join(working_dir, f\"best_model_dp_{dp}.pt\"))\n    )\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, loaders[\"test\"], criterion\n    )\n    experiment_data[\"dropout_tuning\"][tag][\"losses\"][\"test\"] = test_loss\n    experiment_data[\"dropout_tuning\"][tag][\"metrics\"][\"test_f1\"] = test_f1\n    experiment_data[\"dropout_tuning\"][tag][\"predictions\"] = test_preds\n    experiment_data[\"dropout_tuning\"][tag][\"ground_truth\"] = test_gts\n    print(f\"[dropout={dp}] Test MacroF1: {test_f1:.4f}\\n\")\n    if best_val_f1 > best_global_f1:\n        best_global_f1 = best_val_f1\n        best_dropout = dp\n        torch.save(model.state_dict(), best_state_path)\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n\nprint(f\"Best dev MacroF1={best_global_f1:.4f} obtained with dropout={best_dropout}\")\n\n# ---------- final best model on test set ----------\nbest_model = CharTransformer(vocab_size, num_labels, dropout=best_dropout).to(device)\nbest_model.load_state_dict(torch.load(best_state_path))\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_f1, test_preds, test_gts = run_epoch(\n    best_model, loaders[\"test\"], criterion\n)\nprint(f\"Final Test MacroF1 with best dropout ({best_dropout}): {test_f1:.4f}\")\n\n# save final best predictions separately\nexperiment_data[\"best_overall\"] = {\n    \"dropout\": best_dropout,\n    \"test_f1\": test_f1,\n    \"predictions\": test_preds,\n    \"ground_truth\": test_gts,\n}\n\n# ---------- save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n112644.13 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 116308.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 187003.61\nexamples/s]', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 20170.64 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n22847.78 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 23951.44\nexamples/s]', '\\n', '\\n==== Training with max_epochs = 10 ====', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1/10 | train_loss=0.6419\nval_loss=0.5496 val_F1=0.7780', '\\n', 'Epoch 2/10 | train_loss=0.5491\nval_loss=0.5429 val_F1=0.7700', '\\n', 'Epoch 3/10 | train_loss=0.5385\nval_loss=0.5367 val_F1=0.7880', '\\n', 'Epoch 4/10 | train_loss=0.5246\nval_loss=0.5261 val_F1=0.7860', '\\n', 'Epoch 5/10 | train_loss=0.5169\nval_loss=0.5271 val_F1=0.7860', '\\n', 'Epoch 6/10 | train_loss=0.5169\nval_loss=0.5304 val_F1=0.7860', '\\n', 'Epoch 7/10 | train_loss=0.5175\nval_loss=0.5192 val_F1=0.7860', '\\n', 'Epoch 8/10 | train_loss=0.5189\nval_loss=0.5183 val_F1=0.7959', '\\n', 'Epoch 9/10 | train_loss=0.5117\nval_loss=0.5170 val_F1=0.7959', '\\n', 'Epoch 10/10 | train_loss=0.5119\nval_loss=0.5201 val_F1=0.7940', '\\n', 'Test MacroF1 (best ckpt): 0.7930', '\\n',\n'\\n==== Training with max_epochs = 20 ====', '\\n', 'Epoch 1/20 |\ntrain_loss=0.6405 val_loss=0.5508 val_F1=0.7880', '\\n', 'Epoch 2/20 |\ntrain_loss=0.5405 val_loss=0.5375 val_F1=0.7860', '\\n', 'Epoch 3/20 |\ntrain_loss=0.5188 val_loss=0.5479 val_F1=0.7860', '\\n', 'Epoch 4/20 |\ntrain_loss=0.5166 val_loss=0.5203 val_F1=0.7959', '\\n', 'Epoch 5/20 |\ntrain_loss=0.5147 val_loss=0.5250 val_F1=0.7860', '\\n', 'Epoch 6/20 |\ntrain_loss=0.5159 val_loss=0.5109 val_F1=0.7959', '\\n', 'Epoch 7/20 |\ntrain_loss=0.5166 val_loss=0.5371 val_F1=0.7800', '\\n', 'Epoch 8/20 |\ntrain_loss=0.5128 val_loss=0.5134 val_F1=0.7959', '\\n', 'Epoch 9/20 |\ntrain_loss=0.5137 val_loss=0.5247 val_F1=0.7860', '\\n', 'Early stopping\ntriggered.', '\\n', 'Test MacroF1 (best ckpt): 0.7889', '\\n', '\\n==== Training\nwith max_epochs = 30 ====', '\\n', 'Epoch 1/30 | train_loss=0.6259\nval_loss=0.5528 val_F1=0.7680', '\\n', 'Epoch 2/30 | train_loss=0.5425\nval_loss=0.5519 val_F1=0.7599', '\\n', 'Epoch 3/30 | train_loss=0.5403\nval_loss=0.5920 val_F1=0.7456', '\\n', 'Epoch 4/30 | train_loss=0.5431\nval_loss=0.5491 val_F1=0.7959', '\\n', 'Epoch 5/30 | train_loss=0.5323\nval_loss=0.5556 val_F1=0.7599', '\\n', 'Epoch 6/30 | train_loss=0.5249\nval_loss=0.5406 val_F1=0.7820', '\\n', 'Epoch 7/30 | train_loss=0.5188\nval_loss=0.5230 val_F1=0.7860', '\\n', 'Epoch 8/30 | train_loss=0.5111\nval_loss=0.5203 val_F1=0.7959', '\\n', 'Epoch 9/30 | train_loss=0.5107\nval_loss=0.5180 val_F1=0.7940', '\\n', 'Early stopping triggered.', '\\n', 'Test\nMacroF1 (best ckpt): 0.7920', '\\n', '\\n==== Training with max_epochs = 40 ====',\n'\\n', 'Epoch 1/40 | train_loss=0.6533 val_loss=0.5667 val_F1=0.7414', '\\n',\n'Epoch 2/40 | train_loss=0.5362 val_loss=0.5349 val_F1=0.7860', '\\n', 'Epoch\n3/40 | train_loss=0.5292 val_loss=0.5431 val_F1=0.7860', '\\n', 'Epoch 4/40 |\ntrain_loss=0.5496 val_loss=0.5310 val_F1=0.7860', '\\n', 'Epoch 5/40 |\ntrain_loss=0.5219 val_loss=0.5326 val_F1=0.7959', '\\n', 'Epoch 6/40 |\ntrain_loss=0.5156 val_loss=0.5227 val_F1=0.7860', '\\n', 'Epoch 7/40 |\ntrain_loss=0.5113 val_loss=0.5186 val_F1=0.7959', '\\n', 'Epoch 8/40 |\ntrain_loss=0.5127 val_loss=0.5342 val_F1=0.7860', '\\n', 'Epoch 9/40 |\ntrain_loss=0.5128 val_loss=0.5156 val_F1=0.7860', '\\n', 'Epoch 10/40 |\ntrain_loss=0.5093 val_loss=0.5154 val_F1=0.7959', '\\n', 'Early stopping\ntriggered.', '\\n', 'Test MacroF1 (best ckpt): 0.7920', '\\n', '\\n==== Training\nwith max_epochs = 50 ====', '\\n', 'Epoch 1/50 | train_loss=0.5928\nval_loss=0.5914 val_F1=0.7456', '\\n', 'Epoch 2/50 | train_loss=0.5437\nval_loss=0.5459 val_F1=0.7639', '\\n', 'Epoch 3/50 | train_loss=0.5367\nval_loss=0.5365 val_F1=0.7860', '\\n', 'Epoch 4/50 | train_loss=0.5266\nval_loss=0.5331 val_F1=0.7959', '\\n', 'Epoch 5/50 | train_loss=0.5203\nval_loss=0.5235 val_F1=0.7860', '\\n', 'Epoch 6/50 | train_loss=0.5170\nval_loss=0.5374 val_F1=0.7860', '\\n', 'Epoch 7/50 | train_loss=0.5100\nval_loss=0.5190 val_F1=0.7959', '\\n', 'Epoch 8/50 | train_loss=0.5139\nval_loss=0.5194 val_F1=0.7900', '\\n', 'Epoch 9/50 | train_loss=0.5106\nval_loss=0.5158 val_F1=0.7959', '\\n', 'Early stopping triggered.', '\\n', 'Test\nMacroF1 (best ckpt): 0.7940', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n46_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n10/working/experiment_data.npy', '\\n', 'Execution time: 17 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 155157.83\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 101082.18\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 216034.20\nexamples/s]', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 24698.96 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n18590.29 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 24918.34\nexamples/s]', '\\n', '\\n=== Learning rate 1e-05 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Epoch 01: tr_loss=0.6755\nval_loss=0.6714 val_F1=0.6759', '\\n', '  Epoch 02: tr_loss=0.6641\nval_loss=0.6602 val_F1=0.7297', '\\n', '  Epoch 03: tr_loss=0.6537\nval_loss=0.6496 val_F1=0.7396', '\\n', '  Epoch 04: tr_loss=0.6426\nval_loss=0.6393 val_F1=0.7309', '\\n', '  Epoch 05: tr_loss=0.6301\nval_loss=0.6259 val_F1=0.7476', '\\n', '  Epoch 06: tr_loss=0.6164\nval_loss=0.6132 val_F1=0.7476', '\\n', '  Epoch 07: tr_loss=0.6020\nval_loss=0.5990 val_F1=0.7476', '\\n', '  Epoch 08: tr_loss=0.5868\nval_loss=0.5850 val_F1=0.7497', '\\n', '  >> Best val_F1=0.7497 |\nTest_F1=0.7715', '\\n', '\\n=== Learning rate 3e-05 ===', '\\n', '  Epoch 01:\ntr_loss=0.6943 val_loss=0.6798 val_F1=0.5097', '\\n', '  Epoch 02: tr_loss=0.6629\nval_loss=0.6454 val_F1=0.7163', '\\n', '  Epoch 03: tr_loss=0.6252\nval_loss=0.6028 val_F1=0.7760', '\\n', '  Epoch 04: tr_loss=0.5811\nval_loss=0.5632 val_F1=0.7760', '\\n', '  Epoch 05: tr_loss=0.5426\nval_loss=0.5446 val_F1=0.7760', '\\n', '  Epoch 06: tr_loss=0.5290\nval_loss=0.5397 val_F1=0.7800', '\\n', '  Epoch 07: tr_loss=0.5249\nval_loss=0.5383 val_F1=0.7800', '\\n', '  Epoch 08: tr_loss=0.5237\nval_loss=0.5337 val_F1=0.7880', '\\n', '  >> Best val_F1=0.7880 |\nTest_F1=0.7899', '\\n', '\\n=== Learning rate 1e-04 ===', '\\n', '  Epoch 01:\ntr_loss=0.7173 val_loss=0.6749 val_F1=0.3421', '\\n', '  Epoch 02: tr_loss=0.6421\nval_loss=0.5992 val_F1=0.7619', '\\n', '  Epoch 03: tr_loss=0.5651\nval_loss=0.5497 val_F1=0.7680', '\\n', '  Epoch 04: tr_loss=0.5324\nval_loss=0.5445 val_F1=0.7840', '\\n', '  Epoch 05: tr_loss=0.5250\nval_loss=0.5380 val_F1=0.7860', '\\n', '  Epoch 06: tr_loss=0.5235\nval_loss=0.5405 val_F1=0.7860', '\\n', '  Epoch 07: tr_loss=0.5245\nval_loss=0.5333 val_F1=0.7920', '\\n', '  Epoch 08: tr_loss=0.5226\nval_loss=0.5375 val_F1=0.7860', '\\n', '  >> Best val_F1=0.7920 |\nTest_F1=0.7889', '\\n', '\\n=== Learning rate 3e-04 ===', '\\n', '  Epoch 01:\ntr_loss=0.7296 val_loss=0.6894 val_F1=0.3328', '\\n', '  Epoch 02: tr_loss=0.6059\nval_loss=0.5471 val_F1=0.7900', '\\n', '  Epoch 03: tr_loss=0.5302\nval_loss=0.5358 val_F1=0.7860', '\\n', '  Epoch 04: tr_loss=0.5210\nval_loss=0.5442 val_F1=0.7860', '\\n', '  Epoch 05: tr_loss=0.5284\nval_loss=0.5353 val_F1=0.7860', '\\n', '  Epoch 06: tr_loss=0.5222\nval_loss=0.5247 val_F1=0.7860', '\\n', '  Epoch 07: tr_loss=0.5152\nval_loss=0.5335 val_F1=0.7860', '\\n', '  Epoch 08: tr_loss=0.5268\nval_loss=0.5402 val_F1=0.7619', '\\n', '  >> Best val_F1=0.7900 |\nTest_F1=0.7808', '\\n', '\\n=== Learning rate 1e-03 ===', '\\n', '  Epoch 01:\ntr_loss=0.7579 val_loss=0.6168 val_F1=0.5373', '\\n', '  Epoch 02: tr_loss=0.5564\nval_loss=0.5325 val_F1=0.7900', '\\n', '  Epoch 03: tr_loss=0.5243\nval_loss=0.5238 val_F1=0.7959', '\\n', '  Epoch 04: tr_loss=0.5145\nval_loss=0.5144 val_F1=0.7940', '\\n', '  Epoch 05: tr_loss=0.5162\nval_loss=0.5434 val_F1=0.7619', '\\n', '  Epoch 06: tr_loss=0.5172\nval_loss=0.5115 val_F1=0.7959', '\\n', '  Epoch 07: tr_loss=0.5078\nval_loss=0.5320 val_F1=0.7920', '\\n', '  Epoch 08: tr_loss=0.5184\nval_loss=0.5243 val_F1=0.7860', '\\n', '  >> Best val_F1=0.7959 |\nTest_F1=0.7869', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scie\nntist-v2/experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/experiment_data.npy', '\\n', 'Execution time:\n17 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n150198.89 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 96776.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 166500.10\nexamples/s]', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 23579.67 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n18622.65 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 23956.23\nexamples/s]', '\\n', '\\n=== Training with batch_size=32 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.5911\nval_loss=0.5475 val_F1=0.7599', '\\n', 'Epoch 2: train_loss=0.5319\nval_loss=0.5450 val_F1=0.7959', '\\n', 'Epoch 3: train_loss=0.5371\nval_loss=0.5547 val_F1=0.7599', '\\n', 'Epoch 4: train_loss=0.5262\nval_loss=0.5302 val_F1=0.7860', '\\n', 'Epoch 5: train_loss=0.5203\nval_loss=0.5350 val_F1=0.7860', '\\n', 'Epoch 6: train_loss=0.5164\nval_loss=0.5475 val_F1=0.7860', '\\n', 'Epoch 7: train_loss=0.5191\nval_loss=0.5252 val_F1=0.7860', '\\n', 'Epoch 8: train_loss=0.5117\nval_loss=0.5184 val_F1=0.7860', '\\n', 'Epoch 9: train_loss=0.5090\nval_loss=0.5208 val_F1=0.7959', '\\n', 'Epoch 10: train_loss=0.5113\nval_loss=0.5165 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', '\\n=== Training with batch_size=64 ===', '\\n', 'Epoch 1: train_loss=0.5846\nval_loss=0.5408 val_F1=0.7760', '\\n', 'Epoch 2: train_loss=0.5450\nval_loss=0.5326 val_F1=0.7880', '\\n', 'Epoch 3: train_loss=0.5275\nval_loss=0.5307 val_F1=0.7860', '\\n', 'Epoch 4: train_loss=0.5167\nval_loss=0.5201 val_F1=0.7880', '\\n', 'Epoch 5: train_loss=0.5177\nval_loss=0.5193 val_F1=0.7860', '\\n', 'Epoch 6: train_loss=0.5188\nval_loss=0.5168 val_F1=0.7880', '\\n', 'Epoch 7: train_loss=0.5097\nval_loss=0.5148 val_F1=0.7959', '\\n', 'Epoch 8: train_loss=0.5100\nval_loss=0.5100 val_F1=0.7959', '\\n', 'Epoch 9: train_loss=0.5106\nval_loss=0.5414 val_F1=0.7860', '\\n', 'Epoch 10: train_loss=0.5140\nval_loss=0.5123 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', '\\n=== Training with batch_size=128 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6420 val_loss=0.5495 val_F1=0.7880', '\\n', 'Epoch 2:\ntrain_loss=0.5366 val_loss=0.5772 val_F1=0.7558', '\\n', 'Epoch 3:\ntrain_loss=0.5433 val_loss=0.5485 val_F1=0.7959', '\\n', 'Epoch 4:\ntrain_loss=0.5320 val_loss=0.5522 val_F1=0.7599', '\\n', 'Epoch 5:\ntrain_loss=0.5233 val_loss=0.5325 val_F1=0.7860', '\\n', 'Epoch 6:\ntrain_loss=0.5158 val_loss=0.5196 val_F1=0.7860', '\\n', 'Epoch 7:\ntrain_loss=0.5107 val_loss=0.5159 val_F1=0.7959', '\\n', 'Epoch 8:\ntrain_loss=0.5103 val_loss=0.5152 val_F1=0.7959', '\\n', 'Epoch 9:\ntrain_loss=0.5131 val_loss=0.5278 val_F1=0.7860', '\\n', 'Epoch 10:\ntrain_loss=0.5113 val_loss=0.5311 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 |\nTest_F1=0.7950', '\\n', '\\n=== Training with batch_size=256 ===', '\\n', 'Epoch 1:\ntrain_loss=0.7847 val_loss=0.6677 val_F1=0.4410', '\\n', 'Epoch 2:\ntrain_loss=0.6560 val_loss=0.6031 val_F1=0.6050', '\\n', 'Epoch 3:\ntrain_loss=0.5678 val_loss=0.5686 val_F1=0.7959', '\\n', 'Epoch 4:\ntrain_loss=0.5306 val_loss=0.5402 val_F1=0.7959', '\\n', 'Epoch 5:\ntrain_loss=0.5211 val_loss=0.5265 val_F1=0.7860', '\\n', 'Epoch 6:\ntrain_loss=0.5176 val_loss=0.5207 val_F1=0.7880', '\\n', 'Epoch 7:\ntrain_loss=0.5110 val_loss=0.5178 val_F1=0.7959', '\\n', 'Epoch 8:\ntrain_loss=0.5130 val_loss=0.5163 val_F1=0.7920', '\\n', 'Epoch 9:\ntrain_loss=0.5099 val_loss=0.5141 val_F1=0.7959', '\\n', 'Epoch 10:\ntrain_loss=0.5070 val_loss=0.5288 val_F1=0.7959', '\\n', 'Best val_F1=0.7959 |\nTest_F1=0.7950', '\\n', 'Saved results to experiment_data.npy', '\\n', 'Execution\ntime: 21 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n140828.79 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 105262.86\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 166104.47\nexamples/s]', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 22383.83 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n23885.56 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 25082.70\nexamples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[dropout=0.0] Epoch 1:\ntrain_loss=0.6681 val_loss=0.5894 val_F1=0.7227', '\\n', '[dropout=0.0] Epoch 2:\ntrain_loss=0.5548 val_loss=0.5469 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 3:\ntrain_loss=0.5345 val_loss=0.5433 val_F1=0.7599', '\\n', '[dropout=0.0] Epoch 4:\ntrain_loss=0.5194 val_loss=0.5392 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 5:\ntrain_loss=0.5190 val_loss=0.5240 val_F1=0.7920', '\\n', '[dropout=0.0] Epoch 6:\ntrain_loss=0.5156 val_loss=0.5221 val_F1=0.7880', '\\n', '[dropout=0.0] Epoch 7:\ntrain_loss=0.5113 val_loss=0.5161 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 8:\ntrain_loss=0.5087 val_loss=0.5144 val_F1=0.7940', '\\n', '[dropout=0.0] Epoch 9:\ntrain_loss=0.5095 val_loss=0.5141 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 10:\ntrain_loss=0.5090 val_loss=0.5128 val_F1=0.7959', '\\n', '[dropout=0.0] Test\nMacroF1: 0.7940\\n', '\\n', '[dropout=0.05] Epoch 1: train_loss=0.6407\nval_loss=0.5523 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 2: train_loss=0.5398\nval_loss=0.5875 val_F1=0.7496', '\\n', '[dropout=0.05] Epoch 3: train_loss=0.5362\nval_loss=0.5325 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 4: train_loss=0.5201\nval_loss=0.5286 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 5: train_loss=0.5156\nval_loss=0.5225 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 6: train_loss=0.5155\nval_loss=0.5253 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 7: train_loss=0.5126\nval_loss=0.5302 val_F1=0.7920', '\\n', '[dropout=0.05] Epoch 8: train_loss=0.5083\nval_loss=0.5233 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 9: train_loss=0.5065\nval_loss=0.5103 val_F1=0.7959', '\\n', '[dropout=0.05] Epoch 10:\ntrain_loss=0.5132 val_loss=0.5107 val_F1=0.7959', '\\n', '[dropout=0.05] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.1] Epoch 1: train_loss=0.5912\nval_loss=0.5510 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 2: train_loss=0.5278\nval_loss=0.5360 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 3: train_loss=0.5218\nval_loss=0.5415 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 4: train_loss=0.5308\nval_loss=0.5306 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 5: train_loss=0.5170\nval_loss=0.5244 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 6: train_loss=0.5272\nval_loss=0.5278 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 7: train_loss=0.5171\nval_loss=0.5167 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 8: train_loss=0.5153\nval_loss=0.5372 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 9: train_loss=0.5170\nval_loss=0.5330 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 10: train_loss=0.5100\nval_loss=0.5143 val_F1=0.7959', '\\n', '[dropout=0.1] Test MacroF1: 0.7950\\n',\n'\\n', '[dropout=0.2] Epoch 1: train_loss=0.6806 val_loss=0.6008 val_F1=0.7626',\n'\\n', '[dropout=0.2] Epoch 2: train_loss=0.5488 val_loss=0.5673 val_F1=0.7919',\n'\\n', '[dropout=0.2] Epoch 3: train_loss=0.5403 val_loss=0.5376 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 4: train_loss=0.5326 val_loss=0.5439 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 5: train_loss=0.5223 val_loss=0.5298 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 6: train_loss=0.5160 val_loss=0.5254 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 7: train_loss=0.5141 val_loss=0.5161 val_F1=0.7880',\n'\\n', '[dropout=0.2] Epoch 8: train_loss=0.5167 val_loss=0.5150 val_F1=0.7880',\n'\\n', '[dropout=0.2] Epoch 9: train_loss=0.5126 val_loss=0.5209 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 10: train_loss=0.5183 val_loss=0.5257 val_F1=0.7959',\n'\\n', '[dropout=0.2] Test MacroF1: 0.7950\\n', '\\n', '[dropout=0.3] Epoch 1:\ntrain_loss=0.6192 val_loss=0.5546 val_F1=0.7660', '\\n', '[dropout=0.3] Epoch 2:\ntrain_loss=0.5377 val_loss=0.5435 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 3:\ntrain_loss=0.5273 val_loss=0.5389 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 4:\ntrain_loss=0.5356 val_loss=0.5476 val_F1=0.7740', '\\n', '[dropout=0.3] Epoch 5:\ntrain_loss=0.5216 val_loss=0.5276 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 6:\ntrain_loss=0.5264 val_loss=0.5560 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 7:\ntrain_loss=0.5259 val_loss=0.5217 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 8:\ntrain_loss=0.5152 val_loss=0.5211 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 9:\ntrain_loss=0.5118 val_loss=0.5260 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 10:\ntrain_loss=0.5109 val_loss=0.5228 val_F1=0.7940', '\\n', '[dropout=0.3] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.4] Epoch 1: train_loss=0.6452\nval_loss=0.5513 val_F1=0.7740', '\\n', '[dropout=0.4] Epoch 2: train_loss=0.5374\nval_loss=0.5416 val_F1=0.7860', '\\n', '[dropout=0.4] Epoch 3: train_loss=0.5331\nval_loss=0.5850 val_F1=0.7899', '\\n', '[dropout=0.4] Epoch 4: train_loss=0.5186\nval_loss=0.5232 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 5: train_loss=0.5167\nval_loss=0.5349 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 6: train_loss=0.5162\nval_loss=0.5241 val_F1=0.7940', '\\n', '[dropout=0.4] Epoch 7: train_loss=0.5160\nval_loss=0.5254 val_F1=0.7920', '\\n', '[dropout=0.4] Epoch 8: train_loss=0.5100\nval_loss=0.5187 val_F1=0.7940', '\\n', '[dropout=0.4] Epoch 9: train_loss=0.5130\nval_loss=0.5197 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 10: train_loss=0.5114\nval_loss=0.5125 val_F1=0.7959', '\\n', '[dropout=0.4] Test MacroF1: 0.7950\\n',\n'\\n', 'Best dev MacroF1=0.7959 obtained with dropout=0.0', '\\n', 'Final Test\nMacroF1 with best dropout (0.0): 0.7940', '\\n', 'Execution time: 22 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\n=== Training with\nweight_decay=0.0 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Epoch 01: train_loss=0.6419\nval_loss=0.5496 val_F1=0.7780', '\\n', '  Epoch 02: train_loss=0.5491\nval_loss=0.5428 val_F1=0.7700', '\\n', '  Epoch 03: train_loss=0.5385\nval_loss=0.5367 val_F1=0.7880', '\\n', '  Epoch 04: train_loss=0.5246\nval_loss=0.5261 val_F1=0.7860', '\\n', '  Epoch 05: train_loss=0.5169\nval_loss=0.5271 val_F1=0.7860', '\\n', '  Epoch 06: train_loss=0.5169\nval_loss=0.5303 val_F1=0.7860', '\\n', '  Epoch 07: train_loss=0.5175\nval_loss=0.5192 val_F1=0.7860', '\\n', '  Epoch 08: train_loss=0.5189\nval_loss=0.5184 val_F1=0.7959', '\\n', '  Epoch 09: train_loss=0.5117\nval_loss=0.5169 val_F1=0.7959', '\\n', '  Epoch 10: train_loss=0.5119\nval_loss=0.5201 val_F1=0.7940', '\\n', '\\n=== Training with weight_decay=1e-05\n===', '\\n', '  Epoch 01: train_loss=0.6390 val_loss=0.5784 val_F1=0.7352', '\\n',\n'  Epoch 02: train_loss=0.5358 val_loss=0.5416 val_F1=0.7860', '\\n', '  Epoch\n03: train_loss=0.5282 val_loss=0.5369 val_F1=0.7860', '\\n', '  Epoch 04:\ntrain_loss=0.5319 val_loss=0.5257 val_F1=0.7860', '\\n', '  Epoch 05:\ntrain_loss=0.5243 val_loss=0.5349 val_F1=0.7860', '\\n', '  Epoch 06:\ntrain_loss=0.5152 val_loss=0.5156 val_F1=0.7920', '\\n', '  Epoch 07:\ntrain_loss=0.5085 val_loss=0.5193 val_F1=0.7920', '\\n', '  Epoch 08:\ntrain_loss=0.5070 val_loss=0.5146 val_F1=0.7959', '\\n', '  Epoch 09:\ntrain_loss=0.5137 val_loss=0.5192 val_F1=0.7860', '\\n', '  Epoch 10:\ntrain_loss=0.5116 val_loss=0.5299 val_F1=0.7860', '\\n', '\\n=== Training with\nweight_decay=0.0001 ===', '\\n', '  Epoch 01: train_loss=0.6313 val_loss=0.5582\nval_F1=0.7840', '\\n', '  Epoch 02: train_loss=0.5412 val_loss=0.5397\nval_F1=0.7860', '\\n', '  Epoch 03: train_loss=0.5304 val_loss=0.5956\nval_F1=0.7415', '\\n', '  Epoch 04: train_loss=0.5474 val_loss=0.5497\nval_F1=0.7959', '\\n', '  Epoch 05: train_loss=0.5244 val_loss=0.5235\nval_F1=0.7920', '\\n', '  Epoch 06: train_loss=0.5121 val_loss=0.5199\nval_F1=0.7959', '\\n', '  Epoch 07: train_loss=0.5161 val_loss=0.5295\nval_F1=0.7860', '\\n', '  Epoch 08: train_loss=0.5131 val_loss=0.5308\nval_F1=0.7860', '\\n', '  Epoch 09: train_loss=0.5100 val_loss=0.5152\nval_F1=0.7940', '\\n', '  Epoch 10: train_loss=0.5079 val_loss=0.5101\nval_F1=0.7959', '\\n', '\\n=== Training with weight_decay=0.0005 ===', '\\n', '\nEpoch 01: train_loss=0.6859 val_loss=0.6232 val_F1=0.6616', '\\n', '  Epoch 02:\ntrain_loss=0.5547 val_loss=0.6012 val_F1=0.7475', '\\n', '  Epoch 03:\ntrain_loss=0.5721 val_loss=0.5459 val_F1=0.7599', '\\n', '  Epoch 04:\ntrain_loss=0.5306 val_loss=0.5394 val_F1=0.7860', '\\n', '  Epoch 05:\ntrain_loss=0.5210 val_loss=0.5304 val_F1=0.7860', '\\n', '  Epoch 06:\ntrain_loss=0.5158 val_loss=0.5264 val_F1=0.7959', '\\n', '  Epoch 07:\ntrain_loss=0.5175 val_loss=0.5339 val_F1=0.7860', '\\n', '  Epoch 08:\ntrain_loss=0.5163 val_loss=0.5175 val_F1=0.7959', '\\n', '  Epoch 09:\ntrain_loss=0.5157 val_loss=0.5266 val_F1=0.7860', '\\n', '  Epoch 10:\ntrain_loss=0.5109 val_loss=0.5146 val_F1=0.7959', '\\n', '\\n=== Training with\nweight_decay=0.001 ===', '\\n', '  Epoch 01: train_loss=0.6708 val_loss=0.5719\nval_F1=0.7680', '\\n', '  Epoch 02: train_loss=0.5507 val_loss=0.5695\nval_F1=0.7599', '\\n', '  Epoch 03: train_loss=0.5299 val_loss=0.5339\nval_F1=0.7860', '\\n', '  Epoch 04: train_loss=0.5228 val_loss=0.5467\nval_F1=0.7800', '\\n', '  Epoch 05: train_loss=0.5249 val_loss=0.5307\nval_F1=0.7860', '\\n', '  Epoch 06: train_loss=0.5188 val_loss=0.5213\nval_F1=0.7959', '\\n', '  Epoch 07: train_loss=0.5199 val_loss=0.5250\nval_F1=0.7860', '\\n', '  Epoch 08: train_loss=0.5172 val_loss=0.5174\nval_F1=0.7959', '\\n', '  Epoch 09: train_loss=0.5119 val_loss=0.5191\nval_F1=0.7959', '\\n', '  Epoch 10: train_loss=0.5084 val_loss=0.5123\nval_F1=0.7959', '\\n', '\\nBest dev F1=0.7959 achieved with weight_decay=0.0',\n'\\n', 'Test MacroF1: 0.7950', '\\n', 'Execution time: 18 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ', 'train\nexamples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\n=== Training with\nd_model=96 ===', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'd_model 96 | Epoch  1:\ntrain_loss=0.6784 val_loss=0.6158 val_F1=0.7659', '\\n', 'd_model 96 | Epoch  2:\ntrain_loss=0.5755 val_loss=0.5529 val_F1=0.7840', '\\n', 'd_model 96 | Epoch  3:\ntrain_loss=0.5364 val_loss=0.5421 val_F1=0.7820', '\\n', 'd_model 96 | Epoch  4:\ntrain_loss=0.5248 val_loss=0.5402 val_F1=0.7940', '\\n', 'd_model 96 | Epoch  5:\ntrain_loss=0.5274 val_loss=0.5318 val_F1=0.7860', '\\n', 'd_model 96 | Epoch  6:\ntrain_loss=0.5192 val_loss=0.5227 val_F1=0.7959', '\\n', 'd_model 96 | Epoch  7:\ntrain_loss=0.5178 val_loss=0.5257 val_F1=0.7860', '\\n', 'd_model 96 | Epoch  8:\ntrain_loss=0.5127 val_loss=0.5165 val_F1=0.7959', '\\n', 'd_model 96 | Epoch  9:\ntrain_loss=0.5225 val_loss=0.5198 val_F1=0.7860', '\\n', 'd_model 96 | Epoch 10:\ntrain_loss=0.5143 val_loss=0.5379 val_F1=0.7860', '\\n', 'd_model 96 | Test\nMacroF1: 0.7889', '\\n', '\\n=== Training with d_model=128 ===', '\\n', 'd_model\n128 | Epoch  1: train_loss=0.6838 val_loss=0.6129 val_F1=0.6919', '\\n', 'd_model\n128 | Epoch  2: train_loss=0.5566 val_loss=0.5491 val_F1=0.7920', '\\n', 'd_model\n128 | Epoch  3: train_loss=0.5320 val_loss=0.5336 val_F1=0.7860', '\\n', 'd_model\n128 | Epoch  4: train_loss=0.5202 val_loss=0.5513 val_F1=0.7760', '\\n', 'd_model\n128 | Epoch  5: train_loss=0.5207 val_loss=0.5249 val_F1=0.7860', '\\n', 'd_model\n128 | Epoch  6: train_loss=0.5144 val_loss=0.5281 val_F1=0.7940', '\\n', 'd_model\n128 | Epoch  7: train_loss=0.5158 val_loss=0.5161 val_F1=0.7940', '\\n', 'd_model\n128 | Epoch  8: train_loss=0.5159 val_loss=0.5441 val_F1=0.7860', '\\n', 'd_model\n128 | Epoch  9: train_loss=0.5183 val_loss=0.5139 val_F1=0.7959', '\\n', 'd_model\n128 | Epoch 10: train_loss=0.5116 val_loss=0.5198 val_F1=0.7940', '\\n', 'd_model\n128 | Test MacroF1: 0.7920', '\\n', '\\n=== Training with d_model=192 ===', '\\n',\n'd_model 192 | Epoch  1: train_loss=0.6809 val_loss=0.5740 val_F1=0.7819', '\\n',\n'd_model 192 | Epoch  2: train_loss=0.5697 val_loss=0.5438 val_F1=0.7860', '\\n',\n'd_model 192 | Epoch  3: train_loss=0.5482 val_loss=0.5417 val_F1=0.7860', '\\n',\n'd_model 192 | Epoch  4: train_loss=0.5302 val_loss=0.5343 val_F1=0.7860', '\\n',\n'd_model 192 | Epoch  5: train_loss=0.5205 val_loss=0.5425 val_F1=0.7820', '\\n',\n'd_model 192 | Epoch  6: train_loss=0.5237 val_loss=0.5192 val_F1=0.7860', '\\n',\n'd_model 192 | Epoch  7: train_loss=0.5138 val_loss=0.5527 val_F1=0.7840', '\\n',\n'd_model 192 | Epoch  8: train_loss=0.5184 val_loss=0.5172 val_F1=0.7880', '\\n',\n'd_model 192 | Epoch  9: train_loss=0.5122 val_loss=0.5185 val_F1=0.7959', '\\n',\n'd_model 192 | Epoch 10: train_loss=0.5096 val_loss=0.5131 val_F1=0.7959', '\\n',\n'd_model 192 | Test MacroF1: 0.7950', '\\n', '\\n=== Training with d_model=256\n===', '\\n', 'd_model 256 | Epoch  1: train_loss=0.6750 val_loss=0.5498\nval_F1=0.7959', '\\n', 'd_model 256 | Epoch  2: train_loss=0.5452 val_loss=0.5781\nval_F1=0.7959', '\\n', 'd_model 256 | Epoch  3: train_loss=0.5268 val_loss=0.5340\nval_F1=0.7860', '\\n', 'd_model 256 | Epoch  4: train_loss=0.5319 val_loss=0.5699\nval_F1=0.7579', '\\n', 'd_model 256 | Epoch  5: train_loss=0.5341 val_loss=0.5995\nval_F1=0.7878', '\\n', 'd_model 256 | Epoch  6: train_loss=0.5430 val_loss=0.5325\nval_F1=0.7860', '\\n', 'd_model 256 | Epoch  7: train_loss=0.5171 val_loss=0.5182\nval_F1=0.7959', '\\n', 'd_model 256 | Epoch  8: train_loss=0.5198 val_loss=0.5221\nval_F1=0.7860', '\\n', 'd_model 256 | Epoch  9: train_loss=0.5126 val_loss=0.5272\nval_F1=0.7959', '\\n', 'd_model 256 | Epoch 10: train_loss=0.5155 val_loss=0.5222\nval_F1=0.7860', '\\n', 'd_model 256 | Test MacroF1: 0.7889', '\\n', '\\n===\nTraining with d_model=384 ===', '\\n', 'd_model 384 | Epoch  1: train_loss=0.8894\nval_loss=0.6117 val_F1=0.6661', '\\n', 'd_model 384 | Epoch  2: train_loss=0.5762\nval_loss=0.5502 val_F1=0.7619', '\\n', 'd_model 384 | Epoch  3: train_loss=0.5346\nval_loss=0.5260 val_F1=0.7860', '\\n', 'd_model 384 | Epoch  4: train_loss=0.5200\nval_loss=0.5214 val_F1=0.7860', '\\n', 'd_model 384 | Epoch  5: train_loss=0.5154\nval_loss=0.5163 val_F1=0.7860', '\\n', 'd_model 384 | Epoch  6: train_loss=0.5118\nval_loss=0.5244 val_F1=0.7959', '\\n', 'd_model 384 | Epoch  7: train_loss=0.5262\nval_loss=0.5587 val_F1=0.7579', '\\n', 'd_model 384 | Epoch  8: train_loss=0.5259\nval_loss=0.5149 val_F1=0.7880', '\\n', 'd_model 384 | Epoch  9: train_loss=0.5101\nval_loss=0.5119 val_F1=0.7900', '\\n', 'd_model 384 | Epoch 10: train_loss=0.5156\nval_loss=0.5244 val_F1=0.7860', '\\n', 'd_model 384 | Test MacroF1: 0.7889',\n'\\n', '\\nAll experiments completed and saved to experiment_data.npy', '\\n',\n'Execution time: 24 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ',\n'train examples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\rMap:   0%|\n| 0/2000 [00:00<?, ? examples/s]', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 19596.94 examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 19357.76 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n24566.31 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 2189.23\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 2179.95\nexamples/s]', '\\n', '\\n===== Training with nlayers=2 =====', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6273\nval_loss=0.5516 val_F1=0.7680', '\\n', 'Epoch 2: train_loss=0.5420\nval_loss=0.5421 val_F1=0.7860', '\\n', 'Epoch 3: train_loss=0.5406\nval_loss=0.5457 val_F1=0.7880', '\\n', 'Epoch 4: train_loss=0.5307\nval_loss=0.5438 val_F1=0.7820', '\\n', 'Epoch 5: train_loss=0.5260\nval_loss=0.5366 val_F1=0.7860', '\\n', 'Epoch 6: train_loss=0.5218\nval_loss=0.5360 val_F1=0.7860', '\\n', 'Epoch 7: train_loss=0.5217\nval_loss=0.5332 val_F1=0.7860', '\\n', 'Epoch 8: train_loss=0.5251\nval_loss=0.5408 val_F1=0.7959', '\\n', 'Epoch 9: train_loss=0.5223\nval_loss=0.5292 val_F1=0.7860', '\\n', 'Epoch 10: train_loss=0.5202\nval_loss=0.5267 val_F1=0.7860', '\\n', 'best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', '\\n===== Training with nlayers=4 =====', '\\n', 'Epoch 1: train_loss=0.6394\nval_loss=0.5510 val_F1=0.7880', '\\n', 'Epoch 2: train_loss=0.5411\nval_loss=0.5378 val_F1=0.7860', '\\n', 'Epoch 3: train_loss=0.5187\nval_loss=0.5496 val_F1=0.7860', '\\n', 'Epoch 4: train_loss=0.5172\nval_loss=0.5203 val_F1=0.7959', '\\n', 'Epoch 5: train_loss=0.5150\nval_loss=0.5245 val_F1=0.7860', '\\n', 'Epoch 6: train_loss=0.5153\nval_loss=0.5110 val_F1=0.7959', '\\n', 'Epoch 7: train_loss=0.5155\nval_loss=0.5342 val_F1=0.7800', '\\n', 'Epoch 8: train_loss=0.5125\nval_loss=0.5114 val_F1=0.7959', '\\n', 'Epoch 9: train_loss=0.5126\nval_loss=0.5351 val_F1=0.7860', '\\n', 'Epoch 10: train_loss=0.5216\nval_loss=0.5421 val_F1=0.7860', '\\n', 'best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', '\\n===== Training with nlayers=6 =====', '\\n', 'Epoch 1: train_loss=0.6693\nval_loss=0.5623 val_F1=0.7838', '\\n', 'Epoch 2: train_loss=0.5342\nval_loss=0.5551 val_F1=0.7599', '\\n', 'Epoch 3: train_loss=0.5406\nval_loss=0.5281 val_F1=0.7959', '\\n', 'Epoch 4: train_loss=0.5277\nval_loss=0.5519 val_F1=0.7599', '\\n', 'Epoch 5: train_loss=0.5189\nval_loss=0.5311 val_F1=0.7860', '\\n', 'Epoch 6: train_loss=0.5162\nval_loss=0.5336 val_F1=0.7860', '\\n', 'Epoch 7: train_loss=0.5109\nval_loss=0.5127 val_F1=0.7959', '\\n', 'Epoch 8: train_loss=0.5064\nval_loss=0.5097 val_F1=0.7959', '\\n', 'Epoch 9: train_loss=0.5075\nval_loss=0.5091 val_F1=0.7959', '\\n', 'Epoch 10: train_loss=0.5068\nval_loss=0.5223 val_F1=0.7880', '\\n', 'best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', '\\n===== Training with nlayers=8 =====', '\\n', 'Epoch 1: train_loss=0.7205\nval_loss=0.6741 val_F1=0.3421', '\\n', 'Epoch 2: train_loss=0.6069\nval_loss=0.6283 val_F1=0.7435', '\\n', 'Epoch 3: train_loss=0.5871\nval_loss=0.5515 val_F1=0.7860', '\\n', 'Epoch 4: train_loss=0.5272\nval_loss=0.5121 val_F1=0.7959', '\\n', 'Epoch 5: train_loss=0.5098\nval_loss=0.5089 val_F1=0.7959', '\\n', 'Epoch 6: train_loss=0.5087\nval_loss=0.5090 val_F1=0.7959', '\\n', 'Epoch 7: train_loss=0.5092\nval_loss=0.5127 val_F1=0.7920', '\\n', 'Epoch 8: train_loss=0.5156\nval_loss=0.5089 val_F1=0.7959', '\\n', 'Epoch 9: train_loss=0.5104\nval_loss=0.5088 val_F1=0.7959', '\\n', 'Epoch 10: train_loss=0.5074\nval_loss=0.5124 val_F1=0.7920', '\\n', 'best val_F1=0.7959 | Test_F1=0.7950',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 17 seconds seconds\n(time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ',\n'train examples.', '\\n', 'Vocab size:', ' ', '11', '\\n', '\\n==== Training with\nnhead=2 ====', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'nhead=2 | epoch 01 | train_loss\n0.6354 | val_loss 0.5437 | val_f1 0.7720', '\\n', 'nhead=2 | epoch 02 |\ntrain_loss 0.5407 | val_loss 0.5366 | val_f1 0.7860', '\\n', 'nhead=2 | epoch 03\n| train_loss 0.5376 | val_loss 0.5376 | val_f1 0.7940', '\\n', 'nhead=2 | epoch\n04 | train_loss 0.5230 | val_loss 0.5306 | val_f1 0.7860', '\\n', 'nhead=2 |\nepoch 05 | train_loss 0.5176 | val_loss 0.5235 | val_f1 0.7860', '\\n', 'nhead=2\n| epoch 06 | train_loss 0.5158 | val_loss 0.5310 | val_f1 0.7920', '\\n',\n'nhead=2 | epoch 07 | train_loss 0.5157 | val_loss 0.5226 | val_f1 0.7860',\n'\\n', 'nhead=2 | epoch 08 | train_loss 0.5216 | val_loss 0.5272 | val_f1\n0.7959', '\\n', 'nhead=2 | epoch 09 | train_loss 0.5148 | val_loss 0.5161 |\nval_f1 0.7959', '\\n', 'nhead=2 | epoch 10 | train_loss 0.5124 | val_loss 0.5174\n| val_f1 0.7959', '\\n', 'nhead=2 | BEST val_f1 0.7959 | Test f1 0.7950', '\\n',\n'\\n==== Training with nhead=4 ====', '\\n', 'nhead=4 | epoch 01 | train_loss\n0.6404 | val_loss 0.5504 | val_f1 0.7880', '\\n', 'nhead=4 | epoch 02 |\ntrain_loss 0.5412 | val_loss 0.5379 | val_f1 0.7860', '\\n', 'nhead=4 | epoch 03\n| train_loss 0.5195 | val_loss 0.5483 | val_f1 0.7860', '\\n', 'nhead=4 | epoch\n04 | train_loss 0.5164 | val_loss 0.5203 | val_f1 0.7959', '\\n', 'nhead=4 |\nepoch 05 | train_loss 0.5148 | val_loss 0.5235 | val_f1 0.7860', '\\n', 'nhead=4\n| epoch 06 | train_loss 0.5150 | val_loss 0.5114 | val_f1 0.7959', '\\n',\n'nhead=4 | epoch 07 | train_loss 0.5146 | val_loss 0.5312 | val_f1 0.7860',\n'\\n', 'nhead=4 | epoch 08 | train_loss 0.5120 | val_loss 0.5105 | val_f1\n0.7959', '\\n', 'nhead=4 | epoch 09 | train_loss 0.5110 | val_loss 0.5432 |\nval_f1 0.7800', '\\n', 'nhead=4 | epoch 10 | train_loss 0.5218 | val_loss 0.5435\n| val_f1 0.7860', '\\n', 'nhead=4 | BEST val_f1 0.7959 | Test f1 0.7889', '\\n',\n'\\n==== Training with nhead=8 ====', '\\n', 'nhead=8 | epoch 01 | train_loss\n0.6472 | val_loss 0.5574 | val_f1 0.7860', '\\n', 'nhead=8 | epoch 02 |\ntrain_loss 0.5393 | val_loss 0.5809 | val_f1 0.7557', '\\n', 'nhead=8 | epoch 03\n| train_loss 0.5429 | val_loss 0.5485 | val_f1 0.7959', '\\n', 'nhead=8 | epoch\n04 | train_loss 0.5318 | val_loss 0.5540 | val_f1 0.7599', '\\n', 'nhead=8 |\nepoch 05 | train_loss 0.5258 | val_loss 0.5379 | val_f1 0.7860', '\\n', 'nhead=8\n| epoch 06 | train_loss 0.5187 | val_loss 0.5223 | val_f1 0.7860', '\\n',\n'nhead=8 | epoch 07 | train_loss 0.5117 | val_loss 0.5200 | val_f1 0.7959',\n'\\n', 'nhead=8 | epoch 08 | train_loss 0.5100 | val_loss 0.5186 | val_f1\n0.7959', '\\n', 'nhead=8 | epoch 09 | train_loss 0.5134 | val_loss 0.5243 |\nval_f1 0.7860', '\\n', 'nhead=8 | epoch 10 | train_loss 0.5087 | val_loss 0.5325\n| val_f1 0.7959', '\\n', 'nhead=8 | BEST val_f1 0.7959 | Test f1 0.7950', '\\n',\n'Saved experiment data to experiment_data.npy', '\\n', 'Execution time: 11\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ',\n'train examples.', '\\n', 'Vocab size:', ' ', '11', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[dropout=0.0] Epoch 1:\ntrain_loss=0.6368 val_loss=0.5603 val_F1=0.7940', '\\n', '[dropout=0.0] Epoch 2:\ntrain_loss=0.5483 val_loss=0.5462 val_F1=0.7599', '\\n', '[dropout=0.0] Epoch 3:\ntrain_loss=0.5276 val_loss=0.5424 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 4:\ntrain_loss=0.5211 val_loss=0.5399 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 5:\ntrain_loss=0.5288 val_loss=0.5498 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 6:\ntrain_loss=0.5336 val_loss=0.5254 val_F1=0.7920', '\\n', '[dropout=0.0] Epoch 7:\ntrain_loss=0.5173 val_loss=0.5235 val_F1=0.7940', '\\n', '[dropout=0.0] Epoch 8:\ntrain_loss=0.5177 val_loss=0.5147 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 9:\ntrain_loss=0.5119 val_loss=0.5207 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 10:\ntrain_loss=0.5095 val_loss=0.5369 val_F1=0.7860', '\\n', '[dropout=0.0] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.05] Epoch 1: train_loss=0.6346\nval_loss=0.5469 val_F1=0.7680', '\\n', '[dropout=0.05] Epoch 2: train_loss=0.5420\nval_loss=0.5408 val_F1=0.7880', '\\n', '[dropout=0.05] Epoch 3: train_loss=0.5354\nval_loss=0.5559 val_F1=0.7599', '\\n', '[dropout=0.05] Epoch 4: train_loss=0.5233\nval_loss=0.5254 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 5: train_loss=0.5169\nval_loss=0.5199 val_F1=0.7900', '\\n', '[dropout=0.05] Epoch 6: train_loss=0.5143\nval_loss=0.5257 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 7: train_loss=0.5134\nval_loss=0.5146 val_F1=0.7959', '\\n', '[dropout=0.05] Epoch 8: train_loss=0.5096\nval_loss=0.5149 val_F1=0.7959', '\\n', '[dropout=0.05] Epoch 9: train_loss=0.5119\nval_loss=0.5132 val_F1=0.7880', '\\n', '[dropout=0.05] Epoch 10:\ntrain_loss=0.5112 val_loss=0.5116 val_F1=0.7959', '\\n', '[dropout=0.05] Test\nMacroF1: 0.7940\\n', '\\n', '[dropout=0.1] Epoch 1: train_loss=0.6871\nval_loss=0.6000 val_F1=0.7027', '\\n', '[dropout=0.1] Epoch 2: train_loss=0.5412\nval_loss=0.5445 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 3: train_loss=0.5436\nval_loss=0.5787 val_F1=0.7393', '\\n', '[dropout=0.1] Epoch 4: train_loss=0.5261\nval_loss=0.5369 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 5: train_loss=0.5180\nval_loss=0.5247 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 6: train_loss=0.5194\nval_loss=0.5244 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 7: train_loss=0.5158\nval_loss=0.5237 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 8: train_loss=0.5105\nval_loss=0.5155 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 9: train_loss=0.5165\nval_loss=0.5422 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 10: train_loss=0.5219\nval_loss=0.5263 val_F1=0.7860', '\\n', '[dropout=0.1] Test MacroF1: 0.7950\\n',\n'\\n', '[dropout=0.2] Epoch 1: train_loss=0.6191 val_loss=0.5452 val_F1=0.7720',\n'\\n', '[dropout=0.2] Epoch 2: train_loss=0.5403 val_loss=0.5388 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 3: train_loss=0.5376 val_loss=0.5346 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 4: train_loss=0.5211 val_loss=0.5247 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 5: train_loss=0.5214 val_loss=0.5244 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 6: train_loss=0.5219 val_loss=0.5335 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 7: train_loss=0.5183 val_loss=0.5209 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 8: train_loss=0.5120 val_loss=0.5176 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 9: train_loss=0.5084 val_loss=0.5117 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 10: train_loss=0.5095 val_loss=0.5090 val_F1=0.7959',\n'\\n', '[dropout=0.2] Test MacroF1: 0.7950\\n', '\\n', '[dropout=0.3] Epoch 1:\ntrain_loss=0.6351 val_loss=0.5472 val_F1=0.7780', '\\n', '[dropout=0.3] Epoch 2:\ntrain_loss=0.5410 val_loss=0.5444 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 3:\ntrain_loss=0.5251 val_loss=0.5616 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 4:\ntrain_loss=0.5311 val_loss=0.5373 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 5:\ntrain_loss=0.5290 val_loss=0.5934 val_F1=0.7647', '\\n', '[dropout=0.3] Epoch 6:\ntrain_loss=0.5253 val_loss=0.5209 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 7:\ntrain_loss=0.5148 val_loss=0.5171 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 8:\ntrain_loss=0.5125 val_loss=0.5119 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 9:\ntrain_loss=0.5080 val_loss=0.5237 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 10:\ntrain_loss=0.5098 val_loss=0.5159 val_F1=0.7959', '\\n', '[dropout=0.3] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.4] Epoch 1: train_loss=0.6049\nval_loss=0.6422 val_F1=0.7481', '\\n', '[dropout=0.4] Epoch 2: train_loss=0.5370\nval_loss=0.5695 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 3: train_loss=0.5259\nval_loss=0.5333 val_F1=0.7860', '\\n', '[dropout=0.4] Epoch 4: train_loss=0.5201\nval_loss=0.5987 val_F1=0.7877', '\\n', '[dropout=0.4] Epoch 5: train_loss=0.5249\nval_loss=0.5994 val_F1=0.6285', '\\n', '[dropout=0.4] Epoch 6: train_loss=0.5184\nval_loss=0.5268 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 7: train_loss=0.5147\nval_loss=0.5202 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 8: train_loss=0.5187\nval_loss=0.5203 val_F1=0.7940', '\\n', '[dropout=0.4] Epoch 9: train_loss=0.5119\nval_loss=0.5115 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 10: train_loss=0.5133\nval_loss=0.5761 val_F1=0.7001', '\\n', '[dropout=0.4] Test MacroF1: 0.7950\\n',\n'\\n', 'Best dev MacroF1=0.7959 obtained with dropout=0.0', '\\n', 'Final Test\nMacroF1 with best dropout (0.0): 0.7950', '\\n', 'Execution time: 23 seconds\nseconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ',\n'train examples.', '\\n', 'Vocab size:', ' ', '11', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[dropout=0.0] Epoch 1:\ntrain_loss=0.6490 val_loss=0.5974 val_F1=0.7675', '\\n', '[dropout=0.0] Epoch 2:\ntrain_loss=0.5522 val_loss=0.5610 val_F1=0.7599', '\\n', '[dropout=0.0] Epoch 3:\ntrain_loss=0.5362 val_loss=0.5491 val_F1=0.7599', '\\n', '[dropout=0.0] Epoch 4:\ntrain_loss=0.5284 val_loss=0.5313 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 5:\ntrain_loss=0.5269 val_loss=0.5293 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 6:\ntrain_loss=0.5169 val_loss=0.5277 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 7:\ntrain_loss=0.5138 val_loss=0.5194 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 8:\ntrain_loss=0.5070 val_loss=0.5250 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 9:\ntrain_loss=0.5118 val_loss=0.5133 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 10:\ntrain_loss=0.5134 val_loss=0.5267 val_F1=0.7860', '\\n', '[dropout=0.0] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.05] Epoch 1: train_loss=0.6836\nval_loss=0.5849 val_F1=0.7639', '\\n', '[dropout=0.05] Epoch 2: train_loss=0.5447\nval_loss=0.5414 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 3: train_loss=0.5326\nval_loss=0.5319 val_F1=0.7880', '\\n', '[dropout=0.05] Epoch 4: train_loss=0.5188\nval_loss=0.5271 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 5: train_loss=0.5208\nval_loss=0.5189 val_F1=0.7920', '\\n', '[dropout=0.05] Epoch 6: train_loss=0.5118\nval_loss=0.5180 val_F1=0.7900', '\\n', '[dropout=0.05] Epoch 7: train_loss=0.5112\nval_loss=0.5136 val_F1=0.7959', '\\n', '[dropout=0.05] Epoch 8: train_loss=0.5134\nval_loss=0.5526 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 9: train_loss=0.5157\nval_loss=0.5229 val_F1=0.7940', '\\n', '[dropout=0.05] Epoch 10:\ntrain_loss=0.5173 val_loss=0.5180 val_F1=0.7940', '\\n', '[dropout=0.05] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.1] Epoch 1: train_loss=0.6798\nval_loss=0.6003 val_F1=0.7291', '\\n', '[dropout=0.1] Epoch 2: train_loss=0.5522\nval_loss=0.5441 val_F1=0.7800', '\\n', '[dropout=0.1] Epoch 3: train_loss=0.5367\nval_loss=0.5319 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 4: train_loss=0.5314\nval_loss=0.5296 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 5: train_loss=0.5194\nval_loss=0.5273 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 6: train_loss=0.5149\nval_loss=0.5414 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 7: train_loss=0.5239\nval_loss=0.5188 val_F1=0.7920', '\\n', '[dropout=0.1] Epoch 8: train_loss=0.5113\nval_loss=0.5792 val_F1=0.7820', '\\n', '[dropout=0.1] Epoch 9: train_loss=0.5414\nval_loss=0.5484 val_F1=0.7599', '\\n', '[dropout=0.1] Epoch 10: train_loss=0.5137\nval_loss=0.5392 val_F1=0.7860', '\\n', '[dropout=0.1] Test MacroF1: 0.7960\\n',\n'\\n', '[dropout=0.2] Epoch 1: train_loss=0.6511 val_loss=0.5626 val_F1=0.7456',\n'\\n', '[dropout=0.2] Epoch 2: train_loss=0.5512 val_loss=0.5399 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 3: train_loss=0.5451 val_loss=0.5961 val_F1=0.7414',\n'\\n', '[dropout=0.2] Epoch 4: train_loss=0.5363 val_loss=0.5383 val_F1=0.7840',\n'\\n', '[dropout=0.2] Epoch 5: train_loss=0.5246 val_loss=0.5220 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 6: train_loss=0.5150 val_loss=0.5180 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 7: train_loss=0.5114 val_loss=0.5364 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 8: train_loss=0.5191 val_loss=0.5320 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 9: train_loss=0.5159 val_loss=0.5273 val_F1=0.7920',\n'\\n', '[dropout=0.2] Epoch 10: train_loss=0.5144 val_loss=0.5208 val_F1=0.7860',\n'\\n', '[dropout=0.2] Test MacroF1: 0.7950\\n', '\\n', '[dropout=0.3] Epoch 1:\ntrain_loss=0.6892 val_loss=0.6036 val_F1=0.7157', '\\n', '[dropout=0.3] Epoch 2:\ntrain_loss=0.5666 val_loss=0.5658 val_F1=0.7940', '\\n', '[dropout=0.3] Epoch 3:\ntrain_loss=0.5377 val_loss=0.5393 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 4:\ntrain_loss=0.5257 val_loss=0.5383 val_F1=0.7900', '\\n', '[dropout=0.3] Epoch 5:\ntrain_loss=0.5314 val_loss=0.5565 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 6:\ntrain_loss=0.5223 val_loss=0.5339 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 7:\ntrain_loss=0.5186 val_loss=0.5254 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 8:\ntrain_loss=0.5168 val_loss=0.5545 val_F1=0.7919', '\\n', '[dropout=0.3] Epoch 9:\ntrain_loss=0.5139 val_loss=0.5126 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 10:\ntrain_loss=0.5139 val_loss=0.5153 val_F1=0.7959', '\\n', '[dropout=0.3] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.4] Epoch 1: train_loss=0.6531\nval_loss=0.5744 val_F1=0.7758', '\\n', '[dropout=0.4] Epoch 2: train_loss=0.5414\nval_loss=0.5788 val_F1=0.7899', '\\n', '[dropout=0.4] Epoch 3: train_loss=0.5240\nval_loss=0.5383 val_F1=0.7920', '\\n', '[dropout=0.4] Epoch 4: train_loss=0.5258\nval_loss=0.5445 val_F1=0.7860', '\\n', '[dropout=0.4] Epoch 5: train_loss=0.5157\nval_loss=0.5385 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 6: train_loss=0.5174\nval_loss=0.5155 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 7: train_loss=0.5171\nval_loss=0.5152 val_F1=0.7940', '\\n', '[dropout=0.4] Epoch 8: train_loss=0.5119\nval_loss=0.5487 val_F1=0.7818', '\\n', '[dropout=0.4] Epoch 9: train_loss=0.5079\nval_loss=0.5151 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 10: train_loss=0.5075\nval_loss=0.5239 val_F1=0.7959', '\\n', '[dropout=0.4] Test MacroF1: 0.7950\\n',\n'\\n', 'Best dev MacroF1=0.7959 obtained with dropout=0.0', '\\n', 'Final Test\nMacroF1 with best dropout (0.0): 0.7950', '\\n', 'Execution time: 23 seconds\nseconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH with', ' ', '2000', ' ',\n'train examples.', '\\n', 'Vocab size:', ' ', '11', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '[dropout=0.0] Epoch 1:\ntrain_loss=0.6791 val_loss=0.5819 val_F1=0.7599', '\\n', '[dropout=0.0] Epoch 2:\ntrain_loss=0.5440 val_loss=0.5415 val_F1=0.7800', '\\n', '[dropout=0.0] Epoch 3:\ntrain_loss=0.5306 val_loss=0.5363 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 4:\ntrain_loss=0.5232 val_loss=0.5304 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 5:\ntrain_loss=0.5182 val_loss=0.5262 val_F1=0.7959', '\\n', '[dropout=0.0] Epoch 6:\ntrain_loss=0.5203 val_loss=0.5340 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 7:\ntrain_loss=0.5105 val_loss=0.5250 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 8:\ntrain_loss=0.5138 val_loss=0.5213 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 9:\ntrain_loss=0.5173 val_loss=0.5349 val_F1=0.7860', '\\n', '[dropout=0.0] Epoch 10:\ntrain_loss=0.5094 val_loss=0.5152 val_F1=0.7880', '\\n', '[dropout=0.0] Test\nMacroF1: 0.7930\\n', '\\n', '[dropout=0.05] Epoch 1: train_loss=0.6534\nval_loss=0.5547 val_F1=0.7599', '\\n', '[dropout=0.05] Epoch 2: train_loss=0.5457\nval_loss=0.5626 val_F1=0.7538', '\\n', '[dropout=0.05] Epoch 3: train_loss=0.5351\nval_loss=0.5435 val_F1=0.7920', '\\n', '[dropout=0.05] Epoch 4: train_loss=0.5333\nval_loss=0.5484 val_F1=0.7599', '\\n', '[dropout=0.05] Epoch 5: train_loss=0.5297\nval_loss=0.5331 val_F1=0.7900', '\\n', '[dropout=0.05] Epoch 6: train_loss=0.5289\nval_loss=0.5332 val_F1=0.7959', '\\n', '[dropout=0.05] Epoch 7: train_loss=0.5384\nval_loss=0.5306 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 8: train_loss=0.5188\nval_loss=0.5300 val_F1=0.7860', '\\n', '[dropout=0.05] Epoch 9: train_loss=0.5180\nval_loss=0.5218 val_F1=0.7900', '\\n', '[dropout=0.05] Epoch 10:\ntrain_loss=0.5118 val_loss=0.5228 val_F1=0.7959', '\\n', '[dropout=0.05] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.1] Epoch 1: train_loss=0.7127\nval_loss=0.6520 val_F1=0.3421', '\\n', '[dropout=0.1] Epoch 2: train_loss=0.5915\nval_loss=0.5579 val_F1=0.7639', '\\n', '[dropout=0.1] Epoch 3: train_loss=0.5251\nval_loss=0.5353 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 4: train_loss=0.5193\nval_loss=0.5265 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 5: train_loss=0.5198\nval_loss=0.5485 val_F1=0.7800', '\\n', '[dropout=0.1] Epoch 6: train_loss=0.5213\nval_loss=0.5221 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 7: train_loss=0.5162\nval_loss=0.5505 val_F1=0.7860', '\\n', '[dropout=0.1] Epoch 8: train_loss=0.5175\nval_loss=0.5196 val_F1=0.7959', '\\n', '[dropout=0.1] Epoch 9: train_loss=0.5211\nval_loss=0.5228 val_F1=0.7900', '\\n', '[dropout=0.1] Epoch 10: train_loss=0.5110\nval_loss=0.5165 val_F1=0.7880', '\\n', '[dropout=0.1] Test MacroF1: 0.7950\\n',\n'\\n', '[dropout=0.2] Epoch 1: train_loss=0.6401 val_loss=0.5525 val_F1=0.7880',\n'\\n', '[dropout=0.2] Epoch 2: train_loss=0.5313 val_loss=0.5484 val_F1=0.7700',\n'\\n', '[dropout=0.2] Epoch 3: train_loss=0.5349 val_loss=0.5313 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 4: train_loss=0.5194 val_loss=0.5470 val_F1=0.7800',\n'\\n', '[dropout=0.2] Epoch 5: train_loss=0.5445 val_loss=0.5296 val_F1=0.7880',\n'\\n', '[dropout=0.2] Epoch 6: train_loss=0.5213 val_loss=0.5285 val_F1=0.7860',\n'\\n', '[dropout=0.2] Epoch 7: train_loss=0.5186 val_loss=0.5204 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 8: train_loss=0.5106 val_loss=0.5126 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 9: train_loss=0.5091 val_loss=0.5109 val_F1=0.7959',\n'\\n', '[dropout=0.2] Epoch 10: train_loss=0.5076 val_loss=0.5155 val_F1=0.7959',\n'\\n', '[dropout=0.2] Test MacroF1: 0.7950\\n', '\\n', '[dropout=0.3] Epoch 1:\ntrain_loss=0.6682 val_loss=0.5911 val_F1=0.7224', '\\n', '[dropout=0.3] Epoch 2:\ntrain_loss=0.5614 val_loss=0.5485 val_F1=0.7840', '\\n', '[dropout=0.3] Epoch 3:\ntrain_loss=0.5333 val_loss=0.5356 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 4:\ntrain_loss=0.5243 val_loss=0.5356 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 5:\ntrain_loss=0.5228 val_loss=0.5356 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 6:\ntrain_loss=0.5303 val_loss=0.5243 val_F1=0.7860', '\\n', '[dropout=0.3] Epoch 7:\ntrain_loss=0.5175 val_loss=0.5124 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 8:\ntrain_loss=0.5098 val_loss=0.5190 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 9:\ntrain_loss=0.5103 val_loss=0.5115 val_F1=0.7959', '\\n', '[dropout=0.3] Epoch 10:\ntrain_loss=0.5238 val_loss=0.5427 val_F1=0.7860', '\\n', '[dropout=0.3] Test\nMacroF1: 0.7950\\n', '\\n', '[dropout=0.4] Epoch 1: train_loss=0.6120\nval_loss=0.5925 val_F1=0.7619', '\\n', '[dropout=0.4] Epoch 2: train_loss=0.5462\nval_loss=0.5591 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 3: train_loss=0.5327\nval_loss=0.5418 val_F1=0.7860', '\\n', '[dropout=0.4] Epoch 4: train_loss=0.5212\nval_loss=0.5277 val_F1=0.7920', '\\n', '[dropout=0.4] Epoch 5: train_loss=0.5162\nval_loss=0.5329 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 6: train_loss=0.5164\nval_loss=0.5308 val_F1=0.7939', '\\n', '[dropout=0.4] Epoch 7: train_loss=0.5185\nval_loss=0.5210 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 8: train_loss=0.5109\nval_loss=0.5241 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 9: train_loss=0.5159\nval_loss=0.5155 val_F1=0.7959', '\\n', '[dropout=0.4] Epoch 10: train_loss=0.5227\nval_loss=0.5101 val_F1=0.7959', '\\n', '[dropout=0.4] Test MacroF1: 0.7950\\n',\n'\\n', 'Best dev MacroF1=0.7959 obtained with dropout=0.0', '\\n', 'Final Test\nMacroF1 with best dropout (0.0): 0.7930', '\\n', 'Execution time: 22 seconds\nseconds (time limit is 30 minutes).']", ""], "analysis": ["The training and hyperparameter tuning process executed successfully without any\nerrors or bugs. The script performed hyperparameter sweeps for different maximum\nepochs (10, 20, 30, 40, 50) with early stopping based on validation F1 scores.\nThe best model checkpoint was evaluated on the test set for each configuration,\nand results were saved successfully. The implementation adheres to the goals of\nthe sub-stage, focusing on improving validation and test F1 scores while\npreventing overfitting using early stopping.", "The execution completed successfully without any bugs. The training script\nperformed a learning rate sweep and evaluated the model's performance on the\nSynthetic PolyRule Reasoning (SPR) task. The best validation F1 score achieved\nwas 0.7959 with a corresponding test F1 score of 0.7869. All metrics were logged\nand the experiment data was saved as expected. The script demonstrated proper\nfunctionality and achieved reasonable results within the time constraints.", "", "", "The training script executed successfully without any errors or bugs. The model\nwas trained with different weight decay values, and the best validation F1 score\n(0.7959) was achieved with weight_decay=0.0. The corresponding test F1 score was\n0.7950. The output aligns with the goals of the hyperparameter fine-tuning\nstage, and no issues were detected in the implementation or results.", "", "The training and evaluation script executed successfully without any errors or\nbugs. The training process for different numbers of transformer layers (nlayers)\nwas carried out as intended, and the results were saved in\n'experiment_data.npy'. The best validation F1 score achieved was 0.7959, with a\ncorresponding test F1 score of 0.7950. The execution time was well within the\nlimit. No issues were detected, and the implementation aligns with the\nexperimental goals.", "", "The execution output indicates that the training script executed successfully\nwithout any errors or bugs. The script performed hyperparameter tuning on the\ndropout rates, evaluated the model's performance using Macro F1 scores, and\nidentified the best-performing dropout value (0.0). The final test Macro F1\nscore achieved with the best dropout was 0.7950. The results and metrics were\nproperly logged, and the script adhered to the goals of the sub-stage. No issues\nwere detected.", "", "The execution output indicates that the training script ran successfully without\nany errors or bugs. The script explored various dropout rates during\nhyperparameter tuning and reported the best model configuration based on\nvalidation F1 scores. The best dropout rate was found to be 0.0, achieving a\nfinal test Macro F1 score of 0.7930. The results are consistent and align with\nthe goals of the hyperparameter tuning stage.", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is fitting the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5119, "best_value": 0.5119}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating how well the model is generalizing to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5201, "best_value": 0.5201}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.7959}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score on the test set, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.793, "best_value": 0.793}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is fitting the data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5868, "best_value": 0.5868}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training, indicating the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7935, "best_value": 0.7935}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, showing how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5243, "best_value": 0.5243}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The best F1 score during validation, indicating the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score on the test dataset, showing the final model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7899, "best_value": 0.7899}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score on the training dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.7975, "best_value": 0.7975}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score on the validation dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.509, "best_value": 0.509}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5132, "best_value": 0.5132}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.51, "best_value": 0.51}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5183, "best_value": 0.5183}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5109, "best_value": 0.5109}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5114, "best_value": 0.5114}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5128, "best_value": 0.5128}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5107, "best_value": 0.5107}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5143, "best_value": 0.5143}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5257, "best_value": 0.5257}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5228, "best_value": 0.5228}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5125, "best_value": 0.5125}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5099, "best_value": 0.5099}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5092, "best_value": 0.5092}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5284, "best_value": 0.5284}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.542, "best_value": 0.542}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5244, "best_value": 0.5244}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5194, "best_value": 0.5194}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score on the test dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.794, "best_value": 0.794}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "best overall test F1 score", "lower_is_better": false, "description": "Best F1 score across all test datasets", "data": [{"dataset_name": "best_overall", "final_value": 0.794, "best_value": 0.794}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5201, "best_value": 0.5201}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "F1 score measures the balance between precision and recall. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7889, "best_value": 0.795}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "Loss quantifies the error during training. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5244, "best_value": 0.5096}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final and best training loss values for each dataset.", "data": [{"dataset_name": "SPR_BENCH_n2", "final_value": 0.5201653161048889, "best_value": 0.5201653161048889}, {"dataset_name": "SPR_BENCH_n4", "final_value": 0.5215714364051819, "best_value": 0.5215714364051819}, {"dataset_name": "SPR_BENCH_n6", "final_value": 0.5067998228073121, "best_value": 0.5067998228073121}, {"dataset_name": "SPR_BENCH_n8", "final_value": 0.5074102683067322, "best_value": 0.5074102683067322}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final and best validation loss values for each dataset.", "data": [{"dataset_name": "SPR_BENCH_n2", "final_value": 0.5267321410179138, "best_value": 0.5267321410179138}, {"dataset_name": "SPR_BENCH_n4", "final_value": 0.5420828981399536, "best_value": 0.5420828981399536}, {"dataset_name": "SPR_BENCH_n6", "final_value": 0.5222784585952759, "best_value": 0.5222784585952759}, {"dataset_name": "SPR_BENCH_n8", "final_value": 0.5124282159805298, "best_value": 0.5124282159805298}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "Final and best training F1 scores for each dataset.", "data": [{"dataset_name": "SPR_BENCH_n2", "final_value": 0.7909322620529051, "best_value": 0.7909322620529051}, {"dataset_name": "SPR_BENCH_n4", "final_value": 0.7909322620529051, "best_value": 0.7909322620529051}, {"dataset_name": "SPR_BENCH_n6", "final_value": 0.7959869431643625, "best_value": 0.7959869431643625}, {"dataset_name": "SPR_BENCH_n8", "final_value": 0.794983393654886, "best_value": 0.794983393654886}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Final and best validation F1 scores for each dataset.", "data": [{"dataset_name": "SPR_BENCH_n2", "final_value": 0.7959477626272327, "best_value": 0.7959477626272327}, {"dataset_name": "SPR_BENCH_n4", "final_value": 0.7959477626272327, "best_value": 0.7959477626272327}, {"dataset_name": "SPR_BENCH_n6", "final_value": 0.7959477626272327, "best_value": 0.7959477626272327}, {"dataset_name": "SPR_BENCH_n8", "final_value": 0.7959477626272327, "best_value": 0.7959477626272327}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "Final and best test F1 scores for each dataset.", "data": [{"dataset_name": "SPR_BENCH_n2", "final_value": 0.7949899545077709, "best_value": 0.7949899545077709}, {"dataset_name": "SPR_BENCH_n4", "final_value": 0.7949899545077709, "best_value": 0.7949899545077709}, {"dataset_name": "SPR_BENCH_n6", "final_value": 0.7949899545077709, "best_value": 0.7949899545077709}, {"dataset_name": "SPR_BENCH_n8", "final_value": 0.7949899545077709, "best_value": 0.7949899545077709}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7955, "best_value": 0.7955}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.798, "best_value": 0.798}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.797, "best_value": 0.797}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss for the training dataset.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5095, "best_value": 0.5095}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5112, "best_value": 0.5112}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5219, "best_value": 0.5219}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5095, "best_value": 0.5095}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5098, "best_value": 0.5098}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5133, "best_value": 0.5133}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5369, "best_value": 0.5369}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5116, "best_value": 0.5116}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5263, "best_value": 0.5263}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.509, "best_value": 0.509}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5159, "best_value": 0.5159}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5761, "best_value": 0.5761}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss for the test dataset.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5401, "best_value": 0.5401}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5108, "best_value": 0.5108}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5335, "best_value": 0.5335}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5243, "best_value": 0.5243}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.564, "best_value": 0.564}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5702, "best_value": 0.5702}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score for the test dataset.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.794, "best_value": 0.794}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "best model test F1 score", "lower_is_better": false, "description": "Best F1 score achieved by the model on the test dataset.", "data": [{"dataset_name": "best_overall", "final_value": 0.795, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "The F1 score on the training dataset, indicating the model's performance during training.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.7945, "best_value": 0.7945}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7945, "best_value": 0.7945}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score on the validation dataset, used to tune model hyperparameters.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value on the training dataset, indicating the error during training.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5134, "best_value": 0.5134}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5173, "best_value": 0.5173}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5137, "best_value": 0.5137}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5144, "best_value": 0.5144}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5139, "best_value": 0.5139}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5075, "best_value": 0.5075}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, used to tune model hyperparameters.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5267, "best_value": 0.5267}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.518, "best_value": 0.518}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5392, "best_value": 0.5392}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5208, "best_value": 0.5208}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5153, "best_value": 0.5153}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5239, "best_value": 0.5239}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset, used to evaluate final model performance.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5155, "best_value": 0.5155}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.51, "best_value": 0.51}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5233, "best_value": 0.5233}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5175, "best_value": 0.5175}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5604, "best_value": 0.5604}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5442, "best_value": 0.5442}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score on the test dataset, used to evaluate final model performance.", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "best overall test F1 score", "lower_is_better": false, "description": "The best F1 score achieved across all test datasets.", "data": [{"dataset_name": "best_overall", "final_value": 0.795, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score on the training dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7935, "best_value": 0.7935}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7939, "best_value": 0.7939}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score on the validation dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5094, "best_value": 0.5094}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5118, "best_value": 0.5118}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.511, "best_value": 0.511}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5076, "best_value": 0.5076}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5238, "best_value": 0.5238}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5227, "best_value": 0.5227}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5152, "best_value": 0.5152}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5228, "best_value": 0.5228}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5165, "best_value": 0.5165}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5155, "best_value": 0.5155}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5427, "best_value": 0.5427}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5101, "best_value": 0.5101}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.5188, "best_value": 0.5188}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.5304, "best_value": 0.5304}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.5222, "best_value": 0.5222}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.5199, "best_value": 0.5199}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.5338, "best_value": 0.5338}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.5561, "best_value": 0.5561}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score on the test dataset", "data": [{"dataset_name": "SPR_BENCH_dropout_0.0", "final_value": 0.793, "best_value": 0.793}, {"dataset_name": "SPR_BENCH_dropout_0.05", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.1", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.3", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH_dropout_0.4", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "best model test F1 score", "lower_is_better": false, "description": "Best test F1 score across all models", "data": [{"dataset_name": "best_overall", "final_value": 0.793, "best_value": 0.793}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, true, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_testF1_vs_epochs.png"], ["../../logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_valF1_vs_lr.png", "../../logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_testF1_vs_lr.png"], ["../../logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_val_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_best_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_best_val_f1.png", "../../logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_test_f1.png", "../../logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_confusion_matrix_dmodel_192.png"], ["../../logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_confusion_matrix_SPR_BENCH_n2.png"], ["../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead2_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead2_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead4_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead4_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead8_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead8_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_test_f1_summary.png"], ["../../logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_test_f1_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_76abbd9d730a4d5eb877b97796415b50/SPR_BENCH_loss_curves_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_76abbd9d730a4d5eb877b97796415b50/SPR_BENCH_f1_curves_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_76abbd9d730a4d5eb877b97796415b50/SPR_BENCH_test_f1_bar_mean_sem.png"]], "plot_paths": [["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_testF1_vs_epochs.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_valF1_vs_lr.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_testF1_vs_lr.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_val_test_f1_bar.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_best_val_f1_bar.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_best_val_f1.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_test_f1.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_confusion_matrix_dmodel_192.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_confusion_matrix_SPR_BENCH_n2.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead2_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead2_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead4_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead4_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead8_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead8_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_test_f1_summary.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_test_f1_bar.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_76abbd9d730a4d5eb877b97796415b50/SPR_BENCH_loss_curves_mean_sem.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_76abbd9d730a4d5eb877b97796415b50/SPR_BENCH_f1_curves_mean_sem.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_76abbd9d730a4d5eb877b97796415b50/SPR_BENCH_test_f1_bar_mean_sem.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation loss across different epoch budgets (10, 20, 30, 40, and 50). Training loss consistently decreases across all configurations, indicating that the model is learning effectively. However, validation loss does not follow a consistent decreasing trend. For instance, for epoch budgets of 20 and 30, validation loss exhibits fluctuations, suggesting potential overfitting or sensitivity to hyperparameters. The validation loss stabilizes more for higher epoch budgets (40 and 50), indicating better generalization. Overall, while increasing the epoch budget improves training loss, validation loss trends indicate a need for careful hyperparameter tuning and possibly regularization techniques.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates training and validation Macro-F1 scores for different epoch budgets. Both training and validation Macro-F1 scores improve significantly in the initial epochs and stabilize after approximately 5 epochs. Higher epoch budgets (40 and 50) result in slightly higher and more stable validation F1 scores, suggesting that longer training allows the model to better capture the patterns in the data without overfitting. However, the improvements in validation Macro-F1 scores beyond 30 epochs are marginal, indicating diminishing returns with further training. This suggests that an optimal epoch budget might be around 30-40 epochs for this setup.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot shows the test Macro-F1 scores for different epoch budgets. The scores remain relatively stable across all configurations, with slight fluctuations. The highest test Macro-F1 score is achieved with an epoch budget of 50, but the improvement over lower epoch budgets (e.g., 10 and 30) is minimal. This indicates that while increasing the epoch budget does not significantly harm performance, it also does not lead to substantial gains. This stability suggests that other hyperparameters, such as learning rate or batch size, might have a more significant impact on performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bfd7e934c66b4c92909e1d0f2d00fa2c_proc_3160638/SPR_BENCH_testF1_vs_epochs.png"}], [{"analysis": "The first plot shows the training and validation loss curves for different learning rates. Learning rates of 1e-03 and 3e-04 achieve the fastest convergence in terms of both training and validation loss. However, the validation loss for 1e-03 exhibits some instability, suggesting potential overfitting. Learning rates of 1e-05 and 3e-05 converge more slowly and have higher validation loss, indicating underfitting or insufficient learning capacity. The learning rate of 1e-04 strikes a good balance with stable and low validation loss, making it a promising candidate for further optimization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot illustrates the training and validation F1 scores for various learning rates. Learning rates of 1e-03 and 1e-04 reach the highest F1 scores quickly, but 1e-03 shows fluctuations in the validation F1, hinting at overfitting. Learning rates of 1e-05 and 3e-05 show slower improvement and lower final F1 scores, indicating underfitting. Learning rates of 3e-04 and 1e-04 achieve stable and high validation F1 scores, suggesting these are optimal for balancing learning speed and generalization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_f1_curves.png"}, {"analysis": "The third plot summarizes the best validation F1 scores achieved for each learning rate. All learning rates achieve similar high scores, with minimal differences. However, the learning rate of 1e-04 slightly edges out the others in terms of stability and convergence in earlier plots, making it the most reliable choice.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_valF1_vs_lr.png"}, {"analysis": "The fourth plot shows the test F1 scores for different learning rates. The results are consistent across learning rates, with all achieving comparable performance. This indicates that the model generalizes well regardless of the learning rate, provided sufficient training is performed. However, considering earlier convergence and stability, 1e-04 remains the most practical choice for further experimentation.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_53069329f5ee4b2b806ca6523ed1ce3b_proc_3160639/SPR_BENCH_testF1_vs_lr.png"}], [{"analysis": "This plot shows the cross-entropy loss for training and validation across different batch sizes (32, 64, 128, and 256). Smaller batch sizes (e.g., 32 and 64) demonstrate more stable convergence and lower validation loss toward the end of training, indicating better generalization. Larger batch sizes (e.g., 256) show faster initial convergence but higher validation loss, suggesting potential overfitting or insufficient generalization. The validation loss curves are more erratic for larger batch sizes, which could indicate instability during training.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the macro F1 scores for training and validation across batch sizes. The F1 scores for smaller batch sizes (32 and 64) stabilize earlier and remain consistently high, reflecting better generalization. Larger batch sizes (128 and 256) exhibit slower convergence in F1 scores, with validation F1 scores sometimes lagging behind training scores, indicating potential overfitting. Batch size 256 shows a significant gap between training and validation F1 in the early epochs, which narrows as training progresses.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart compares the best validation F1 scores with the test F1 scores for each batch size. The results show minimal difference between validation and test F1 scores, suggesting that the model generalizes well across all batch sizes. However, smaller batch sizes (32 and 64) achieve slightly higher F1 scores, aligning with the observations from the previous plots that smaller batch sizes lead to better generalization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7b4fd3f0460345bda4d0986b351535c0_proc_3160640/SPR_BENCH_val_test_f1_bar.png"}], [{"analysis": "The training loss plots show a consistent decrease across all dropout rates, indicating that the model is effectively learning from the training data. However, the validation loss exhibits more variability, particularly in the earlier epochs, which could suggest some sensitivity to dropout rates. Dropout rates of 0.0 and 0.2 seem to lead to lower validation loss towards the later epochs, potentially indicating a better generalization capability compared to other rates.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_loss_curves.png"}, {"analysis": "The training Macro-F1 scores quickly converge to a high value (~0.80) across all dropout rates, showing that the model learns well from the training set. On the validation Macro-F1 scores, dropout rates of 0.2 and 0.3 show more stability and higher scores in later epochs, suggesting better generalization. Dropout rates of 0.0 and 0.1 exhibit more fluctuations, which might indicate overfitting or sensitivity to noise in the validation set.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_f1_curves.png"}, {"analysis": "The test Macro-F1 scores are nearly identical across all dropout rates, with a best overall score of 0.794. This suggests that dropout has a minimal impact on the final test performance, and the model achieves comparable results regardless of the dropout rate. However, the slight edge at certain dropout rates (e.g., 0.2) might be worth further investigation to confirm its consistency.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1b26a93c3d094cee8a32cbed61d33fa7_proc_3160641/SPR_BENCH_test_f1_bar.png"}], [{"analysis": "The F1 scores for both training and validation converge to approximately 0.80 across all weight decay values. While there is some initial divergence in the first few epochs, the model stabilizes quickly. This indicates that weight decay has only a minor impact on the overall learning dynamics and performance of the model. The consistent performance across weight decay values suggests that the model is robust to this hyperparameter.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_f1_curves.png"}, {"analysis": "The bar chart shows that the best validation F1 score remains constant at approximately 0.80 across all tested weight decay values. This reinforces the observation that weight decay does not significantly affect the model's performance on the validation set. The lack of variation suggests that other hyperparameters or model adjustments may have a more significant influence on performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_best_val_f1_bar.png"}, {"analysis": "The confusion matrix indicates a well-balanced performance on the test set, with a high number of correct predictions for both classes. The model appears to perform equally well across the two classes, with no significant bias or imbalance in predictions. This suggests that the model has learned the underlying patterns in the data effectively and generalizes well to unseen data.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8bdec1ffa63746bd8bfdfa4b3ef42943_proc_3160640/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves show that the training loss decreases steadily across all configurations of d_model, indicating that the model is learning effectively. The validation loss also follows a decreasing trend initially, but it exhibits some fluctuations after a few epochs, suggesting potential overfitting for certain configurations. The differences in loss values across different d_model configurations are minor, indicating that the choice of d_model does not significantly impact the convergence of the loss.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves demonstrate that the model achieves rapid improvements in performance within the first few epochs, after which the improvement plateaus. Validation Macro-F1 scores show slight fluctuations, indicating that some configurations may be more prone to overfitting than others. Overall, different d_model values seem to achieve similar Macro-F1 performance, with no significant advantage for any specific configuration.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_f1_curves.png"}, {"analysis": "The bar plot shows that the best validation Macro-F1 scores are nearly identical across all d_model configurations. This suggests that the choice of d_model has minimal impact on the validation performance, and other factors such as learning rate or batch size might be more critical in determining performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_best_val_f1.png"}, {"analysis": "The test Macro-F1 bar plot confirms that the performance on the test set is consistent across different d_model configurations. This reinforces the observation that varying d_model does not significantly affect the model's generalization ability on the SPR_BENCH dataset.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_test_f1.png"}, {"analysis": "The confusion matrix for d_model=192 shows that the model performs well in distinguishing between the two classes. The high values along the diagonal indicate that the model has a strong ability to correctly classify samples. However, there is a slight imbalance in the misclassification rates, which could be addressed with additional fine-tuning or adjustments to the training process.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_90875fb5600f4ebcaf2062ce0849503d_proc_3160639/SPR_BENCH_confusion_matrix_dmodel_192.png"}], [{"analysis": "The loss curves show a steady decline in training loss for all configurations, indicating the model is learning effectively. Validation loss stabilizes after a few epochs, suggesting that the model does not overfit significantly. However, the red curve (SPR_BENCH_n8) exhibits a slightly higher initial loss and slower convergence compared to the others, which might indicate that the n8 configuration is less optimal for this task.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_loss_curves.png"}, {"analysis": "The F1 score curves indicate that all configurations achieve similar performance on the training and validation sets, converging to approximately 0.8. The red curve (SPR_BENCH_n8) shows slower initial improvement, but it eventually catches up. This suggests that n8 might require more epochs to fully optimize, but its final performance is comparable to other configurations.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_f1_curves.png"}, {"analysis": "The test F1 scores for all configurations are identical at 0.795, indicating that the number of layers (n2, n4, n6, n8) does not significantly impact the final performance on the test set. This suggests that the model's architecture is robust across these variations.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for SPR_BENCH_n2 shows a balanced performance between the two classes, with a slightly higher number of true positives (401) compared to true negatives (394). The false positive and false negative rates are also relatively low, indicating good overall classification performance. This supports the conclusion that the model performs well on this task.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_57c397eab351412593170a2fa7a24ffa_proc_3160638/SPR_BENCH_confusion_matrix_SPR_BENCH_n2.png"}], [{"analysis": "The loss curves show a consistent decrease in training loss over the epochs, with validation loss following a similar trend but at a slightly higher level. This suggests that the model is learning effectively without significant overfitting. However, the validation loss plateauing around epoch 8 indicates the model may have reached its learning capacity with the current configuration.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead2_loss_curves.png"}, {"analysis": "The F1 score curves demonstrate a rapid improvement in the early epochs, with both training and validation F1 scores stabilizing around 0.78 to 0.80. The close alignment of these curves suggests a good balance between training and validation performance, and minimal overfitting is observed.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead2_f1_curves.png"}, {"analysis": "The training loss decreases steadily, while the validation loss fluctuates slightly but remains relatively stable after epoch 4. This indicates that the model is learning effectively, though the slight fluctuations in validation loss suggest minor instability in generalization performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead4_loss_curves.png"}, {"analysis": "The F1 score curves show a rapid rise in the first few epochs, with both training and validation scores stabilizing around 0.78 to 0.80. The alignment of the curves indicates that the model is generalizing well, with minimal overfitting.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead4_f1_curves.png"}, {"analysis": "The training loss shows a consistent decrease across epochs, while the validation loss exhibits more pronounced fluctuations compared to the other configurations. This suggests some instability in the model's generalization performance, potentially due to over-parameterization or suboptimal hyperparameter settings.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead8_loss_curves.png"}, {"analysis": "The F1 score curves reveal a stable performance after initial rapid improvement, with both training and validation scores converging around 0.78 to 0.80. However, the fluctuations in validation F1 suggest that the model's generalization may be slightly less stable than in other configurations.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_nhead8_f1_curves.png"}, {"analysis": "The bar chart shows that the test F1 scores for nhead values of 2 and 8 are slightly higher (0.795) compared to nhead=4 (0.789). This suggests that nhead=2 and nhead=8 configurations provide marginally better test performance, though the differences are minimal and may not be statistically significant.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e0194a600c3641b088291b125cf0d912_proc_3160641/SPR_BENCH_test_f1_summary.png"}], [{"analysis": "The training loss consistently decreases across all dropout configurations, indicating that the model is learning effectively. However, the validation loss shows some fluctuations, particularly for higher dropout rates like 0.3, which may suggest instability during training. Lower dropout rates (e.g., 0.05 and 0.1) generally result in smoother validation loss curves, indicating better generalization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_loss_curves.png"}, {"analysis": "The training Macro-F1 scores quickly converge to high values above 0.78 for all dropout configurations, demonstrating that the model is effectively capturing patterns in the training data. The validation Macro-F1 scores show more variability, particularly for dropout rates of 0.2 and 0.3, suggesting that higher dropout rates might introduce instability. Lower dropout rates (e.g., 0.05 and 0.1) appear to maintain more consistent validation performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_f1_curves.png"}, {"analysis": "The test Macro-F1 scores for all dropout configurations are close to the best overall score of 0.795, indicating that the dropout parameter does not significantly impact final test performance. This suggests that the model is robust to dropout variations within the range tested.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/SPR_BENCH_test_f1_bar.png"}], [{"analysis": "The training loss plot shows a consistent decrease across all dropout rates, indicating that the model is learning effectively during training. However, the validation loss plot exhibits more variability, with some dropout rates (e.g., 0.1 and 0.3) showing occasional spikes. This suggests that certain dropout rates may lead to overfitting or instability during validation. Overall, dropout rates of 0.2 and 0.05 seem to provide more stable validation loss trends.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_loss_curves.png"}, {"analysis": "The training Macro-F1 scores indicate rapid improvement during the initial epochs, plateauing around 0.8 for all dropout rates. This suggests that the model quickly learns to generalize well on the training data. On the other hand, the validation Macro-F1 scores show more fluctuation, particularly for dropout rates of 0.2 and 0.3, which might indicate overfitting or sensitivity to dropout. Dropout rates of 0.0 and 0.05 appear to provide the most stable and consistent validation performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_f1_curves.png"}, {"analysis": "The test Macro-F1 plot highlights that all dropout rates achieve similar performance, with a best overall Macro-F1 score of 0.795. This suggests that the choice of dropout rate has minimal impact on the final test performance, although lower dropout rates (e.g., 0.0 and 0.05) might be slightly more reliable given their stability during training and validation.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/SPR_BENCH_test_f1_bar.png"}], [{"analysis": "The training loss plot shows consistent convergence for all dropout values, with losses stabilizing around 0.5 by epoch 10. The validation loss plot indicates similar trends, with slightly higher variability among dropout values. Dropout 0.2 and 0.3 appear to achieve slightly better validation loss stability, suggesting a potential benefit of regularization at these levels. However, the differences are marginal, and all configurations seem to perform comparably well in terms of loss reduction.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_loss_curves.png"}, {"analysis": "The training Macro-F1 scores exhibit rapid improvement within the first two epochs, stabilizing close to 0.8 for all dropout configurations. This indicates that the model quickly learns the patterns in the training data. The validation Macro-F1 scores also converge around 0.8 across all dropout values, suggesting that the model generalizes well to unseen data. The differences between dropout values are minimal, indicating that the choice of dropout may not significantly impact F1 performance in this setup.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_f1_curves.png"}, {"analysis": "The bar chart of test Macro-F1 scores across different dropout values shows that all configurations achieve nearly identical performance, with the best overall Macro-F1 being 0.793. This further supports the observation that dropout has minimal impact on the final performance of the model on the test set. The slight variations are likely within the margin of error, and no particular dropout value stands out as significantly better or worse.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/SPR_BENCH_test_f1_bar.png"}], []], "vlm_feedback_summary": ["The plots provide valuable insights into the effects of varying epoch budgets on\ntraining, validation, and test performance. While higher epoch budgets improve\ntraining loss and lead to slightly better validation and test scores, the gains\nare marginal beyond 30-40 epochs. This suggests a need for further\nhyperparameter tuning and regularization to achieve more consistent\nimprovements.", "The analysis indicates that a learning rate of 1e-04 is the most balanced\nchoice, achieving stable and high F1 scores across both validation and test\nsets. While other learning rates perform similarly, 1e-04 shows the best trade-\noff between convergence speed and generalization. Further experiments could\nfocus on fine-tuning other hyperparameters using this learning rate.", "The experimental plots reveal that smaller batch sizes (32 and 64) generally\nlead to better generalization and stability, as evidenced by lower validation\nloss and higher F1 scores. Larger batch sizes (128 and 256) show faster initial\nconvergence but suffer from instability and potential overfitting. The\ncomparison of validation and test F1 scores confirms good generalization across\nall configurations, with smaller batch sizes slightly outperforming larger ones.", "The plots provide insights into the impact of different dropout rates on\ntraining and validation performance. Dropout rates of 0.2 and 0.3 appear to\nstrike a balance between training and validation performance, potentially\nindicating better generalization. The test Macro-F1 scores suggest minimal\nsensitivity to dropout rates, with a consistent best overall score of 0.794.", "The plots demonstrate that the model achieves consistent performance across\ndifferent weight decay values, with an F1 score of approximately 0.80. The\nconfusion matrix confirms that the model generalizes well to the test set,\nshowing balanced and accurate predictions for both classes.", "The results indicate that different d_model configurations have minimal impact\non the model's performance metrics. The training and validation loss curves\nsuggest effective learning, though some overfitting is observed. The Macro-F1\nscores are stable across configurations, with rapid early improvements. The\nconfusion matrix highlights strong classification performance with minor\nmisclassification imbalances. Further fine-tuning of other hyperparameters may\nyield additional improvements.", "The analysis reveals that the model performs consistently across different\nconfigurations, with minimal impact from the number of layers. The loss and F1\nscore curves demonstrate effective learning and stable performance. The\nconfusion matrix confirms good classification accuracy, with low error rates.\nOverall, the results suggest that the proposed approach is robust and achieves\ncompetitive performance.", "The plots provide insights into the model's learning dynamics and generalization\nperformance across different configurations. The model demonstrates effective\nlearning and minimal overfitting, with test F1 scores showing slight variations\nacross nhead values. The nhead=2 and nhead=8 configurations achieve marginally\nbetter test performance, indicating potential hyperparameter sensitivity.", "The results indicate that lower dropout rates (e.g., 0.05 and 0.1) generally\nlead to better stability and generalization during training and validation.\nWhile all dropout configurations achieve similar test performance, the observed\nfluctuations in validation metrics for higher dropout rates suggest that lower\ndropout rates may be preferable for this task.", "The plots illustrate that the model learns effectively during training, with\nsome variability in validation performance across different dropout rates. Lower\ndropout rates (0.0 and 0.05) appear to provide more stable and consistent\nresults, while higher dropout rates show occasional instability. The final test\nresults indicate that dropout has a limited effect on ultimate performance, with\nall configurations achieving similar Macro-F1 scores.", "The plots indicate that the model performs consistently well across different\ndropout values, with minimal impact on loss and Macro-F1 metrics. Training and\nvalidation losses converge smoothly, and Macro-F1 scores stabilize around 0.8,\ndemonstrating good generalization. Dropout regularization appears to have\nnegligible effect on performance in this experiment.", "[]"], "exec_time": [17.720833778381348, 17.4319589138031, 21.680989980697632, 22.025863647460938, 18.09143829345703, 24.200924396514893, 17.883678674697876, 11.998880624771118, 23.268936157226562, 23.332881689071655, 22.81694793701172, null], "exec_time_feedback": ["", "", "", "Implementation works but runs too quickly (0.37 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH_n2'", "'SPR_BENCH_n4'", "'SPR_BENCH_n6'", "'SPR_BENCH_n8']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"num_epochs\"][\"SPR_BENCH\"]\n    epochs_grid = spr_data[\"tried_epochs\"]\n    curves = spr_data[\"epoch_curves\"]\n    best_val_f1 = spr_data[\"best_val_f1\"]\n    test_f1 = spr_data[\"test_f1\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    curves, epochs_grid, best_val_f1, test_f1 = None, [], [], []\n\n# -------- Plot 1: Loss curves --------\ntry:\n    if curves:\n        plt.figure()\n        for i, max_ep in enumerate(epochs_grid):\n            x = np.arange(1, len(curves[\"train_loss\"][i]) + 1)\n            plt.plot(\n                x,\n                curves[\"train_loss\"][i],\n                label=f\"train_loss (max_ep={max_ep})\",\n                linestyle=\"-\",\n            )\n            plt.plot(\n                x,\n                curves[\"val_loss\"][i],\n                label=f\"val_loss (max_ep={max_ep})\",\n                linestyle=\"--\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\n            \"SPR_BENCH: Training vs Validation Loss\\nLeft: Train, Right: Validation\"\n        )\n        plt.legend(fontsize=\"small\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname, dpi=150)\n        plt.close()\n        print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# -------- Plot 2: F1 curves --------\ntry:\n    if curves:\n        plt.figure()\n        for i, max_ep in enumerate(epochs_grid):\n            x = np.arange(1, len(curves[\"train_f1\"][i]) + 1)\n            plt.plot(\n                x,\n                curves[\"train_f1\"][i],\n                label=f\"train_F1 (max_ep={max_ep})\",\n                linestyle=\"-\",\n            )\n            plt.plot(\n                x,\n                curves[\"val_f1\"][i],\n                label=f\"val_F1 (max_ep={max_ep})\",\n                linestyle=\"--\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\n            \"SPR_BENCH: Training vs Validation Macro-F1\\nLeft: Train, Right: Validation\"\n        )\n        plt.legend(fontsize=\"small\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname, dpi=150)\n        plt.close()\n        print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# -------- Plot 3: Test F1 vs epochs --------\ntry:\n    if epochs_grid and test_f1:\n        plt.figure()\n        plt.plot(epochs_grid, test_f1, marker=\"o\")\n        for x, y in zip(epochs_grid, test_f1):\n            plt.text(x, y, f\"{y:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n        plt.xlabel(\"max_epochs tried\")\n        plt.ylabel(\"Test Macro-F1\")\n        plt.title(\"SPR_BENCH: Test Macro-F1 vs Epoch Budget\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_testF1_vs_epochs.png\")\n        plt.savefig(fname, dpi=150)\n        plt.close()\n        print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating Test F1 plot: {e}\")\n    plt.close()\n\n# -------- Print numeric summary --------\nprint(\"Tried max_epochs:\", epochs_grid)\nprint(\"Best Val Macro-F1 per run:\", best_val_f1)\nprint(\"Test Macro-F1 per run:\", test_f1)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    ed = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\n    lrs = ed[\"lrs\"]\n    epochs_lists = ed[\"epochs\"]\n    tr_losses = ed[\"losses\"][\"train\"]\n    val_losses = ed[\"losses\"][\"val\"]\n    tr_f1s = ed[\"metrics\"][\"train_f1\"]\n    val_f1s = ed[\"metrics\"][\"val_f1\"]\n    best_val_f1 = ed[\"best_val_f1\"]\n    preds_by_lr = ed[\"predictions\"]\n    gts = ed[\"ground_truth\"]\n    test_f1 = [f1_score(gts, preds, average=\"macro\") for preds in preds_by_lr]\n\n    # print summary table\n    print(\"LR\\tBestValF1\\tTestF1\")\n    for lr, bv, tf in zip(lrs, best_val_f1, test_f1):\n        print(f\"{lr:.0e}\\t{bv:.4f}\\t{tf:.4f}\")\n\n    # ---------- Plot 1: Loss curves ----------\n    try:\n        plt.figure()\n        for lr, ep, tl, vl in zip(lrs, epochs_lists, tr_losses, val_losses):\n            plt.plot(ep, tl, label=f\"Train lr={lr:.0e}\")\n            plt.plot(ep, vl, \"--\", label=f\"Val lr={lr:.0e}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend(fontsize=8)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 2: F1 curves ----------\n    try:\n        plt.figure()\n        for lr, ep, tf1, vf1 in zip(lrs, epochs_lists, tr_f1s, val_f1s):\n            plt.plot(ep, tf1, label=f\"Train lr={lr:.0e}\")\n            plt.plot(ep, vf1, \"--\", label=f\"Val lr={lr:.0e}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation F1\")\n        plt.legend(fontsize=8)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 3: Best val F1 vs LR ----------\n    try:\n        plt.figure()\n        x = range(len(lrs))\n        plt.bar(x, best_val_f1, tick_label=[f\"{lr:.0e}\" for lr in lrs])\n        plt.ylabel(\"Best Validation Macro F1\")\n        plt.xlabel(\"Learning Rate\")\n        plt.title(\"SPR_BENCH: Best Validation F1 by Learning Rate\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_valF1_vs_lr.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val-F1 vs LR plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 4: Test F1 vs LR ----------\n    try:\n        plt.figure()\n        x = range(len(lrs))\n        plt.bar(x, test_f1, tick_label=[f\"{lr:.0e}\" for lr in lrs])\n        plt.ylabel(\"Test Macro F1\")\n        plt.xlabel(\"Learning Rate\")\n        plt.title(\"SPR_BENCH: Test F1 by Learning Rate\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_testF1_vs_lr.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test-F1 vs LR plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\nexp_path_try = [os.path.join(working_dir, \"experiment_data.npy\"), \"experiment_data.npy\"]\nexperiment_data = None\nfor p in exp_path_try:\n    try:\n        experiment_data = np.load(p, allow_pickle=True).item()\n        break\n    except Exception as e:\n        print(f\"Failed loading from {p}: {e}\")\nif experiment_data is None:\n    raise FileNotFoundError(\"experiment_data.npy not found in expected paths\")\n\nspr_hist = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n\n# ---------- loss curves ----------\ntry:\n    plt.figure()\n    for i, (bs, hist) in enumerate(spr_hist.items()):\n        epochs = hist[\"epochs\"]\n        plt.plot(\n            epochs, hist[\"losses\"][\"train\"], \"--\", color=colors[i], label=f\"{bs}-train\"\n        )\n        plt.plot(epochs, hist[\"losses\"][\"val\"], \"-\", color=colors[i], label=f\"{bs}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation (all batch sizes)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ---------- F1 curves ----------\ntry:\n    plt.figure()\n    for i, (bs, hist) in enumerate(spr_hist.items()):\n        epochs = hist[\"epochs\"]\n        plt.plot(\n            epochs,\n            hist[\"metrics\"][\"train_f1\"],\n            \"--\",\n            color=colors[i],\n            label=f\"{bs}-train\",\n        )\n        plt.plot(\n            epochs, hist[\"metrics\"][\"val_f1\"], \"-\", color=colors[i], label=f\"{bs}-val\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro F1\")\n    plt.title(\n        \"SPR_BENCH Macro-F1 Curves\\nLeft: Train, Right: Validation (all batch sizes)\"\n    )\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# ---------- bar chart of best val & test F1 ----------\ntry:\n    plt.figure()\n    bs_labels, best_vals, tests = [], [], []\n    for bs, hist in spr_hist.items():\n        bs_labels.append(bs.replace(\"bs_\", \"\"))\n        best_vals.append(hist[\"best_val_f1\"])\n        tests.append(hist[\"test_f1\"])\n    x = np.arange(len(bs_labels))\n    width = 0.35\n    plt.bar(x - width / 2, best_vals, width, label=\"Best Val F1\")\n    plt.bar(x + width / 2, tests, width, label=\"Test F1\")\n    plt.xticks(x, bs_labels)\n    plt.ylim(0, 1)\n    plt.ylabel(\"Macro F1\")\n    plt.title(\"SPR_BENCH Validation vs Test F1 by Batch Size\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Batch Size | Best Val F1 | Test F1\")\nfor bs, hist in spr_hist.items():\n    print(\n        f\"{bs.replace('bs_',''):>9} | {hist['best_val_f1']:.4f}     | {hist['test_f1']:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndropout_dict = experiment_data.get(\"dropout_tuning\", {})\ntags = list(dropout_dict.keys())[:5]  # plot at most first 5 dropouts\n\n\n# Helper to fetch arrays\ndef get_arr(tag, key1, key2):\n    return np.asarray(dropout_dict[tag][key1][key2])\n\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training loss\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"train\"), label=tag\n        )\n    plt.title(\"Left: Training Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation loss\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"val\"), label=tag)\n    plt.title(\"Right: Validation Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) F1 curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training F1\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"train_f1\"), label=tag\n        )\n    plt.title(\"Left: Training Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation F1\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"val_f1\"), label=tag\n        )\n    plt.title(\"Right: Validation Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Test-set F1 per dropout\ntry:\n    plt.figure()\n    test_f1s = [dropout_dict[tag][\"metrics\"][\"test_f1\"] for tag in tags]\n    plt.bar(range(len(tags)), test_f1s, tick_label=[t.split(\"_\")[-1] for t in tags])\n    best_overall = experiment_data.get(\"best_overall\", {})\n    best_f1 = best_overall.get(\"test_f1\", None)\n    if best_f1 is not None:\n        plt.axhline(\n            best_f1, color=\"r\", linestyle=\"--\", label=f\"Best Overall = {best_f1:.3f}\"\n        )\n    plt.title(\"Test Macro-F1 by Dropout - SPR_BENCH\")\n    plt.xlabel(\"Dropout\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating test-F1 bar: {e}\")\n    plt.close()\n\n# print best overall metric\nbo = experiment_data.get(\"best_overall\", {})\nif bo:\n    print(\n        f\"Best Dropout: {bo.get('dropout')} | Final Test Macro-F1: {bo.get('test_f1'):.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef is_float(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    spr_exp = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wd_keys = [k for k in spr_exp.keys() if is_float(k)]  # numeric keys only\n\n    # 1) Train / Val F1 curves\n    try:\n        plt.figure()\n        for k in wd_keys:\n            epochs = spr_exp[k][\"epochs\"]\n            plt.plot(\n                epochs, spr_exp[k][\"metrics\"][\"train_f1\"], \"--\", label=f\"train wd={k}\"\n            )\n            plt.plot(epochs, spr_exp[k][\"metrics\"][\"val_f1\"], \"-\", label=f\"val wd={k}\")\n        plt.title(\n            \"SPR_BENCH: Train vs Validation F1 across epochs\\n(hyper-parameter sweep)\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.legend(fontsize=8)\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # 2) Best Val F1 per weight decay\n    try:\n        best_vals = [max(spr_exp[k][\"metrics\"][\"val_f1\"]) for k in wd_keys]\n        plt.figure()\n        plt.bar(range(len(wd_keys)), best_vals, tick_label=wd_keys)\n        plt.title(\"SPR_BENCH: Best Validation F1 vs Weight Decay\")\n        plt.xlabel(\"Weight Decay\")\n        plt.ylabel(\"Best Val Macro F1\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_best_val_f1_bar.png\")\n        plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating best-val-F1 bar plot: {e}\")\n        plt.close()\n\n    # 3) Confusion matrix for best model\n    try:\n        preds = (\n            np.array(spr_exp[\"predictions\"])\n            if \"predictions\" in spr_exp\n            else np.array([])\n        )\n        gts = (\n            np.array(spr_exp[\"ground_truth\"])\n            if \"ground_truth\" in spr_exp\n            else np.array([])\n        )\n        if preds.size and gts.size:\n            n_labels = int(max(gts.max(), preds.max()) + 1)\n            conf = np.zeros((n_labels, n_labels), dtype=int)\n            for gt, pr in zip(gts, preds):\n                conf[gt, pr] += 1\n            plt.figure()\n            im = plt.imshow(conf, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(\n                \"SPR_BENCH: Confusion Matrix on Test Set\\n(Left axis: Ground Truth, Bottom axis: Predictions)\"\n            )\n            plt.xlabel(\"Predicted label\")\n            plt.ylabel(\"True label\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            print(\"Predictions / ground truth not found, skipping confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"d_model_tuning\", {}).get(\"SPR_BENCH\", {})\nif not spr_data:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\nd_models = sorted(spr_data.keys())\nepochs = spr_data[d_models[0]][\"epochs\"]\n\n# Gather metrics\ntrain_losses = {dm: spr_data[dm][\"losses\"][\"train\"] for dm in d_models}\nval_losses = {dm: spr_data[dm][\"losses\"][\"val\"] for dm in d_models}\ntrain_f1s = {dm: spr_data[dm][\"metrics\"][\"train\"] for dm in d_models}\nval_f1s = {dm: spr_data[dm][\"metrics\"][\"val\"] for dm in d_models}\nbest_val_f1 = {dm: spr_data[dm][\"best_val_f1\"] for dm in d_models}\ntest_f1 = {dm: spr_data[dm][\"test_f1\"] for dm in d_models}\n\n# -------- 1) Loss curves --------\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs, train_losses[dm], label=f\"train d_model={dm}\", linestyle=\"--\")\n        plt.plot(epochs, val_losses[dm], label=f\"val d_model={dm}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (Char Transformer)\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# -------- 2) F1 curves --------\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs, train_f1s[dm], label=f\"train d_model={dm}\", linestyle=\"--\")\n        plt.plot(epochs, val_f1s[dm], label=f\"val d_model={dm}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Char Transformer)\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# -------- 3) Bar chart: best val F1 --------\ntry:\n    plt.figure()\n    plt.bar(range(len(d_models)), [best_val_f1[dm] for dm in d_models])\n    plt.xticks(range(len(d_models)), d_models)\n    plt.ylabel(\"Best Validation Macro-F1\")\n    plt.title(\"SPR_BENCH Best Validation F1 vs d_model\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_best_val_f1.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating best val F1 plot: {e}\")\n    plt.close()\n\n# -------- 4) Bar chart: test F1 --------\ntry:\n    plt.figure()\n    plt.bar(range(len(d_models)), [test_f1[dm] for dm in d_models])\n    plt.xticks(range(len(d_models)), d_models)\n    plt.ylabel(\"Test Macro-F1\")\n    plt.title(\"SPR_BENCH Test F1 vs d_model\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test F1 plot: {e}\")\n    plt.close()\n\n# -------- 5) Confusion Matrix for best model --------\ntry:\n    best_dm = max(test_f1, key=test_f1.get)\n    preds = np.array(spr_data[best_dm][\"predictions\"])\n    gts = np.array(spr_data[best_dm][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar()\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(\n        f\"SPR_BENCH Confusion Matrix (d_model={best_dm})\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    plt.tight_layout()\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_dmodel_{best_dm}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# -------- print summary --------\nprint(\"\\n=== Summary of Results ===\")\nfor dm in d_models:\n    print(\n        f\"d_model {dm}:  best_val_F1={best_val_f1[dm]:.4f} | test_F1={test_f1[dm]:.4f}\"\n    )\nprint(f\"\\nBest test model: d_model={max(test_f1, key=test_f1.get)}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\nimport itertools\n\n# -------- setup --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# -------- helper --------\nnlayers_runs = experiment_data.get(\"nlayers\", {})\ntest_f1_scores = {}\nfor k, exp in nlayers_runs.items():\n    preds, gts = exp.get(\"predictions\", []), exp.get(\"ground_truth\", [])\n    if preds and gts:\n        test_f1_scores[k] = f1_score(gts, preds, average=\"macro\")\n\n# -------- plot 1: loss curves --------\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for k, exp in nlayers_runs.items():\n        epochs = exp[\"epochs\"]\n        axes[0].plot(epochs, exp[\"losses\"][\"train\"], label=k)\n        axes[1].plot(epochs, exp[\"losses\"][\"val\"], label=k)\n    axes[0].set_title(\"Train Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n    fig.suptitle(\"SPR_BENCH Loss Curves\\nLeft: Train Loss, Right: Validation Loss\")\n    fig.legend(fontsize=8)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# -------- plot 2: F1 curves --------\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for k, exp in nlayers_runs.items():\n        epochs = exp[\"epochs\"]\n        axes[0].plot(epochs, exp[\"metrics\"][\"train_f1\"], label=k)\n        axes[1].plot(epochs, exp[\"metrics\"][\"val_f1\"], label=k)\n    axes[0].set_title(\"Train Macro F1\")\n    axes[1].set_title(\"Validation Macro F1\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Macro F1\")\n    fig.suptitle(\"SPR_BENCH F1 Curves\\nLeft: Train F1, Right: Validation F1\")\n    fig.legend(fontsize=8)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# -------- plot 3: test F1 bar chart --------\ntry:\n    if test_f1_scores:\n        keys, vals = list(test_f1_scores.keys()), list(test_f1_scores.values())\n        plt.figure(figsize=(6, 4))\n        plt.bar(keys, vals, color=\"skyblue\")\n        plt.ylabel(\"Macro F1\")\n        plt.xlabel(\"nlayers setting\")\n        plt.title(\"SPR_BENCH Test F1 vs nlayers\")\n        plt.xticks(rotation=45, ha=\"right\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.005, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test F1 bar chart: {e}\")\n    plt.close()\n\n# -------- plot 4: confusion matrix of best model --------\ntry:\n    if test_f1_scores:\n        best_key = max(test_f1_scores, key=test_f1_scores.get)\n        best_exp = nlayers_runs[best_key]\n        cm = confusion_matrix(best_exp[\"ground_truth\"], best_exp[\"predictions\"])\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH Confusion Matrix ({best_key})\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=7\n            )\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_{best_key}.png\")\n        )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# -------- print metrics --------\nfor k, v in test_f1_scores.items():\n    print(f\"{k}: Test Macro F1 = {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\nexperiment_path_candidates = [\n    os.path.join(working_dir, \"experiment_data.npy\"),\n    os.path.join(os.getcwd(), \"experiment_data.npy\"),\n]\nexperiment_data = None\nfor p in experiment_path_candidates:\n    try:\n        experiment_data = np.load(p, allow_pickle=True).item()\n        break\n    except Exception:\n        pass\nif experiment_data is None:\n    raise FileNotFoundError(\"experiment_data.npy not found in expected locations.\")\n\nresults = experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][\"results\"]\n\n# ---------- per-nhead plots ----------\nfor nhead, res in results.items():\n    hist = res[\"history\"]\n    epochs = hist[\"epochs\"]\n    # Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, hist[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, hist[\"losses\"][\"val\"], label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\n            f\"SPR_BENCH Loss Curves (nhead={nhead})\\nLeft: Train, Right: Validation\"\n        )\n        plt.legend()\n        fname = f\"SPR_BENCH_nhead{nhead}_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss curves for nhead={nhead}: {e}\")\n        plt.close()\n    # F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, hist[\"metrics\"][\"train_f1\"], label=\"Train F1\")\n        plt.plot(epochs, hist[\"metrics\"][\"val_f1\"], label=\"Validation F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(\n            f\"SPR_BENCH F1 Curves (nhead={nhead})\\nLeft: Train, Right: Validation\"\n        )\n        plt.legend()\n        fname = f\"SPR_BENCH_nhead{nhead}_f1_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting F1 curves for nhead={nhead}: {e}\")\n        plt.close()\n\n# ---------- summary bar chart ----------\ntry:\n    plt.figure()\n    heads = sorted(results.keys())\n    test_f1s = [results[h][\"test_f1\"] for h in heads]\n    plt.bar([str(h) for h in heads], test_f1s, color=\"skyblue\")\n    plt.xlabel(\"nhead\")\n    plt.ylabel(\"Test Macro F1\")\n    plt.title(\"SPR_BENCH Test F1 by nhead\\nLeft: nhead value, Right: Macro F1\")\n    for i, v in enumerate(test_f1s):\n        plt.text(i, v + 0.005, f\"{v:.3f}\", ha=\"center\")\n    fname = \"SPR_BENCH_test_f1_summary.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting summary bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndropout_dict = experiment_data.get(\"dropout_tuning\", {})\ntags = list(dropout_dict.keys())[:5]  # plot at most first 5 dropouts\n\n\n# Helper to fetch arrays\ndef get_arr(tag, key1, key2):\n    return np.asarray(dropout_dict[tag][key1][key2])\n\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training loss\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"train\"), label=tag\n        )\n    plt.title(\"Left: Training Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation loss\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"val\"), label=tag)\n    plt.title(\"Right: Validation Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) F1 curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training F1\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"train_f1\"), label=tag\n        )\n    plt.title(\"Left: Training Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation F1\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"val_f1\"), label=tag\n        )\n    plt.title(\"Right: Validation Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Test-set F1 per dropout\ntry:\n    plt.figure()\n    test_f1s = [dropout_dict[tag][\"metrics\"][\"test_f1\"] for tag in tags]\n    plt.bar(range(len(tags)), test_f1s, tick_label=[t.split(\"_\")[-1] for t in tags])\n    best_overall = experiment_data.get(\"best_overall\", {})\n    best_f1 = best_overall.get(\"test_f1\", None)\n    if best_f1 is not None:\n        plt.axhline(\n            best_f1, color=\"r\", linestyle=\"--\", label=f\"Best Overall = {best_f1:.3f}\"\n        )\n    plt.title(\"Test Macro-F1 by Dropout - SPR_BENCH\")\n    plt.xlabel(\"Dropout\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating test-F1 bar: {e}\")\n    plt.close()\n\n# print best overall metric\nbo = experiment_data.get(\"best_overall\", {})\nif bo:\n    print(\n        f\"Best Dropout: {bo.get('dropout')} | Final Test Macro-F1: {bo.get('test_f1'):.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndropout_dict = experiment_data.get(\"dropout_tuning\", {})\ntags = list(dropout_dict.keys())[:5]  # plot at most first 5 dropouts\n\n\n# Helper to fetch arrays\ndef get_arr(tag, key1, key2):\n    return np.asarray(dropout_dict[tag][key1][key2])\n\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training loss\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"train\"), label=tag\n        )\n    plt.title(\"Left: Training Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation loss\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"val\"), label=tag)\n    plt.title(\"Right: Validation Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) F1 curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training F1\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"train_f1\"), label=tag\n        )\n    plt.title(\"Left: Training Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation F1\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"val_f1\"), label=tag\n        )\n    plt.title(\"Right: Validation Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Test-set F1 per dropout\ntry:\n    plt.figure()\n    test_f1s = [dropout_dict[tag][\"metrics\"][\"test_f1\"] for tag in tags]\n    plt.bar(range(len(tags)), test_f1s, tick_label=[t.split(\"_\")[-1] for t in tags])\n    best_overall = experiment_data.get(\"best_overall\", {})\n    best_f1 = best_overall.get(\"test_f1\", None)\n    if best_f1 is not None:\n        plt.axhline(\n            best_f1, color=\"r\", linestyle=\"--\", label=f\"Best Overall = {best_f1:.3f}\"\n        )\n    plt.title(\"Test Macro-F1 by Dropout - SPR_BENCH\")\n    plt.xlabel(\"Dropout\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating test-F1 bar: {e}\")\n    plt.close()\n\n# print best overall metric\nbo = experiment_data.get(\"best_overall\", {})\nif bo:\n    print(\n        f\"Best Dropout: {bo.get('dropout')} | Final Test Macro-F1: {bo.get('test_f1'):.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndropout_dict = experiment_data.get(\"dropout_tuning\", {})\ntags = list(dropout_dict.keys())[:5]  # plot at most first 5 dropouts\n\n\n# Helper to fetch arrays\ndef get_arr(tag, key1, key2):\n    return np.asarray(dropout_dict[tag][key1][key2])\n\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training loss\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"train\"), label=tag\n        )\n    plt.title(\"Left: Training Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation loss\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(dropout_dict[tag][\"epochs\"], get_arr(tag, \"losses\", \"val\"), label=tag)\n    plt.title(\"Right: Validation Loss - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) F1 curves\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left subplot: training F1\n    plt.subplot(1, 2, 1)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"train_f1\"), label=tag\n        )\n    plt.title(\"Left: Training Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    # Right subplot: validation F1\n    plt.subplot(1, 2, 2)\n    for tag in tags:\n        plt.plot(\n            dropout_dict[tag][\"epochs\"], get_arr(tag, \"metrics\", \"val_f1\"), label=tag\n        )\n    plt.title(\"Right: Validation Macro-F1 - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Test-set F1 per dropout\ntry:\n    plt.figure()\n    test_f1s = [dropout_dict[tag][\"metrics\"][\"test_f1\"] for tag in tags]\n    plt.bar(range(len(tags)), test_f1s, tick_label=[t.split(\"_\")[-1] for t in tags])\n    best_overall = experiment_data.get(\"best_overall\", {})\n    best_f1 = best_overall.get(\"test_f1\", None)\n    if best_f1 is not None:\n        plt.axhline(\n            best_f1, color=\"r\", linestyle=\"--\", label=f\"Best Overall = {best_f1:.3f}\"\n        )\n    plt.title(\"Test Macro-F1 by Dropout - SPR_BENCH\")\n    plt.xlabel(\"Dropout\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating test-F1 bar: {e}\")\n    plt.close()\n\n# print best overall metric\nbo = experiment_data.get(\"best_overall\", {})\nif bo:\n    print(\n        f\"Best Dropout: {bo.get('dropout')} | Final Test Macro-F1: {bo.get('test_f1'):.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------------------------------------------------------------\n# basic set-up\n# -------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------------------\n# load all experiment_data files that were supplied\n# -------------------------------------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eca2e524b5c34ba391a176baea24e0bf_proc_3160641/experiment_data.npy\",\n    \"experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_1195467d8fb944a799d6f096b897d8d1_proc_3160639/experiment_data.npy\",\n    \"experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_18f10014191c46559582eb4fe8ed013c_proc_3160638/experiment_data.npy\",\n]\nall_experiment_data = []\ntry:\n    for path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), path)\n        all_experiment_data.append(np.load(full_path, allow_pickle=True).item())\n    print(f\"Loaded {len(all_experiment_data)} experiment files\")\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n\n# -------------------------------------------------------------\n# Helper utilities\n# -------------------------------------------------------------\ndef align_and_stack(arr_list):\n    \"\"\"\n    Trim every 1-D array in arr_list to the minimum length, then stack.\n    Returns ndarray of shape [n_runs, min_len]\n    \"\"\"\n    if not arr_list:\n        return None\n    min_len = min(len(x) for x in arr_list)\n    arr_list = [np.asarray(x)[:min_len] for x in arr_list]\n    return np.stack(arr_list, axis=0)  # [runs, epochs]\n\n\ndef mean_sem(arr, axis=0):\n    \"\"\"\n    Given ndarray arr, returns mean and standard error along axis.\n    \"\"\"\n    if arr is None or arr.size == 0:\n        return None, None\n    mean = arr.mean(axis=axis)\n    sem = arr.std(axis=axis, ddof=1) / np.sqrt(arr.shape[axis])\n    return mean, sem\n\n\n# -------------------------------------------------------------\n# Aggregate data across runs\n# -------------------------------------------------------------\n# gather every dropout tag observed in any run\nall_dropout_tags = []\nfor exp in all_experiment_data:\n    all_dropout_tags.extend(list(exp.get(\"dropout_tuning\", {}).keys()))\nall_dropout_tags = sorted(list(set(all_dropout_tags)))[:5]  # keep at most five\n\naggregated = {}  # tag -> dict of aggregated arrays\nfor tag in all_dropout_tags:\n    train_loss_runs, val_loss_runs = [], []\n    train_f1_runs, val_f1_runs = [], []\n    test_f1_runs = []\n    for exp in all_experiment_data:\n        d = exp.get(\"dropout_tuning\", {}).get(tag, None)\n        if d is None:\n            continue\n        train_loss_runs.append(np.asarray(d[\"losses\"][\"train\"]))\n        val_loss_runs.append(np.asarray(d[\"losses\"][\"val\"]))\n        train_f1_runs.append(np.asarray(d[\"metrics\"][\"train_f1\"]))\n        val_f1_runs.append(np.asarray(d[\"metrics\"][\"val_f1\"]))\n        test_f1_runs.append(float(d[\"metrics\"][\"test_f1\"]))\n    aggregated[tag] = {\n        \"train_loss\": align_and_stack(train_loss_runs),\n        \"val_loss\": align_and_stack(val_loss_runs),\n        \"train_f1\": align_and_stack(train_f1_runs),\n        \"val_f1\": align_and_stack(val_f1_runs),\n        \"test_f1\": np.asarray(test_f1_runs) if test_f1_runs else None,\n    }\n\n# -------------------------------------------------------------\n# 1) aggregated loss curves (mean \u00b1 SEM)\n# -------------------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left \u2011 training loss\n    plt.subplot(1, 2, 1)\n    for tag in aggregated:\n        arr = aggregated[tag][\"train_loss\"]\n        if arr is None:\n            continue\n        mu, se = mean_sem(arr)\n        epochs = np.arange(len(mu))\n        plt.plot(epochs, mu, label=f\"{tag} mean\")\n        plt.fill_between(epochs, mu - se, mu + se, alpha=0.2)\n    plt.title(\"Left: Training Loss (mean \u00b1 SEM) - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n\n    # Right \u2011 validation loss\n    plt.subplot(1, 2, 2)\n    for tag in aggregated:\n        arr = aggregated[tag][\"val_loss\"]\n        if arr is None:\n            continue\n        mu, se = mean_sem(arr)\n        epochs = np.arange(len(mu))\n        plt.plot(epochs, mu, label=f\"{tag} mean\")\n        plt.fill_between(epochs, mu - se, mu + se, alpha=0.2)\n    plt.title(\"Right: Validation Loss (mean \u00b1 SEM) - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(fontsize=6)\n\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_mean_sem.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curves: {e}\")\n    plt.close()\n\n# -------------------------------------------------------------\n# 2) aggregated F1 curves (mean \u00b1 SEM)\n# -------------------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left \u2013 training F1\n    plt.subplot(1, 2, 1)\n    for tag in aggregated:\n        arr = aggregated[tag][\"train_f1\"]\n        if arr is None:\n            continue\n        mu, se = mean_sem(arr)\n        epochs = np.arange(len(mu))\n        plt.plot(epochs, mu, label=f\"{tag} mean\")\n        plt.fill_between(epochs, mu - se, mu + se, alpha=0.2)\n    plt.title(\"Left: Training Macro-F1 (mean \u00b1 SEM) - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n\n    # Right \u2013 validation F1\n    plt.subplot(1, 2, 2)\n    for tag in aggregated:\n        arr = aggregated[tag][\"val_f1\"]\n        if arr is None:\n            continue\n        mu, se = mean_sem(arr)\n        epochs = np.arange(len(mu))\n        plt.plot(epochs, mu, label=f\"{tag} mean\")\n        plt.fill_between(epochs, mu - se, mu + se, alpha=0.2)\n    plt.title(\"Right: Validation Macro-F1 (mean \u00b1 SEM) - SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend(fontsize=6)\n\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves_mean_sem.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated F1 curves: {e}\")\n    plt.close()\n\n# -------------------------------------------------------------\n# 3) Test-set Macro-F1 bar plot (mean \u00b1 SEM)\n# -------------------------------------------------------------\ntry:\n    plt.figure()\n    tags = list(aggregated.keys())\n    means = []\n    sems = []\n    xtick_lbls = []\n    for tag in tags:\n        tf1 = aggregated[tag][\"test_f1\"]\n        if tf1 is None or tf1.size == 0:\n            means.append(np.nan)\n            sems.append(0)\n        else:\n            means.append(tf1.mean())\n            sems.append(tf1.std(ddof=1) / np.sqrt(len(tf1)))\n        xtick_lbls.append(tag.split(\"_\")[-1])  # show only dropout value\n    x = np.arange(len(tags))\n    plt.bar(x, means, yerr=sems, capsize=4)\n    plt.title(\"Test Macro-F1 by Dropout (mean \u00b1 SEM) - SPR_BENCH\")\n    plt.xlabel(\"Dropout\")\n    plt.ylabel(\"Macro-F1\")\n    plt.xticks(x, xtick_lbls)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar_mean_sem.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated test-F1 bar: {e}\")\n    plt.close()\n\n# -------------------------------------------------------------\n# Print aggregated scalar results to console\n# -------------------------------------------------------------\nprint(\"Aggregated Test Macro-F1 (mean \u00b1 SEM):\")\nfor tag in aggregated:\n    tf1 = aggregated[tag][\"test_f1\"]\n    if tf1 is None or tf1.size == 0:\n        continue\n    mu = tf1.mean()\n    se = tf1.std(ddof=1) / np.sqrt(len(tf1))\n    print(f\"  {tag}: {mu:.4f} \u00b1 {se:.4f}\")\n"], "plot_plan": ["We first load experiment_data.npy, focus on the \u201cSPR_BENCH\u201d entry, and unpack the lists of epoch-wise curves, the list of tried max_epochs, and the final test-F1 scores.  \nFor the learning dynamics we create two line plots: one for loss (train vs. validation) and one for macro-F1 (train vs. validation).  Because each hyper-parameter run may stop early, each curve is plotted against its own epoch index.  To keep the figure count low we draw all runs in the same axes but differentiate them with labels that include the run\u2019s max_epochs.  \nA third plot shows the final test macro-F1 obtained from every run as a function of the max_epochs grid; this gives an at-a-glance view of sweep performance.  \nEach figure is wrapped in an individual try\u2013except block, is given a clear title and subtitle, saved in working_dir with descriptive filenames, and then closed.  \nAll plotting follows the guidelines: imports at the top, basic matplotlib, no fabricated data, and at most three figures total, staying well under the five-figure limit.  \nFinally, the script prints each metric array so the user can quickly inspect the numeric values after running.", "We load experiment_data.npy, extract the SPR_BENCH learning-rate sweep results, and precompute per-LR arrays: epochs, train/val losses, train/val F1, best val F1, and test F1 (re-computed from stored predictions and the common ground-truth). Four figures are created in independent try-except blocks: (1) combined train/val loss curves across epochs for every LR, (2) combined train/val F1 curves, (3) bar chart of best validation F1 versus LR, and (4) bar chart of test F1 versus LR. Each figure is titled clearly, saved to working_dir with descriptive filenames, and then closed to free resources. To keep plots readable we draw a single multi-line panel per metric instead of one plot per LR, ensuring we remain under the 5-figure limit. The code prints a small table of LR, best-val F1 and test F1 so users can quickly inspect the key metrics in the console. All plotting uses basic matplotlib with no extra styles and only data that exist in the .npy file. Every step is wrapped in minimal error handling so that failures in one plot don\u2019t block the others.", "We first load the saved experiment_data, gather the per-batch-size histories under SPR_BENCH, and collect epochs, losses, and F1 scores. We create a working directory and, within separate try-except blocks, generate three plots: (1) train vs. validation loss curves for all batch sizes, (2) train vs. validation macro-F1 curves, and (3) a bar chart of best validation and test F1 for each batch size (limited to \u22645 bars). Each plot is titled clearly, saved with descriptive filenames, and the figure is closed regardless of errors. Finally, we print a small summary table of the best validation and test F1 scores so the notebook log provides immediate feedback even without inspecting the images.", "We first load the saved experiment_data.npy and extract the \u201cdropout_tuning\u201d dictionary.  For every dropout value we read the per-epoch arrays of training/validation loss and F1 stored during the sweep.  We then create three concise visualizations: (1) a combined loss-curve figure with two subplots (left = training, right = validation) where each line corresponds to one dropout; (2) an analogous combined F1-curve figure; and (3) a bar chart of the final test-set macro-F1 achieved by each dropout together with a dashed red line showing the best overall value.  Each figure is generated inside its own try-except block, saved to working_dir with descriptive names that include \u201cSPR_BENCH\u201d and the plot type, and the figure is always closed afterward.  We additionally print the best dropout and its test macro-F1 so the user sees the key metric in the console.  All plotting code uses plain matplotlib, only accesses data present in experiment_data.npy, and limits the number of figures to three as required.  No artificial data is generated; everything comes directly from the stored arrays.  The code is compact and self-contained so it can be run as-is inside the prescribed environment.", "We load the saved experiment_data.npy, isolate the SPR_BENCH records and identify which keys correspond to individual weight-decay runs. For every run we extract the stored train/val F1 lists and their epoch indices, then plot all curves in a single figure so users can compare learning dynamics across hyper-parameters. We next compute the best validation F1 obtained in each run (max over epochs) and visualise these values with a bar plot, providing a quick summary of the sweep. Finally, using the stored test predictions and ground-truth labels of the best model, we build a confusion matrix (via simple numpy counting) and render it with a heat-map. Each figure is wrapped in its own try-except block, titled clearly, saved in the working directory with descriptive file names, and closed to prevent memory leaks. No synthetic data are generated\u2014everything comes from experiment_data.npy. The script starts with the mandated imports, creates working_dir if needed, and prints basic status messages. Only three figures are produced, comfortably under the five-figure limit, while satisfying the requirement to include training/validation curves.", "We will load experiment_data.npy, focus on the \u2018d_model_tuning\u2019 results for the SPR_BENCH dataset, and extract epochs, train/val losses, and train/val macro-F1 values. Five concise visualizations will be generated: (1) a single figure with train/val loss curves for every d_model, (2) a similar figure for macro-F1 curves, (3) a bar chart of best validation F1 per d_model, (4) a bar chart of test F1 per d_model, and (5) a confusion-matrix heat-map on the test set for the best-performing d_model.  Each plot is wrapped in its own try-except block, titled clearly, saved to working_dir with descriptive names, and the figure is always closed afterwards.  We pick at most five plots to respect the interval guideline (loss/F1 curves aggregate all settings, avoiding per-epoch multi-figure spam).  After plotting we print a small summary table of best_val_F1 and test_F1 for all d_model values, plus the identity of the best model.  All code runs with basic matplotlib, uses only values present in experiment_data.npy, and adheres to the required imports and saving rules.", "We load the saved experiment_data.npy, iterate through each nlayers run, and collect epoch-wise train/validation losses, F1 scores, and final test predictions. Four figures are produced (\u22645 as required): (1) a two-panel plot of train and validation loss across epochs for every nlayers setting; (2) a two-panel plot of train and validation macro-F1; (3) a bar chart comparing the best test-set F1 obtained by each nlayers configuration; and (4) a confusion-matrix heat-map for the configuration with the highest test F1. Every figure is wrapped in its own try-except block, saved with descriptive filenames inside working_dir, and properly closed to free memory. All plots explicitly mention the SPR_BENCH dataset and include clear subtitles such as \u201cLeft: Train Loss, Right: Validation Loss\u201d. Only data already present in experiment_data.npy are visualised, and when computing test F1 we reuse the stored predictions/ground-truth without re-running the model. The script starts with the mandated imports, creates working_dir if missing, prints the computed test F1 scores for quick inspection, and otherwise relies only on numpy, matplotlib, and sklearn (for F1/confusion matrix). Executing the code will therefore generate self-contained, standard visualisations of the experiment results.", "We will load the serialized experiment_data dictionary, iterate through the three \u201fnhead\u201d settings and extract the stored training history. For every head value we create two line plots: (1) training vs. validation loss and (2) training vs. validation macro-F1 across epochs. We then produce a bar chart that summarizes the final test-set macro-F1 for each model. All plots are wrapped in individual try-except blocks, saved with descriptive file names inside working_dir, and the figures are always closed. No artificial data are generated and at most four figures are created, satisfying the plotting constraints.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["num_epochs", "learning_rate", "batch_size", "dropout", "weight_decay", "d_model", "nlayers", "nhead", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["Below is a concise script that loads the saved NumPy results file, identifies\nthe hyper-parameter run with the highest validation F1 score, and prints the\nfinal/best metrics for that run. It follows the directory conventions and\nprinting rules specified.", "The script will locate the numpy file inside the working directory, load it as a\nPython dict, and iterate through every dataset stored under the \u201clearning_rate\u201d\nkey.   For each dataset and every learning-rate run, it will retrieve (i) the\nlast training loss and F1 score, (ii) the last validation loss, (iii) the best\nvalidation F1 score already stored, and (iv) compute the test F1 score from\nsaved predictions and ground-truth labels.   Metric names are printed explicitly\nbefore their values, and the dataset name precedes its block of metrics.   The\ncode is executed at global scope so the information is displayed immediately\nwhen the script is run.", "", "The script will load the saved numpy file from the working directory, iterate\nover each dropout-specific sub-experiment (and the overall best run), and print\nthe requested \u201cbest or final\u201d statistics with explicit metric names.  For every\ndropout setting it reports the maximum train/validation F1 scores, the last\nrecorded train/validation losses, and the stored test loss/F1 score; for the\noverall best model it shows the chosen dropout and its test F1 score.", "We load the experiment_data.npy file from the \u201cworking\u201d directory, locate the\nsection corresponding to the SPR_BENCH dataset, and identify the weight-decay\nrun that achieved the best validation F1 score (stored under \u201cbest_wd\u201d). From\nthat run we report the final training macro-F1 score, the best validation\nmacro-F1 score, the final validation loss, and (from the top level) the held-out\ntest macro-F1 score. Each metric is clearly labeled when printed, and the script\nexecutes immediately without any special entry point.", "The script will load the serialized `experiment_data.npy` file from the\n`working` directory, traverse the nested dictionary produced by the original\ntraining script, and for each dataset (currently only \u201cSPR_BENCH\u201d) it will\niterate over every `d_model` setting that was evaluated.   For every setting it\nprints the most relevant end-of-run numbers: the final training F1 score, the\nbest validation F1 score (stored explicitly), the test F1 score, the final\ntraining loss, and the final validation loss.   All metric names are spelled out\nclearly and the code executes immediately on import/run, with no special entry\npoint required.", "The script will locate the \u201cworking\u201d directory, load the saved numpy dictionary,\nand iterate over every stored sub-experiment (e.g., \u201cSPR_BENCH_n2\u201d,\n\u201cSPR_BENCH_n4\u201d, \u2026).   For each sub-experiment, it will derive the final training\nloss/F1, the final validation loss, the best validation F1 score, and compute\nthe test F1 score directly from the saved predictions and ground-truth labels.\nThe results are printed with clear dataset and metric names, complying with all\nstructural constraints\u2014no plots, no `if __name__ == \"__main__\":`, and the code\nruns immediately on execution.", "", "The script will load the saved numpy file from the working directory, iterate\nover each dropout-specific sub-experiment (and the overall best run), and print\nthe requested \u201cbest or final\u201d statistics with explicit metric names.  For every\ndropout setting it reports the maximum train/validation F1 scores, the last\nrecorded train/validation losses, and the stored test loss/F1 score; for the\noverall best model it shows the chosen dropout and its test F1 score.", "The script will load the saved numpy file from the working directory, iterate\nover each dropout-specific sub-experiment (and the overall best run), and print\nthe requested \u201cbest or final\u201d statistics with explicit metric names.  For every\ndropout setting it reports the maximum train/validation F1 scores, the last\nrecorded train/validation losses, and the stored test loss/F1 score; for the\noverall best model it shows the chosen dropout and its test F1 score.", "The script will load the saved numpy file from the working directory, iterate\nover each dropout-specific sub-experiment (and the overall best run), and print\nthe requested \u201cbest or final\u201d statistics with explicit metric names.  For every\ndropout setting it reports the maximum train/validation F1 scores, the last\nrecorded train/validation losses, and the stored test loss/F1 score; for the\noverall best model it shows the chosen dropout and its test F1 score.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------- locate and load results ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------- helper: pick best run ----------------\nbench_key = \"SPR_BENCH\"\ndata = experiment_data[\"num_epochs\"][bench_key]\n\n# index of run with highest validation F1\nbest_run_idx = int(np.argmax(data[\"best_val_f1\"]))\n\n# fetch curves for that run\ncurves = {k: v[best_run_idx] for k, v in data[\"epoch_curves\"].items()}\nbest_val_f1 = data[\"best_val_f1\"][best_run_idx]\ntest_f1 = data[\"test_f1\"][best_run_idx]\n\n# final epoch values (last element in each curve)\nfinal_train_loss = curves[\"train_loss\"][-1]\nfinal_val_loss = curves[\"val_loss\"][-1]\nfinal_train_f1 = curves[\"train_f1\"][-1]\nfinal_val_f1 = curves[\"val_f1\"][-1]\n\n# ---------------- printing ----------------\nprint(\"Dataset: SPR_BENCH\")\nprint(f\"Final training loss: {final_train_loss:.4f}\")\nprint(f\"Final validation loss: {final_val_loss:.4f}\")\nprint(f\"Final training F1 score: {final_train_f1:.4f}\")\nprint(f\"Final validation F1 score: {final_val_f1:.4f}\")\nprint(f\"Best validation F1 score: {best_val_f1:.4f}\")\nprint(f\"Test F1 score (from best checkpoint): {test_f1:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ------------------------------------------------------------------\n# locate and load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment file at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# iterate over datasets inside the learning-rate sweep\n# ------------------------------------------------------------------\nsweep_key = \"learning_rate\"\nfor dataset_name, dset_dict in experiment_data.get(sweep_key, {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # unpack shared arrays\n    ground_truth = np.array(dset_dict.get(\"ground_truth\", []))\n\n    for idx, lr in enumerate(dset_dict.get(\"lrs\", [])):\n        lr_str = f\"{lr:.0e}\" if isinstance(lr, (float, int)) else str(lr)\n        print(f\"\\n  Learning rate: {lr_str}\")\n\n        # ------- training metrics (final epoch) -------\n        final_train_loss = dset_dict[\"losses\"][\"train\"][idx][-1]\n        final_train_f1 = dset_dict[\"metrics\"][\"train_f1\"][idx][-1]\n        print(f\"    training loss (final): {final_train_loss:.4f}\")\n        print(f\"    training F1 score (final): {final_train_f1:.4f}\")\n\n        # ------- validation metrics -------\n        final_val_loss = dset_dict[\"losses\"][\"val\"][idx][-1]\n        best_val_f1 = dset_dict[\"best_val_f1\"][idx]\n        print(f\"    validation loss (final): {final_val_loss:.4f}\")\n        print(f\"    validation F1 score (best): {best_val_f1:.4f}\")\n\n        # ------- test metrics -------\n        preds = np.array(dset_dict[\"predictions\"][idx])\n        if preds.size and ground_truth.size:\n            test_f1 = f1_score(ground_truth, preds, average=\"macro\")\n            print(f\"    test F1 score: {test_f1:.4f}\")\n        else:\n            print(\"    test F1 score: N/A (predictions or ground truth missing)\")\n", "", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper for pretty printing ----------\ndef print_metric(name: str, value: float):\n    \"\"\"Print the metric name followed by its value formatted to 4 decimals.\"\"\"\n    print(f\"{name}: {value:.4f}\")\n\n\n# ---------- iterate over each dropout run ----------\nfor tag, ed in experiment_data.get(\"dropout_tuning\", {}).items():\n    print(f\"\\nDataset: {tag}\")\n\n    # best / final values\n    best_train_f1 = (\n        max(ed[\"metrics\"][\"train_f1\"]) if ed[\"metrics\"][\"train_f1\"] else float(\"nan\")\n    )\n    best_val_f1 = (\n        max(ed[\"metrics\"][\"val_f1\"]) if ed[\"metrics\"][\"val_f1\"] else float(\"nan\")\n    )\n    final_train_loss = (\n        ed[\"losses\"][\"train\"][-1] if ed[\"losses\"][\"train\"] else float(\"nan\")\n    )\n    final_val_loss = ed[\"losses\"][\"val\"][-1] if ed[\"losses\"][\"val\"] else float(\"nan\")\n    test_loss = ed[\"losses\"].get(\"test\", float(\"nan\"))\n    test_f1 = ed[\"metrics\"].get(\"test_f1\", float(\"nan\"))\n\n    # print metrics\n    print_metric(\"Best train F1 score\", best_train_f1)\n    print_metric(\"Best validation F1 score\", best_val_f1)\n    print_metric(\"Final train loss\", final_train_loss)\n    print_metric(\"Final validation loss\", final_val_loss)\n    print_metric(\"Test loss\", test_loss)\n    print_metric(\"Test F1 score\", test_f1)\n\n# ---------- print overall best model ----------\nbest_overall = experiment_data.get(\"best_overall\", {})\nif best_overall:\n    print(f\"\\nDataset: best_overall\")\n    print_metric(\"Best model test F1 score\", best_overall.get(\"test_f1\", float(\"nan\")))\n    dropout_used = best_overall.get(\"dropout\", None)\n    if dropout_used is not None:\n        print(f\"Chosen dropout value: {dropout_used}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate and load the saved experiment dictionary\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# extract information for each dataset\n# -------------------------------------------------\nfor dataset_name, dataset_dict in experiment_data[\"weight_decay\"].items():\n    # identify the best weight-decay setting chosen during the sweep\n    best_wd = dataset_dict.get(\"best_wd\")\n    best_run = dataset_dict[str(best_wd)]\n\n    # gather required metrics\n    final_train_f1 = best_run[\"metrics\"][\"train_f1\"][-1]  # final epoch\n    best_val_f1 = max(best_run[\"metrics\"][\"val_f1\"])  # best across epochs\n    final_val_loss = best_run[\"losses\"][\"val\"][-1]  # final epoch\n    test_f1 = dataset_dict.get(\"test_f1\")  # held-out test\n\n    # -------------------------------------------------\n    # print results with clear labels\n    # -------------------------------------------------\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Best weight decay: {best_wd}\")\n    print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(f\"Test macro F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment results ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate over sweeps / datasets / hyper-params ----------\nfor sweep_name, sweep_content in experiment_data.items():  # e.g. \"d_model_tuning\"\n    for dataset_name, dataset_results in sweep_content.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\n{dataset_name}\")  # dataset header\n        for hyper_param, run_data in dataset_results.items():  # e.g. 96, 128, ...\n            epochs = run_data[\"epochs\"]\n            # Retrieve metric lists\n            train_f1_history = run_data[\"metrics\"][\"train\"]\n            val_f1_history = run_data[\"metrics\"][\"val\"]\n            train_loss_hist = run_data[\"losses\"][\"train\"]\n            val_loss_hist = run_data[\"losses\"][\"val\"]\n\n            # Compute required values\n            final_train_f1 = train_f1_history[-1] if train_f1_history else None\n            best_val_f1 = run_data.get(\n                \"best_val_f1\", val_f1_history[-1] if val_f1_history else None\n            )\n            test_f1 = run_data.get(\"test_f1\", None)\n            final_train_loss = train_loss_hist[-1] if train_loss_hist else None\n            final_val_loss = val_loss_hist[-1] if val_loss_hist else None\n\n            # Print metrics for this hyper-parameter setting\n            print(f\"  d_model = {hyper_param}\")\n            if final_train_f1 is not None:\n                print(f\"    train F1 score: {final_train_f1:.4f}\")\n            if best_val_f1 is not None:\n                print(f\"    validation F1 score: {best_val_f1:.4f}\")\n            if test_f1 is not None:\n                print(f\"    test F1 score: {test_f1:.4f}\")\n            if final_train_loss is not None:\n                print(f\"    train loss: {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"    validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# -------------------------------------------------\n# locate and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# iterate through each stored dataset / setting\n# -------------------------------------------------\nfor dataset_name, record in experiment_data.get(\"nlayers\", {}).items():\n    metrics = record.get(\"metrics\", {})\n    losses = record.get(\"losses\", {})\n    preds = record.get(\"predictions\", [])\n    gts = record.get(\"ground_truth\", [])\n\n    # derive required values\n    train_loss_final = losses.get(\"train\", [None])[-1]\n    val_loss_final = losses.get(\"val\", [None])[-1]\n    train_f1_final = metrics.get(\"train_f1\", [None])[-1]\n    val_f1_best = max(metrics.get(\"val_f1\", [0.0]))\n    test_f1 = f1_score(gts, preds, average=\"macro\") if preds and gts else None\n\n    # print results\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"final training loss: {train_loss_final}\")\n    print(f\"final validation loss: {val_loss_final}\")\n    print(f\"final training F1 score: {train_f1_final}\")\n    print(f\"best validation F1 score: {val_f1_best}\")\n    print(f\"test F1 score: {test_f1}\")\n", "", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper for pretty printing ----------\ndef print_metric(name: str, value: float):\n    \"\"\"Print the metric name followed by its value formatted to 4 decimals.\"\"\"\n    print(f\"{name}: {value:.4f}\")\n\n\n# ---------- iterate over each dropout run ----------\nfor tag, ed in experiment_data.get(\"dropout_tuning\", {}).items():\n    print(f\"\\nDataset: {tag}\")\n\n    # best / final values\n    best_train_f1 = (\n        max(ed[\"metrics\"][\"train_f1\"]) if ed[\"metrics\"][\"train_f1\"] else float(\"nan\")\n    )\n    best_val_f1 = (\n        max(ed[\"metrics\"][\"val_f1\"]) if ed[\"metrics\"][\"val_f1\"] else float(\"nan\")\n    )\n    final_train_loss = (\n        ed[\"losses\"][\"train\"][-1] if ed[\"losses\"][\"train\"] else float(\"nan\")\n    )\n    final_val_loss = ed[\"losses\"][\"val\"][-1] if ed[\"losses\"][\"val\"] else float(\"nan\")\n    test_loss = ed[\"losses\"].get(\"test\", float(\"nan\"))\n    test_f1 = ed[\"metrics\"].get(\"test_f1\", float(\"nan\"))\n\n    # print metrics\n    print_metric(\"Best train F1 score\", best_train_f1)\n    print_metric(\"Best validation F1 score\", best_val_f1)\n    print_metric(\"Final train loss\", final_train_loss)\n    print_metric(\"Final validation loss\", final_val_loss)\n    print_metric(\"Test loss\", test_loss)\n    print_metric(\"Test F1 score\", test_f1)\n\n# ---------- print overall best model ----------\nbest_overall = experiment_data.get(\"best_overall\", {})\nif best_overall:\n    print(f\"\\nDataset: best_overall\")\n    print_metric(\"Best model test F1 score\", best_overall.get(\"test_f1\", float(\"nan\")))\n    dropout_used = best_overall.get(\"dropout\", None)\n    if dropout_used is not None:\n        print(f\"Chosen dropout value: {dropout_used}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper for pretty printing ----------\ndef print_metric(name: str, value: float):\n    \"\"\"Print the metric name followed by its value formatted to 4 decimals.\"\"\"\n    print(f\"{name}: {value:.4f}\")\n\n\n# ---------- iterate over each dropout run ----------\nfor tag, ed in experiment_data.get(\"dropout_tuning\", {}).items():\n    print(f\"\\nDataset: {tag}\")\n\n    # best / final values\n    best_train_f1 = (\n        max(ed[\"metrics\"][\"train_f1\"]) if ed[\"metrics\"][\"train_f1\"] else float(\"nan\")\n    )\n    best_val_f1 = (\n        max(ed[\"metrics\"][\"val_f1\"]) if ed[\"metrics\"][\"val_f1\"] else float(\"nan\")\n    )\n    final_train_loss = (\n        ed[\"losses\"][\"train\"][-1] if ed[\"losses\"][\"train\"] else float(\"nan\")\n    )\n    final_val_loss = ed[\"losses\"][\"val\"][-1] if ed[\"losses\"][\"val\"] else float(\"nan\")\n    test_loss = ed[\"losses\"].get(\"test\", float(\"nan\"))\n    test_f1 = ed[\"metrics\"].get(\"test_f1\", float(\"nan\"))\n\n    # print metrics\n    print_metric(\"Best train F1 score\", best_train_f1)\n    print_metric(\"Best validation F1 score\", best_val_f1)\n    print_metric(\"Final train loss\", final_train_loss)\n    print_metric(\"Final validation loss\", final_val_loss)\n    print_metric(\"Test loss\", test_loss)\n    print_metric(\"Test F1 score\", test_f1)\n\n# ---------- print overall best model ----------\nbest_overall = experiment_data.get(\"best_overall\", {})\nif best_overall:\n    print(f\"\\nDataset: best_overall\")\n    print_metric(\"Best model test F1 score\", best_overall.get(\"test_f1\", float(\"nan\")))\n    dropout_used = best_overall.get(\"dropout\", None)\n    if dropout_used is not None:\n        print(f\"Chosen dropout value: {dropout_used}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper for pretty printing ----------\ndef print_metric(name: str, value: float):\n    \"\"\"Print the metric name followed by its value formatted to 4 decimals.\"\"\"\n    print(f\"{name}: {value:.4f}\")\n\n\n# ---------- iterate over each dropout run ----------\nfor tag, ed in experiment_data.get(\"dropout_tuning\", {}).items():\n    print(f\"\\nDataset: {tag}\")\n\n    # best / final values\n    best_train_f1 = (\n        max(ed[\"metrics\"][\"train_f1\"]) if ed[\"metrics\"][\"train_f1\"] else float(\"nan\")\n    )\n    best_val_f1 = (\n        max(ed[\"metrics\"][\"val_f1\"]) if ed[\"metrics\"][\"val_f1\"] else float(\"nan\")\n    )\n    final_train_loss = (\n        ed[\"losses\"][\"train\"][-1] if ed[\"losses\"][\"train\"] else float(\"nan\")\n    )\n    final_val_loss = ed[\"losses\"][\"val\"][-1] if ed[\"losses\"][\"val\"] else float(\"nan\")\n    test_loss = ed[\"losses\"].get(\"test\", float(\"nan\"))\n    test_f1 = ed[\"metrics\"].get(\"test_f1\", float(\"nan\"))\n\n    # print metrics\n    print_metric(\"Best train F1 score\", best_train_f1)\n    print_metric(\"Best validation F1 score\", best_val_f1)\n    print_metric(\"Final train loss\", final_train_loss)\n    print_metric(\"Final validation loss\", final_val_loss)\n    print_metric(\"Test loss\", test_loss)\n    print_metric(\"Test F1 score\", test_f1)\n\n# ---------- print overall best model ----------\nbest_overall = experiment_data.get(\"best_overall\", {})\nif best_overall:\n    print(f\"\\nDataset: best_overall\")\n    print_metric(\"Best model test F1 score\", best_overall.get(\"test_f1\", float(\"nan\")))\n    dropout_used = best_overall.get(\"dropout\", None)\n    if dropout_used is not None:\n        print(f\"Chosen dropout value: {dropout_used}\")\n", ""], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', 'Final training loss: 0.5119', '\\n', 'Final\nvalidation loss: 0.5201', '\\n', 'Final training F1 score: 0.7950', '\\n', 'Final\nvalidation F1 score: 0.7940', '\\n', 'Best validation F1 score: 0.7959', '\\n',\n'Test F1 score (from best checkpoint): 0.7930', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '\\n  Learning rate: 1e-05', '\\n', '    training\nloss (final): 0.5868', '\\n', '    training F1 score (final): 0.7612', '\\n', '\nvalidation loss (final): 0.5850', '\\n', '    validation F1 score (best):\n0.7497', '\\n', '    test F1 score: 0.7715', '\\n', '\\n  Learning rate: 3e-05',\n'\\n', '    training loss (final): 0.5237', '\\n', '    training F1 score (final):\n0.7879', '\\n', '    validation loss (final): 0.5337', '\\n', '    validation F1\nscore (best): 0.7880', '\\n', '    test F1 score: 0.7899', '\\n', '\\n  Learning\nrate: 1e-04', '\\n', '    training loss (final): 0.5226', '\\n', '    training F1\nscore (final): 0.7935', '\\n', '    validation loss (final): 0.5375', '\\n', '\nvalidation F1 score (best): 0.7920', '\\n', '    test F1 score: 0.7889', '\\n',\n'\\n  Learning rate: 3e-04', '\\n', '    training loss (final): 0.5268', '\\n', '\ntraining F1 score (final): 0.7889', '\\n', '    validation loss (final): 0.5402',\n'\\n', '    validation F1 score (best): 0.7900', '\\n', '    test F1 score:\n0.7808', '\\n', '\\n  Learning rate: 1e-03', '\\n', '    training loss (final):\n0.5184', '\\n', '    training F1 score (final): 0.7879', '\\n', '    validation\nloss (final): 0.5243', '\\n', '    validation F1 score (best): 0.7959', '\\n', '\ntest F1 score: 0.7869', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "", "['\\nDataset: SPR_BENCH_dropout_0.0', '\\n', 'Best train F1 score: 0.7960', '\\n',\n'Best validation F1 score: 0.7959', '\\n', 'Final train loss: 0.5090', '\\n',\n'Final validation loss: 0.5128', '\\n', 'Test loss: 0.5099', '\\n', 'Test F1\nscore: 0.7940', '\\n', '\\nDataset: SPR_BENCH_dropout_0.05', '\\n', 'Best train F1\nscore: 0.7965', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train\nloss: 0.5132', '\\n', 'Final validation loss: 0.5107', '\\n', 'Test loss: 0.5092',\n'\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.1', '\\n',\n'Best train F1 score: 0.7950', '\\n', 'Best validation F1 score: 0.7959', '\\n',\n'Final train loss: 0.5100', '\\n', 'Final validation loss: 0.5143', '\\n', 'Test\nloss: 0.5284', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset:\nSPR_BENCH_dropout_0.2', '\\n', 'Best train F1 score: 0.7975', '\\n', 'Best\nvalidation F1 score: 0.7959', '\\n', 'Final train loss: 0.5183', '\\n', 'Final\nvalidation loss: 0.5257', '\\n', 'Test loss: 0.5420', '\\n', 'Test F1 score:\n0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.3', '\\n', 'Best train F1 score:\n0.7965', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train loss:\n0.5109', '\\n', 'Final validation loss: 0.5228', '\\n', 'Test loss: 0.5244', '\\n',\n'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.4', '\\n', 'Best\ntrain F1 score: 0.7960', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final\ntrain loss: 0.5114', '\\n', 'Final validation loss: 0.5125', '\\n', 'Test loss:\n0.5194', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: best_overall', '\\n',\n'Best model test F1 score: 0.7940', '\\n', 'Chosen dropout value: 0.0', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best weight decay: 0.0', '\\n', 'Final training\nmacro F1 score: 0.7950', '\\n', 'Best validation macro F1 score: 0.7959', '\\n',\n'Final validation loss: 0.5201', '\\n', 'Test macro F1 score: 0.7950', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', '  d_model = 96', '\\n', '    train F1 score: 0.7930',\n'\\n', '    validation F1 score: 0.7959', '\\n', '    test F1 score: 0.7889',\n'\\n', '    train loss: 0.5143', '\\n', '    validation loss: 0.5379', '\\n', '\nd_model = 128', '\\n', '    train F1 score: 0.7965', '\\n', '    validation F1\nscore: 0.7959', '\\n', '    test F1 score: 0.7920', '\\n', '    train loss:\n0.5116', '\\n', '    validation loss: 0.5198', '\\n', '  d_model = 192', '\\n', '\ntrain F1 score: 0.7950', '\\n', '    validation F1 score: 0.7959', '\\n', '\ntest F1 score: 0.7950', '\\n', '    train loss: 0.5096', '\\n', '    validation\nloss: 0.5131', '\\n', '  d_model = 256', '\\n', '    train F1 score: 0.7950',\n'\\n', '    validation F1 score: 0.7959', '\\n', '    test F1 score: 0.7889',\n'\\n', '    train loss: 0.5155', '\\n', '    validation loss: 0.5222', '\\n', '\nd_model = 384', '\\n', '    train F1 score: 0.7930', '\\n', '    validation F1\nscore: 0.7959', '\\n', '    test F1 score: 0.7889', '\\n', '    train loss:\n0.5156', '\\n', '    validation loss: 0.5244', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH_n2', '\\n', 'final training loss: 0.5201653161048889',\n'\\n', 'final validation loss: 0.5267321410179138', '\\n', 'final training F1\nscore: 0.7909322620529051', '\\n', 'best validation F1 score:\n0.7959477626272327', '\\n', 'test F1 score: 0.7949899545077709', '\\n',\n'\\nDataset: SPR_BENCH_n4', '\\n', 'final training loss: 0.5215714364051819',\n'\\n', 'final validation loss: 0.5420828981399536', '\\n', 'final training F1\nscore: 0.7909322620529051', '\\n', 'best validation F1 score:\n0.7959477626272327', '\\n', 'test F1 score: 0.7949899545077709', '\\n',\n'\\nDataset: SPR_BENCH_n6', '\\n', 'final training loss: 0.5067998228073121',\n'\\n', 'final validation loss: 0.5222784585952759', '\\n', 'final training F1\nscore: 0.7959869431643625', '\\n', 'best validation F1 score:\n0.7959477626272327', '\\n', 'test F1 score: 0.7949899545077709', '\\n',\n'\\nDataset: SPR_BENCH_n8', '\\n', 'final training loss: 0.5074102683067322',\n'\\n', 'final validation loss: 0.5124282159805298', '\\n', 'final training F1\nscore: 0.794983393654886', '\\n', 'best validation F1 score: 0.7959477626272327',\n'\\n', 'test F1 score: 0.7949899545077709', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "", "['\\nDataset: SPR_BENCH_dropout_0.0', '\\n', 'Best train F1 score: 0.7960', '\\n',\n'Best validation F1 score: 0.7959', '\\n', 'Final train loss: 0.5095', '\\n',\n'Final validation loss: 0.5369', '\\n', 'Test loss: 0.5401', '\\n', 'Test F1\nscore: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.05', '\\n', 'Best train F1\nscore: 0.7955', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train\nloss: 0.5112', '\\n', 'Final validation loss: 0.5116', '\\n', 'Test loss: 0.5108',\n'\\n', 'Test F1 score: 0.7940', '\\n', '\\nDataset: SPR_BENCH_dropout_0.1', '\\n',\n'Best train F1 score: 0.7950', '\\n', 'Best validation F1 score: 0.7959', '\\n',\n'Final train loss: 0.5219', '\\n', 'Final validation loss: 0.5263', '\\n', 'Test\nloss: 0.5335', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset:\nSPR_BENCH_dropout_0.2', '\\n', 'Best train F1 score: 0.7980', '\\n', 'Best\nvalidation F1 score: 0.7959', '\\n', 'Final train loss: 0.5095', '\\n', 'Final\nvalidation loss: 0.5090', '\\n', 'Test loss: 0.5243', '\\n', 'Test F1 score:\n0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.3', '\\n', 'Best train F1 score:\n0.7970', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train loss:\n0.5098', '\\n', 'Final validation loss: 0.5159', '\\n', 'Test loss: 0.5640', '\\n',\n'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.4', '\\n', 'Best\ntrain F1 score: 0.7955', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final\ntrain loss: 0.5133', '\\n', 'Final validation loss: 0.5761', '\\n', 'Test loss:\n0.5702', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: best_overall', '\\n',\n'Best model test F1 score: 0.7950', '\\n', 'Chosen dropout value: 0.0', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH_dropout_0.0', '\\n', 'Best train F1 score: 0.7965', '\\n',\n'Best validation F1 score: 0.7959', '\\n', 'Final train loss: 0.5134', '\\n',\n'Final validation loss: 0.5267', '\\n', 'Test loss: 0.5155', '\\n', 'Test F1\nscore: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.05', '\\n', 'Best train F1\nscore: 0.7960', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train\nloss: 0.5173', '\\n', 'Final validation loss: 0.5180', '\\n', 'Test loss: 0.5100',\n'\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.1', '\\n',\n'Best train F1 score: 0.7945', '\\n', 'Best validation F1 score: 0.7959', '\\n',\n'Final train loss: 0.5137', '\\n', 'Final validation loss: 0.5392', '\\n', 'Test\nloss: 0.5233', '\\n', 'Test F1 score: 0.7960', '\\n', '\\nDataset:\nSPR_BENCH_dropout_0.2', '\\n', 'Best train F1 score: 0.7950', '\\n', 'Best\nvalidation F1 score: 0.7959', '\\n', 'Final train loss: 0.5144', '\\n', 'Final\nvalidation loss: 0.5208', '\\n', 'Test loss: 0.5175', '\\n', 'Test F1 score:\n0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.3', '\\n', 'Best train F1 score:\n0.7945', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train loss:\n0.5139', '\\n', 'Final validation loss: 0.5153', '\\n', 'Test loss: 0.5604', '\\n',\n'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.4', '\\n', 'Best\ntrain F1 score: 0.7950', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final\ntrain loss: 0.5075', '\\n', 'Final validation loss: 0.5239', '\\n', 'Test loss:\n0.5442', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: best_overall', '\\n',\n'Best model test F1 score: 0.7950', '\\n', 'Chosen dropout value: 0.0', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH_dropout_0.0', '\\n', 'Best train F1 score: 0.7960', '\\n',\n'Best validation F1 score: 0.7959', '\\n', 'Final train loss: 0.5094', '\\n',\n'Final validation loss: 0.5152', '\\n', 'Test loss: 0.5188', '\\n', 'Test F1\nscore: 0.7930', '\\n', '\\nDataset: SPR_BENCH_dropout_0.05', '\\n', 'Best train F1\nscore: 0.7935', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train\nloss: 0.5118', '\\n', 'Final validation loss: 0.5228', '\\n', 'Test loss: 0.5304',\n'\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.1', '\\n',\n'Best train F1 score: 0.7960', '\\n', 'Best validation F1 score: 0.7959', '\\n',\n'Final train loss: 0.5110', '\\n', 'Final validation loss: 0.5165', '\\n', 'Test\nloss: 0.5222', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset:\nSPR_BENCH_dropout_0.2', '\\n', 'Best train F1 score: 0.7965', '\\n', 'Best\nvalidation F1 score: 0.7959', '\\n', 'Final train loss: 0.5076', '\\n', 'Final\nvalidation loss: 0.5155', '\\n', 'Test loss: 0.5199', '\\n', 'Test F1 score:\n0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.3', '\\n', 'Best train F1 score:\n0.7939', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final train loss:\n0.5238', '\\n', 'Final validation loss: 0.5427', '\\n', 'Test loss: 0.5338', '\\n',\n'Test F1 score: 0.7950', '\\n', '\\nDataset: SPR_BENCH_dropout_0.4', '\\n', 'Best\ntrain F1 score: 0.7955', '\\n', 'Best validation F1 score: 0.7959', '\\n', 'Final\ntrain loss: 0.5227', '\\n', 'Final validation loss: 0.5101', '\\n', 'Test loss:\n0.5561', '\\n', 'Test F1 score: 0.7950', '\\n', '\\nDataset: best_overall', '\\n',\n'Best model test F1 score: 0.7930', '\\n', 'Chosen dropout value: 0.0', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}