<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 5], [0, 6], [0, 7], [0, 8], [0, 3], [0, 2], [0, 1], [0, 4]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0], [1.0, 1.0]], "plan": ["We extend the previous char-plus-bigram transformer by injecting an explicit\n\u201ccount pathway\u201d that lets the model see global symbol-frequency information, a\ncue known to be useful for shape-count and parity rules.  For every sequence we\nbuild a histogram over the character vocabulary and feed it through a small MLP\nto obtain a dense vector.  At inference we average-pool the transformer\u2019s\ncontextual token states, concatenate this pooled vector with the count-pathway\nvector, and pass the result to the classifier.  This light architectural change\nkeeps training cost almost identical while giving the network direct access to\nglobal statistics that the self-attention mechanism often struggles to compute\nexactly.  We reuse the same char/bigram vocab and dataloaders, simply adding the\ncount vector to each batch.  Training is run with early stopping on dev Macro-F1\nand all required metrics are stored in `experiment_data.npy` for later analysis.\nThe code below is self-contained and should finish within 30 minutes on a single\nGPU.", "Ablation name: No-Count-Vector Pathway (CBC \u2192 CB Transformer). Below is a\nstreamlined re-implementation that ablates the count-vector pathway.  The model\nnow relies solely on the sequential character + bigram embeddings processed by\nthe Transformer; the count branch is excised and the classifier consumes only\nthe pooled Transformer output.  All bookkeeping, training, early-stopping, and\nresult saving follow the earlier baseline so results can be compared directly.", "Ablation name: No-Bigram-Embedding Pathway (C + C Transformer). The ablation\nremoves the bigram-embedding pathway, so token representations use only\ncharacter and positional embeddings while all other components remain identical.\nWe train this Char + Count model on SPR-BENCH, track losses/F1, and save\neverything to experiment_data.npy for later comparison with the full baseline.", "Ablation name: No-Positional-Embedding (CBC-w/o-Pos). The solution reuses the\nbaseline data-processing, vocabulary building, and training logic, but\nintroduces a new CBCTransformerNoPos model that omits the learnable\npositional\u2013embedding term when composing token embeddings. All other pathways\n(character, bigram, and count-vector) and the training/evaluation loop remain\nunchanged, allowing us to isolate the impact of positional information. Results\nare stored under the ablation key \u201cno_positional_embedding\u201d and saved to\nexperiment_data.npy for later plotting/analysis.", "Ablation name: No-Char-Embedding Pathway (Bigram + Count Transformer). Below is\nthe ablation implementation: the character\u2013embedding stream is removed, leaving\nonly the bigram + positional embeddings and the count-vector pathway. All\nbookkeeping, training, early-stopping, and result saving follow the baseline.\nThe model\u2019s forward still receives char_ids for compatibility but ignores them,\nthereby isolating the effect of removing single-character information.", "Ablation name: Count-Only (Bag-of-Characters) Model. The ablation replaces the\nfull char-bigram Transformer with a bag-of-characters classifier: raw character-\nfrequency vectors are fed through the original count _proj MLP followed by one\noutput layer. All sequence-aware components (char/bigram/positional embeddings\nand the Transformer encoder) are removed, so any performance drop directly\nreflects the value of order-sensitive information. The script below faithfully\nreproduces data loading/encoding, trains the count-only model with early\nstopping, evaluates on the SPR-BENCH splits, and stores losses, F1 scores,\npredictions, and gold labels in the required `experiment_data.npy`.", "Ablation name: Bigram-Only Transformer (B-Only). We remove the character-\nembedding and count-vector channels, retaining only bigram embeddings that are\nposition-encoded and passed through the Transformer encoder. Everything else\n(data loading, vocabulary creation, batching, early stopping, metric logging,\nsaving) stays identical, so we can fairly compare this \u201cB-Only\u201d ablation to the\nCBC baseline. The script below builds the bigram-only model, trains with early-\nstopping on dev F1, evaluates on the test split, and stores all results in the\nprescribed experiment_data.npy file.", "Ablation name: No-Transformer-Context (Mean-Pool Embedding + Count). The\nablation replaces the Transformer encoder with simple mean pooling: the per-\ntoken representation is the element-wise sum of character, bigram and positional\nembeddings; these token vectors are averaged over the true sequence length\n(masking pads).  The pooled token vector is concatenated with the projected\ncount-vector and fed to the same final classifier.  Everything else (data\nloading, training loop, early stopping, metrics storage) is identical to the\nbaseline, and results are stored under the key \"no_transformer_context\".  Below\nis a complete, runnable script.", "Ablation name: CBC-CLS (CLS-Token Pooling instead of Mean Pool). We add a\ndedicated learnable <CLS> token that is prepended to every sequence. Char- and\nbigram-vocabularies are extended accordingly, positional index 0 is reserved for\nthe CLS embedding and the Transformer\u2019s output at position 0 substitutes the\nformer length-normalised mean pool. The rest of the CBC architecture (count-\nvector branch, training loop, I/O, saving) is kept unchanged, so the ablation\nisolates the pooling strategy. Below is the complete, executable script\nimplementing this CLS-token ablation and logging the results."], "code": ["import os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------\n# working dir & device\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------\n# experiment data dict\n# -------------------------------------------------------\nexperiment_data = {\n    \"char_bigram_count\": {\n        \"epochs\": [],\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n        \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------\n# data loading helper exactly as given in baseline\n# -------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# -------------------------------------------------------\n# build vocabularies\n# -------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = char_vocab[\"<PAD>\"], char_vocab[\"<UNK>\"], char_vocab[\"<SOS>\"]\n\nbigram_vocab = {tok: idx for idx, tok in enumerate([\"<PAD>\", \"<UNK>\"])}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\n\nprint(f\"Char vocab size {len(char_vocab)}, Bigram vocab size {len(bigram_vocab)}\")\n\n\n# -------------------------------------------------------\n# encode samples\n# -------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids, bigram_ids = [], []\n    prev = \"<SOS>\"\n    for ch in seq:\n        char_ids.append(char_vocab.get(ch, unk_id))\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    # count vector\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for idx in char_ids:\n        counts[idx] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# -------------------------------------------------------\n# collate\n# -------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"char_ids\"]) for b in batch)\n    B = len(batch)\n    char_tensor = torch.full((B, max_len), pad_id, dtype=torch.long)\n    bigram_tensor = torch.full_like(char_tensor, bigram_vocab[\"<PAD>\"])\n    attn_mask = torch.zeros_like(char_tensor, dtype=torch.bool)\n    counts_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"char_ids\"])\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"], dtype=torch.long)\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n    return {\n        \"char_ids\": char_tensor,\n        \"bigram_ids\": bigram_tensor,\n        \"attention_mask\": attn_mask,\n        \"count_vec\": counts_tensor,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# -------------------------------------------------------\n# model\n# -------------------------------------------------------\nclass CBCTransformer(nn.Module):\n    \"\"\"\n    Char + Bigram + Count pathway Transformer\n    \"\"\"\n\n    def __init__(\n        self,\n        char_vocab,\n        bigram_vocab,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_feedforward=512,\n        dropout=0.2,\n        max_len=512,\n    ):\n        super().__init__()\n        self.char_emb = nn.Embedding(len(char_vocab), d_model, padding_idx=pad_id)\n        self.bigram_emb = nn.Embedding(\n            len(bigram_vocab), d_model, padding_idx=bigram_vocab[\"<PAD>\"]\n        )\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n\n        # count pathway\n        self.count_proj = nn.Sequential(\n            nn.Linear(len(char_vocab), d_model), nn.ReLU(), nn.Dropout(dropout)\n        )\n        # classifier on concatenated representation\n        self.classifier = nn.Linear(d_model * 2, num_labels)\n\n    def forward(self, char_ids, bigram_ids, attention_mask, count_vec):\n        L = char_ids.shape[1]\n        tok_emb = (\n            self.char_emb(char_ids) + self.bigram_emb(bigram_ids) + self.pos_emb[:L]\n        )\n        enc_out = self.encoder(tok_emb, src_key_padding_mask=~attention_mask)\n        pooled = (enc_out * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        count_rep = self.count_proj(count_vec)\n        repr_cat = torch.cat([pooled, count_rep], dim=-1)\n        return self.classifier(repr_cat)\n\n\nmodel = CBCTransformer(char_vocab, bigram_vocab, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# -------------------------------------------------------\n# helpers\n# -------------------------------------------------------\ndef run_epoch(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"count_vec\"],\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------\n# training loop with early stopping\n# -------------------------------------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"cbc_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    tic = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_F1 = {val_f1:.4f}\")\n    ed = experiment_data[\"char_bigram_count\"]\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), save_path)\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  epoch time {time.time()-tic:.1f}s  best_val_F1={best_val_f1:.4f}\")\n\n# -------------------------------------------------------\n# test evaluation\n# -------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro F1: {test_f1:.4f}\")\ned = experiment_data[\"char_bigram_count\"]\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# -------------------------------------------------------\n# save experiment data\n# -------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------------- directories / device ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# --------------------------- experiment store --------------------------------\nexperiment_data = {\n    \"char_bigram_only\": {  # ablation key\n        \"SPR_BENCH\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ---------------------------- data loading -----------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# ----------------------------- vocab build -----------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {t: i for i, t in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = char_vocab[\"<PAD>\"], char_vocab[\"<UNK>\"], char_vocab[\"<SOS>\"]\n\nbigram_vocab = {t: i for i, t in enumerate([\"<PAD>\", \"<UNK>\"])}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\nprint(f\"Char vocab {len(char_vocab)}, Bigram vocab {len(bigram_vocab)}\")\n\n\n# ----------------------------- encoding --------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids, bigram_ids = [], []\n    prev = \"<SOS>\"\n    for ch in seq:\n        char_ids.append(char_vocab.get(ch, unk_id))\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    # count vector retained for compatibility although unused in ablation\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for idx in char_ids:\n        counts[idx] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# ----------------------------- collate ---------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"char_ids\"]) for b in batch)\n    B = len(batch)\n    char_tensor = torch.full((B, max_len), pad_id, dtype=torch.long)\n    bigram_tensor = torch.full_like(char_tensor, bigram_vocab[\"<PAD>\"])\n    attn_mask = torch.zeros_like(char_tensor, dtype=torch.bool)\n    counts_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"char_ids\"])\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"], dtype=torch.long)\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n    return {\n        \"char_ids\": char_tensor,\n        \"bigram_ids\": bigram_tensor,\n        \"attention_mask\": attn_mask,\n        \"count_vec\": counts_tensor,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        num_workers=2,\n        pin_memory=True,\n        collate_fn=collate,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ------------------------------ Model ----------------------------------------\nclass CBTransformer(nn.Module):\n    \"\"\"\n    Character + Bigram Transformer WITHOUT count-vector branch.\n    \"\"\"\n\n    def __init__(\n        self,\n        char_vocab,\n        bigram_vocab,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_feedforward=512,\n        dropout=0.2,\n        max_len=512,\n    ):\n        super().__init__()\n        self.char_emb = nn.Embedding(len(char_vocab), d_model, padding_idx=pad_id)\n        self.bigram_emb = nn.Embedding(\n            len(bigram_vocab), d_model, padding_idx=bigram_vocab[\"<PAD>\"]\n        )\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, char_ids, bigram_ids, attention_mask, count_vec=None):\n        L = char_ids.size(1)\n        tok_emb = (\n            self.char_emb(char_ids) + self.bigram_emb(bigram_ids) + self.pos_emb[:L]\n        )\n        enc_out = self.encoder(tok_emb, src_key_padding_mask=~attention_mask)\n        pooled = (enc_out * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\nmodel = CBTransformer(char_vocab, bigram_vocab, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# ------------------------------ helpers --------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"count_vec\"],\n            )  # ignored\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# --------------------------- train / early stop ------------------------------\nbest_val, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"cb_only_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"])\n    print(f\"Epoch {epoch}: val_loss {val_loss:.4f}  val_F1 {val_f1:.4f}\")\n    ed = experiment_data[\"char_bigram_only\"][\"SPR_BENCH\"]\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n    if val_f1 > best_val:\n        best_val, wait = val_f1, 0\n        torch.save(model.state_dict(), save_path)\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping triggered.\")\n            break\n    print(f\"  epoch time {(time.time()-t0):.1f}s  best_val_F1={best_val:.4f}\")\n\n# ------------------------------ test -----------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, preds, gts = run_epoch(loaders[\"test\"])\nprint(f\"Test Macro F1: {test_f1:.4f}\")\ned = experiment_data[\"char_bigram_only\"][\"SPR_BENCH\"]\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = preds\ned[\"ground_truth\"] = gts\n\n# ------------------------------ save -----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Results saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "\"\"\"\nNo-Bigram-Embedding Pathway (C + C Transformer) \u2013 ablation study.\nTrains a character-only transformer with a parallel count-vector pathway on SPR-BENCH.\nAll plottable data are stored in experiment_data.npy under key\n'no_bigram_char_count' \u2192 'spr_bench'.\n\"\"\"\n\nimport os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------\n# experiment data dict (follow required format)\n# -------------------------------------------------------\nexperiment_data = {\n    \"no_bigram_char_count\": {\n        \"spr_bench\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\ned = experiment_data[\"no_bigram_char_count\"][\"spr_bench\"]\n\n# -------------------------------------------------------\n# working dir & device\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------\n# data loading helper (same as baseline)\n# -------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# -------------------------------------------------------\n# build vocabularies (identical to baseline)\n# -------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = char_vocab[\"<PAD>\"], char_vocab[\"<UNK>\"], char_vocab[\"<SOS>\"]\n\nbigram_vocab = {tok: idx for idx, tok in enumerate([\"<PAD>\", \"<UNK>\"])}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\nprint(f\"Char vocab size {len(char_vocab)}, Bigram vocab size {len(bigram_vocab)}\")\n\n\n# -------------------------------------------------------\n# encode samples\n# -------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids, bigram_ids = [], []\n    prev = \"<SOS>\"\n    for ch in seq:\n        char_ids.append(char_vocab.get(ch, unk_id))\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for idx in char_ids:\n        counts[idx] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# -------------------------------------------------------\n# collate fn\n# -------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"char_ids\"]) for b in batch)\n    B = len(batch)\n    char_tensor = torch.full((B, max_len), pad_id, dtype=torch.long)\n    bigram_tensor = torch.full_like(char_tensor, bigram_vocab[\"<PAD>\"])\n    attn_mask = torch.zeros_like(char_tensor, dtype=torch.bool)\n    counts_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"char_ids\"])\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"], dtype=torch.long)\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n    return {\n        \"char_ids\": char_tensor,\n        \"bigram_ids\": bigram_tensor,  # kept for compatibility, ignored by model\n        \"attention_mask\": attn_mask,\n        \"count_vec\": counts_tensor,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# -------------------------------------------------------\n# model (Char + Count, NO bigram embedding)\n# -------------------------------------------------------\nclass CCTransformer(nn.Module):\n    \"\"\"\n    Character-only token embedding + count-vector pathway transformer.\n    \"\"\"\n\n    def __init__(\n        self,\n        char_vocab,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_feedforward=512,\n        dropout=0.2,\n        max_len=512,\n    ):\n        super().__init__()\n        self.char_emb = nn.Embedding(len(char_vocab), d_model, padding_idx=pad_id)\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        # count pathway\n        self.count_proj = nn.Sequential(\n            nn.Linear(len(char_vocab), d_model), nn.ReLU(), nn.Dropout(dropout)\n        )\n        self.classifier = nn.Linear(d_model * 2, num_labels)\n\n    def forward(self, char_ids, bigram_ids, attention_mask, count_vec):\n        # bigram_ids is ignored by design\n        L = char_ids.shape[1]\n        tok_emb = self.char_emb(char_ids) + self.pos_emb[:L]\n        enc_out = self.encoder(tok_emb, src_key_padding_mask=~attention_mask)\n        pooled = (enc_out * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        count_rep = self.count_proj(count_vec)\n        features = torch.cat([pooled, count_rep], dim=-1)\n        return self.classifier(features)\n\n\nmodel = CCTransformer(char_vocab, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# -------------------------------------------------------\n# helpers\n# -------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"count_vec\"],\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------\n# training loop with early stopping\n# -------------------------------------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"cc_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    tic = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, val_F1={val_f1:.4f}\")\n    # record\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n    # early stopping\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), save_path)\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  epoch time {time.time()-tic:.1f}s  best_val_F1={best_val_f1:.4f}\")\n\n# -------------------------------------------------------\n# test evaluation\n# -------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro F1: {test_f1:.4f}\")\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# -------------------------------------------------------\n# save experiment data\n# -------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "# -------------------------------------------------------\n# CBC-w/o-Positional-Embedding ablation\n# -------------------------------------------------------\nimport os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------\n# experiment bookkeeping dict (follow required schema)\n# -------------------------------------------------------\nexperiment_data = {\n    \"no_positional_embedding\": {\n        \"spr_bench\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\ned = experiment_data[\"no_positional_embedding\"][\"spr_bench\"]\n\n# -------------------------------------------------------\n# working dir & device\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------\n# data loader identical to baseline\n# -------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# -------------------------------------------------------\n# vocab build (char & bigram)\n# -------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = char_vocab[\"<PAD>\"], char_vocab[\"<UNK>\"], char_vocab[\"<SOS>\"]\n\nbigram_vocab = {tok: idx for idx, tok in enumerate([\"<PAD>\", \"<UNK>\"])}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\n\nprint(f\"Char vocab {len(char_vocab)}, Bigram vocab {len(bigram_vocab)}\")\n\n\n# -------------------------------------------------------\n# encode samples\n# -------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids, bigram_ids = [], []\n    prev = \"<SOS>\"\n    for ch in seq:\n        char_ids.append(char_vocab.get(ch, unk_id))\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for idx in char_ids:\n        counts[idx] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# -------------------------------------------------------\n# collate fn\n# -------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"char_ids\"]) for b in batch)\n    B = len(batch)\n    char_tensor = torch.full((B, max_len), pad_id, dtype=torch.long)\n    bigram_tensor = torch.full_like(char_tensor, bigram_vocab[\"<PAD>\"])\n    attn_mask = torch.zeros_like(char_tensor, dtype=torch.bool)\n    counts_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"char_ids\"])\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"], dtype=torch.long)\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n    return {\n        \"char_ids\": char_tensor,\n        \"bigram_ids\": bigram_tensor,\n        \"attention_mask\": attn_mask,\n        \"count_vec\": counts_tensor,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# -------------------------------------------------------\n# CBC model WITHOUT positional embeddings\n# -------------------------------------------------------\nclass CBCTransformerNoPos(nn.Module):\n    def __init__(\n        self,\n        char_vocab,\n        bigram_vocab,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_feedforward=512,\n        dropout=0.2,\n    ):\n        super().__init__()\n        self.char_emb = nn.Embedding(len(char_vocab), d_model, padding_idx=pad_id)\n        self.bigram_emb = nn.Embedding(\n            len(bigram_vocab), d_model, padding_idx=bigram_vocab[\"<PAD>\"]\n        )\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n\n        # count pathway\n        self.count_proj = nn.Sequential(\n            nn.Linear(len(char_vocab), d_model), nn.ReLU(), nn.Dropout(dropout)\n        )\n\n        self.classifier = nn.Linear(d_model * 2, num_labels)\n\n    def forward(self, char_ids, bigram_ids, attention_mask, count_vec):\n        tok_emb = self.char_emb(char_ids) + self.bigram_emb(bigram_ids)  # NO pos emb\n        enc_out = self.encoder(tok_emb, src_key_padding_mask=~attention_mask)\n        # mean-pool over valid positions\n        pooled = (enc_out * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        count_rep = self.count_proj(count_vec)\n        return self.classifier(torch.cat([pooled, count_rep], dim=-1))\n\n\nmodel = CBCTransformerNoPos(char_vocab, bigram_vocab, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# -------------------------------------------------------\n# helper: run one epoch\n# -------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"count_vec\"],\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------\n# training loop with early stopping\n# -------------------------------------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"cbc_no_pos_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    tic = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}  val_F1={val_f1:.4f}\")\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), save_path)\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  epoch time {time.time()-tic:.1f}s   best_val_F1={best_val_f1:.4f}\")\n\n# -------------------------------------------------------\n# test evaluation\n# -------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro F1: {test_f1:.4f}\")\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# -------------------------------------------------------\n# save experiment data\n# -------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "\"\"\"\nNo-Char-Embedding Pathway (Bigram + Count Transformer) ablation.\nA self-contained script \u2013 run directly after putting the SPR-BENCH csv\nfiles into DATA_PATH.\n\"\"\"\n\nimport os, pathlib, time, numpy as np, torch, warnings\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# ------------------------------------------------------------------\n# experiment bookkeeping dict\n# ------------------------------------------------------------------\nexperiment_data = {\n    \"no_char_bigram_count\": {\n        \"spr_bench\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------\n# device & working dir\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------------------------------------------------------\n# load SPR-BENCH\n# ------------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\n# set this to your dataset location\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------------\n# vocabularies\n# ------------------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = [char_vocab[tok] for tok in [\"<PAD>\", \"<UNK>\", \"<SOS>\"]]\n\nbigram_vocab = {tok: idx for idx, tok in enumerate([\"<PAD>\", \"<UNK>\"])}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\nprint(f\"Char vocab: {len(char_vocab)},  Bigram vocab: {len(bigram_vocab)}\")\n\n\n# ------------------------------------------------------------------\n# encoding\n# ------------------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids, bigram_ids = [], []\n    prev = \"<SOS>\"\n    for ch in seq:\n        cid = char_vocab.get(ch, unk_id)\n        char_ids.append(cid)\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for cid in char_ids:\n        counts[cid] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# ------------------------------------------------------------------\n# collate fn\n# ------------------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"bigram_ids\"]) for b in batch)\n    B = len(batch)\n    bigram_tensor = torch.full((B, max_len), bigram_vocab[\"<PAD>\"], dtype=torch.long)\n    attn_mask = torch.zeros_like(bigram_tensor, dtype=torch.bool)\n    char_tensor = torch.full_like(bigram_tensor, pad_id)  # kept for count path\n    counts_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"bigram_ids\"])\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"], dtype=torch.long)\n    return {\n        \"char_ids\": char_tensor,  # only for count construction; ignored by encoder\n        \"bigram_ids\": bigram_tensor,\n        \"attention_mask\": attn_mask,\n        \"count_vec\": counts_tensor,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ------------------------------------------------------------------\n# Model WITHOUT char-embedding stream\n# ------------------------------------------------------------------\nclass BigramCountTransformer(nn.Module):\n    \"\"\"\n    Bigram + Positional embedding through Transformer Encoder\n    plus count-vector pathway (no character embeddings).\n    \"\"\"\n\n    def __init__(\n        self,\n        bigram_vocab_sz,\n        char_vocab_sz,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_feedforward=512,\n        dropout=0.2,\n        max_len=512,\n    ):\n        super().__init__()\n        self.bigram_emb = nn.Embedding(bigram_vocab_sz, d_model, padding_idx=0)\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n\n        # count vector projection\n        self.count_proj = nn.Sequential(\n            nn.Linear(char_vocab_sz, d_model), nn.ReLU(), nn.Dropout(dropout)\n        )\n        self.classifier = nn.Linear(d_model * 2, num_labels)\n\n    def forward(self, char_ids, bigram_ids, attention_mask, count_vec):\n        \"\"\"\n        char_ids is ignored (kept for API compatibility).\n        \"\"\"\n        L = bigram_ids.size(1)\n        tok_emb = self.bigram_emb(bigram_ids) + self.pos_emb[:L]\n        enc_out = self.encoder(tok_emb, src_key_padding_mask=~attention_mask)\n        pooled = (enc_out * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        count_rep = self.count_proj(count_vec)\n        feats = torch.cat([pooled, count_rep], dim=-1)\n        return self.classifier(feats)\n\n\nmodel = BigramCountTransformer(\n    bigram_vocab_sz=len(bigram_vocab),\n    char_vocab_sz=len(char_vocab),\n    num_labels=num_labels,\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# ------------------------------------------------------------------\n# train / eval helpers\n# ------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"count_vec\"],\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, dim=1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ------------------------------------------------------------------\n# training loop with early stopping\n# ------------------------------------------------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"no_char_bigram_count_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    tic = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(\n        f\"Epoch {epoch}: train_loss {tr_loss:.4f}  val_loss {val_loss:.4f}  val_F1 {val_f1:.4f}\"\n    )\n\n    ed = experiment_data[\"no_char_bigram_count\"][\"spr_bench\"]\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), save_path)\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  epoch time {(time.time()-tic):.1f}s  best_val_F1 {best_val_f1:.4f}\")\n\n# ------------------------------------------------------------------\n# test evaluation\n# ------------------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro-F1: {test_f1:.4f}\")\n\ned = experiment_data[\"no_char_bigram_count\"][\"spr_bench\"]\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# ------------------------------------------------------------------\n# save experiment data\n# ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------- dirs / device ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------------------------- experiment dict --------------------------\nexperiment_data = {\n    \"count_only\": {\n        \"spr_bench\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\ned = experiment_data[\"count_only\"][\"spr_bench\"]\n\n\n# ---------------------------- dataset ----------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# ---------------------------- vocab & encode ---------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = char_vocab[\"<PAD>\"], char_vocab[\"<UNK>\"], char_vocab[\"<SOS>\"]\n\n\ndef encode(example):\n    seq = example[\"sequence\"]\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for ch in seq:\n        counts[char_vocab.get(ch, unk_id)] += 1\n    return {\"count_vec\": counts.tolist()}\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# ---------------------------- collate ----------------------------------\ndef collate(batch):\n    count_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return {\"count_vec\": count_tensor, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------------------------- model ------------------------------------\nclass CountOnlyClassifier(nn.Module):\n    def __init__(self, vocab_size, num_labels, d_model=256, dropout=0.2):\n        super().__init__()\n        self.count_proj = nn.Sequential(\n            nn.Linear(vocab_size, d_model),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        )\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, count_vec):\n        rep = self.count_proj(count_vec)\n        return self.classifier(rep)\n\n\nmodel = CountOnlyClassifier(len(char_vocab), num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# ---------------------------- train / eval -----------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        count_vec = batch[\"count_vec\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        with torch.set_grad_enabled(train):\n            logits = model(count_vec)\n            loss = criterion(logits, labels)\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        tot_loss += loss.item() * labels.size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(labels.cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, f1, preds, gts\n\n\n# ---------------------------- training loop ----------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"count_only_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    tic = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f} val_F1={val_f1:.4f}\")\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), save_path)\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  time {time.time()-tic:.1f}s  best_val_F1={best_val_f1:.4f}\")\n\n# ---------------------------- testing ----------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro F1: {test_f1:.4f}\")\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# ---------------------------- save data --------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Bigram-Only Transformer (B-Only) ablation\nimport os, pathlib, time, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# reproducibility\n# ------------------------------------------------------------------\nseed = 37\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# ------------------------------------------------------------------\n# experiment bookkeeping ------------------------------------------------\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"bigram_only\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n        \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# ------------------------------------------------------------------\n# data -----------------------------------------------------------------\n# ------------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------------\n# vocab -----------------------------------------------------------------\n# ------------------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: i for i, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id = [char_vocab[tok] for tok in [\"<PAD>\", \"<UNK>\", \"<SOS>\"]]\n\nbigram_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\nprint(f\"Char vocab {len(char_vocab)} | Bigram vocab {len(bigram_vocab)}\")\n\n\n# ------------------------------------------------------------------\n# encode ----------------------------------------------------------------\n# ------------------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    bigram_ids = []\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    return {\"bigram_ids\": bigram_ids}\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# ------------------------------------------------------------------\n# collate --------------------------------------------------------------\n# ------------------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"bigram_ids\"]) for b in batch)\n    B = len(batch)\n    bigram_t = torch.full((B, max_len), bigram_vocab[\"<PAD>\"], dtype=torch.long)\n    attn_mask = torch.zeros_like(bigram_t, dtype=torch.bool)\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"bigram_ids\"])\n        bigram_t[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n    return {\"bigram_ids\": bigram_t, \"attention_mask\": attn_mask, \"labels\": labels}\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=torch.cuda.is_available(),\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ------------------------------------------------------------------\n# model ----------------------------------------------------------------\n# ------------------------------------------------------------------\nclass BigramOnlyTransformer(nn.Module):\n    def __init__(\n        self,\n        bigram_vocab_size,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_ff=512,\n        dropout=0.2,\n        max_len=512,\n    ):\n        super().__init__()\n        self.bigram_emb = nn.Embedding(\n            bigram_vocab_size, d_model, padding_idx=bigram_vocab[\"<PAD>\"]\n        )\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, bigram_ids, attention_mask):\n        L = bigram_ids.size(1)\n        x = self.bigram_emb(bigram_ids) + self.pos_emb[:L]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\nmodel = BigramOnlyTransformer(len(bigram_vocab), num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# ------------------------------------------------------------------\n# helpers -------------------------------------------------------------\n# ------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"bigram_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ------------------------------------------------------------------\n# training loop ------------------------------------------------------\n# ------------------------------------------------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"b_only_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(f\"Epoch {epoch:2d}  val_loss {val_loss:.4f}  val_F1 {val_f1:.4f}\")\n\n    ed = experiment_data[\"bigram_only\"]\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), save_path)\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  time {time.time()-t0:.1f}s  best_val_F1 {best_val_f1:.4f}\")\n\n# ------------------------------------------------------------------\n# test ----------------------------------------------------------------\n# ------------------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro-F1: {test_f1:.4f}\")\n\ned = experiment_data[\"bigram_only\"]\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# ------------------------------------------------------------------\n# save ----------------------------------------------------------------\n# ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# No-Transformer-Context ablation for SPR-BENCH\nimport os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------\n# experiment bookkeeping\n# -------------------------------------------------------\nexperiment_data = {\n    \"no_transformer_context\": {\n        \"spr_bench\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\ned = experiment_data[\"no_transformer_context\"][\"spr_bench\"]\n\n# -------------------------------------------------------\n# misc\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# -------------------------------------------------------\n# data helpers (same as baseline)\n# -------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# -------------------------------------------------------\n# vocab construction\n# -------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        char_vocab.setdefault(ch, len(char_vocab))\npad_id, unk_id, sos_id = char_vocab[\"<PAD>\"], char_vocab[\"<UNK>\"], char_vocab[\"<SOS>\"]\n\nbigram_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<SOS>\"\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\nprint(f\"Char vocab: {len(char_vocab)}, Bigram vocab: {len(bigram_vocab)}\")\n\n\n# -------------------------------------------------------\n# encoding\n# -------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids, bigram_ids = [], []\n    prev = \"<SOS>\"\n    for ch in seq:\n        char_ids.append(char_vocab.get(ch, unk_id))\n        bigram_ids.append(bigram_vocab.get(prev + ch, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for idx in char_ids:\n        counts[idx] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# -------------------------------------------------------\n# collator\n# -------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"char_ids\"]) for b in batch)\n    B = len(batch)\n    char_tensor = torch.full((B, max_len), pad_id, dtype=torch.long)\n    bigram_tensor = torch.full_like(char_tensor, bigram_vocab[\"<PAD>\"])\n    mask = torch.zeros_like(char_tensor, dtype=torch.bool)\n    counts = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"char_ids\"])\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"])\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"])\n        mask[i, :L] = 1\n    return {\n        \"char_ids\": char_tensor,\n        \"bigram_ids\": bigram_tensor,\n        \"mask\": mask,\n        \"count_vec\": counts,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# -------------------------------------------------------\n# Model without Transformer\n# -------------------------------------------------------\nclass CBCMeanPool(nn.Module):\n    def __init__(\n        self,\n        char_vocab_size,\n        bigram_vocab_size,\n        num_labels,\n        d_model=256,\n        dropout=0.2,\n        max_len=512,\n    ):\n        super().__init__()\n        self.char_emb = nn.Embedding(char_vocab_size, d_model, padding_idx=pad_id)\n        self.bigram_emb = nn.Embedding(\n            bigram_vocab_size, d_model, padding_idx=bigram_vocab[\"<PAD>\"]\n        )\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        self.count_proj = nn.Sequential(\n            nn.Linear(char_vocab_size, d_model), nn.ReLU(), nn.Dropout(dropout)\n        )\n        self.classifier = nn.Linear(d_model * 2, num_labels)\n\n    def forward(self, char_ids, bigram_ids, mask, count_vec):\n        L = char_ids.size(1)\n        tok_repr = (\n            self.char_emb(char_ids) + self.bigram_emb(bigram_ids) + self.pos_emb[:L]\n        )\n        masked_sum = (tok_repr * mask.unsqueeze(-1)).sum(1)\n        denom = mask.sum(1, keepdim=True).clamp(min=1)\n        seq_repr = masked_sum / denom\n        count_repr = self.count_proj(count_vec)\n        cat = torch.cat([seq_repr, count_repr], dim=-1)\n        return self.classifier(cat)\n\n\nmodel = CBCMeanPool(len(char_vocab), len(bigram_vocab), num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# -------------------------------------------------------\n# epoch runner\n# -------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"mask\"],\n                batch[\"count_vec\"],\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------------------------------------------\n# training loop\n# -------------------------------------------------------\nbest_val, wait, patience = 0.0, 0, 3\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"cbc_meanpool_best.pt\")\n\nfor epoch in range(1, max_epochs + 1):\n    st = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"])\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}  val_F1={val_f1:.4f}\")\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n    if val_f1 > best_val:\n        best_val, wait = val_f1, 0\n        torch.save(model.state_dict(), save_path)\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  epoch time {time.time()-st:.1f}s  best_val_F1={best_val:.4f}\")\n\n# -------------------------------------------------------\n# test evaluation\n# -------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"])\nprint(\"Test Macro F1:\", test_f1)\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# -------------------------------------------------------\n# save artifacts\n# -------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# -------------------------------------------------------\n# CBC-CLS ablation : single-file executable\n# -------------------------------------------------------\nimport os, pathlib, time, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# reproducibility (optional)\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# -------------------------------------------------------\n# working dir & device\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------\n# experiment container\n# -------------------------------------------------------\nexperiment_data = {\n    \"cls_pooling\": {\n        \"spr_bench\": {\n            \"epochs\": [],\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None},\n            \"losses\": {\"train\": [], \"val\": [], \"test\": None},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# -------------------------------------------------------\n# dataset helpers (unchanged)\n# -------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\nnum_labels = len(set(spr[\"train\"][\"label\"]))\n\n# -------------------------------------------------------\n# vocabularies with extra CLS token\n# -------------------------------------------------------\nspecial_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<CLS>\"]\nchar_vocab = {tok: idx for idx, tok in enumerate(special_tokens)}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for ch in seq:\n        if ch not in char_vocab:\n            char_vocab[ch] = len(char_vocab)\npad_id, unk_id, sos_id, cls_id = (\n    char_vocab[\"<PAD>\"],\n    char_vocab[\"<UNK>\"],\n    char_vocab[\"<SOS>\"],\n    char_vocab[\"<CLS>\"],\n)\n\nbigram_vocab = {tok: idx for idx, tok in enumerate([\"<PAD>\", \"<UNK>\", \"<SOS><CLS>\"])}\nfor seq in spr[\"train\"][\"sequence\"]:\n    prev = \"<CLS>\"  # first bigram will be <CLS> + first_char\n    for ch in seq:\n        bg = prev + ch\n        if bg not in bigram_vocab:\n            bigram_vocab[bg] = len(bigram_vocab)\n        prev = ch\nprint(f\"Char vocab size {len(char_vocab)}, Bigram vocab size {len(bigram_vocab)}\")\n\n\n# -------------------------------------------------------\n# encoding with CLS prepended\n# -------------------------------------------------------\ndef encode(example):\n    seq = example[\"sequence\"]\n    char_ids = [cls_id]  # prepend CLS\n    bigram_ids = [bigram_vocab[\"<SOS><CLS>\"]]  # first bigram\n    prev = \"<CLS>\"\n    for ch in seq:\n        char_ids.append(char_vocab.get(ch, unk_id))\n        bg = prev + ch\n        bigram_ids.append(bigram_vocab.get(bg, bigram_vocab[\"<UNK>\"]))\n        prev = ch\n    # count vector for real characters only\n    counts = np.zeros(len(char_vocab), dtype=np.int16)\n    for idx in char_ids[1:]:\n        counts[idx] += 1\n    return {\n        \"char_ids\": char_ids,\n        \"bigram_ids\": bigram_ids,\n        \"count_vec\": counts.tolist(),\n    }\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(encode, remove_columns=[])\n\n\n# -------------------------------------------------------\n# collate fn\n# -------------------------------------------------------\ndef collate(batch):\n    max_len = max(len(b[\"char_ids\"]) for b in batch)\n    B = len(batch)\n    char_tensor = torch.full((B, max_len), pad_id, dtype=torch.long)\n    bigram_tensor = torch.full_like(char_tensor, bigram_vocab[\"<PAD>\"])\n    attn_mask = torch.zeros_like(char_tensor, dtype=torch.bool)\n    counts_tensor = torch.stack(\n        [torch.tensor(b[\"count_vec\"], dtype=torch.float32) for b in batch]\n    )\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    for i, b in enumerate(batch):\n        L = len(b[\"char_ids\"])\n        char_tensor[i, :L] = torch.tensor(b[\"char_ids\"], dtype=torch.long)\n        bigram_tensor[i, :L] = torch.tensor(b[\"bigram_ids\"], dtype=torch.long)\n        attn_mask[i, :L] = 1\n    return {\n        \"char_ids\": char_tensor,\n        \"bigram_ids\": bigram_tensor,\n        \"attention_mask\": attn_mask,\n        \"count_vec\": counts_tensor,\n        \"labels\": labels,\n    }\n\n\nbatch_size = 128\nloaders = {\n    split: DataLoader(\n        spr[split],\n        batch_size=batch_size,\n        shuffle=(split == \"train\"),\n        collate_fn=collate,\n        num_workers=2,\n        pin_memory=True,\n    )\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# -------------------------------------------------------\n# CBC-CLS Transformer model\n# -------------------------------------------------------\nclass CBCTransformerCLS(nn.Module):\n    def __init__(\n        self,\n        char_vocab_size,\n        bigram_vocab_size,\n        num_labels,\n        d_model=256,\n        nhead=8,\n        nlayers=4,\n        dim_feedforward=512,\n        dropout=0.2,\n        max_len=513,  # +1 for CLS\n    ):\n        super().__init__()\n        self.char_emb = nn.Embedding(char_vocab_size, d_model, padding_idx=pad_id)\n        self.bigram_emb = nn.Embedding(\n            bigram_vocab_size, d_model, padding_idx=bigram_vocab[\"<PAD>\"]\n        )\n        self.pos_emb = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        # count pathway\n        self.count_proj = nn.Sequential(\n            nn.Linear(char_vocab_size, d_model), nn.ReLU(), nn.Dropout(dropout)\n        )\n        # final classifier\n        self.classifier = nn.Linear(d_model * 2, num_labels)\n\n    def forward(self, char_ids, bigram_ids, attention_mask, count_vec):\n        L = char_ids.size(1)\n        tok_emb = (\n            self.char_emb(char_ids) + self.bigram_emb(bigram_ids) + self.pos_emb[:L]\n        )\n        enc_out = self.encoder(tok_emb, src_key_padding_mask=~attention_mask)\n        pooled = enc_out[:, 0]  # CLS hidden state\n        count_rep = self.count_proj(count_vec)\n        repr_cat = torch.cat([pooled, count_rep], dim=-1)\n        return self.classifier(repr_cat)\n\n\nmodel = CBCTransformerCLS(len(char_vocab), len(bigram_vocab), num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\n# -------------------------------------------------------\n# epoch runner\n# -------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(\n                batch[\"char_ids\"],\n                batch[\"bigram_ids\"],\n                batch[\"attention_mask\"],\n                batch[\"count_vec\"],\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------\n# training loop with early stopping\n# -------------------------------------------------------\nbest_val_f1, patience, wait = 0.0, 3, 0\nmax_epochs = 15\nsave_path = os.path.join(working_dir, \"cbc_cls_best.pt\")\ned = experiment_data[\"cls_pooling\"][\"spr_bench\"]\n\nfor epoch in range(1, max_epochs + 1):\n    tic = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(loaders[\"train\"], train=True)\n    val_loss, val_f1, _, _ = run_epoch(loaders[\"dev\"], train=False)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, val_F1={val_f1:.4f}\")\n    ed[\"epochs\"].append(epoch)\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(val_f1)\n\n    if val_f1 > best_val_f1:\n        best_val_f1, wait = val_f1, 0\n        torch.save(model.state_dict(), save_path)\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n    print(f\"  time={time.time()-tic:.1f}s  best_val_F1={best_val_f1:.4f}\")\n\n# -------------------------------------------------------\n# final test evaluation\n# -------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\ntest_loss, test_f1, test_preds, test_gts = run_epoch(loaders[\"test\"], train=False)\nprint(f\"Test Macro F1: {test_f1:.4f}\")\ned[\"losses\"][\"test\"] = test_loss\ned[\"metrics\"][\"test_f1\"] = test_f1\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\n\n# -------------------------------------------------------\n# save experiment data\n# -------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved.\")\n"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Char vocab size 12, Bigram vocab size 30', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '\\rMap:  30%|##9       | 595/2000 [00:00<00:00,\n5902.30 examples/s]', '\\rMap:  67%|######7   | 1344/2000 [00:00<00:00, 5276.19\nexamples/s]', '\\rMap:  97%|#########7| 1945/2000 [00:00<00:00, 5562.47\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5233.50\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5316.24 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  61%|######1\n| 612/1000 [00:00<00:00, 6093.94 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5180.39 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.5672,\nval_F1 = 0.7780', '\\n', '  epoch time 1.0s  best_val_F1=0.7780', '\\n', 'Epoch 2:\nvalidation_loss = 0.5774, val_F1 = 0.7918', '\\n', '  epoch time 0.7s\nbest_val_F1=0.7918', '\\n', 'Epoch 3: validation_loss = 0.5412, val_F1 = 0.7880',\n'\\n', '  epoch time 0.8s  best_val_F1=0.7918', '\\n', 'Epoch 4: validation_loss =\n0.5370, val_F1 = 0.7919', '\\n', '  epoch time 0.7s  best_val_F1=0.7919', '\\n',\n'Epoch 5: validation_loss = 0.5733, val_F1 = 0.7740', '\\n', '  epoch time 0.7s\nbest_val_F1=0.7919', '\\n', 'Epoch 6: validation_loss = 0.5172, val_F1 = 0.7940',\n'\\n', '  epoch time 0.7s  best_val_F1=0.7940', '\\n', 'Epoch 7: validation_loss =\n0.5843, val_F1 = 0.7720', '\\n', '  epoch time 0.7s  best_val_F1=0.7940', '\\n',\n'Epoch 8: validation_loss = 0.5337, val_F1 = 0.7940', '\\n', '  epoch time 0.7s\nbest_val_F1=0.7940', '\\n', 'Epoch 9: validation_loss = 0.5160, val_F1 = 0.7959',\n'\\n', '  epoch time 0.7s  best_val_F1=0.7959', '\\n', 'Epoch 10: validation_loss\n= 0.5668, val_F1 = 0.7820', '\\n', '  epoch time 0.7s  best_val_F1=0.7959', '\\n',\n'Epoch 11: validation_loss = 0.5470, val_F1 = 0.7940', '\\n', '  epoch time 0.7s\nbest_val_F1=0.7959', '\\n', 'Epoch 12: validation_loss = 0.5481, val_F1 =\n0.7940', '\\n', 'Early stopping.', '\\n', 'Test Macro F1: 0.7980', '\\n',\n'Execution time: 12 seconds seconds (time limit is 30 minutes).']", "['Device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 87571.98\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 63377.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 100732.60\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Char\nvocab 12, Bigram vocab 30', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '\\rMap:  31%|###       | 613/2000 [00:00<00:00, 6095.22\nexamples/s]', '\\rMap:  70%|######9   | 1394/2000 [00:00<00:00, 5480.81\nexamples/s]', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5280.60\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5361.22\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5363.18 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  61%|######1\n| 613/1000 [00:00<00:00, 6094.94 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5374.34 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss 0.5450  val_F1\n0.7840', '\\n', '  epoch time 1.0s  best_val_F1=0.7840', '\\n', 'Epoch 2: val_loss\n0.5696  val_F1 0.7878', '\\n', '  epoch time 0.7s  best_val_F1=0.7878', '\\n',\n'Epoch 3: val_loss 0.5268  val_F1 0.7840', '\\n', '  epoch time 0.7s\nbest_val_F1=0.7878', '\\n', 'Epoch 4: val_loss 0.5349  val_F1 0.7820', '\\n', '\nepoch time 0.8s  best_val_F1=0.7878', '\\n', 'Epoch 5: val_loss 0.5220  val_F1\n0.7959', '\\n', '  epoch time 0.7s  best_val_F1=0.7959', '\\n', 'Epoch 6: val_loss\n0.5514  val_F1 0.7919', '\\n', '  epoch time 0.7s  best_val_F1=0.7959', '\\n',\n'Epoch 7: val_loss 0.5497  val_F1 0.7920', '\\n', '  epoch time 0.7s\nbest_val_F1=0.7959', '\\n', 'Epoch 8: val_loss 0.5518  val_F1 0.7735', '\\n',\n'Early stopping triggered.', '\\n', 'Test Macro F1: 0.7950', '\\n', 'Results saved\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n46_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n20/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 107188.96\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 73825.18\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 90570.16\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Char\nvocab size 12, Bigram vocab size 30', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '\\rMap:  29%|##8       | 573/2000 [00:00<00:00,\n5690.54 examples/s]', '\\rMap:  66%|######6   | 1330/2000 [00:00<00:00, 5243.52\nexamples/s]', '\\rMap:  96%|#########6| 1928/2000 [00:00<00:00, 5528.46\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5185.75\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5303.75 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  59%|#####8\n| 588/1000 [00:00<00:00, 5851.73 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5218.45 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.5414,\nval_F1=0.7780', '\\n', '  epoch time 1.1s  best_val_F1=0.7780', '\\n', 'Epoch 2:\nval_loss=0.6061, val_F1=0.7619', '\\n', '  epoch time 0.8s  best_val_F1=0.7780',\n'\\n', 'Epoch 3: val_loss=0.5359, val_F1=0.7860', '\\n', '  epoch time 0.8s\nbest_val_F1=0.7860', '\\n', 'Epoch 4: val_loss=0.5357, val_F1=0.7860', '\\n', '\nepoch time 0.7s  best_val_F1=0.7860', '\\n', 'Epoch 5: val_loss=0.5431,\nval_F1=0.7860', '\\n', '  epoch time 0.7s  best_val_F1=0.7860', '\\n', 'Epoch 6:\nval_loss=0.5208, val_F1=0.7959', '\\n', '  epoch time 0.7s  best_val_F1=0.7959',\n'\\n', 'Epoch 7: val_loss=0.5304, val_F1=0.7959', '\\n', '  epoch time 0.8s\nbest_val_F1=0.7959', '\\n', 'Epoch 8: val_loss=0.5490, val_F1=0.7940', '\\n', '\nepoch time 0.7s  best_val_F1=0.7959', '\\n', 'Epoch 9: val_loss=0.5194,\nval_F1=0.7939', '\\n', 'Early stopping.', '\\n', 'Test Macro F1: 0.7920', '\\n',\n'Saved experiment data to working/experiment_data.npy', '\\n', 'Execution time:\n10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 143844.99\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 99684.00\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 195120.21\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Char\nvocab 12, Bigram vocab 30', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '\\rMap:  29%|##8       | 572/2000 [00:00<00:00, 5693.95\nexamples/s]', '\\rMap:  66%|######6   | 1327/2000 [00:00<00:00, 5234.85\nexamples/s]', '\\rMap:  96%|#########5| 1918/2000 [00:00<00:00, 5499.08\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5163.48\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5336.48 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  59%|#####8\n| 588/1000 [00:00<00:00, 5837.25 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5197.33 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.5796\nval_F1=0.7371', '\\n', '  epoch time 1.1s   best_val_F1=0.7371', '\\n', 'Epoch 2:\nval_loss=0.5545  val_F1=0.7880', '\\n', '  epoch time 0.7s   best_val_F1=0.7880',\n'\\n', 'Epoch 3: val_loss=0.5416  val_F1=0.7880', '\\n', '  epoch time 0.8s\nbest_val_F1=0.7880', '\\n', 'Epoch 4: val_loss=0.5481  val_F1=0.7840', '\\n', '\nepoch time 0.7s   best_val_F1=0.7880', '\\n', 'Epoch 5: val_loss=0.5427\nval_F1=0.7878', '\\n', 'Early stopping.', '\\n', 'Test Macro F1: 0.7910', '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n100279.83 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 72044.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 128628.07\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Char\nvocab: 12,  Bigram vocab: 30', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '\\rMap:  29%|##8       | 576/2000 [00:00<00:00, 5731.55\nexamples/s]', '\\rMap:  67%|######7   | 1349/2000 [00:00<00:00, 5333.37\nexamples/s]', '\\rMap:  98%|#########8| 1964/2000 [00:00<00:00, 5650.22\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5274.40\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5321.59 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  59%|#####8\n| 586/1000 [00:00<00:00, 5840.14 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5238.55 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss 0.8522\nval_loss 0.6612  val_F1 0.7330', '\\n', '  epoch time 1.2s  best_val_F1 0.7330',\n'\\n', 'Epoch 2: train_loss 0.7119  val_loss 0.5478  val_F1 0.7820', '\\n', '\nepoch time 0.8s  best_val_F1 0.7820', '\\n', 'Epoch 3: train_loss 0.6539\nval_loss 0.5260  val_F1 0.7880', '\\n', '  epoch time 0.8s  best_val_F1 0.7880',\n'\\n', 'Epoch 4: train_loss 0.6394  val_loss 0.5447  val_F1 0.7740', '\\n', '\nepoch time 0.8s  best_val_F1 0.7880', '\\n', 'Epoch 5: train_loss 0.6060\nval_loss 0.5462  val_F1 0.7940', '\\n', '  epoch time 0.9s  best_val_F1 0.7940',\n'\\n', 'Epoch 6: train_loss 0.6066  val_loss 0.5359  val_F1 0.7900', '\\n', '\nepoch time 0.8s  best_val_F1 0.7940', '\\n', 'Epoch 7: train_loss 0.5822\nval_loss 0.5374  val_F1 0.7940', '\\n', '  epoch time 0.8s  best_val_F1 0.7940',\n'\\n', 'Epoch 8: train_loss 0.5706  val_loss 0.5146  val_F1 0.7959', '\\n', '\nepoch time 1.0s  best_val_F1 0.7959', '\\n', 'Epoch 9: train_loss 0.5596\nval_loss 0.5231  val_F1 0.7896', '\\n', '  epoch time 1.0s  best_val_F1 0.7959',\n'\\n', 'Epoch 10: train_loss 0.5676  val_loss 0.5404  val_F1 0.7859', '\\n', '\nepoch time 0.8s  best_val_F1 0.7959', '\\n', 'Epoch 11: train_loss 0.5429\nval_loss 0.5399  val_F1 0.7940', '\\n', 'Early stopping.', '\\n', 'Test Macro-F1:\n0.7980', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 13 seconds\nseconds (time limit is 30 minutes).']", "['Device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\",\n'\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  27%|##7\n| 549/2000 [00:00<00:00, 5442.64 examples/s]', '\\rMap:  57%|#####6    |\n1133/2000 [00:00<00:00, 5670.22 examples/s]', '\\rMap:  89%|########8 | 1779/2000\n[00:00<00:00, 6024.40 examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 5847.99 examples/s]', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00,\n6174.76 examples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '\\rMap:  66%|######5   | 658/1000 [00:00<00:00, 6554.58\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 6241.60\nexamples/s]', '\\n', 'Epoch 1: val_loss=0.7648 val_F1=0.4510', '\\n', '  time 0.5s\nbest_val_F1=0.4510', '\\n', 'Epoch 2: val_loss=0.6386 val_F1=0.6476', '\\n', '\ntime 0.3s  best_val_F1=0.6476', '\\n', 'Epoch 3: val_loss=0.5987 val_F1=0.7000',\n'\\n', '  time 0.3s  best_val_F1=0.7000', '\\n', 'Epoch 4: val_loss=0.5815\nval_F1=0.7203', '\\n', '  time 0.3s  best_val_F1=0.7203', '\\n', 'Epoch 5:\nval_loss=0.5885 val_F1=0.7287', '\\n', '  time 0.3s  best_val_F1=0.7287', '\\n',\n'Epoch 6: val_loss=0.5721 val_F1=0.7840', '\\n', '  time 0.3s\nbest_val_F1=0.7840', '\\n', 'Epoch 7: val_loss=0.5674 val_F1=0.7517', '\\n', '\ntime 0.4s  best_val_F1=0.7840', '\\n', 'Epoch 8: val_loss=0.5672 val_F1=0.7538',\n'\\n', '  time 0.3s  best_val_F1=0.7840', '\\n', 'Epoch 9: val_loss=0.5670\nval_F1=0.7558', '\\n', 'Early stopping.', '\\n', 'Test Macro F1: 0.7879', '\\n',\n'Execution time: 6 seconds seconds (time limit is 30 minutes).']", "['Device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\",\n'\\n', 'Char vocab 12 | Bigram vocab 30', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n20133.51 examples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 21279.65\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]',\n'', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 22376.90 examples/s]',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch  1  val_loss 0.5748  val_F1\n0.7877', '\\n', '  time 1.0s  best_val_F1 0.7877', '\\n', 'Epoch  2  val_loss\n0.5493  val_F1 0.7599', '\\n', '  time 0.6s  best_val_F1 0.7877', '\\n', 'Epoch  3\nval_loss 0.5254  val_F1 0.7860', '\\n', '  time 0.6s  best_val_F1 0.7877', '\\n',\n'Epoch  4  val_loss 0.5069  val_F1 0.7959', '\\n', '  time 0.7s  best_val_F1\n0.7959', '\\n', 'Epoch  5  val_loss 0.5274  val_F1 0.7900', '\\n', '  time 0.7s\nbest_val_F1 0.7959', '\\n', 'Epoch  6  val_loss 0.5248  val_F1 0.7820', '\\n', '\ntime 0.7s  best_val_F1 0.7959', '\\n', 'Epoch  7  val_loss 0.5223  val_F1\n0.7940', '\\n', 'Early stopping.', '\\n', 'Test Macro-F1: 0.7950', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time limit is 30\nminutes).']", "['Device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\",\n'\\n', 'Char vocab: 12, Bigram vocab: 30', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '\\rMap:  27%|##6       | 534/2000 [00:00<00:00,\n5226.78 examples/s]', '\\rMap:  65%|######5   | 1300/2000 [00:00<00:00, 4943.43\nexamples/s]', '\\rMap:  94%|#########4| 1883/2000 [00:00<00:00, 5278.86\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 4961.05\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5249.88 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  61%|######\n| 607/1000 [00:00<00:00, 6038.81 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5095.55 examples/s]', '\\n', 'Epoch 1: val_loss=0.6445\nval_F1=0.6300', '\\n', '  epoch time 0.8s  best_val_F1=0.6300', '\\n', 'Epoch 2:\nval_loss=0.5956  val_F1=0.7186', '\\n', '  epoch time 0.5s  best_val_F1=0.7186',\n'\\n', 'Epoch 3: val_loss=0.5541  val_F1=0.7659', '\\n', '  epoch time 0.8s\nbest_val_F1=0.7659', '\\n', 'Epoch 4: val_loss=0.5559  val_F1=0.7559', '\\n', '\nepoch time 0.6s  best_val_F1=0.7659', '\\n', 'Epoch 5: val_loss=0.5534\nval_F1=0.7700', '\\n', '  epoch time 0.7s  best_val_F1=0.7700', '\\n', 'Epoch 6:\nval_loss=0.5526  val_F1=0.7720', '\\n', '  epoch time 0.5s  best_val_F1=0.7720',\n'\\n', 'Epoch 7: val_loss=0.5527  val_F1=0.7579', '\\n', '  epoch time 0.5s\nbest_val_F1=0.7720', '\\n', 'Epoch 8: val_loss=0.5550  val_F1=0.7559', '\\n', '\nepoch time 0.6s  best_val_F1=0.7720', '\\n', 'Epoch 9: val_loss=0.5488\nval_F1=0.7800', '\\n', '  epoch time 0.5s  best_val_F1=0.7800', '\\n', 'Epoch 10:\nval_loss=0.5482  val_F1=0.7700', '\\n', '  epoch time 1.1s  best_val_F1=0.7800',\n'\\n', 'Epoch 11: val_loss=0.5466  val_F1=0.7720', '\\n', '  epoch time 0.5s\nbest_val_F1=0.7800', '\\n', 'Epoch 12: val_loss=0.5489  val_F1=0.7599', '\\n',\n'Early stopping.', '\\n', 'Test Macro F1:', ' ', '0.7828851462423623', '\\n',\n'Execution time: 11 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Char vocab size 13, Bigram vocab size 31', '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '\\rMap:  29%|##9       | 587/2000 [00:00<00:00,\n5776.17 examples/s]', '\\rMap:  68%|######7   | 1350/2000 [00:00<00:00, 5302.16\nexamples/s]', '\\rMap:  98%|#########8| 1961/2000 [00:00<00:00, 5613.05\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5257.85\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 5326.70 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:  61%|######\n| 608/1000 [00:00<00:00, 6049.29 examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 5323.63 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.5967,\nval_F1=0.7248', '\\n', '  time=1.1s  best_val_F1=0.7248', '\\n', 'Epoch 2:\nval_loss=0.5545, val_F1=0.7720', '\\n', '  time=0.8s  best_val_F1=0.7720', '\\n',\n'Epoch 3: val_loss=0.5993, val_F1=0.7599', '\\n', '  time=0.7s\nbest_val_F1=0.7720', '\\n', 'Epoch 4: val_loss=0.5816, val_F1=0.7740', '\\n', '\ntime=0.9s  best_val_F1=0.7740', '\\n', 'Epoch 5: val_loss=0.5360, val_F1=0.7900',\n'\\n', '  time=1.0s  best_val_F1=0.7900', '\\n', 'Epoch 6: val_loss=0.6008,\nval_F1=0.7719', '\\n', '  time=0.8s  best_val_F1=0.7900', '\\n', 'Epoch 7:\nval_loss=0.5471, val_F1=0.7800', '\\n', '  time=0.8s  best_val_F1=0.7900', '\\n',\n'Epoch 8: val_loss=0.5197, val_F1=0.7940', '\\n', '  time=1.0s\nbest_val_F1=0.7940', '\\n', 'Epoch 9: val_loss=0.5253, val_F1=0.7940', '\\n', '\ntime=0.8s  best_val_F1=0.7940', '\\n', 'Epoch 10: val_loss=0.5536,\nval_F1=0.7659', '\\n', '  time=0.8s  best_val_F1=0.7940', '\\n', 'Epoch 11:\nval_loss=0.5306, val_F1=0.7959', '\\n', '  time=1.0s  best_val_F1=0.7959', '\\n',\n'Epoch 12: val_loss=0.5246, val_F1=0.7919', '\\n', '  time=0.8s\nbest_val_F1=0.7959', '\\n', 'Epoch 13: val_loss=0.5208, val_F1=0.7959', '\\n', '\ntime=0.8s  best_val_F1=0.7959', '\\n', 'Epoch 14: val_loss=0.5838,\nval_F1=0.7679', '\\n', 'Early stopping.', '\\n', 'Test Macro F1: 0.7970', '\\n',\n'Experiment data saved.', '\\n', 'Execution time: 15 seconds seconds (time limit\nis 30 minutes).']"], "analysis": ["", "", "", "The code executed successfully, completing the training and evaluation of the\nCBC model without positional embeddings. The model achieved a Test Macro F1\nscore of 0.7910, and early stopping was correctly applied after the validation\nF1 score plateaued. No bugs were identified in the output or execution process.", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "char_bigram_count", "final_value": 0.5229, "best_value": 0.5229}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "char_bigram_count", "final_value": 0.516, "best_value": 0.516}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss calculated on the test dataset.", "data": [{"dataset_name": "char_bigram_count", "final_value": 0.5056, "best_value": 0.5056}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score calculated on the training dataset.", "data": [{"dataset_name": "char_bigram_count", "final_value": 0.7925, "best_value": 0.7925}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score calculated on the validation dataset.", "data": [{"dataset_name": "char_bigram_count", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score calculated on the test dataset.", "data": [{"dataset_name": "char_bigram_count", "final_value": 0.798, "best_value": 0.798}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "Measures the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH (Ablation: char_bigram_only)", "final_value": 0.795, "best_value": 0.7959}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or difference between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH (Ablation: char_bigram_only)", "final_value": 0.5175, "best_value": 0.5143}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "The F1 score is the harmonic mean of precision and recall, used to evaluate classification models.", "data": [{"dataset_name": "spr_bench", "final_value": 0.792, "best_value": 0.7939}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error between predicted and actual values, used to optimize models.", "data": [{"dataset_name": "spr_bench", "final_value": 0.518342, "best_value": 0.518342}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5974, "best_value": 0.5974}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5427, "best_value": 0.5427}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5496, "best_value": 0.5496}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score during training.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7504, "best_value": 0.7504}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7878, "best_value": 0.7878}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score on the test dataset.", "data": [{"dataset_name": "spr_bench", "final_value": 0.791, "best_value": 0.791}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final and best training loss values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5429, "best_value": 0.5429}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final and best validation loss values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5146, "best_value": 0.5146}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "Final and best training F1 score values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7775, "best_value": 0.7775}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Final and best validation F1 score values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Final and best test loss values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5161, "best_value": 0.5161}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "Final and best test F1 score values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.798, "best_value": 0.798}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "Measures the harmonic mean of precision and recall, used to evaluate classification models.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7879, "best_value": 0.7879}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or deviation of the model's predictions from the actual values.", "data": [{"dataset_name": "spr_bench", "final_value": 0.559528, "best_value": 0.559528}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "bigram_only", "final_value": 0.510434, "best_value": 0.510434}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "bigram_only", "final_value": 0.522263, "best_value": 0.522263}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Measures the error on the test dataset. Lower values indicate better performance.", "data": [{"dataset_name": "bigram_only", "final_value": 0.514678, "best_value": 0.514678}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "Measures the F1 score during training. Higher values indicate better performance.", "data": [{"dataset_name": "bigram_only", "final_value": 0.796984, "best_value": 0.796984}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "Measures the F1 score during validation. Higher values indicate better performance.", "data": [{"dataset_name": "bigram_only", "final_value": 0.795948, "best_value": 0.795948}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "Measures the F1 score on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "bigram_only", "final_value": 0.79499, "best_value": 0.79499}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5575, "best_value": 0.5575}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5466, "best_value": 0.5466}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5362, "best_value": 0.5362}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7494, "best_value": 0.7494}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation.", "data": [{"dataset_name": "spr_bench", "final_value": 0.78, "best_value": 0.78}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing.", "data": [{"dataset_name": "spr_bench", "final_value": 0.7829, "best_value": 0.7829}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "F1 score measures the balance between precision and recall, ranging from 0 to 1. Higher is better.", "data": [{"dataset_name": "spr_bench", "final_value": 0.797, "best_value": 0.797}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss quantifies the error in predictions, where lower values indicate better performance.", "data": [{"dataset_name": "spr_bench", "final_value": 0.5326, "best_value": 0.5303}]}]}], "is_best_node": [false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_confusion.png"], ["../../logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_loss_curve.png", "../../logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_f1_curve.png", "../../logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_f1_curve.png", "../../logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_f1_bar.png", "../../logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_f1_curves.png", "../../logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_train_val_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_train_val_f1_curve.png", "../../logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_val_vs_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_confusion_matrix.png"]], "plot_paths": [["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_confusion.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_loss_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_f1_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_loss_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_f1_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_f1_bar.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_train_val_loss_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_train_val_f1_curve.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_val_vs_test_f1_bar.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_f1_curves.png", "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_confusion_matrix.png"]], "plot_analyses": [[{"analysis": "The training loss plot shows a steady decrease over the epochs, indicating that the model is learning effectively during training. However, the validation loss plot exhibits fluctuations, suggesting potential overfitting or instability in the model's generalization capability. This could be due to the complex nature of the SPR_BENCH task or the need for better regularization techniques.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_loss_curves.png"}, {"analysis": "The training Macro-F1 score improves consistently over epochs, which aligns with the observed decrease in training loss. This suggests that the model is learning to classify symbolic sequences better over time. However, the validation Macro-F1 score fluctuates significantly, mirroring the behavior of the validation loss. This variability indicates that the model's performance on unseen data is inconsistent, possibly due to overfitting or insufficient robustness to the SPR_BENCH dataset's complexities.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_f1_curves.png"}, {"analysis": "The confusion matrix highlights the classification performance for the top-5 classes. The diagonal dominance suggests that the model performs well on certain classes, but the off-diagonal elements indicate misclassifications. This could imply that the model struggles with certain symbolic rules or patterns, necessitating further investigation into these specific cases.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_4867cf4077184200a6ae09a914d28ac5_proc_3167394/SPR_BENCH_char_bigram_count_confusion.png"}], [{"analysis": "The loss curves indicate a steady decrease in training loss across epochs, with validation loss showing a similar trend but with slight fluctuations. Training loss stabilizes around epoch 5, suggesting effective learning. However, the validation loss does not show significant improvement after epoch 5 and remains slightly higher than the test loss (0.518). This could indicate mild overfitting or suboptimal generalization. Further regularization or early stopping could improve performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show significant improvement in training F1 scores during the first few epochs, stabilizing near the test F1 score of 0.795. Validation F1 scores closely follow the training F1 trend, indicating that the model generalizes well. The slight decline in F1 scores after epoch 6 suggests potential overfitting or diminishing returns from training. This trend implies the model performs well but might benefit from additional regularization or hyperparameter tuning.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix reveals that the model correctly classified a majority of the samples, with 394 true negatives and 401 true positives. However, there are 104 false positives and 101 false negatives, indicating room for improvement in both precision and recall. Strategies such as adjusting the decision threshold or incorporating a loss function that penalizes misclassifications differently could enhance performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_cf40ca240f1b42e9888209f021825e62_proc_3174204/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that the model is effectively learning from the training data. The training loss steadily decreases over epochs, showing convergence. However, the validation loss exhibits a slight increase after epoch 7, suggesting potential overfitting. This could be mitigated by applying regularization techniques or early stopping. The overall trend is promising, but care should be taken to ensure generalization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_loss_curves.png"}, {"analysis": "The F1 score curves show that the model's performance improves consistently on the training set. The validation F1 score stabilizes after epoch 4 and remains relatively constant, indicating that the model generalizes well up to a certain point. The gap between training and validation F1 scores narrows, which is a positive sign of reduced overfitting. Further fine-tuning could help improve the validation F1 score.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix for the test set shows a balanced performance across the two classes. The diagonal dominance indicates that the model is correctly classifying a majority of instances. However, there is still room for improvement in reducing misclassifications, particularly in the off-diagonal cells. Techniques such as data augmentation or class-weighted loss functions could be explored to address this.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_024186e2ce7b4e079d67796dcab573c9_proc_3174205/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curve indicates a consistent decrease in training loss over the epochs, suggesting that the model is learning effectively from the training data. Validation loss remains relatively stable after an initial decline, which could indicate that the model is not overfitting significantly but may have reached its capacity for generalization within the given setup. Further tuning of hyperparameters or model architecture might help to reduce validation loss further.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_loss_curve.png"}, {"analysis": "The F1 curve demonstrates an improvement in both train and validation macro-F1 scores over the epochs. The validation macro-F1 stabilizes at a high value, which is a positive sign of the model's ability to generalize to unseen data. However, the gap between train and validation F1 scores suggests there might still be potential for further optimization to close the gap and improve generalization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_f1_curve.png"}, {"analysis": "The confusion matrix reveals the distribution of predictions compared to the true labels. While the diagonal dominance indicates that the model performs well in predicting the correct classes, there might be some misclassification in certain classes. Analyzing the specific misclassified instances could provide insights into potential weaknesses of the model and guide further improvements.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_bc1e88e6947d44438acfbb28b43b04f3_proc_3174206/spr_bench_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over epochs. The training loss decreases steadily, indicating that the model is learning effectively from the training data. The validation loss also decreases, but at a slower rate, which suggests that the model is generalizing well to unseen data. However, the slight gap between the training and validation loss could be a sign of minor overfitting, which might require regularization or early stopping to address.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_loss_curves.png"}, {"analysis": "This plot illustrates the macro-F1 score for both training and validation sets over epochs. The training macro-F1 score improves consistently, demonstrating that the model's performance is getting better with more training. The validation macro-F1 score starts high and remains relatively stable, with slight fluctuations, indicating that the model maintains good generalization capabilities. The convergence of the training and validation scores suggests that the model is not overfitting significantly.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_f1_curves.png"}, {"analysis": "This confusion matrix provides a comparison of the predicted and ground truth labels. The darker diagonal elements indicate that the model is making accurate predictions for the majority of the samples. However, the presence of lighter off-diagonal elements suggests some misclassifications, which could be addressed by fine-tuning the model or employing techniques to handle class imbalance if it exists.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_aa1ca0d0468d468e87a7a61e3f56074e_proc_3174207/spr_bench_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over 9 epochs. The training loss decreases steadily, indicating that the model is learning from the training data. The validation loss also decreases initially but plateaus after epoch 5, suggesting that the model generalizes well without overfitting. However, the gap between training and validation loss indicates some level of overfitting, which might be mitigated by regularization techniques or early stopping.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_loss_curve.png"}, {"analysis": "This plot illustrates the training and validation Macro-F1 scores across epochs. Both metrics improve over time, with the validation Macro-F1 reaching a plateau around epoch 5. The consistent improvement in training F1 and the validation F1 plateau suggest that the model is learning effectively without significant overfitting. However, the slight divergence between training and validation F1 towards the end could be indicative of minor overfitting.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_f1_curve.png"}, {"analysis": "This bar chart compares the best validation Macro-F1 score (0.784) with the test Macro-F1 score (0.788). The close alignment of these scores indicates that the model generalizes well to unseen data, demonstrating robust performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_f1_bar.png"}, {"analysis": "The confusion matrix for the test set shows that the model performs reasonably well in both classes, with 385 true negatives, 403 true positives, 113 false positives, and 99 false negatives. While the true positive and true negative rates are high, the false positive and false negative counts suggest room for improvement in precision and recall. Balancing these metrics could enhance the overall performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d10b2936a8694b11a272f55e0bb1a677_proc_3174204/spr_bench_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over seven epochs for the 'bigram_only' model. The training loss decreases consistently, indicating that the model is learning effectively from the training data. However, the validation loss decreases initially and then stabilizes with minor fluctuations, suggesting that the model's ability to generalize to unseen data is adequate but may require further tuning to minimize overfitting. The gap between training and validation loss is small, which is a positive sign of the model's generalization capability.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation Macro-F1 scores over seven epochs. Both scores improve significantly in the initial epochs, with the training Macro-F1 score catching up to the validation score by the third epoch. The scores then stabilize, with slight fluctuations, indicating that the model achieves consistent performance on both training and validation sets. The convergence of the two curves suggests good alignment between training and validation performance, though further analysis may be needed to confirm robustness.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_f1_curves.png"}, {"analysis": "The confusion matrix for the 'bigram_only' model on the test dataset indicates strong performance, with the majority of predictions aligning with the ground truth. The diagonal dominance shows high accuracy in both classes, while the relatively low off-diagonal values suggest minimal misclassifications. This indicates that the model generalizes well to the test data and effectively distinguishes between the classes.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d3120e09a3cb4173be885a68aca9569e_proc_3174205/bigram_only_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over 12 epochs. The training loss decreases steadily, indicating that the model is learning effectively during training. However, the validation loss plateaus after a few epochs, suggesting that the model's ability to generalize to unseen data stabilizes early. The gap between the training and validation loss remains consistent, indicating that overfitting is not a significant concern at this stage.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_train_val_loss_curve.png"}, {"analysis": "This plot compares the macro-F1 scores for the training and validation datasets over 12 epochs. Both metrics improve as training progresses, with the validation macro-F1 score reaching a peak around epoch 10 and remaining relatively stable. This suggests that the model is consistently improving its classification performance and generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_train_val_f1_curve.png"}, {"analysis": "This bar chart compares the best validation macro-F1 score with the test macro-F1 score. The test macro-F1 score slightly surpasses the validation score, indicating that the model performs well on unseen data and that the validation set was a good proxy for the test set during training.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_val_vs_test_f1_bar.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's performance on the test set. The model correctly classifies 380 samples as class 0 and 403 samples as class 1. However, it misclassifies 118 samples as class 1 when they are actually class 0, and 99 samples as class 0 when they are actually class 1. These results indicate a slight imbalance in classification errors but overall good performance.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ba1e3fd2501b472596f7245a82b88894_proc_3174206/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curve indicates that the training loss decreases steadily over epochs, showing convergence. The validation loss also decreases initially but exhibits some fluctuations, especially around the mid-training epochs. This suggests that while the model is learning, it may be slightly overfitting or encountering challenges generalizing to unseen data. The gap between training and validation loss is relatively small, which is a positive sign of generalization.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_loss_curves.png"}, {"analysis": "The F1 score curves for both training and validation datasets show an upward trend initially, stabilizing close to the 0.8 mark. The test F1 score, indicated by the red dashed line, is slightly below the validation score, which may suggest minor overfitting to the validation set. Overall, the model achieves performance close to the state-of-the-art benchmark of 0.8, demonstrating the effectiveness of the approach.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix shows a clear diagonal dominance, indicating that the model is correctly classifying the majority of samples. The off-diagonal elements are minimal, suggesting low misclassification rates. This confirms that the model has learned to distinguish between classes effectively, though further analysis of specific misclassified examples might provide insights for improvement.", "plot_path": "experiments/2025-08-17_00-44-46_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e580495aa632498eb543123e4a659111_proc_3174207/spr_bench_confusion_matrix.png"}]], "vlm_feedback_summary": ["The plots reveal that while the model learns effectively during training, its\nperformance on the validation set is inconsistent, as evidenced by fluctuating\nvalidation loss and Macro-F1 scores. The confusion matrix suggests that the\nmodel performs well on some classes but struggles with others, highlighting\nareas for further improvement.", "The plots provide insights into the model's training dynamics, generalization,\nand classification performance. The loss and F1 curves suggest effective\nlearning with some room for improvement in generalization. The confusion matrix\nhighlights balanced but slightly imperfect precision and recall, indicating\npotential areas for optimization.", "The results show that the model is learning effectively and generalizing\nreasonably well. The loss curves suggest potential overfitting after epoch 7,\nwhile the F1 curves indicate stable validation performance. The confusion matrix\nhighlights balanced classification but suggests room for improvement in reducing\nerrors.", "The plots show promising trends in model training and validation. The loss\ncurves indicate effective learning with minimal overfitting concerns. The F1\ncurves demonstrate good generalization capabilities, though there is room for\noptimization. The confusion matrix suggests overall strong performance but\nhighlights areas for improvement in reducing misclassifications.", "The plots indicate that the model is learning effectively and generalizing well,\nwith minor overfitting observed. The macro-F1 scores show stable performance\nacross epochs, and the confusion matrix highlights that most predictions are\naccurate, with some room for improvement in misclassified samples.", "The plots indicate that the model is learning effectively, with decreasing\nlosses and improving Macro-F1 scores. The validation and test performance are\nclosely aligned, suggesting good generalization. However, there is minor\noverfitting and some imbalances in the confusion matrix that could be addressed\nto further refine the model.", "The experimental results show promising signs of effective learning and\ngeneralization. The training and validation losses indicate good convergence\nwith minimal overfitting, while the Macro-F1 scores reflect consistent\nperformance improvements and alignment. The confusion matrix demonstrates strong\nclassification accuracy, with minimal misclassification errors. Overall, the\n'bigram_only' model appears to be performing well, but further ablation studies\nand hyperparameter tuning could help optimize its performance further.", "The plots collectively demonstrate that the model effectively learns and\ngeneralizes well to unseen data. The consistent gap between training and\nvalidation loss, improving macro-F1 scores, and a well-performing confusion\nmatrix indicate a robust model. The slight imbalance in classification errors,\nas seen in the confusion matrix, could be addressed in future iterations.", "The plots indicate that the model is converging and performing well, with an F1\nscore close to the state-of-the-art benchmark. However, there are minor signs of\noverfitting and fluctuations in validation loss. The confusion matrix highlights\nstrong classification performance with minimal misclassifications."], "exec_time": [12.80115032196045, 9.733130693435669, 10.670262336730957, 7.583498477935791, 13.569158792495728, 6.282108306884766, 8.009112358093262, 11.543391227722168, 15.519115447998047], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["[\"SPR_BENCH\"]"], ["[\"spr_bench\"]"], ["[\"spr_bench\"]"], ["[\"spr_bench\"]"], ["['SPR_BENCH']"], ["['bigram_only']"], ["[\"SPR-BENCH\"]"], ["['spr_bench']"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# setup + load\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ncbc = experiment_data.get(\"char_bigram_count\", {})\n\nepochs = np.asarray(cbc.get(\"epochs\", []))\ntrain_loss = np.asarray(cbc.get(\"losses\", {}).get(\"train\", []))\nval_loss = np.asarray(cbc.get(\"losses\", {}).get(\"val\", []))\ntrain_f1 = np.asarray(cbc.get(\"metrics\", {}).get(\"train_f1\", []))\nval_f1 = np.asarray(cbc.get(\"metrics\", {}).get(\"val_f1\", []))\ntest_f1 = cbc.get(\"metrics\", {}).get(\"test_f1\", None)\npreds = np.asarray(cbc.get(\"predictions\", []))\ngts = np.asarray(cbc.get(\"ground_truth\", []))\n\n# ------------------------------------------------------------------\n# 1) Loss curves\n# ------------------------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left: training loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, label=\"train\")\n    plt.title(\"Left: Training Loss - SPR_BENCH (char_bigram_count)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    # Right: validation loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, val_loss, label=\"val\", color=\"orange\")\n    plt.title(\"Right: Validation Loss - SPR_BENCH (char_bigram_count)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_char_bigram_count_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) F1 curves\n# ------------------------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    # Left: training F1\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_f1, label=\"train\")\n    plt.title(\"Left: Training Macro-F1 - SPR_BENCH (char_bigram_count)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    # Right: validation F1\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, val_f1, label=\"val\", color=\"orange\")\n    plt.title(\"Right: Validation Macro-F1 - SPR_BENCH (char_bigram_count)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_char_bigram_count_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Confusion-matrix heat-map (at most 5\u00d75 shown)\n# ------------------------------------------------------------------\ntry:\n    if preds.size and gts.size:\n        n_cls = max(int(preds.max()), int(gts.max())) + 1\n        mat = np.zeros((n_cls, n_cls), dtype=int)\n        for t, p in zip(gts, preds):\n            mat[int(t), int(p)] += 1\n        view = mat[:5, :5]  # keep figure small if many classes\n        plt.figure(figsize=(4, 4))\n        im = plt.imshow(view, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.title(\"Confusion Matrix (Top-5 classes) - SPR_BENCH\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_char_bigram_count_confusion.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# print evaluation metric\n# ------------------------------------------------------------------\nif test_f1 is not None:\n    print(f\"Final Test Macro-F1: {test_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# -------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------- load experiment data -----------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"char_bigram_only\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    epochs = data[\"epochs\"]\n    tr_loss = data[\"losses\"][\"train\"]\n    val_loss = data[\"losses\"][\"val\"]\n    tr_f1 = data[\"metrics\"][\"train_f1\"]\n    val_f1 = data[\"metrics\"][\"val_f1\"]\n    test_loss = data[\"losses\"].get(\"test\", None)\n    test_f1 = data[\"metrics\"].get(\"test_f1\", None)\n    preds = np.array(data.get(\"predictions\", []))\n    gts = np.array(data.get(\"ground_truth\", []))\n\n    # ------------------- 1. Loss curve --------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        if test_loss is not None:\n            plt.hlines(\n                test_loss,\n                epochs[0],\n                epochs[-1],\n                colors=\"grey\",\n                linestyles=\"--\",\n                label=f\"Test Loss={test_loss:.3f}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Training, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n    finally:\n        plt.close()\n\n    # ------------------- 2. F1 curve ----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train F1\")\n        plt.plot(epochs, val_f1, label=\"Validation F1\")\n        if test_f1 is not None:\n            plt.hlines(\n                test_f1,\n                epochs[0],\n                epochs[-1],\n                colors=\"grey\",\n                linestyles=\"--\",\n                label=f\"Test F1={test_f1:.3f}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(\"SPR_BENCH Macro-F1 Curves\\nLeft: Training, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n    finally:\n        plt.close()\n\n    # ------------------- 3. Confusion matrix --------------------------\n    if preds.size and gts.size:\n        try:\n            cm = confusion_matrix(gts, preds)\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                \"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n        finally:\n            plt.close()\nelse:\n    print(\"No data available to plot.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    ed = experiment_data[\"no_bigram_char_count\"][\"spr_bench\"]\n    epochs = np.array(ed[\"epochs\"])\n    tr_loss = np.array(ed[\"losses\"][\"train\"])\n    val_loss = np.array(ed[\"losses\"][\"val\"])\n    tr_f1 = np.array(ed[\"metrics\"][\"train_f1\"])\n    val_f1 = np.array(ed[\"metrics\"][\"val_f1\"])\n    preds = np.array(ed[\"predictions\"])\n    gts = np.array(ed[\"ground_truth\"])\n\n    # ----------------------------- Plot 1 --------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR-BENCH \u2013 Loss Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ----------------------------- Plot 2 --------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Val Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR-BENCH \u2013 F1 Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # ----------------------------- Plot 3 --------------------------\n    try:\n        # Only attempt confusion matrix if predictions exist\n        if preds.size and gts.size:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds)\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\"SPR-BENCH \u2013 Confusion Matrix (Test Set)\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch nested dict safely\ndef get(d, *keys, default=None):\n    for k in keys:\n        if d is None:\n            return default\n        d = d.get(k, None)\n    return d if d is not None else default\n\n\n# select our run\nrun_key = \"no_positional_embedding\"\nds_key = \"spr_bench\"\ned = get(experiment_data, run_key, ds_key, default={})\n\nepochs = np.array(get(ed, \"epochs\", default=[]))\ntrain_loss = np.array(get(ed, \"losses\", \"train\", default=[]))\nval_loss = np.array(get(ed, \"losses\", \"val\", default=[]))\ntrain_f1 = np.array(get(ed, \"metrics\", \"train_f1\", default=[]))\nval_f1 = np.array(get(ed, \"metrics\", \"val_f1\", default=[]))\npreds = np.array(get(ed, \"predictions\", default=[]))\ngts = np.array(get(ed, \"ground_truth\", default=[]))\n\n# 1) Loss curve\ntry:\n    if epochs.size and train_loss.size and val_loss.size:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"spr_bench Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"spr_bench_loss_curve.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) F1 curve\ntry:\n    if epochs.size and train_f1.size and val_f1.size:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"spr_bench F1 Curve\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"spr_bench_f1_curve.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 3) Confusion matrix (heat-map)\ntry:\n    if preds.size and gts.size and preds.shape[0] == gts.shape[0]:\n        num_labels = int(max(max(preds), max(gts)) + 1)\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for p, g in zip(preds, gts):\n            cm[g, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"spr_bench Confusion Matrix\\nLeft: Ground Truth (rows), Right: Predictions (cols)\"\n        )\n        fname = os.path.join(working_dir, \"spr_bench_confusion_matrix.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment results\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nrun = experiment_data.get(\"no_char_bigram_count\", {}).get(\"spr_bench\", {})\n\n\n# helper\ndef safe_get(dic, *keys, default=None):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic if dic else default\n\n\nepochs = safe_get(run, \"epochs\", default=[])\ntrain_loss = safe_get(run, \"losses\", \"train\", default=[])\nval_loss = safe_get(run, \"losses\", \"val\", default=[])\ntrain_f1 = safe_get(run, \"metrics\", \"train_f1\", default=[])\nval_f1 = safe_get(run, \"metrics\", \"val_f1\", default=[])\npreds = safe_get(run, \"predictions\", default=[])\ngts = safe_get(run, \"ground_truth\", default=[])\ntest_f1 = safe_get(run, \"metrics\", \"test_f1\", default=None)\n\n# ------------------------------------------------------------------\n# 1) Loss curves\n# ------------------------------------------------------------------\ntry:\n    if epochs and train_loss and val_loss:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"spr_bench: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) F1 curves\n# ------------------------------------------------------------------\ntry:\n    if epochs and train_f1 and val_f1:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"spr_bench: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_f1_curves.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Confusion matrix\n# ------------------------------------------------------------------\ntry:\n    if preds and gts:\n        num_labels = max(max(preds), max(gts)) + 1\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\n            \"spr_bench: Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# print evaluation metric\n# ------------------------------------------------------------------\nif test_f1 is not None:\n    print(f\"Test Macro-F1: {test_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------ load experiment data -------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ned = experiment_data.get(\"count_only\", {}).get(\"spr_bench\", {})\n\nepochs = ed.get(\"epochs\", [])\ntrain_loss = ed.get(\"losses\", {}).get(\"train\", [])\nval_loss = ed.get(\"losses\", {}).get(\"val\", [])\ntrain_f1 = ed.get(\"metrics\", {}).get(\"train_f1\", [])\nval_f1 = ed.get(\"metrics\", {}).get(\"val_f1\", [])\ntest_f1 = ed.get(\"metrics\", {}).get(\"test_f1\", None)\npreds = ed.get(\"predictions\", [])\ngts = ed.get(\"ground_truth\", [])\n\n# ----------------------------- plots -----------------------------------\n# 1. Loss curve\ntry:\n    if epochs and train_loss and val_loss:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"spr_bench_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. F1 curve\ntry:\n    if epochs and train_f1 and val_f1:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train F1\")\n        plt.plot(epochs, val_f1, label=\"Validation F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"spr_bench_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 3. Test vs Best-Val F1 bar\ntry:\n    if test_f1 is not None and val_f1:\n        best_val = max(val_f1)\n        plt.figure()\n        plt.bar([\"Best Val\", \"Test\"], [best_val, test_f1], color=[\"skyblue\", \"salmon\"])\n        for i, v in enumerate([best_val, test_f1]):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH: Best Validation vs Test Macro-F1\")\n        fname = os.path.join(working_dir, \"spr_bench_f1_bar.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 bar: {e}\")\n    plt.close()\n\n# 4. Confusion matrix\ntry:\n    if preds and gts:\n        import itertools\n\n        labels = sorted(list(set(gts)))\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for p, g in zip(preds, gts):\n            cm[g][p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(labels)\n        plt.yticks(labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH: Confusion Matrix (Test Set)\")\n        # annotate\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n        fname = os.path.join(working_dir, \"spr_bench_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ----------------------------- log -------------------------------------\nif test_f1 is not None:\n    print(f\"Best Val F1: {max(val_f1):.4f} | Test F1: {test_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ensure working directory exists\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ned = experiment_data.get(\"bigram_only\", {})\nepochs = np.array(ed.get(\"epochs\", []))\ntr_loss = np.array(ed.get(\"losses\", {}).get(\"train\", []))\nval_loss = np.array(ed.get(\"losses\", {}).get(\"val\", []))\ntr_f1 = np.array(ed.get(\"metrics\", {}).get(\"train_f1\", []))\nval_f1 = np.array(ed.get(\"metrics\", {}).get(\"val_f1\", []))\npreds = np.array(ed.get(\"predictions\", []))\ngts = np.array(ed.get(\"ground_truth\", []))\n\n# ------------------------------------------------------------------\n# 1) Loss curves\n# ------------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, tr_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"bigram_only: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"bigram_only_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) F1 curves\n# ------------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n    plt.plot(epochs, val_f1, label=\"Val Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"bigram_only: Training vs Validation Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"bigram_only_f1_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Confusion matrix (only if predictions exist)\n# ------------------------------------------------------------------\ntry:\n    if preds.size and gts.size:\n        num_labels = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"bigram_only: Test Confusion Matrix\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"bigram_only_confusion_matrix.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ned = experiment_data.get(\"no_transformer_context\", {}).get(\"spr_bench\", {})\n\nepochs = ed.get(\"epochs\", [])\ntr_loss = ed.get(\"losses\", {}).get(\"train\", [])\nval_loss = ed.get(\"losses\", {}).get(\"val\", [])\ntr_f1 = ed.get(\"metrics\", {}).get(\"train_f1\", [])\nval_f1 = ed.get(\"metrics\", {}).get(\"val_f1\", [])\ntest_f1 = ed.get(\"metrics\", {}).get(\"test_f1\", None)\npreds = ed.get(\"predictions\", [])\ngts = ed.get(\"ground_truth\", [])\n\n# ------------------------------------------------------------------\n# 1) Train / Val loss curve\n# ------------------------------------------------------------------\ntry:\n    if epochs and tr_loss and val_loss:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR-BENCH: Train vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"spr_bench_train_val_loss_curve.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Train / Val F1 curve\n# ------------------------------------------------------------------\ntry:\n    if epochs and tr_f1 and val_f1:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR-BENCH: Train vs Validation Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"spr_bench_train_val_f1_curve.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Final Test vs Best-Val F1 bar\n# ------------------------------------------------------------------\ntry:\n    if test_f1 is not None and val_f1:\n        plt.figure()\n        bars = [\"Best Val F1\", \"Test F1\"]\n        vals = [max(val_f1), test_f1]\n        plt.bar(bars, vals, color=[\"steelblue\", \"orange\"])\n        plt.ylim(0, 1)\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.title(\"SPR-BENCH: Validation vs Test Macro-F1\")\n        fname = os.path.join(working_dir, \"spr_bench_val_vs_test_f1_bar.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 4) Confusion matrix\n# ------------------------------------------------------------------\ntry:\n    if preds and gts:\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        num_labels = cm.shape[0]\n        ticks = np.arange(num_labels)\n        plt.xticks(ticks, ticks)\n        plt.yticks(ticks, ticks)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"SPR-BENCH: Confusion Matrix (Test Set)\")\n        for i in range(num_labels):\n            for j in range(num_labels):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    fontsize=8,\n                )\n        fname = os.path.join(working_dir, \"spr_bench_confusion_matrix.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    edict = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = edict[\"cls_pooling\"][\"spr_bench\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed is not None:\n    epochs = ed[\"epochs\"]\n    tr_loss = ed[\"losses\"][\"train\"]\n    val_loss = ed[\"losses\"][\"val\"]\n    tr_f1 = ed[\"metrics\"][\"train_f1\"]\n    val_f1 = ed[\"metrics\"][\"val_f1\"]\n    test_f1 = ed[\"metrics\"][\"test_f1\"]\n    preds = ed.get(\"predictions\", [])\n    gts = ed.get(\"ground_truth\", [])\n\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"spr_bench Loss Curves\\nLeft: Training, Right: Validation\")\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        if test_f1 is not None:\n            plt.hlines(\n                test_f1,\n                xmin=epochs[0],\n                xmax=epochs[-1],\n                colors=\"r\",\n                linestyles=\"dashed\",\n                label=f\"Test={test_f1:.3f}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"spr_bench F1 Curves\\nLeft: Training, Right: Validation\")\n        plt.legend()\n        plt.ylim(0, 1)\n        plt.grid(True, alpha=0.3)\n        fname = os.path.join(working_dir, \"spr_bench_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix (optional)\n    if preds and gts:\n        try:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(\n                \"spr_bench Normalized Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.tight_layout()\n            fname = os.path.join(working_dir, \"spr_bench_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n"], "plot_plan": ["Load the stored experiment_data.npy, retrieve the single run \u201cchar_bigram_count\u201d, and pull out epochs, loss arrays, macro-F1 arrays, predictions and ground-truth. Build three visualizations: (1) a two-panel loss curve (train vs. val), (2) a two-panel macro-F1 curve (train vs. val), and (3) a confusion-matrix heat-map on the test set (limited to the first 5\u00d75 block if classes >5 to keep the figure readable). Wrap each plot in its own try-except so that a failure in one plot doesn\u2019t break the rest, and always close figures. Name each file descriptively and save under working_dir. Finally, print the stored test macro-F1 so the script outputs a numeric metric as well. All plotting uses plain matplotlib, no simulated data, and we use imshow for the heat-map to stay within basic matplotlib functionality.", "We will load experiment_data.npy, extract the SPR_BENCH sub-dictionary, and pull out the epoch list together with the recorded training / validation losses and macro-F1 scores. Using matplotlib we will first draw a loss curve, then an F1 curve, each with clear legends and subtitles describing the dataset (SPR_BENCH) and the plotted quantities. If test metrics are present we\u2019ll indicate them as horizontal dashed lines. Third, if predictions and ground-truth arrays are non-empty we will compute a confusion matrix with sklearn and visualise it with imshow, adding numeric annotations for clarity. Each figure will be produced in its own try-except block, saved to working_dir with descriptive filenames, and the figure will be closed in both success and failure cases to avoid resource leaks. We abide strictly by the data that exists inside experiment_data.npy and create no synthetic values. No more than three figures are generated, well under the limit of five. All plotting code starts with the required imports and ensures the working_dir exists before saving. Finally the script prints a short summary of saved files and closes cleanly.", "The code will load the saved experiment_data.npy, focus on the entry stored under the No-Bigram model for SPR-BENCH, and pull out epochs, losses, F1 scores, predictions and ground-truth labels. It will then make at most three basic matplotlib plots: (1) training vs. validation loss across epochs, (2) training vs. validation macro-F1 across epochs, and (3) a confusion-matrix style heat-map comparing ground-truth and model predictions on the test set. Each plot is wrapped in its own try-except so that failure in one plot does not affect the others, and every figure is closed after saving. Titles will clearly state the dataset (SPR-BENCH) and the content of the figure; file names will be equally descriptive. Figures are saved into the existing working directory that the training script already created. Only data coming directly from experiment_data.npy are used\u2014no synthetic points or extra epochs are fabricated. The script prints nothing except error messages (if any) and automatically creates working_dir when missing.", "We will load experiment_data.npy from the working directory, extract the sole split (\u201cspr_bench\u201d) under the \u201cno_positional_embedding\u201d ablation, and retrieve epochs, train/val losses, train/val F1, and the test-set predictions / ground-truth labels.  \nThree plots will be generated (\u22645 as required):  \n1) Loss curve: epochs vs. train and validation loss.  \n2) F1 curve: epochs vs. train and validation macro-F1.  \n3) Confusion-matrix heat-map on the test set (if both predictions and ground_truth exist and have matching length).  \nEach plot will be wrapped in its own try-except block, employ clear titles/sub-titles indicating dataset and content, and be saved into working_dir using descriptive filenames.  \nMatplotlib figures are always closed after saving (or after an error) to prevent memory leaks.  \nWe use only the data present in experiment_data.npy\u2014no synthetic values.  \nA maximum of five figures is respected, and the code is concise, fully self-contained, and starts with the mandatory imports.", "We will load the stored experiment_data.npy, extract the single run under key no_char_bigram_count \u2192 spr_bench, and retrieve loss, macro-F1 and prediction arrays. Three plots are produced: (1) training and validation loss versus epoch, (2) training and validation macro-F1 versus epoch, and (3) a confusion-matrix heat-map built from final test predictions. Each figure is wrapped in its own try-except block, saved into working_dir with descriptive names, titled clearly with dataset name and subtitles, and the figure is always closed. We only draw data that exist (checking length before plotting) and never exceed five figures. Finally the script prints the stored test macro-F1 so the user sees the main evaluation metric. All requirements for imports, directory handling, figure management and naming conventions are respected. The code is concise and self-contained\u2014just run it after the experiment has finished and saved experiment_data.npy.", "We load the stored experiment_data.npy, isolate the \u201ccount_only/spr_bench\u201d entry, and extract epochs, train/val losses, train/val F1 scores, test F1, ground-truth labels, and predictions. Four figures are generated: (1) a line plot of training vs. validation loss over epochs, (2) a line plot of training vs. validation macro-F1 over epochs, (3) a single-bar plot comparing test macro-F1 with the best validation F1, and (4) a confusion-matrix heat-map for the test set. Each figure is created in its own try-except block, saved into working_dir with descriptive names that include the dataset (\u201cspr_bench\u201d) and plot type, titled appropriately, and the figure is always closed afterward. We respect the five-figure ceiling by limiting ourselves to these four essentials. All plotting relies strictly on the values already stored in experiment_data.npy; nothing is synthesized. The script begins with the mandated imports, ensures the working directory exists, and prints the metrics after plotting so users can verify numbers in the logs. Finally, everything is wrapped in concise, fail-safe code so any missing keys or empty arrays are caught and reported without halting the run.", "We will first load experiment_data.npy, extract the bigram-only sub-dictionary, and pull out the training/validation losses and F1 scores along with the epoch indices.  Using these arrays we will create two simple line plots: (1) loss vs. epoch and (2) macro-F1 vs. epoch, with both training and validation curves shown.  If prediction and label arrays are present, we will also build a confusion-matrix heat-map for the test set.  Every figure will be generated inside an individual try-except so that failures in one plot do not halt the rest; each figure is saved to working_dir with descriptive filenames that include \u201cbigram_only\u201d and the plot type.  We cap the epoch axis to the actual number of recorded epochs (\u226415) so no extra sampling logic is needed.  Titles and axis labels are added for clarity, and figures are always closed in a finally or after save.  Basic matplotlib is used without any custom styles, adhering strictly to the available data.  Finally, the script prints the saved filenames so a user can easily locate the outputs.", "We need to load the saved experiment_data.npy, pick the SPR-BENCH section for the no-transformer ablation, and visualize: (1) training vs validation loss across epochs, (2) training vs validation macro-F1 across epochs, (3) a bar chart comparing final test F1 to the best validation F1, and (4) a confusion-matrix heat-map on the test split.  Each figure is produced inside its own try-except, saved with a descriptive filename that includes the dataset name and plot type, then closed.  All plots are written to \u201cworking/\u201d.  We guard every plot with presence checks so we never access missing keys, respect the 5-figure limit, and stick to plain matplotlib defaults.", "We first load experiment_data.npy and safely unwrap the nested dict to reach the single run stored at experiment_data['cls_pooling']['spr_bench']. From this we pull the epoch list, loss dictionaries, and metric dictionaries in order to build two time\u2013series plots: (1) training vs. validation loss per epoch and (2) training vs. validation macro-F1 per epoch, adding the test-set F1 as a horizontal dashed line for context. If prediction and ground-truth arrays are non-empty, we add a third figure containing a normalized confusion matrix to give a qualitative view of classification errors. Every plot is wrapped in its own try-except so a failure in one will not abort the rest, and each figure is closed after saving. Filenames include the dataset (spr_bench) and plot type, and all plots are written to the working directory created by the experiment script. We keep the code concise, respect the 5-figure limit, and never fabricate data\u2014everything plotted comes straight from experiment_data.npy. The code adheres to the required imports, path handling, titles, subtitles, and figure-closing conventions."], "ablation_name": [null, "No-Count-Vector Pathway (CBC \u2192 CB Transformer)", "No-Bigram-Embedding Pathway (C + C Transformer)", "No-Positional-Embedding (CBC-w/o-Pos)", "No-Char-Embedding Pathway (Bigram + Count Transformer)", "Count-Only (Bag-of-Characters) Model", "Bigram-Only Transformer (B-Only)", "No-Transformer-Context (Mean-Pool Embedding + Count)", "CBC-CLS (CLS-Token Pooling instead of Mean Pool)"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["We load the saved experiment_data.npy from the \u201cworking\u201d folder, turn it back\ninto a Python dict, then iterate over every dataset key it contains.   For each\ndataset we retrieve the loss arrays (training, validation) and F1 arrays and\ncompute:   \u2022 best (minimum) training loss and best (minimum) validation loss   \u2022\nfinal training F1 score, best (maximum) validation F1 score, and test F1 score\n\u2022 final test loss (already stored as a scalar).   We then print the dataset name\nfollowed by each clearly-labeled metric value.", "The script will load the saved NumPy file from the working directory, convert it\nback to a Python dictionary, and iterate through every experiment and dataset it\ncontains. For each dataset, it will compute the best (maximum) F1 scores and the\nlowest losses observed during training and validation, and print those along\nwith the single test metrics already stored. Metrics are labelled explicitly so\nit is clear which split and measure each value corresponds to. The code is\nwritten at the top level so that it executes immediately when run.", "The script will load working/experiment_data.npy, navigate to the key hierarchy\n(model \u2192 dataset), and read the lists of F1 scores and losses. For\ntrain/validation metrics it prints the last recorded value (i.e., the final\nepoch), while test metrics are single scalars. Each metric is printed with a\nclear, descriptive label after first printing the dataset name.", "Below is the plan: Load experiment_data.npy from the \u201cworking\u201d directory, walk\nthrough every experiment entry, then every dataset entry, and finally print only\nthe final (last) or single value for each stored metric/loss with explicit,\nself-describing names. No figures are produced and the script executes\nimmediately.", "The script will load the stored numpy dictionary from the working directory,\niterate over every experiment and nested dataset, and then print the most\nrelevant values: the final training loss, the minimum validation loss, the best\nvalidation F1 score, and the held-out test F1 score (if present). Each dataset\u2019s\nname is shown before its metrics, and each metric is reported with an explicit,\ndescriptive label so there is no ambiguity about what is being displayed.", "The script loads the saved NumPy dictionary, navigates its nested structure,\ncomputes the final (or best, when relevant) values for each recorded metric, and\nprints them with explicit, descriptive names. It automatically handles multiple\nexperiments or datasets if they exist in the file.", "The script will locate the working directory, load the saved experiment_data.npy\nfile into a Python dictionary, and iterate through each recorded experiment\n(e.g., \"bigram_only\"). For every experiment it will extract the final training\nloss/F1, the final validation loss, the best (maximum) validation F1 score, and\nthe stored test loss and test F1 score. Each experiment\u2019s name is printed first,\nfollowed by clearly-labeled metric/value pairs. All logic sits at top level so\nthe file runs immediately when executed, and no plots or special entry points\nare used.", "The script will: (1) locate the working directory created by the training\nscript; (2) load the serialized experiment_data dictionary from\nexperiment_data.npy; (3) iterate through every experiment and nested dataset;\n(4) extract the final training loss/F1, the best validation loss/F1, and the\nstored test loss/F1; (5) print them with clear metric names exactly as required.\nEverything runs at import-time, with no \u201cmain\u201d guard.", "The script will (1) locate the working directory created by the training script,\n(2) load experiment_data.npy, (3) iterate over every dataset inside it, and (4)\nprint the final training metrics, the best validation metric, and the test\nmetrics with explicit, descriptive labels. All logic is placed at global scope\nso the file runs immediately."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------------\n# locate and load experiment data\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------\n# helper to print metrics\n# -------------------------------------------------------\ndef print_metrics(name: str, record: dict):\n    losses = record[\"losses\"]\n    metrics = record[\"metrics\"]\n\n    # training losses / F1\n    best_train_loss = min(losses[\"train\"]) if losses[\"train\"] else None\n    best_val_loss = min(losses[\"val\"]) if losses[\"val\"] else None\n    test_loss = losses[\"test\"]\n\n    final_train_f1 = metrics[\"train_f1\"][-1] if metrics[\"train_f1\"] else None\n    best_val_f1 = max(metrics[\"val_f1\"]) if metrics[\"val_f1\"] else None\n    test_f1 = metrics[\"test_f1\"]\n\n    print(f\"Dataset: {name}\")\n    if best_train_loss is not None:\n        print(f\"  best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"  best validation loss: {best_val_loss:.4f}\")\n    if test_loss is not None:\n        print(f\"  test loss: {test_loss:.4f}\")\n\n    if final_train_f1 is not None:\n        print(f\"  final training F1 score: {final_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"  best validation F1 score: {best_val_f1:.4f}\")\n    if test_f1 is not None:\n        print(f\"  test F1 score: {test_f1:.4f}\")\n    print()  # blank line for readability\n\n\n# -------------------------------------------------------\n# iterate through each dataset in the dict\n# -------------------------------------------------------\nfor ds_name, ds_record in experiment_data.items():\n    print_metrics(ds_name, ds_record)\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Load the experiment results\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------\n# Helper for safe aggregation\n# -----------------------------------------------------------------------------\ndef best_value(values, mode=\"max\"):\n    \"\"\"\n    Return the best value from a list according to the mode.\n    mode == 'max' -> maximum, mode == 'min' -> minimum.\n    If the list is empty, returns None.\n    \"\"\"\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\n# -----------------------------------------------------------------------------\n# Iterate and display metrics\n# -----------------------------------------------------------------------------\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, res in datasets.items():\n        print(f\"Dataset: {dataset_name}  (Ablation: {ablation_name})\")\n\n        # F1 scores\n        best_train_f1 = best_value(res[\"metrics\"][\"train_f1\"], mode=\"max\")\n        best_val_f1 = best_value(res[\"metrics\"][\"val_f1\"], mode=\"max\")\n        test_f1 = res[\"metrics\"][\"test_f1\"]\n\n        # Losses\n        lowest_train_loss = best_value(res[\"losses\"][\"train\"], mode=\"min\")\n        lowest_val_loss = best_value(res[\"losses\"][\"val\"], mode=\"min\")\n        test_loss = res[\"losses\"][\"test\"]\n\n        # Print metrics with explicit labels\n        if best_train_f1 is not None:\n            print(f\"  Best training F1 score:      {best_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"  Best validation F1 score:    {best_val_f1:.4f}\")\n        if test_f1 is not None:\n            print(f\"  Test F1 score:              {test_f1:.4f}\")\n\n        if lowest_train_loss is not None:\n            print(f\"  Lowest training loss:        {lowest_train_loss:.4f}\")\n        if lowest_val_loss is not None:\n            print(f\"  Lowest validation loss:      {lowest_val_loss:.4f}\")\n        if test_loss is not None:\n            print(f\"  Test loss:                   {test_loss:.4f}\")\n\n        # Spacer between datasets\n        print()\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------\n# locate and load experiment data\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -------------------------------------------------------\n# iterate over all stored runs and datasets\n# -------------------------------------------------------\nfor run_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # --- F1 scores ---\n        train_f1_list = data[\"metrics\"].get(\"train_f1\", [])\n        val_f1_list = data[\"metrics\"].get(\"val_f1\", [])\n        test_f1 = data[\"metrics\"].get(\"test_f1\", None)\n\n        if train_f1_list:\n            print(f\"train F1 score: {train_f1_list[-1]:.4f}\")\n        if val_f1_list:\n            print(f\"validation F1 score: {val_f1_list[-1]:.4f}\")\n        if test_f1 is not None:\n            print(f\"test F1 score: {test_f1:.4f}\")\n\n        # --- Losses ---\n        train_loss_list = data[\"losses\"].get(\"train\", [])\n        val_loss_list = data[\"losses\"].get(\"val\", [])\n        test_loss = data[\"losses\"].get(\"test\", None)\n\n        if train_loss_list:\n            print(f\"train loss: {train_loss_list[-1]:.6f}\")\n        if val_loss_list:\n            print(f\"validation loss: {val_loss_list[-1]:.6f}\")\n        if test_loss is not None:\n            print(f\"test loss: {test_loss:.6f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------\n# Locate and load the saved experiment file\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------\n# Helper to fetch \u201cfinal\u201d value in a list\n# -------------------------------------------------------\ndef last_or_none(seq):\n    \"\"\"Return final element in list or None if list empty/None.\"\"\"\n    if isinstance(seq, list) and seq:\n        return seq[-1]\n    return seq  # might already be scalar (e.g., None)\n\n\n# -------------------------------------------------------\n# Iterate through experiments and datasets\n# -------------------------------------------------------\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, info in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # ----- losses -----\n        train_loss = last_or_none(info[\"losses\"].get(\"train\"))\n        val_loss = last_or_none(info[\"losses\"].get(\"val\"))\n        test_loss = info[\"losses\"].get(\"test\")\n\n        if train_loss is not None:\n            print(f\"Final training loss: {train_loss:.4f}\")\n        if val_loss is not None:\n            print(f\"Final validation loss: {val_loss:.4f}\")\n        if test_loss is not None:\n            print(f\"Test loss: {test_loss:.4f}\")\n\n        # ----- F1 metrics -----\n        train_f1 = last_or_none(info[\"metrics\"].get(\"train_f1\"))\n        val_f1 = last_or_none(info[\"metrics\"].get(\"val_f1\"))\n        test_f1 = info[\"metrics\"].get(\"test_f1\")\n\n        if train_f1 is not None:\n            print(f\"Final training macro F1 score: {train_f1:.4f}\")\n        if val_f1 is not None:\n            print(f\"Final validation macro F1 score: {val_f1:.4f}\")\n        if test_f1 is not None:\n            print(f\"Test macro F1 score: {test_f1:.4f}\")\n\n        # Separate outputs for readability\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(npy_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {npy_path}\")\n\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef _safe_last(lst):\n    return lst[-1] if lst else None\n\n\ndef _safe_min(lst):\n    return min(lst) if lst else None\n\n\ndef _safe_max(lst):\n    return max(lst) if lst else None\n\n\n# ------------------------------------------------------------------\n# iterate through experiments and datasets, printing metrics\n# ------------------------------------------------------------------\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # losses\n        final_train_loss = _safe_last(data[\"losses\"].get(\"train\", []))\n        best_val_loss = _safe_min(data[\"losses\"].get(\"val\", []))\n        test_loss = data[\"losses\"].get(\"test\")\n\n        # F1 scores\n        best_val_f1 = _safe_max(data[\"metrics\"].get(\"val_f1\", []))\n        final_train_f1 = _safe_last(data[\"metrics\"].get(\"train_f1\", []))\n        test_f1 = data[\"metrics\"].get(\"test_f1\")\n\n        # print only if the metric exists\n        if final_train_loss is not None:\n            print(f\"Final training loss: {final_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"Best validation loss: {best_val_loss:.4f}\")\n        if final_train_f1 is not None:\n            print(f\"Final training F1 score: {final_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n        if test_loss is not None:\n            print(f\"Test loss: {test_loss:.4f}\")\n        if test_f1 is not None:\n            print(f\"Test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------- load data ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------- helper funcs --------------------------\ndef safe_last(lst):\n    return lst[-1] if lst else None\n\n\ndef safe_max(lst):\n    return max(lst) if lst else None\n\n\ndef safe_min(lst):\n    return min(lst) if lst else None\n\n\n# ---------------------------- iterate & report ----------------------\nfor experiment_name, experiment_dict in experiment_data.items():\n    for dataset_name, data in experiment_dict.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # ---------------- metrics ----------------\n        metrics = data.get(\"metrics\", {})\n        train_f1_final = safe_last(metrics.get(\"train_f1\", []))\n        val_f1_best = safe_max(metrics.get(\"val_f1\", []))\n        test_f1 = metrics.get(\"test_f1\")\n\n        if train_f1_final is not None:\n            print(f\"Final train F1 score: {train_f1_final:.4f}\")\n        if val_f1_best is not None:\n            print(f\"Best validation F1 score: {val_f1_best:.4f}\")\n        if test_f1 is not None:\n            print(f\"Test F1 score: {test_f1:.4f}\")\n\n        # ---------------- losses -----------------\n        losses = data.get(\"losses\", {})\n        train_loss_final = safe_last(losses.get(\"train\", []))\n        val_loss_best = safe_min(losses.get(\"val\", []))\n        test_loss = losses.get(\"test\")\n\n        if train_loss_final is not None:\n            print(f\"Final train loss: {train_loss_final:.6f}\")\n        if val_loss_best is not None:\n            print(f\"Best validation loss: {val_loss_best:.6f}\")\n        if test_loss is not None:\n            print(f\"Test loss: {test_loss:.6f}\")\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper to safely fetch list elements\n# ------------------------------------------------------------------\ndef last_or_none(seq):\n    return seq[-1] if seq else None\n\n\n# ------------------------------------------------------------------\n# iterate through experiments and print metrics\n# ------------------------------------------------------------------\nfor exp_name, exp_dict in experiment_data.items():\n    print(f\"Dataset: {exp_name}\")\n\n    # ----- losses -----\n    final_train_loss = last_or_none(exp_dict[\"losses\"].get(\"train\", []))\n    final_val_loss = last_or_none(exp_dict[\"losses\"].get(\"val\", []))\n    test_loss = exp_dict[\"losses\"].get(\"test\")\n\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.6f}\")\n    if final_val_loss is not None:\n        print(f\"Final validation loss: {final_val_loss:.6f}\")\n    if test_loss is not None:\n        print(f\"Test loss: {test_loss:.6f}\")\n\n    # ----- F1 scores -----\n    final_train_f1 = last_or_none(exp_dict[\"metrics\"].get(\"train_f1\", []))\n    val_f1_list = exp_dict[\"metrics\"].get(\"val_f1\", [])\n    best_val_f1 = max(val_f1_list) if val_f1_list else None\n    test_f1 = exp_dict[\"metrics\"].get(\"test_f1\")\n\n    if final_train_f1 is not None:\n        print(f\"Final training F1 score: {final_train_f1:.6f}\")\n    if best_val_f1 is not None:\n        print(f\"Best validation F1 score: {best_val_f1:.6f}\")\n    if test_f1 is not None:\n        print(f\"Test F1 score: {test_f1:.6f}\")\n\n    print(\"\")  # blank line between datasets\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------\n# locate and load experiment results\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------\n# helper for printing one dataset block\n# -------------------------------------------------------\ndef print_metrics(exp_name: str, ds_name: str, data: dict):\n    losses = data[\"losses\"]\n    metrics = data[\"metrics\"]\n\n    # final or best values\n    train_loss_final = losses[\"train\"][-1] if losses[\"train\"] else None\n    val_loss_best = min(losses[\"val\"]) if losses[\"val\"] else None\n    test_loss = losses[\"test\"]\n\n    train_f1_final = metrics[\"train_f1\"][-1] if metrics[\"train_f1\"] else None\n    val_f1_best = max(metrics[\"val_f1\"]) if metrics[\"val_f1\"] else None\n    test_f1 = metrics[\"test_f1\"]\n\n    print(f\"Dataset: {ds_name}  (experiment: {exp_name})\")\n    if train_loss_final is not None:\n        print(f\"  final training loss: {train_loss_final:.4f}\")\n    if val_loss_best is not None:\n        print(f\"  best validation loss: {val_loss_best:.4f}\")\n    if test_loss is not None:\n        print(f\"  test loss: {test_loss:.4f}\")\n\n    if train_f1_final is not None:\n        print(f\"  final training F1 score: {train_f1_final:.4f}\")\n    if val_f1_best is not None:\n        print(f\"  best validation F1 score: {val_f1_best:.4f}\")\n    if test_f1 is not None:\n        print(f\"  test F1 score: {test_f1:.4f}\")\n    print(\"\")  # blank line for readability\n\n\n# -------------------------------------------------------\n# iterate over all experiments / datasets\n# -------------------------------------------------------\nfor exp_name, exp_dict in experiment_data.items():\n    for ds_name, ds_data in exp_dict.items():\n        print_metrics(exp_name, ds_name, ds_data)\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------\n# locate and load the saved experiment data\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# -------------------------------------------------------\n# helper for safe indexing\n# -------------------------------------------------------\ndef _last(lst):\n    return lst[-1] if lst else None\n\n\ndef _best(lst, maximize=True):\n    if not lst:\n        return None\n    return max(lst) if maximize else min(lst)\n\n\n# -------------------------------------------------------\n# iterate and print metrics\n# -------------------------------------------------------\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(dataset_name)  # dataset header\n\n        # F1 scores\n        train_f1_final = _last(data[\"metrics\"].get(\"train_f1\", []))\n        val_f1_best = _best(data[\"metrics\"].get(\"val_f1\", []), maximize=True)\n        test_f1 = data[\"metrics\"].get(\"test_f1\")\n\n        # Losses\n        train_loss_final = _last(data[\"losses\"].get(\"train\", []))\n        val_loss_final = _last(data[\"losses\"].get(\"val\", []))\n        test_loss = data[\"losses\"].get(\"test\")\n\n        if train_f1_final is not None:\n            print(f\"training F1 score: {train_f1_final:.4f}\")\n        if val_f1_best is not None:\n            print(f\"validation F1 score (best): {val_f1_best:.4f}\")\n        if test_f1 is not None:\n            print(f\"test F1 score: {test_f1:.4f}\")\n\n        if train_loss_final is not None:\n            print(f\"training loss: {train_loss_final:.4f}\")\n        if val_loss_final is not None:\n            print(f\"validation loss: {val_loss_final:.4f}\")\n        if test_loss is not None:\n            print(f\"test loss: {test_loss:.4f}\")\n\n        print()  # blank line between datasets for readability\n"], "parse_term_out": ["['Dataset: char_bigram_count', '\\n', '  best training loss: 0.5229', '\\n', '\nbest validation loss: 0.5160', '\\n', '  test loss: 0.5056', '\\n', '  final\ntraining F1 score: 0.7925', '\\n', '  best validation F1 score: 0.7959', '\\n', '\ntest F1 score: 0.7980', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH  (Ablation: char_bigram_only)', '\\n', '  Best training F1\nscore:      0.7950', '\\n', '  Best validation F1 score:    0.7959', '\\n', '\nTest F1 score:              0.7950', '\\n', '  Lowest training loss:\n0.5143', '\\n', '  Lowest validation loss:      0.5220', '\\n', '  Test loss:\n0.5175', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: spr_bench', '\\n', 'train F1 score: 0.7815', '\\n', 'validation F1\nscore: 0.7939', '\\n', 'test F1 score: 0.7920', '\\n', 'train loss: 0.561486',\n'\\n', 'validation loss: 0.519403', '\\n', 'test loss: 0.518342', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: spr_bench', '\\n', 'Final training loss: 0.5974', '\\n', 'Final\nvalidation loss: 0.5427', '\\n', 'Test loss: 0.5496', '\\n', 'Final training macro\nF1 score: 0.7504', '\\n', 'Final validation macro F1 score: 0.7878', '\\n', 'Test\nmacro F1 score: 0.7910', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: spr_bench', '\\n', 'Final training loss: 0.5429', '\\n', 'Best\nvalidation loss: 0.5146', '\\n', 'Final training F1 score: 0.7775', '\\n', 'Best\nvalidation F1 score: 0.7959', '\\n', 'Test loss: 0.5161', '\\n', 'Test F1 score:\n0.7980', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: spr_bench', '\\n', 'Final train F1 score: 0.7318', '\\n', 'Best\nvalidation F1 score: 0.7840', '\\n', 'Test F1 score: 0.7879', '\\n', 'Final train\nloss: 0.615817', '\\n', 'Best validation loss: 0.567000', '\\n', 'Test loss:\n0.559528', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Dataset: bigram_only', '\\n', 'Final training loss: 0.510434', '\\n', 'Final\nvalidation loss: 0.522263', '\\n', 'Test loss: 0.514678', '\\n', 'Final training\nF1 score: 0.796984', '\\n', 'Best validation F1 score: 0.795948', '\\n', 'Test F1\nscore: 0.794990', '\\n', '', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: spr_bench  (experiment: no_transformer_context)', '\\n', '  final\ntraining loss: 0.5575', '\\n', '  best validation loss: 0.5466', '\\n', '  test\nloss: 0.5362', '\\n', '  final training F1 score: 0.7494', '\\n', '  best\nvalidation F1 score: 0.7800', '\\n', '  test F1 score: 0.7829', '\\n', '', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['spr_bench', '\\n', 'training F1 score: 0.7935', '\\n', 'validation F1 score\n(best): 0.7959', '\\n', 'test F1 score: 0.7970', '\\n', 'training loss: 0.5303',\n'\\n', 'validation loss: 0.5838', '\\n', 'test loss: 0.5326', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"], "current_stage": "Stage_4"};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
