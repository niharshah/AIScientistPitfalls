DatasetDict({
    train: Dataset({
        features: ['id', 'sequence', 'label', 'tokens', 'num_tokens'],
        num_rows: 20000
    })
    dev: Dataset({
        features: ['id', 'sequence', 'label', 'tokens', 'num_tokens'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'sequence', 'label', 'tokens', 'num_tokens'],
        num_rows: 10000
    })
})
Dataset splits available: ['train', 'dev', 'test']
Vocabulary size (including PAD): 17
Using device: cpu

Starting training for Robust PolyRuleNet for SPR.
This experiment demonstrates a two-stage model:
 - Stage 1: Discrete Symbolic Tokenization using Gumbel-Softmax to create explicit, one-hot symbols from token embeddings.
 - Stage 2: Rule Induction via a lightweight MLP that aggregates symbolic representations to verify if the sequence meets a hidden poly-factor rule.
Evaluation uses Shape-Weighted Accuracy (SWA): accuracy weighted by the diversity of shapes (first character of tokens) in the sequence.


Epoch 1/2:
 - This epoch's average training loss reflects the model's convergence.
   Average Training Loss = 0.6884

Development Set Evaluation:
 - Binary Accuracy shows the raw fraction of correct predictions.
 - Shape-Weighted Accuracy (SWA) weights correctness by the diversity of shapes in each sequence (more unique shapes yield higher weight).
   Dev Binary Accuracy = 0.5846
   Dev SWA = 0.5827

Epoch 2/2:
 - This epoch's average training loss reflects the model's convergence.
   Average Training Loss = 0.6611

Development Set Evaluation:
 - Binary Accuracy shows the raw fraction of correct predictions.
 - Shape-Weighted Accuracy (SWA) weights correctness by the diversity of shapes in each sequence (more unique shapes yield higher weight).
   Dev Binary Accuracy = 0.5848
   Dev SWA = 0.5860

Figure_1.png saved: This figure displays the decrease in training loss over epochs, demonstrating model convergence.

Final Evaluation on Test Set:
 - Binary Accuracy reflects the overall correctness on unseen data.
 - Shape-Weighted Accuracy (SWA) emphasizes sequences with a richer diversity of shapes.
   Test Binary Accuracy = 0.6028
   Test SWA = 0.6058

Figure_2.png saved: This chart compares the SWA on the Dev and Test sets, showcasing the model's generalization capability.

Success: Model achieved non-zero accuracy on the test set.
