DatasetDict({
    train: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 20000
    })
    dev: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 10000
    })
})
Loaded dataset splits: DatasetDict({
    train: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 20000
    })
    dev: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 10000
    })
})
Vocabulary size: 16
Using device: cpu

Starting training. This loop aims to optimize the HIPA model for the SPR task.
Results include loss progression and Dev Shape-Weighted Accuracy (SWA), measuring performance considering token shape diversity.

Epoch 1/5:
  Average Train Loss = 0.6938269186019898
  Average Dev Loss = 0.6944368958473206
  Dev Shape-Weighted Accuracy (SWA) = 0.4736070381231672 

Epoch 2/5:
  Average Train Loss = 0.6934887752532959
  Average Dev Loss = 0.6936771774291992
  Dev Shape-Weighted Accuracy (SWA) = 0.4780058651026393 

Epoch 3/5:
  Average Train Loss = 0.6932518000602722
  Average Dev Loss = 0.6932686376571655
  Dev Shape-Weighted Accuracy (SWA) = 0.4838709677419355 

Epoch 4/5:
  Average Train Loss = 0.6931726861000062
  Average Dev Loss = 0.6931129503250122
  Dev Shape-Weighted Accuracy (SWA) = 0.5043988269794721 

Epoch 5/5:
  Average Train Loss = 0.6931136193275451
  Average Dev Loss = 0.6928163790702819
  Dev Shape-Weighted Accuracy (SWA) = 0.5263929618768328 


Final results on the Test set:
  Test Shape-Weighted Accuracy (SWA): 0.46088193456614507
Success: Non-zero accuracy achieved. The HIPA model functions correctly.

Figure_1.png saved: This figure displays the training and development loss curves, indicating learning convergence.
Figure_2.png saved: This figure shows the progression of Dev SWA, reflecting improvement in capturing token shape diversity over epochs.

Training, evaluation, and result visualization complete.
