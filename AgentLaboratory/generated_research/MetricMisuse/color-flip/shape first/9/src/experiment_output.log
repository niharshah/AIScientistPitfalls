Train split count: 20000
Dev split count: 5000
Test split count: 10000
Example train instance: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0}
Full dataset sizes:
 Train split count: 20000
 Dev split count: 5000
 Test split count: 10000
Example train instance: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0}
Using device: cpu

=== Stage 1 Pretraining ===
Objective: Pretrain the discrete tokenization module using a JEPA-inspired auxiliary contrastive loss.
Method: Two dropout-augmented views of the token representations are generated and their MSE is minimized.

[Pretraining] Epoch 1/1 completed. Average Aux Loss: 0.0114

=== Stage 2 Finetuning ===
Objective: Jointly fine-tune the Transformer encoder and predicate reasoning head on labeled data.
Method: Optimize the sum of cross-entropy classification loss and auxiliary contrastive loss.

[Finetuning] Epoch 1/1 completed. Joint Loss: 0.7113
Dev Evaluation: Assessing binary classification performance weighted by unique shape counts (SWA).
Development Set Shape-Weighted Accuracy (SWA): 45.59%


=== Final Evaluation on Test Set ===
Objective: Evaluate the final model on unseen test data using the SWA metric.

Final Test Shape-Weighted Accuracy (SWA): 48.31%

Figure_1.png saved: Displays the training loss progression demonstrating model convergence.

Figure_2.png saved: Shows the progression of Development Set SWA across finetuning epochs.

All experiments completed successfully. The figures and metrics demonstrate the model's performance trends on the SPR_BENCH benchmark.
