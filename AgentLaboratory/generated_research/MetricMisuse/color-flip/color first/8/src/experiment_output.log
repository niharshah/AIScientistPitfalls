Dataset splits loaded: ['train', 'dev', 'test']

Example from train split:
{'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0, 'color_variety': 3, 'shape_variety': 3}
Beginning experiments for SPR task. Each print statement details the experiment objective and expected outcomes.

Experiment A: Full Hierarchical VAE-Enhanced Transformer.
Objective: Utilize latent segmentation, atomic predicate extraction, and hierarchical composition to decide if an L-token sequence meets the hidden rule.

Model A - Epoch 1: Training Loss = 0.7483, Dev Accuracy = 0.4800
Model A - Epoch 2: Training Loss = 0.7157, Dev Accuracy = 0.4800
Final Test Accuracy for Model A: 0.5400

Experiment B: Baseline Transformer Classifier.
Objective: Provide a baseline by averaging transformer outputs without latent segmentation/symbolic extraction.

Model B - Epoch 1: Training Loss = 0.6763, Dev Accuracy = 0.5700
Model B - Epoch 2: Training Loss = 0.6677, Dev Accuracy = 0.4900
Final Test Accuracy for Model B: 0.5500

Experiment C: Variant Model without Hierarchical Composition.
Objective: Examine the impact of removing the hierarchical composition layer, simply averaging predicate scores.

Model C - Epoch 1: Training Loss = 0.7181, Dev Accuracy = 0.4800
Model C - Epoch 2: Training Loss = 0.6648, Dev Accuracy = 0.4600
Final Test Accuracy for Model C: 0.5300

Summary of final test accuracies (using Shape-Weighted Accuracy as the evaluation metric):
Model A (Full Hierarchical): Test Accuracy = 0.5400
Model B (Baseline Transformer): Test Accuracy = 0.5500
Model C (Without Hierarchical Composition): Test Accuracy = 0.5300
The above results compare the full hierarchical model with its ablation variants. The aim is to surpass the current SOTA baseline for SPR_BENCH.
