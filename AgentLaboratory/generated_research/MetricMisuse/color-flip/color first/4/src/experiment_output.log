Dataset splits loaded: ['train', 'dev', 'test']
Sample training instance: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0}
Using device: cpu

Loading SPR_BENCH dataset splits:
Dataset splits loaded: ['train', 'dev', 'test']
Sample training instance: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0}

Starting Data Preprocessing: Building vocabulary and encoding sequences.
Vocabulary size: 17
Maximum sequence length found: 6

Defining the SPR Model: A simple GRU-based classifier for Symbolic Pattern Recognition.
Model architecture:
SPRModel(
  (embedding): Embedding(17, 50)
  (gru): GRU(50, 64, batch_first=True)
  (fc): Linear(in_features=64, out_features=2, bias=True)
)

Starting Training Procedure:
This experiment trains on the Train split (subsampled if needed) to learn the mapping from an L-token sequence to its label (i.e. if the sequence satisfies the hidden target rule). Dev set accuracy (using SWA) is monitored for tuning.
Epoch 1 - Training Loss: 0.6649
Epoch 1 - Dev Accuracy (SWA metric): 0.6516
Epoch 1 - Dev Accuracy (CWA metric): 0.6684

Final Evaluation on Test Set:
The final experiment evaluates the trained model on the Test split using the SWA (Shape-Weighted Accuracy) metric, reflecting generalization to unseen data.
Final Test Accuracy (SWA metric): 0.5781
Final Test Accuracy (CWA metric): 0.5840

Generating Figure_1.png:
Figure_1.png saved: This figure displays the decrease in training loss over epochs, demonstrating model convergence.

Generating Figure_2.png:
Figure_2.png saved: This figure displays the confusion matrix, showing counts of correct and incorrect classifications on the test set.

Comparison with SOTA Baseline:
Our model achieved a Test SWA of 0.5781 (SOTA baseline: 0.7500) and a Test CWA of 0.5840 (SOTA baseline: 0.7000)
The model does not yet surpass the current SOTA performance on SPR_BENCH for the selected metric (SWA and CWA). Further tuning or model enhancements may be required.

All experiments complete.
