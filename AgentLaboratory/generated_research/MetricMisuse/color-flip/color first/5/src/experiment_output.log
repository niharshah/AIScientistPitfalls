Loaded dataset splits: ['train', 'dev', 'test']
Example from training set: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0}
Loaded dataset splits: ['train', 'dev', 'test']
Example from training set: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0}

Building vocabulary from training data (mapping symbolic tokens to integer indices)...
Vocabulary size (including PAD): 17

Constructing label mapping from training data...
Number of classes: 2

Preprocessing datasets: converting token sequences into padded integer id lists...
Completed dataset preprocessing.

Creating PyTorch datasets and dataloaders (using num_workers=0)...

Defining the LSTM-based classifier model for the SPR task.
The model embeds tokens, processes them through an LSTM, applies mean pooling over the sequence, and outputs class scores.

Using device: cpu

Starting training process. For each epoch, the average training loss is printed (demonstrating convergence) and development SWA (Shape-Weighted Accuracy) is evaluated (indicating generalization).
Epoch 1/2: Avg Training Loss = 0.6689, Dev Accuracy (SWA) = 0.7500
Epoch 2/2: Avg Training Loss = 0.6031, Dev Accuracy (SWA) = 0.7800

Evaluating final model on the Test set using SWA (Shape-Weighted Accuracy).
This evaluation shows how well the model generalizes to unseen data.
Final Test Set SWA (Shape-Weighted Accuracy): 0.6650

Generating Figure_1.png: Training Loss per Epoch.
This figure illustrates the decrease in training loss across epochs to demonstrate model convergence.
Figure_1.png generated.

Generating Figure_2.png: Development Accuracy (SWA) per Epoch.
This figure shows the improvement in development accuracy, indicating better pattern recognition and generalization.
Figure_2.png generated.

Final Performance Comparison:
Your Model Test SWA: 0.6650
SOTA SWA: 0.8500
Your model does not yet surpass the SOTA baseline. Further model improvements are needed.

Total execution time: 22.81 seconds
