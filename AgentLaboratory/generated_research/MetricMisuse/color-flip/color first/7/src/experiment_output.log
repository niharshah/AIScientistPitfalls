Train sample:
{'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0, 'color_complexity': 3, 'shape_complexity': 3}
max_len set to: 6
Vocabulary size: 17
Maximum sequence length: 6
Using device: cpu

Starting Experiment 1: Baseline (Transformer-only) Model
This experiment trains a transformer-only model on the SPR task. It shows the training loss per epoch, development accuracy, and evaluates the test set using the Shape-Weighted Accuracy (SWA) metric.
Baseline Epoch 1/1: Training Loss = 0.7054, Dev Accuracy = 0.8267

Baseline Model Test Evaluation:
The transformer-only model's test performance is evaluated using Shape-Weighted Accuracy (SWA), which weights correct predictions by sequence shape complexity.
Baseline Test SWA: 0.6244, Test CWA: 0.6250

Starting Experiment 2: Hybrid Neuro-Symbolic Model
This experiment trains a hybrid model that combines transformer-based representations with an explicit rule extraction branch and a differentiable logic layer.
Hybrid Epoch 1/1: Training Loss = 0.8642, Dev Accuracy = 0.8267

Hybrid Model Test Evaluation:
The hybrid model's performance is evaluated on the test set via SWA, combining both contextual and symbolic cues.
Hybrid Test SWA: 0.6444
Hybrid Test CWA: 0.6535

Performance Comparison:
Baseline Model SWA: 0.6244
Hybrid Model SWA: 0.6444
Baseline Model CWA: 0.6250
Hybrid Model CWA: 0.6535
Assumed SOTA: 0.8000
Result: The hybrid model does not yet surpass the current SOTA performance on SPR_BENCH.

Generating Figure_1.png: Training Loss Curves for Both Models.

Generating Figure_2.png: Development Accuracy Curves for Both Models.

Figures saved as 'Figure_1.png' (Training Loss Curves) and 'Figure_2.png' (Development Accuracy Curves).
End of Experiments.
