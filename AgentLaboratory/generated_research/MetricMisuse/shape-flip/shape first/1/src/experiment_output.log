SPR_BENCH Dataset:
Train instances: 20000
Dev instances: 5000
Test instances: 10000
CUDA is available but will not be used (forced CPU).
SPR_BENCH Dataset:
Train instances: 20000
Dev instances: 5000
Test instances: 10000

Starting Training: This experiment trains a Neuro-Symbolic Transformer to decide if an L-token sequence satisfies a hidden poly-factor rule.
The model uses a Transformer encoder with 4 predicate extraction heads (for shape-count, color-position, parity, and order) and a rule verifier module.
Training is performed on the subsampled Train split using binary cross-entropy loss, tuned on the Dev split using the Shape-Weighted Accuracy (SWA) metric.

Epoch 1: Train Loss = 0.6875, Dev Loss = 0.6852, Dev SWA = 0.5556
Epoch 2: Train Loss = 0.6800, Dev Loss = 0.6826, Dev SWA = 0.5556
Epoch 3: Train Loss = 0.6628, Dev Loss = 0.6457, Dev SWA = 0.6385

Evaluating final performance on the Test set:
Test SWA (Shape-Weighted Accuracy): 0.6518

Figure_1.png generated: This figure shows the training and development loss curves across epochs, indicating the model's convergence.
Figure_2.png generated: This figure shows the evolution of Shape-Weighted Accuracy on the dev split over epochs.

Final model training and evaluation complete. The Neuro-Symbolic Transformer was trained to decide if a given L-token sequence conforms to a hidden poly-factor rule. The reported Test SWA, along with the generated figures, provides a performance evaluation relative to state-of-the-art methods.
