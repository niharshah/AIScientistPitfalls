DatasetDict({
    train: Dataset({
        features: ['id', 'sequence', 'label'],
        num_rows: 20000
    })
    validation: Dataset({
        features: ['id', 'sequence', 'label'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'sequence', 'label'],
        num_rows: 10000
    })
})
Loaded dataset structure:
DatasetDict({
    train: Dataset({
        features: ['id', 'sequence', 'label'],
        num_rows: 20000
    })
    validation: Dataset({
        features: ['id', 'sequence', 'label'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'sequence', 'label'],
        num_rows: 10000
    })
})

Extracting features for training set...
Training set feature shape: (20000, 3)

Extracting features for development (validation) set...
Development set feature shape: (5000, 3)

Extracting features for test set...
Test set feature shape: (10000, 3)

Experiment 1: Training Baseline Logistic Regression Model
Dev SWA for Logistic Regression (Baseline): 53.57%
Test SWA for Logistic Regression (Baseline): 55.32%

Experiment 2: Training Improved Random Forest Classifier Model
Dev SWA for Random Forest (Improved): 53.02%
Test SWA for Random Forest (Improved): 54.89%

Based on development set performance, the best model is: Logistic Regression
Best model Test SWA: 55.32%

Figure_1.png generated: Bar chart comparing dev SWA for Logistic Regression and Random Forest.
Figure_2.png generated: Confusion matrix of the best model on the test set.

Summary of Experiments:
------------------------------------------------
Baseline Logistic Regression:
  Dev SWA: 53.57%  |  Test SWA: 55.32%
Improved Random Forest:
  Dev SWA: 53.02%  |  Test SWA: 54.89%
Selected Best Model: Logistic Regression
Final Test SWA (Best Model): 55.32%
