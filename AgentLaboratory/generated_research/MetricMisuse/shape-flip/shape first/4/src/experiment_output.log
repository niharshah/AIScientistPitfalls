SPR_BENCH dataset loaded and preprocessed:
DatasetDict({
    train: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 20000
    })
    dev: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'sequence', 'label', 'tokens'],
        num_rows: 10000
    })
})
Synthetic dataset created:
Dataset({
    features: ['id', 'sequence', 'tokens', 'label'],
    num_rows: 100
})
Vocabulary Size: 16

--- Experiment 1: PRT Model on Synthetic Dataset ---
This experiment trains the PRT model on synthetic data where each sequence contains 10 tokens (shape+color).
The final output includes per-epoch training loss, test accuracy, SWA (weighted by unique shapes),
and two figures: 'Figure_1.png' showing the loss curve and 'Figure_2.png' showing predicate activations.
[PRT Synthetic] Epoch 1/20: Training Loss = 2.8278
[PRT Synthetic] Epoch 2/20: Training Loss = 2.3353
[PRT Synthetic] Epoch 3/20: Training Loss = 1.9458
[PRT Synthetic] Epoch 4/20: Training Loss = 1.6104
[PRT Synthetic] Epoch 5/20: Training Loss = 1.3152
[PRT Synthetic] Epoch 6/20: Training Loss = 1.0697
[PRT Synthetic] Epoch 7/20: Training Loss = 0.8606
[PRT Synthetic] Epoch 8/20: Training Loss = 0.6810
[PRT Synthetic] Epoch 9/20: Training Loss = 0.5305
[PRT Synthetic] Epoch 10/20: Training Loss = 0.4048
[PRT Synthetic] Epoch 11/20: Training Loss = 0.3021
[PRT Synthetic] Epoch 12/20: Training Loss = 0.2211
[PRT Synthetic] Epoch 13/20: Training Loss = 0.1593
[PRT Synthetic] Epoch 14/20: Training Loss = 0.1136
[PRT Synthetic] Epoch 15/20: Training Loss = 0.0804
[PRT Synthetic] Epoch 16/20: Training Loss = 0.0566
[PRT Synthetic] Epoch 17/20: Training Loss = 0.0401
[PRT Synthetic] Epoch 18/20: Training Loss = 0.0287
[PRT Synthetic] Epoch 19/20: Training Loss = 0.0205
[PRT Synthetic] Epoch 20/20: Training Loss = 0.0150

[PRT Synthetic] Final Test Accuracy on Synthetic Dataset: 1.0000
[PRT Synthetic] Shape-Weighted Accuracy (SWA): 1.0000

--- Experiment 2: Baseline Transformer Classifier on SPR_BENCH (Dev Split) ---
This experiment trains a baseline transformer (without symbolic rule induction) on the SPR_BENCH training set.
It then evaluates the model on the dev set and computes accuracy and SWA (weighted by unique shapes).
[Baseline SPR_BENCH] Epoch 1/20: Training Loss = 0.6993
[Baseline SPR_BENCH] Epoch 2/20: Training Loss = 0.6832
[Baseline SPR_BENCH] Epoch 3/20: Training Loss = 0.6786
[Baseline SPR_BENCH] Epoch 4/20: Training Loss = 0.6719
[Baseline SPR_BENCH] Epoch 5/20: Training Loss = 0.6667
[Baseline SPR_BENCH] Epoch 6/20: Training Loss = 0.6566
[Baseline SPR_BENCH] Epoch 7/20: Training Loss = 0.6455
[Baseline SPR_BENCH] Epoch 8/20: Training Loss = 0.6341
[Baseline SPR_BENCH] Epoch 9/20: Training Loss = 0.6264
[Baseline SPR_BENCH] Epoch 10/20: Training Loss = 0.6183
[Baseline SPR_BENCH] Epoch 11/20: Training Loss = 0.6138
[Baseline SPR_BENCH] Epoch 12/20: Training Loss = 0.6093
[Baseline SPR_BENCH] Epoch 13/20: Training Loss = 0.6005
[Baseline SPR_BENCH] Epoch 14/20: Training Loss = 0.5916
[Baseline SPR_BENCH] Epoch 15/20: Training Loss = 0.5820
[Baseline SPR_BENCH] Epoch 16/20: Training Loss = 0.5735
[Baseline SPR_BENCH] Epoch 17/20: Training Loss = 0.5657
[Baseline SPR_BENCH] Epoch 18/20: Training Loss = 0.5607
[Baseline SPR_BENCH] Epoch 19/20: Training Loss = 0.5529
[Baseline SPR_BENCH] Epoch 20/20: Training Loss = 0.5443

[Baseline SPR_BENCH] Final Dev Accuracy on SPR_BENCH: 0.7276
[Baseline SPR_BENCH] Shape-Weighted Accuracy (SWA) on Dev Split: 0.6925

===== Summary of Results =====
[Experiment 1: PRT on Synthetic Dataset] -> Test Accuracy: 1.0000, SWA: 1.0000
[Experiment 2: Baseline on SPR_BENCH Dev] -> Dev Accuracy: 0.7276, SWA: 0.6925
Generated Figures: 'Figure_1.png' (Training Loss Curve) and 'Figure_2.png' (Predicate Activations Heatmap)
