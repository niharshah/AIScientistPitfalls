Dataset splits: ['train', 'dev', 'test']
Example from train split: {'id': 'SPR_train_0', 'sequence': '●y ●g ●r ■r ▲y ▲g', 'label': 0, 'color_count': 3, 'shape_count': 3}
Starting full-model training experiment:
This experiment demonstrates the complete architecture integrating:
  - A Transformer encoder for L-token sequences,
  - A Graph Attention Network (GAT) for relational token representations,
  - A Differentiable Logic Reasoning Layer producing soft predicate scores, and
  - An RL-based Rule Prototype Generator for candidate symbolic rules.
Metrics include overall accuracy, Color-Weighted Accuracy (CWA), and Shape-Weighted Accuracy (SWA).

Epoch 1/1 - Avg Loss: 0.8009 - Dev Acc: 48.67%, CWA: 47.78%, SWA: 46.37%

Starting ablation experiment: Removing the GAT module.
This experiment substitutes the GAT output with the average output from the Transformer encoder.
Ablation (No GAT) - Dev Acc: 48.67%, CWA: 47.78%, SWA: 46.37%

Final evaluation on Test set using the full model:
Test Set - Accuracy: 60.60%, CWA: 61.37%, SWA: 60.25%

Figures saved as Figure_1.png and Figure_2.png.
Figure_1.png shows the training loss curve; Figure_2.png compares Accuracy, CWA, and SWA for the full model vs. the model without GAT.
