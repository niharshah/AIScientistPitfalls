Benchmark: SFRFG
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Benchmark: IJSJF
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Benchmark: GURSG
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Benchmark: TSHUY
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Using device: cpu
Benchmark: SFRFG
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Benchmark: IJSJF
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Benchmark: GURSG
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances
Benchmark: TSHUY
  train: 2000 instances
  dev: 500 instances
  test: 1000 instances

===================================================================
Starting experiment for benchmark SFRFG.
This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,
and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the
fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.
This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.

Epoch 1/3 completed. Average Training Loss: 0.6684
Epoch 2/3 completed. Average Training Loss: 0.4628
Epoch 3/3 completed. Average Training Loss: 0.4054

Evaluating on Dev split to measure generalization on held-out tuning data:
Dev Accuracy: 81.80%

Evaluating on Test split to assess final model generalization on unseen data:
Test Accuracy: 85.30%

===================================================================
Starting experiment for benchmark IJSJF.
This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,
and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the
fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.
This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.

Epoch 1/3 completed. Average Training Loss: 0.6756
Epoch 2/3 completed. Average Training Loss: 0.6026
Epoch 3/3 completed. Average Training Loss: 0.5723

Evaluating on Dev split to measure generalization on held-out tuning data:
Dev Accuracy: 71.80%

Evaluating on Test split to assess final model generalization on unseen data:
Test Accuracy: 69.30%

===================================================================
Starting experiment for benchmark GURSG.
This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,
and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the
fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.
This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.

Epoch 1/3 completed. Average Training Loss: 0.6196
Epoch 2/3 completed. Average Training Loss: 0.3298
Epoch 3/3 completed. Average Training Loss: 0.2874

Evaluating on Dev split to measure generalization on held-out tuning data:
Dev Accuracy: 86.80%

Evaluating on Test split to assess final model generalization on unseen data:
Test Accuracy: 88.90%

===================================================================
Starting experiment for benchmark TSHUY.
This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,
and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the
fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.
This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.

Epoch 1/3 completed. Average Training Loss: 0.5513
Epoch 2/3 completed. Average Training Loss: 0.2209
Epoch 3/3 completed. Average Training Loss: 0.1481

Evaluating on Dev split to measure generalization on held-out tuning data:
Dev Accuracy: 96.00%

Evaluating on Test split to assess final model generalization on unseen data:
Test Accuracy: 96.20%

Figure_1.png saved: Bar plot of final test accuracies across benchmarks.

Generating Figure_2.png: Confusion matrix for benchmark SFRFG.
Figure_2.png saved: Confusion matrix for benchmark SFRFG

Final Test Accuracies per Benchmark:
SFRFG: 85.30%
IJSJF: 69.30%
GURSG: 88.90%
TSHUY: 96.20%
