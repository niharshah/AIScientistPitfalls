SFRFG dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
IJSJF dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
GURSG dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
TSHUY dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
SFRFG dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
IJSJF dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
GURSG dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}
TSHUY dataset splits: {'train': 2000, 'dev': 500, 'test': 1000}

================================================================
Starting experiment for benchmark: SFRFG
This experiment demonstrates the dual-branch model's integration of graph-based attention and differentiable soft symbolic reasoning. Branch A captures sequential and semantic token relationships, while Branch B infers soft symbolic rule scores. We train on a subsampled train split, tune on a subsampled dev split, and evaluate on a subsampled test split.

Epoch [1/2] for SFRFG: Avg Training Loss = 0.7353
Epoch [2/2] for SFRFG: Avg Training Loss = 0.6222
Dev Set Accuracy for SFRFG: 64.00%
Test Set Accuracy for SFRFG: 54.00%

================================================================
Starting experiment for benchmark: IJSJF
This experiment demonstrates the dual-branch model's integration of graph-based attention and differentiable soft symbolic reasoning. Branch A captures sequential and semantic token relationships, while Branch B infers soft symbolic rule scores. We train on a subsampled train split, tune on a subsampled dev split, and evaluate on a subsampled test split.

Epoch [1/2] for IJSJF: Avg Training Loss = 0.7525
Epoch [2/2] for IJSJF: Avg Training Loss = 0.7488
Dev Set Accuracy for IJSJF: 48.00%
Test Set Accuracy for IJSJF: 50.00%

================================================================
Starting experiment for benchmark: GURSG
This experiment demonstrates the dual-branch model's integration of graph-based attention and differentiable soft symbolic reasoning. Branch A captures sequential and semantic token relationships, while Branch B infers soft symbolic rule scores. We train on a subsampled train split, tune on a subsampled dev split, and evaluate on a subsampled test split.

Epoch [1/2] for GURSG: Avg Training Loss = 0.6962
Epoch [2/2] for GURSG: Avg Training Loss = 0.3830
Dev Set Accuracy for GURSG: 94.00%
Test Set Accuracy for GURSG: 90.00%

================================================================
Starting experiment for benchmark: TSHUY
This experiment demonstrates the dual-branch model's integration of graph-based attention and differentiable soft symbolic reasoning. Branch A captures sequential and semantic token relationships, while Branch B infers soft symbolic rule scores. We train on a subsampled train split, tune on a subsampled dev split, and evaluate on a subsampled test split.

Epoch [1/2] for TSHUY: Avg Training Loss = 0.6300
Epoch [2/2] for TSHUY: Avg Training Loss = 0.3779
Dev Set Accuracy for TSHUY: 98.00%
Test Set Accuracy for TSHUY: 100.00%

Final Test Accuracies Across Benchmarks:
SFRFG: 54.00%
IJSJF: 50.00%
GURSG: 90.00%
TSHUY: 100.00%

Figure_1.png generated - It shows the training loss convergence for the SFRFG benchmark.

Figure_2.png generated - It displays the confusion matrix for the IJSJF benchmark test set.

All experiments completed. The dual-branch neuro-symbolic model successfully integrates graph-based attention with differentiable soft logic for SPR tasks. The generated figures provide insights into training convergence and classification performance.
