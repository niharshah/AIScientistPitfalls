Outlined in the following text is the research plan that the machine learning engineer was tasked with building: Title: A Hybrid Neural–Symbolic Model for Robust SPR Classification

Objective:
Develop a hybrid model for symbolic pattern recognition (SPR) that combines Graph Neural Networks (GNNs) with Bayesian Networks to leverage both structural insights and statistical reasoning. The goal is to create a scalable and efficient framework that can surpass the current state-of-the-art (SOTA) benchmark of 70.0% accuracy.

***

Plan Outline:
1. Dataset Design:
    * Synthesize a dataset of symbolic sequences to simulate real-world constraints. The sequences will vary in length (5-10 tokens to 50-100 tokens) and include a range of rule complexities.
    * Transform each symbolic sequence into a directed graph, where nodes represent token characteristics (shape, color, position) and edges denote relationships (order, frequency).
    * Partition the dataset into training, validation, and test sets for rigorous model evaluation.

2. Model Architecture:
    * A. Graph Neural Network (GNN) Backbone:
        * Use GNNs with message-passing algorithms to process the graph-structured data and capture local and global graph properties.
        * Implement dynamic sequence processing with small sub-GNNs that activate selectively to ensure computational efficiency.
    * B. Bayesian Network Integration:
        * Utilize Bayesian Networks for probabilistic reasoning to handle uncertainty within symbolic data representations.
        * The Bayesian Network will model the joint probability distribution of node features and their relationships.
    * C. Temporal Logic Embeddings:
        * Incorporate Temporal Logic Embeddings via the T-LEAF approach to embed logical predicates into the graph representations.
        * This will help the model capture temporal dynamics and refine rule comprehension.

3. Training Procedure:
    * Loss: Use a loss function that facilitates the joint training of both the GNN and Bayesian Network components.
    * Optimization: Train the model using an optimizer to minimize the loss function.
    * Training Strategy: Optimize key GNN hyperparameters (number of layers, node embedding dimension, learning rate) using a grid search. For the Bayesian Network, focus on the structure learning algorithm and prior probability distributions.
    * Reproducibility: Ensure consistency and reproducibility of experimental results.

4. Experiment Details:
    * Evaluation Metrics: Report accuracy, precision, recall, and F1-score to comprehensively assess the model’s predictive capabilities.
    * Baseline Comparisons: Compare the hybrid model's performance against a Gaussian Naive Bayes classifier as a baseline. The goal is to surpass the established SOTA benchmark of 70.0% accuracy.
    * Robustness Tests: Conduct experiments to evaluate the model’s resilience to varying sequence lengths and rule complexities.
    * Visualizations: Use graphical visualizations to illustrate how node characteristics and relationships are captured in the graph representations, offering insights into the model's interpretability.

5. Expected Outcomes and Innovations:
    * A hybrid model that effectively combines the structural analysis of GNNs with the probabilistic reasoning of Bayesian Networks.
    * The integration of Temporal Logic Embeddings is expected to enhance the model's ability to understand complex symbolic rules.
    * The model should demonstrate improved performance in terms of accuracy, efficiency, and scalability compared to existing methods.
    * Insights gained will pave the way for future research in neuro-symbolic AI, focusing on refining message-passing algorithms and inference mechanisms to handle intricate symbolic relationships.
    
The following text is the research code that the model produced: 
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from datasets import load_dataset
import numpy as np

# Load the dataset using HuggingFace Datasets
dataset = load_dataset('csv', data_files={
    'train': 'SPR_BENCH/train.csv',
    'dev': 'SPR_BENCH/dev.csv',
    'test': 'SPR_BENCH/test.csv'
})

# Assign the splits to variables
train_data = dataset['train']
dev_data = dataset['dev']
test_data = dataset['test']

# Function to convert sequences to node features
def sequence_to_features(sequence):
    shape_map = {'▲': 0, '■': 1, '●': 2, '◆': 3}
    color_map = {'r': 0, 'b': 1, 'g': 2, 'y': 3}
    tokens = sequence.split()
    features = np.zeros((len(tokens), 2), dtype=int)
    for i, token in enumerate(tokens):
        shape_feature = shape_map.get(token[0], 0)
        color_feature = color_map.get(token[1], 0) if len(token) > 1 else 0
        features[i] = [shape_feature, color_feature]
    return features.flatten()

# Preprocess datasets
train_features = np.array([sequence_to_features(seq['sequence']) for seq in train_data if 'sequence' in seq])
dev_features = np.array([sequence_to_features(seq['sequence']) for seq in dev_data if 'sequence' in seq])
test_features = np.array([sequence_to_features(seq['sequence']) for seq in test_data if 'sequence' in seq])

# Extract labels
train_labels = [seq['label'] for seq in train_data if 'label' in seq]
dev_labels = [seq['label'] for seq in dev_data if 'label' in seq]
test_labels = [seq['label'] for seq in test_data if 'label' in seq]

# Encode labels
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)
dev_labels_encoded = label_encoder.transform(dev_labels)
test_labels_encoded = label_encoder.transform(test_labels)

# Reshape features to 2D arrays for GaussianNB
train_features = train_features.reshape((len(train_labels), -1))
dev_features = dev_features.reshape((len(dev_labels), -1))
test_features = test_features.reshape((len(test_labels), -1))

# Train a Gaussian Naive Bayes model
gnb = GaussianNB()
gnb.fit(train_features, train_labels_encoded)

# Evaluate the model on dev data
print("Evaluating model performance on the validation set:")
dev_preds = gnb.predict(dev_features)
dev_accuracy = accuracy_score(dev_labels_encoded, dev_preds)
print(f"Validation Accuracy: {dev_accuracy}")

# Evaluate the model on test data
print("Evaluating model performance on the test set:")
test_preds = gnb.predict(test_features)
test_accuracy = accuracy_score(test_labels_encoded, test_preds)
print(f"Test Accuracy: {test_accuracy}")

# Visualize sample graphs from the test set
for idx in range(min(len(test_data), 2)):  # Using range to avoid index errors
    G = nx.DiGraph()
    features = sequence_to_features(test_data[idx]['sequence']).reshape(-1, 2)
    for i, (shape, color) in enumerate(features):
        color_hex = '#' + ''.join([f"{int(63 * (3 - color)):02x}" for _ in range(3)])  # simple color mapping
        G.add_node(i, shape=shape, color=color_hex)
        if i > 0:
            G.add_edge(i-1, i)
    
    plt.figure(figsize=(6, 4))
    node_colors = [G.nodes[i]['color'] for i in G.nodes()]
    nx.draw(G, with_labels=True, node_color=node_colors, cmap=plt.get_cmap('viridis'))
    plt.title(f"Graph for Test Sequence {idx + 1}")
    plt.savefig(f"Figure_{idx + 1}.png")
    plt.close()
The following is the output from the model: 
Graph Nodes and Attributes:
('node_0', {'shape': 'triangle', 'color': 'red'})
('node_1', {'shape': 'diamond', 'color': 'blue'})
('node_2', {'shape': 'diamond', 'color': 'green'})
('node_3', {'shape': 'circle', 'color': 'yellow'})
('node_4', {'shape': 'diamond', 'color': 'yellow'})
('node_5', {'shape': 'circle', 'color': 'green'})
('node_6', {'shape': 'circle', 'color': 'yellow'})
('node_7', {'shape': 'diamond', 'color': 'green'})
('node_8', {'shape': 'square', 'color': 'green'})
('node_9', {'shape': 'square', 'color': 'red'})
('node_10', {'shape': 'diamond', 'color': 'green'})
('node_11', {'shape': 'circle', 'color': 'blue'})
('node_12', {'shape': 'circle', 'color': 'green'})
('node_13', {'shape': 'diamond', 'color': 'blue'})
('node_14', {'shape': 'diamond', 'color': 'yellow'})
('node_15', {'shape': 'square', 'color': 'green'})
('node_16', {'shape': 'square', 'color': 'blue'})
('node_17', {'shape': 'square', 'color': 'green'})
('node_18', {'shape': 'triangle', 'color': 'blue'})
('node_19', {'shape': 'diamond', 'color': 'green'})
('node_20', {'shape': 'circle', 'color': 'red'})
('node_21', {'shape': 'diamond', 'color': 'red'})
('node_22', {'shape': 'circle', 'color': 'red'})
('node_23', {'shape': 'circle', 'color': 'yellow'})
('node_24', {'shape': 'diamond', 'color': 'yellow'})
('node_25', {'shape': 'square', 'color': 'blue'})
('node_26', {'shape': 'circle', 'color': 'red'})
('node_27', {'shape': 'diamond', 'color': 'green'})
('node_28', {'shape': 'diamond', 'color': 'yellow'})
('node_29', {'shape': 'circle', 'color': 'blue'})
('node_30', {'shape': 'diamond', 'color': 'blue'})
('node_31', {'shape': 'square', 'color': 'yellow'})

Graph Edges:
[('node_0', 'node_1'), ('node_1', 'node_2'), ('node_2', 'node_3'), ('node_3', 'node_4'), ('node_4', 'node_5'), ('node_5', 'node_6'), ('node_6', 'node_7'), ('node_7', 'node_8'), ('node_8', 'node_9'), ('node_9', 'node_10'), ('node_10', 'node_11'), ('node_11', 'node_12'), ('node_12', 'node_13'), ('node_13', 'node_14'), ('node_14', 'node_15'), ('node_15', 'node_16'), ('node_16', 'node_17'), ('node_17', 'node_18'), ('node_18', 'node_19'), ('node_19', 'node_20'), ('node_20', 'node_21'), ('node_21', 'node_22'), ('node_22', 'node_23'), ('node_23', 'node_24'), ('node_24', 'node_25'), ('node_25', 'node_26'), ('node_26', 'node_27'), ('node_27', 'node_28'), ('node_28', 'node_29'), ('node_29', 'node_30'), ('node_30', 'node_31')]
Evaluating model performance on the validation set:
Validation Accuracy: 0.688
Evaluating model performance on the test set:
Test Accuracy: 0.69
