{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4a4a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Force CPU usage by disabling CUDA devices.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nnb\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Reproducibility and Device\n",
    "# ----------------------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d2b84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2000 examples [00:00, 74316.36 examples/s]\n",
      "Generating dev split: 500 examples [00:00, 94671.00 examples/s]\n",
      "Generating test split: 1000 examples [00:00, 145262.31 examples/s]\n",
      "Generating train split: 2000 examples [00:00, 227815.11 examples/s]\n",
      "Generating dev split: 500 examples [00:00, 98880.29 examples/s]\n",
      "Generating test split: 1000 examples [00:00, 188161.32 examples/s]\n",
      "Generating train split: 2000 examples [00:00, 7096.63 examples/s]\n",
      "Generating dev split: 500 examples [00:00, 100428.69 examples/s]\n",
      "Generating test split: 1000 examples [00:00, 199140.82 examples/s]\n",
      "Generating train split: 2000 examples [00:00, 232939.24 examples/s]\n",
      "Generating dev split: 500 examples [00:00, 157372.96 examples/s]\n",
      "Generating test split: 1000 examples [00:00, 201320.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: SFRFG\n",
      "  train: 2000 instances\n",
      "  dev: 500 instances\n",
      "  test: 1000 instances\n",
      "Benchmark: IJSJF\n",
      "  train: 2000 instances\n",
      "  dev: 500 instances\n",
      "  test: 1000 instances\n",
      "Benchmark: GURSG\n",
      "  train: 2000 instances\n",
      "  dev: 500 instances\n",
      "  test: 1000 instances\n",
      "Benchmark: TSHUY\n",
      "  train: 2000 instances\n",
      "  dev: 500 instances\n",
      "  test: 1000 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Dataset Loading (assumed to be pre-loaded by provided code, but we include here for completeness)\n",
    "# ----------------------------\n",
    "benchmarks = [\"SFRFG\", \"IJSJF\", \"GURSG\", \"TSHUY\"]\n",
    "all_datasets = {}\n",
    "for bm in benchmarks:\n",
    "    files = {\n",
    "        \"train\": f\"../../../SPR_BENCH/{bm}/train.csv\",\n",
    "        \"dev\": f\"../../../SPR_BENCH/{bm}/dev.csv\",\n",
    "        \"test\": f\"../../../SPR_BENCH/{bm}/test.csv\"\n",
    "    }\n",
    "    dataset = load_dataset(\"csv\", data_files=files)\n",
    "    all_datasets[bm] = dataset\n",
    "\n",
    "for bm, ds in all_datasets.items():\n",
    "    print(f\"Benchmark: {bm}\")\n",
    "    for split in ds.keys():\n",
    "        print(f\"  {split}: {len(ds[split])} instances\")\n",
    "\n",
    "# ----------------------------\n",
    "# Data Preprocessing: Build vocabulary and define SPRDataset\n",
    "# ----------------------------\n",
    "class SPRDataset(Dataset):\n",
    "    def __init__(self, examples, vocab, max_len):\n",
    "        self.examples = examples\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        tokens = ex[\"sequence\"].strip().split()\n",
    "        indices = [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in tokens]\n",
    "        if len(indices) < self.max_len:\n",
    "            indices = indices + [self.vocab[\"<pad>\"]] * (self.max_len - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_len]\n",
    "        label = int(ex[\"label\"])\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c348d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {}\n",
    "for bm in benchmarks:\n",
    "    ds = all_datasets[bm]\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    token_set = set()\n",
    "    max_len = 0\n",
    "    for ex in ds[\"train\"]:\n",
    "        tokens = ex[\"sequence\"].strip().split()\n",
    "        max_len = max(max_len, len(tokens))\n",
    "        token_set.update(tokens)\n",
    "    for token in sorted(token_set):\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "    processed_data[bm] = {\n",
    "        \"max_len\": max_len,\n",
    "        \"vocab\": vocab,\n",
    "        \"train\": SPRDataset(ds[\"train\"], vocab, max_len),\n",
    "        \"dev\": SPRDataset(ds[\"dev\"], vocab, max_len),\n",
    "        \"test\": SPRDataset(ds[\"test\"], vocab, max_len)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6504ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Model Definition: PositionalEncoding and SimpleTransformer\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, emb_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, emb_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, nhead, num_layers, num_classes, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=emb_dim*2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(emb_dim, num_classes)\n",
    "        # Extra chain-of-thought head to produce symbolic intermediate representations\n",
    "        self.ct_head = nn.Linear(emb_dim, emb_dim)\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len)\n",
    "        emb = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "        emb = self.pos_encoder(emb)\n",
    "        emb = emb.transpose(0, 1)  # (seq_len, batch, emb_dim)\n",
    "        transformer_out = self.transformer_encoder(emb)\n",
    "        transformer_out = transformer_out.transpose(0, 1)  # (batch, seq_len, emb_dim)\n",
    "        pooled = transformer_out.mean(dim=1)  # simple average pooling for sequence\n",
    "        ct_vector = torch.tanh(self.ct_head(pooled))\n",
    "        logits = self.fc(self.dropout(pooled))\n",
    "        return logits, ct_vector\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameters for all experiments\n",
    "# ----------------------------\n",
    "emb_dim = 32\n",
    "nhead = 4\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "learning_rate = 0.001\n",
    "fusion_scale = 2.0  # scaling factor for symbolic fusion\n",
    "\n",
    "# Dictionary to store final test accuracies and predictions for each benchmark\n",
    "results = {}\n",
    "model_store = {}\n",
    "all_preds_dict = {}\n",
    "all_labels_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd38830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================\n",
      "Starting experiment for benchmark SFRFG.\n",
      "This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,\n",
      "and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the\n",
      "fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.\n",
      "This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxl240011/anaconda3/envs/agentlab/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Average Training Loss: 0.6684\n",
      "Epoch 2/3 completed. Average Training Loss: 0.4628\n",
      "Epoch 3/3 completed. Average Training Loss: 0.4054\n",
      "\n",
      "Evaluating on Dev split to measure generalization on held-out tuning data:\n",
      "Dev Accuracy: 81.80%\n",
      "\n",
      "Evaluating on Test split to assess final model generalization on unseen data:\n",
      "Test Accuracy: 85.30%\n",
      "\n",
      "===================================================================\n",
      "Starting experiment for benchmark IJSJF.\n",
      "This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,\n",
      "and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the\n",
      "fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.\n",
      "This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxl240011/anaconda3/envs/agentlab/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Average Training Loss: 0.6756\n",
      "Epoch 2/3 completed. Average Training Loss: 0.6026\n",
      "Epoch 3/3 completed. Average Training Loss: 0.5723\n",
      "\n",
      "Evaluating on Dev split to measure generalization on held-out tuning data:\n",
      "Dev Accuracy: 71.80%\n",
      "\n",
      "Evaluating on Test split to assess final model generalization on unseen data:\n",
      "Test Accuracy: 69.30%\n",
      "\n",
      "===================================================================\n",
      "Starting experiment for benchmark GURSG.\n",
      "This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,\n",
      "and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the\n",
      "fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.\n",
      "This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.\n",
      "\n",
      "Epoch 1/3 completed. Average Training Loss: 0.6196\n",
      "Epoch 2/3 completed. Average Training Loss: 0.3298\n",
      "Epoch 3/3 completed. Average Training Loss: 0.2874\n",
      "\n",
      "Evaluating on Dev split to measure generalization on held-out tuning data:\n",
      "Dev Accuracy: 86.80%\n",
      "\n",
      "Evaluating on Test split to assess final model generalization on unseen data:\n",
      "Test Accuracy: 88.90%\n",
      "\n",
      "===================================================================\n",
      "Starting experiment for benchmark TSHUY.\n",
      "This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,\n",
      "and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the\n",
      "fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.\n",
      "This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.\n",
      "\n",
      "Epoch 1/3 completed. Average Training Loss: 0.5513\n",
      "Epoch 2/3 completed. Average Training Loss: 0.2209\n",
      "Epoch 3/3 completed. Average Training Loss: 0.1481\n",
      "\n",
      "Evaluating on Dev split to measure generalization on held-out tuning data:\n",
      "Dev Accuracy: 96.00%\n",
      "\n",
      "Evaluating on Test split to assess final model generalization on unseen data:\n",
      "Test Accuracy: 96.20%\n"
     ]
    }
   ],
   "source": [
    "for bm in benchmarks:\n",
    "    print(\"\\n===================================================================\")\n",
    "    print(f\"Starting experiment for benchmark {bm}.\")\n",
    "    print(\"This experiment trains a lightweight transformer classifier on the training split, uses the dev split for tuning,\")\n",
    "    print(\"and finally evaluates on the hidden test split. The symbolic module computes a verification score based on the\")\n",
    "    print(\"fraction of tokens starting with '▲'. The score is fused with the neural logits by adjusting the positive class logit.\")\n",
    "    print(\"This experiment is designed to assess accuracy and generalization across sequence lengths and rule complexity.\\n\")\n",
    "    \n",
    "    # Retrieve processed data for benchmark\n",
    "    data = processed_data[bm]\n",
    "    train_dataset = data[\"train\"]\n",
    "    dev_dataset = data[\"dev\"]\n",
    "    test_dataset = data[\"test\"]\n",
    "    max_len = data[\"max_len\"]\n",
    "    vocab = data[\"vocab\"]\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    # Create DataLoaders (num_workers=0 for stability)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = SimpleTransformer(vocab_size, emb_dim, nhead, num_layers, num_classes, max_len)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Monkey-patch optimizer to avoid CUDA graph capture health check when using CPU.\n",
    "    if device.type != \"cuda\":\n",
    "        optimizer._cuda_graph_capture_health_check = lambda: None\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_tokens, batch_labels in train_loader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(batch_tokens)\n",
    "            # Build reverse vocab mapping: index -> token\n",
    "            rev_vocab = {i: token for token, i in vocab.items()}\n",
    "            symbolic_scores = []\n",
    "            # Calculate symbolic verification score for each instance: fraction of tokens starting with '▲'\n",
    "            batch_tokens_cpu = batch_tokens.cpu().numpy()\n",
    "            for tokens in batch_tokens_cpu:\n",
    "                token_list = [rev_vocab[idx] for idx in tokens if idx != vocab[\"<pad>\"]]\n",
    "                if len(token_list) == 0:\n",
    "                    score = 0.0\n",
    "                else:\n",
    "                    score = sum(1 for tok in token_list if tok.startswith(\"▲\")) / len(token_list)\n",
    "                symbolic_scores.append(score)\n",
    "            symbolic_scores = torch.tensor(symbolic_scores, dtype=torch.float32, device=device)\n",
    "            # Fuse symbolic scores with neural logits: adjust positive class logit (index 1)\n",
    "            adjusted_logits = logits.clone()\n",
    "            adjusted_logits[:, 1] = adjusted_logits[:, 1] + fusion_scale * (symbolic_scores - 0.5)\n",
    "            loss = criterion(adjusted_logits, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_tokens.size(0)\n",
    "        avg_loss = epoch_loss / len(train_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation on Dev Set\n",
    "    print(\"\\nEvaluating on Dev split to measure generalization on held-out tuning data:\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens, batch_labels in dev_loader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            logits, _ = model(batch_tokens)\n",
    "            rev_vocab = {i: token for token, i in vocab.items()}\n",
    "            symbolic_scores = []\n",
    "            batch_tokens_cpu = batch_tokens.cpu().numpy()\n",
    "            for tokens in batch_tokens_cpu:\n",
    "                token_list = [rev_vocab[idx] for idx in tokens if idx != vocab[\"<pad>\"]]\n",
    "                if len(token_list) == 0:\n",
    "                    score = 0.0\n",
    "                else:\n",
    "                    score = sum(1 for tok in token_list if tok.startswith(\"▲\")) / len(token_list)\n",
    "                symbolic_scores.append(score)\n",
    "            symbolic_scores = torch.tensor(symbolic_scores, dtype=torch.float32, device=device)\n",
    "            adjusted_logits = logits.clone()\n",
    "            adjusted_logits[:, 1] = adjusted_logits[:, 1] + fusion_scale * (symbolic_scores - 0.5)\n",
    "            preds = adjusted_logits.argmax(dim=1)\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "    dev_accuracy = (correct / total) * 100\n",
    "    print(f\"Dev Accuracy: {dev_accuracy:.2f}%\")\n",
    "    \n",
    "    # Evaluation on Test Set (hidden data evaluation)\n",
    "    print(\"\\nEvaluating on Test split to assess final model generalization on unseen data:\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens, batch_labels in test_loader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            logits, _ = model(batch_tokens)\n",
    "            rev_vocab = {i: token for token, i in vocab.items()}\n",
    "            symbolic_scores = []\n",
    "            batch_tokens_cpu = batch_tokens.cpu().numpy()\n",
    "            for tokens in batch_tokens_cpu:\n",
    "                token_list = [rev_vocab[idx] for idx in tokens if idx != vocab[\"<pad>\"]]\n",
    "                if len(token_list) == 0:\n",
    "                    score = 0.0\n",
    "                else:\n",
    "                    score = sum(1 for tok in token_list if tok.startswith(\"▲\")) / len(token_list)\n",
    "                symbolic_scores.append(score)\n",
    "            symbolic_scores = torch.tensor(symbolic_scores, dtype=torch.float32, device=device)\n",
    "            adjusted_logits = logits.clone()\n",
    "            adjusted_logits[:, 1] = adjusted_logits[:, 1] + fusion_scale * (symbolic_scores - 0.5)\n",
    "            preds = adjusted_logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "    test_accuracy = (correct / total) * 100\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    results[bm] = test_accuracy\n",
    "    model_store = model_store if 'model_store' in globals() else {}\n",
    "    model_store[bm] = model\n",
    "    all_preds_dict[bm] = all_preds\n",
    "    all_labels_dict[bm] = all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbaf5b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxl240011/anaconda3/envs/agentlab/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Average Training Loss: 0.5118\n",
      "Epoch 2/3 completed. Average Training Loss: 0.2134\n",
      "Epoch 3/3 completed. Average Training Loss: 0.1621\n",
      "\n",
      "Evaluating on Dev split to measure generalization on held-out tuning data:\n",
      "Dev Accuracy: 96.60%\n",
      "\n",
      "Evaluating on Test split to assess final model generalization on unseen data:\n",
      "Test Accuracy: 96.40%\n"
     ]
    }
   ],
   "source": [
    "bm = \"TSHUY\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Retrieve processed data for benchmark\n",
    "data = processed_data[bm]\n",
    "train_dataset = data[\"train\"]\n",
    "dev_dataset = data[\"dev\"]\n",
    "test_dataset = data[\"test\"]\n",
    "max_len = data[\"max_len\"]\n",
    "vocab = data[\"vocab\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create DataLoaders (num_workers=0 for stability)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SimpleTransformer(vocab_size, emb_dim, nhead, num_layers, num_classes, max_len)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Monkey-patch optimizer to avoid CUDA graph capture health check when using CPU.\n",
    "if device.type != \"cuda\":\n",
    "    optimizer._cuda_graph_capture_health_check = lambda: None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_tokens, batch_labels in train_loader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(batch_tokens)\n",
    "        # Build reverse vocab mapping: index -> token\n",
    "        rev_vocab = {i: token for token, i in vocab.items()}\n",
    "        symbolic_scores = []\n",
    "        # Calculate symbolic verification score for each instance: fraction of tokens starting with '▲'\n",
    "        batch_tokens_cpu = batch_tokens.cpu().numpy()\n",
    "        for tokens in batch_tokens_cpu:\n",
    "            token_list = [rev_vocab[idx] for idx in tokens if idx != vocab[\"<pad>\"]]\n",
    "            if len(token_list) == 0:\n",
    "                score = 0.0\n",
    "            else:\n",
    "                score = sum(1 for tok in token_list if tok.startswith(\"▲\")) / len(token_list)\n",
    "            symbolic_scores.append(score)\n",
    "        symbolic_scores = torch.tensor(symbolic_scores, dtype=torch.float32, device=device)\n",
    "        # Fuse symbolic scores with neural logits: adjust positive class logit (index 1)\n",
    "        adjusted_logits = logits.clone()\n",
    "        adjusted_logits[:, 1] = adjusted_logits[:, 1] + fusion_scale * (symbolic_scores - 0.5)\n",
    "        loss = criterion(adjusted_logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_tokens.size(0)\n",
    "    avg_loss = epoch_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation on Dev Set\n",
    "print(\"\\nEvaluating on Dev split to measure generalization on held-out tuning data:\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_labels in dev_loader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        logits, _ = model(batch_tokens)\n",
    "        rev_vocab = {i: token for token, i in vocab.items()}\n",
    "        symbolic_scores = []\n",
    "        batch_tokens_cpu = batch_tokens.cpu().numpy()\n",
    "        for tokens in batch_tokens_cpu:\n",
    "            token_list = [rev_vocab[idx] for idx in tokens if idx != vocab[\"<pad>\"]]\n",
    "            if len(token_list) == 0:\n",
    "                score = 0.0\n",
    "            else:\n",
    "                score = sum(1 for tok in token_list if tok.startswith(\"▲\")) / len(token_list)\n",
    "            symbolic_scores.append(score)\n",
    "        symbolic_scores = torch.tensor(symbolic_scores, dtype=torch.float32, device=device)\n",
    "        adjusted_logits = logits.clone()\n",
    "        adjusted_logits[:, 1] = adjusted_logits[:, 1] + fusion_scale * (symbolic_scores - 0.5)\n",
    "        preds = adjusted_logits.argmax(dim=1)\n",
    "        correct += (preds == batch_labels).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "dev_accuracy = (correct / total) * 100\n",
    "print(f\"Dev Accuracy: {dev_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on Test Set (hidden data evaluation)\n",
    "print(\"\\nEvaluating on Test split to assess final model generalization on unseen data:\")\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_labels in test_loader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        logits, _ = model(batch_tokens)\n",
    "        rev_vocab = {i: token for token, i in vocab.items()}\n",
    "        symbolic_scores = []\n",
    "        batch_tokens_cpu = batch_tokens.cpu().numpy()\n",
    "        for tokens in batch_tokens_cpu:\n",
    "            token_list = [rev_vocab[idx] for idx in tokens if idx != vocab[\"<pad>\"]]\n",
    "            if len(token_list) == 0:\n",
    "                score = 0.0\n",
    "            else:\n",
    "                score = sum(1 for tok in token_list if tok.startswith(\"▲\")) / len(token_list)\n",
    "            symbolic_scores.append(score)\n",
    "        symbolic_scores = torch.tensor(symbolic_scores, dtype=torch.float32, device=device)\n",
    "        adjusted_logits = logits.clone()\n",
    "        adjusted_logits[:, 1] = adjusted_logits[:, 1] + fusion_scale * (symbolic_scores - 0.5)\n",
    "        preds = adjusted_logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "        correct += (preds == batch_labels).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "test_accuracy = (correct / total) * 100\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "results[bm] = test_accuracy\n",
    "model_store = model_store if 'model_store' in globals() else {}\n",
    "model_store[bm] = model\n",
    "all_preds_dict[bm] = all_preds\n",
    "all_labels_dict[bm] = all_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
