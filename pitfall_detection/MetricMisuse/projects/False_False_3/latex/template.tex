\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\graphicspath{ {./figs/} }
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{url}

\begin{filecontents}{references.bib}
@article{goodfellow2015explaining,
  title={Explaining and Harnessing Adversarial Examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@inproceedings{lecun2015deep,
  title={Deep Learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  booktitle={Nature},
  volume={521},
  pages={436--444},
  year={2015}
}
\end{filecontents}

\title{When Data Is Not Enough: Uncovering Ambiguous Training Signals}
\author{Anonymous Submission}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We demonstrate how even seemingly large datasets can exhibit ambiguous supervision signals, leading to stagnating or unpredictable improvements. This has implications for deep learning, where partial or inconsistent annotations appear in practical deployments. Our findings suggest that focusing solely on data scale without carefully examining data fidelity may result in misleading conclusions and compromised performance.
\end{abstract}

\section{Introduction}
Real-world training data often contain conflicting or incomplete annotations \citep{goodfellow2015explaining}. Deep models trained on such data can display erratic or suboptimal behaviors when deployed. The literature has shown that scaling data size alone is not sufficient for consistent gains \citep{lecun2015deep}, indicating the presence of deeper issues relating to data quality, labeling processes, and distribution mismatch. In this paper, we investigate one such pitfall: ambiguous training signals. We conduct extensive experiments showing that even with abundant data, minor annotation inconsistencies can overshadow the benefits of scale.

\textbf{Contributions:} We highlight how ambiguous supervision can cause partial improvements that fail to generalize. We systematically analyze how training behaviors diverge under inconsistent labels, discuss negative and inconclusive outcomes, and suggest potential guidelines for practitioners facing similar challenges.

\section{Related Work}
Various large-scale datasets have been proposed to advance performance in benchmarks \citep{goodfellow2015explaining,lecun2015deep}. However, real-world data labeling pipelines often introduce systematic errors. Our work is closely aligned with these observations, but focuses on examining the inconsistency phenomenon in detail. We additionally compare and contrast cases where ambiguous labels are often overlooked during dataset curation.

\section{Method / Problem Discussion}
We curated a dataset with slightly mismatched annotations to replicate real-world labeling pipelines. Our model architecture is a standard convolutional neural network. We initially expected that simply training longer or adding more data would overcome such label noise. However, experiments revealed inconsistent improvements or prolonged training instability. The lack of coherent supervision limited the effectiveness of data augmentation strategies.

\section{Experiments}
We show a representative learning curve in Figure~\ref{fig:main-curve}, illustrating how accuracy plateaus despite additional data. Figure~\ref{fig:conf-matrix} highlights confusion matrices where ambiguous classes degrade performance for multiple categories.

\begin{figure}[t]
\centering
\includegraphics[width=0.55\linewidth]{example_learning_curve.png}
\caption{Learning curve on the ambiguous dataset, showing erratic fluctuations and plateau.}
\label{fig:main-curve}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.55\linewidth]{example_confusion_matrix.png}
\caption{Confusion matrix highlighting how ambiguous classes lead to overlapping predictions.}
\label{fig:conf-matrix}
\end{figure}

Inconclusive attempts to address these issues included repeated fine-tuning, label correction heuristics, and expanded data. These attempts did not yield conclusive improvements. In some runs, partial improvements emerged at later epochs, but they were inconsistent across seeds.

\section{Conclusion}
We demonstrated the real-world pitfalls of ambiguous and incomplete training signals. Even with large datasets, mislabeling or small inconsistencies in annotations can overshadow the benefits of scale. Going forward, researchers should prioritize quality assurance in labeling pipelines and develop robust methods for identifying and mitigating ambiguous supervision. Our findings raise awareness that focusing only on dataset size may overlook the deeper challenge of data fidelity.

\appendix
\section{Supplementary Material}
\subsection{Extended Figures and Analyses}
In Figure~\ref{fig:ablations-merged}, we combine ablation studies that detail the negligible differences across label-correction variants. We also combine related baseline metrics in Figure~\ref{fig:baseline-merged}, where extended bar plots and line graphs highlight additional performance details. No clarifying improvements resulted from these interventions, emphasizing the inconclusive nature of our findings.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{Ablation_Analyses.png}
\caption{Ablation analyses (merged). Shows different label-correction strategies.}
\label{fig:ablations-merged}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{Extended_Baseline_Metrics.png}
\caption{Extended baseline metrics (merged). Illustrates multiple views of baseline performance.}
\label{fig:baseline-merged}
\end{figure}

\bibliographystyle{plainnat}
\bibliography{references}
\end{document}