\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{goodfellow2016deep}
\citation{sir2024acp,bader2005dimensionson}
\citation{wang2019aso,nagar2025howdt,yi2025ironydr}
\citation{bader2005dimensionson,ylmaz2016apf}
\citation{li2025reinforcementlw}
\citation{cao2020linguisticallydg,wu2022rnnctpsan}
\citation{yu2023ann}
\citation{xie2025finchainas,xu2024macab}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{1}{section.3}\protected@file@percent }
\bibdata{references}
\bibcite{bader2005dimensionson}{{1}{2005}{{Bader \& Hitzler}}{{Bader and Hitzler}}}
\@writefile{toc}{\contentsline {paragraph}{Real-World Pitfalls.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}{section.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test performance on \texttt  {SPR\_BENCH}.}}{2}{table.1}\protected@file@percent }
\newlabel{tab:main}{{1}{2}{Test performance on \texttt {SPR\_BENCH}}{table.1}{}}
\newlabel{tab:main@cref}{{[table][1][]1}{[1][2][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Sample baseline curves: the model learns effectively on training, but test accuracy stalls near $0.62$. RCWA (right y-axis in (a)) also remains below $0.77$ on validation.}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Baseline: Loss/RCWA}}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Baseline: Test Acc.}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:baseline_main}{{1}{2}{Sample baseline curves: the model learns effectively on training, but test accuracy stalls near $0.62$. RCWA (right y-axis in (a)) also remains below $0.77$ on validation}{figure.1}{}}
\newlabel{fig:baseline_main@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {paragraph}{Ablation.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{2}{section.5}\protected@file@percent }
\bibcite{cao2020linguisticallydg}{{2}{2020}{{Cao et~al.}}{{Cao, Liang, Wang, and Lin}}}
\bibcite{goodfellow2016deep}{{3}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{wu2022rnnctpsan}{{4}{2022}{{hao Wu \& Li}}{{hao Wu and Li}}}
\bibcite{li2025reinforcementlw}{{5}{2025}{{Li et~al.}}{{Li, Lei, Yin, and Hu}}}
\bibcite{nagar2025howdt}{{6}{2025}{{Nagar et~al.}}{{Nagar, Rawal, Dhanania, and Tan}}}
\bibcite{wang2019aso}{{7}{2019}{{Wang et~al.}}{{Wang, Zheng, Yu, and Miao}}}
\bibcite{xie2025finchainas}{{8}{2025}{{Xie et~al.}}{{Xie, Sahnan, Banerjee, Georgiev, Thareja, Madmoun, Su, Singh, Wang, Xing, Koto, Li, Koychev, Chakraborty, Lahlou, Stoyanov, and Nakov}}}
\bibcite{xu2024macab}{{9}{2024}{{Xu et~al.}}{{Xu, Wang, Hu, Lin, Du, and Wu}}}
\bibcite{yi2025ironydr}{{10}{2025}{{Yi \& Xia}}{{Yi and Xia}}}
\bibcite{yu2023ann}{{11}{2023}{{Yu et~al.}}{{Yu, Liu, Pan, Li, and Yang}}}
\bibcite{ylmaz2016apf}{{12}{2016}{{Özgür Yılmaz et~al.}}{{Özgür Yılmaz, Garcez, and Silver}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Our neural-symbolic approach yields improved shape-weighted accuracy across epochs but only modest gains in final test accuracy (to about $0.65$).}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Research: Loss/SWA}}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Research: Test Acc.}}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:research_main}{{2}{3}{Our neural-symbolic approach yields improved shape-weighted accuracy across epochs but only modest gains in final test accuracy (to about $0.65$)}{figure.2}{}}
\newlabel{fig:research_main@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Ablation: removing symbolic vectors. Zero-shot accuracy degrades and off-diagonal confusion increases.}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:ablation_add}{{3}{3}{Ablation: removing symbolic vectors. Zero-shot accuracy degrades and off-diagonal confusion increases}{figure.3}{}}
\newlabel{fig:ablation_add@cref}{{[figure][3][]3}{[1][2][]3}}
\bibcite{sir2024acp}{{13}{2024}{{Šír}}{{}}}
\bibstyle{iclr2025}
\newlabel{sec:appendix}{{5}{4}{\LARGE Supplementary Material}{section*.4}{}}
\newlabel{sec:appendix@cref}{{[section][5][]5}{[1][4][]4}}
\@writefile{toc}{\contentsline {section}{Appendix}{4}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Implementation Details}{4}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Ablations and Figures}{4}{appendix.B}\protected@file@percent }
\gdef \@abspage@last{4}
