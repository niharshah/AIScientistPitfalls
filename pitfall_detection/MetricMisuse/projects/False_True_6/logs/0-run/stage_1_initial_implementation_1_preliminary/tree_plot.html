<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[3, 4], [3, 7], [3, 5], [3, 6]], "layout": [[0.0, 0.0], [0.2222222222222222, 0.0], [0.4444444444444444, 0.0], [0.6666666666666666, 0.0], [0.3333333333333333, 1.0], [0.5555555555555556, 1.0], [0.7777777777777778, 1.0], [1.0, 1.0]], "plan": ["We will start with a minimal neural-symbolic baseline: treat every shape-color\ntoken as a discrete symbol, embed it, average-pool the embeddings to obtain a\nsequence representation, and feed this to a linear classifier.  This gives us a\nfast model that can learn correlations between symbolic tokens and labels yet\nstill runs on GPU.  We build a lightweight PyTorch pipeline with a custom\ncollate function, train for only a few epochs, and report: validation loss,\nShape-Weighted Accuracy, Color-Weighted Accuracy, overall accuracy, and our\nNovel Rule Generalization Score (NRGS) which is computed on the subset of\ndev/test sequences whose simple (length, #shape-types, #color-types) signature\nnever appears in the training split.  All metrics, losses, predictions, and\nsignatures are stored in `experiment_data` and saved to the `working` directory.\nIf the real SPR_BENCH folder is missing we automatically fall back to a tiny\nsynthetic dataset so the script always runs.  The code respects all device-\nplacement guidelines and prints a progress line for every epoch.", "We start with a lightweight neural-symbolic baseline.  Each SPR sequence is\ntokenised at the \u201cshape-color\u201d token level; an Embedding + bi-directional LSTM\nturns the symbol string into a dense vector, after which we concatenate two\nsymbolic statistics (shape-variety and color-variety counts).  A small feed-\nforward layer maps this joint representation to the target rule label.  The\nmodel is trained for a few epochs with cross-entropy on the training split and\nvalidated on the dev split while tracking loss, Shape-Weighted Accuracy, Color-\nWeighted Accuracy and the Novel Rule Generalisation Score (NRGS).  NRGS is\ncomputed by withholding all test examples whose shape-only template occurs in\nthe training data, thereby measuring zero-shot rule transfer.  All metrics and\nlosses are stored in the required experiment_data structure and saved as a NumPy\nfile; prediction arrays are also saved for later analysis.  The code runs on GPU\nif available, follows the mandatory device handling rules, and produces final\nmetric prints plus a small plot image under ./working for quick visual\ninspection.  This gives us a first functional baseline that we can later extend\nwith more sophisticated neuro-symbolic reasoning components.", "We will build a very small neural-symbolic baseline that learns a token-level\nembedding of the SPR sequences with an LSTM while simultaneously keeping light-\nweight symbolic statistics for zero-shot rule detection. Tokens (e.g. \u201caR\u201d for\nshape \u2018a\u2019, colour \u2018R\u2019) are embedded, a single-layer bidirectional LSTM encodes\nthe sequence, and the last hidden state is classified with a linear layer. For\nNovel Rule Generalisation Score (NRGS) we treat the ordered shape string\n(token[0] for every token) as the \u201crule-signature\u201d.  Any test sequence whose\nsignature never appeared in training is counted as \u201cunseen\u201d; NRGS is the\naccuracy on that subset.  We train for only a few epochs, report validation loss\nevery epoch, and compute Accuracy, SWA, CWA, and NRGS on dev & test.  All\nmetrics, losses, predictions and ground-truth labels are stored in\nexperiment_data and saved to ./working.  The code complies with all device and\ndata-saving constraints and executes immediately.", "We begin with a minimalist baseline that treats SPR as a sequence-classification\nproblem.  Each token (e.g. \u201ca1\u201d) is split into an atomic symbol that is embedded\nand passed through a small GRU; its final hidden state feeds a linear layer that\npredicts the label.  A bespoke collate_fn pads sequences, and the whole model\nruns on GPU if available.  Besides standard accuracy we compute SWA, CWA (from\nthe proposal) and the Novel Rule Generalization Score (NRGS).  NRGS is obtained\nby building a set of \u201crule signatures\u201d \u2013 the ordered list of shapes in each\nsequence \u2013 from the training set and evaluating accuracy only on examples whose\nsignature never appears in that set.  All metrics, losses, predictions and\nground-truth labels are stored in a structured experiment_data dict and saved\nwith np.save; a quick bar-plot is also written to ./working.  The code completes\na few epochs (default = 5) to stay within time limits yet demonstrates the full\npipeline from data loading to evaluation/visualisation.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, time, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom collections import Counter\nfrom datasets import DatasetDict\n\n# ------------------------------------------------------------------\n# Required working directory & experiment dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"signatures\": [],\n    }\n}\n# ------------------------------------------------------------------\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----------  SPR I/O (from given utility, slightly wrapped) --------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\n# ------------------------------------------------------------------\n# -----------------  Fallback synthetic data  -----------------------\ndef build_synthetic_dataset(n_train=5000, n_dev=1000, n_test=1000) -> DatasetDict:\n    shapes = \"SCRTP\"  # Square, Circle, Rectangle, Triangle, Pentagon\n    colors = \"RGBYM\"  # Red, Green, Blue, Yellow, Magenta\n\n    def rand_seq():\n        length = random.randint(4, 12)\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors) for _ in range(length)\n        )\n\n    def gen_split(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            s = rand_seq()\n            # arbitrary synthetic label rule: 1 if more unique shapes than colors else 0\n            label = int(count_shape_variety(s) > count_color_variety(s))\n            seqs.append(s)\n            labels.append(label)\n        return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n    hf = DatasetDict()\n    import datasets\n\n    hf[\"train\"] = datasets.Dataset.from_dict(gen_split(n_train))\n    hf[\"dev\"] = datasets.Dataset.from_dict(gen_split(n_dev))\n    hf[\"test\"] = datasets.Dataset.from_dict(gen_split(n_test))\n    return hf\n\n\n# ------------------------------------------------------------------\n# ------------------  Load dataset (real or fake) -------------------\nSPR_ROOT = pathlib.Path(\"./SPR_BENCH\")\nif SPR_ROOT.exists():\n    print(\"Loading real SPR_BENCH dataset \u2026\")\n    ds = load_spr_bench(SPR_ROOT)\nelse:\n    print(\"Real SPR_BENCH not found, generating synthetic data \u2026\")\n    ds = build_synthetic_dataset()\n\nlabel_set = sorted(set(ds[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nnum_labels = len(label2id)\nprint(f\"Number of labels = {num_labels}\")\n\n\n# ------------------------------------------------------------------\n# --------------------  Vocabulary ---------------------------------\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    counter = Counter()\n    for seq in sequences:\n        counter.update(seq.strip().split())\n    vocab = {\n        tok: i + 2 for i, (tok, _) in enumerate(counter.most_common())\n    }  # 0 PAD, 1 UNK\n    vocab[\"<PAD>\"] = 0\n    vocab[\"<UNK>\"] = 1\n    return vocab\n\n\nvocab = build_vocab(ds[\"train\"][\"sequence\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size = {vocab_size}\")\n\n\ndef encode_seq(seq: str) -> List[int]:\n    return [vocab.get(tok, 1) for tok in seq.strip().split()]\n\n\n# ------------------------------------------------------------------\n# -------------- PyTorch Dataset & DataLoader ----------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.ids = hf_split[\"id\"]\n        self.sequences = hf_split[\"sequence\"]\n        self.labels = [label2id[l] for l in hf_split[\"label\"]]\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"id\": self.ids[idx],\n            \"input\": torch.tensor(encode_seq(self.sequences[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.sequences[idx],\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(item[\"input\"]) for item in batch]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, item in enumerate(batch):\n        padded[i, : lengths[i]] = item[\"input\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw_seqs = [b[\"raw_seq\"] for b in batch]\n    ids = [b[\"id\"] for b in batch]\n    return {\n        \"ids\": ids,\n        \"input\": padded,\n        \"label\": labels,\n        \"raw_seq\": raw_seqs,\n        \"lengths\": lengths,\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(ds[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n)\nval_loader = DataLoader(\n    SPRTorchDataset(ds[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(ds[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\n\n\n# ------------------------------------------------------------------\n# -------------------  Simple Mean-Pool model ----------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Linear(emb_dim, num_labels)\n\n    def forward(self, x):  # x: (B, L)\n        emb = self.emb(x)  # (B, L, D)\n        mask = (x != 0).unsqueeze(-1)  # (B, L, 1)\n        summed = (emb * mask).sum(1)  # (B, D)\n        lengths = mask.sum(1).clamp(min=1)  # (B,1)\n        mean = summed / lengths\n        return self.classifier(mean)\n\n\nmodel = MeanPoolClassifier(vocab_size, emb_dim=64, num_labels=num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------------------------------------------------------\n# ----------- Helper: rule signatures & NRGS -----------------------\ndef seq_signature(seq: str):\n    return (len(seq.split()), count_shape_variety(seq), count_color_variety(seq))\n\n\ntrain_signatures = set(seq_signature(s) for s in ds[\"train\"][\"sequence\"])\n\n\ndef compute_metrics(all_raw, all_truth, all_pred):\n    acc = np.mean(np.array(all_truth) == np.array(all_pred))\n    swa = shape_weighted_accuracy(all_raw, all_truth, all_pred)\n    cwa = color_weighted_accuracy(all_raw, all_truth, all_pred)\n    # NRGS\n    mask_unseen = [seq_signature(s) not in train_signatures for s in all_raw]\n    if any(mask_unseen):\n        true_unseen = [t for t, m in zip(all_truth, mask_unseen) if m]\n        pred_unseen = [p for p, m in zip(all_pred, mask_unseen) if m]\n        nrg = np.mean(np.array(true_unseen) == np.array(pred_unseen))\n    else:\n        nrg = float(\"nan\")\n    return {\"acc\": acc, \"swa\": swa, \"cwa\": cwa, \"nrg\": nrg}\n\n\n# ------------------------------------------------------------------\n# ------------------------- Training loop --------------------------\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        inp = batch[\"input\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        out = model(inp)\n        loss = criterion(out, lbl)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * inp.size(0)\n    train_loss = epoch_loss / len(train_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    val_loss = 0.0\n    all_raw, all_truth, all_pred = [], [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inp = batch[\"input\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            out = model(inp)\n            loss = criterion(out, lbl)\n            val_loss += loss.item() * inp.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            truths = lbl.cpu().tolist()\n            all_pred.extend(preds)\n            all_truth.extend(truths)\n            all_raw.extend(batch[\"raw_seq\"])\n    val_loss /= len(val_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    metrics = compute_metrics(all_raw, all_truth, all_pred)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(metrics)\n    print(\n        f'Epoch {epoch}: validation_loss = {val_loss:.4f} | ACC={metrics[\"acc\"]:.3f} SWA={metrics[\"swa\"]:.3f} CWA={metrics[\"cwa\"]:.3f} NRGS={metrics[\"nrg\"]:.3f}'\n    )\n\n# ------------------------------------------------------------------\n# ------------------  Final evaluation on TEST ---------------------\nmodel.eval()\nall_raw, all_truth, all_pred = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        inp = batch[\"input\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        out = model(inp)\n        preds = out.argmax(1).cpu().tolist()\n        truths = lbl.cpu().tolist()\n        all_pred.extend(preds)\n        all_truth.extend(truths)\n        all_raw.extend(batch[\"raw_seq\"])\ntest_metrics = compute_metrics(all_raw, all_truth, all_pred)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = test_metrics\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_truth\nexperiment_data[\"SPR_BENCH\"][\"signatures\"] = [seq_signature(s) for s in all_raw]\n\nprint(\"\\nTest set results:\")\nfor k, v in test_metrics.items():\n    print(f\"  {k.upper():4s}: {v:.3f}\")\n\n# ------------------------------------------------------------------\n# ------------------  Save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f\"\\nAll experiment data saved to {os.path.join(working_dir, 'experiment_data.npy')}\"\n)\n", "import os, pathlib, re, time, math, random, json\nfrom collections import Counter\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# WORK DIR & DEVICE\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# DATA LOADING UTILITIES  (copied / slightly tweaked from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [count_shape_variety(s) for s in seqs]\n    corr = [w if yt == yp else 0 for w, yt, yp in zip(ws, y_true, y_pred)]\n    return sum(corr) / sum(ws) if sum(ws) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [count_color_variety(s) for s in seqs]\n    corr = [w if yt == yp else 0 for w, yt, yp in zip(ws, y_true, y_pred)]\n    return sum(corr) / sum(ws) if sum(ws) else 0.0\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# LOAD DATA\nSPR_PATH = pathlib.Path(os.getenv(\"SPR_DATA_DIR\", \"./SPR_BENCH\"))\nif not SPR_PATH.exists():\n    raise FileNotFoundError(\n        f\"SPR_BENCH data folder not found at {SPR_PATH}. \"\n        \"Set env var SPR_DATA_DIR if located elsewhere.\"\n    )\ndsets = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# VOCABULARY BUILDING\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = Counter()\nfor ex in dsets[\"train\"]:\n    vocab.update(tokenize(ex[\"sequence\"]))\ntok2id = {tok: i + 2 for i, tok in enumerate(sorted(vocab))}\ntok2id[\"<pad>\"] = 0\ntok2id[\"<unk>\"] = 1\nid2tok = {i: t for t, i in tok2id.items()}\nvocab_size = len(tok2id)\nprint(\"Vocab size:\", vocab_size)\n\n# LABEL ENCODING\nlabels = sorted(set(ex[\"label\"] for ex in dsets[\"train\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nid2lab = {i: l for l, i in lab2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# DATASET & DATALOADER\nMAX_LEN = 40  # sequences are short; pad/truncate to 40 tokens\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        tokens = [tok2id.get(t, 1) for t in tokenize(seq)][:MAX_LEN]\n        length = len(tokens)\n        if length < MAX_LEN:\n            tokens += [0] * (MAX_LEN - length)\n        shape_var = count_shape_variety(seq)\n        color_var = count_color_variety(seq)\n        return {\n            \"input_ids\": torch.tensor(tokens, dtype=torch.long),\n            \"shape_var\": torch.tensor([shape_var], dtype=torch.float),\n            \"color_var\": torch.tensor([color_var], dtype=torch.float),\n            \"label\": torch.tensor(lab2id[self.labels[idx]], dtype=torch.long),\n            \"raw_seq\": seq,\n        }\n\n\ndef collate_fn(batch):\n    batch_out = {\n        k: torch.stack([item[k] for item in batch])\n        for k in [\"input_ids\", \"shape_var\", \"color_var\", \"label\"]\n    }\n    batch_out[\"raw_seq\"] = [item[\"raw_seq\"] for item in batch]\n    return batch_out\n\n\nBATCH = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BATCH, shuffle=False, collate_fn=collate_fn\n)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# MODEL\nclass NeuroSymbolicSPR(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2 + 2, num_classes)\n\n    def forward(self, input_ids, sym_features):\n        x = self.emb(input_ids)  # [B, L, E]\n        x, _ = self.lstm(x)  # [B, L, 2H]\n        x = x.mean(dim=1)  # simple mean-pool\n        x = torch.cat([x, sym_features], dim=1)\n        return self.fc(x)\n\n\nmodel = NeuroSymbolicSPR(\n    vocab_size, emb_dim=64, hidden_dim=64, num_classes=num_classes\n).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# EXPERIMENT DATA STRUCTURE\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"val_SWA\": [],\n            \"val_CWA\": [],\n            \"val_NRGS\": [],\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"raw_sequences\": [],\n    }\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# HELPER \u2011 build training rule templates for NRGS\ntrain_templates = set(\n    \" \".join(tok[0] for tok in tokenize(s)) for s in dsets[\"train\"][\"sequence\"]\n)\n\n\ndef compute_NRGS(seqs, y_true, y_pred):\n    novel_idx = [\n        i\n        for i, s in enumerate(seqs)\n        if \" \".join(tok[0] for tok in tokenize(s)) not in train_templates\n    ]\n    if not novel_idx:\n        return 0.0\n    correct = sum(1 for i in novel_idx if y_true[i] == y_pred[i])\n    return correct / len(novel_idx)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# TRAINING LOOP\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        sym = torch.cat([batch[\"shape_var\"], batch[\"color_var\"]], dim=1).to(device)\n        logits = model(batch[\"input_ids\"], sym)\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"label\"].size(0)\n    avg_train_loss = running_loss / len(train_loader.dataset)\n\n    # VALIDATION\n    model.eval()\n    val_loss, y_true, y_pred, val_seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            sym = torch.cat([batch_t[\"shape_var\"], batch_t[\"color_var\"]], dim=1)\n            logits = model(batch_t[\"input_ids\"], sym)\n            loss = criterion(logits, batch_t[\"label\"])\n            val_loss += loss.item() * batch_t[\"label\"].size(0)\n            preds = logits.argmax(dim=1).cpu().tolist()\n            y_pred.extend(preds)\n            y_true.extend(batch[\"label\"].tolist())\n            val_seqs.extend(batch[\"raw_seq\"])\n    avg_val_loss = val_loss / len(dev_loader.dataset)\n    swa = shape_weighted_accuracy(val_seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(val_seqs, y_true, y_pred)\n    nrgs = compute_NRGS(val_seqs, y_true, y_pred)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(avg_train_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(avg_val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_SWA\"].append(swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_CWA\"].append(cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_NRGS\"].append(nrgs)\n\n    print(\n        f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}, \"\n        f\"val_loss={avg_val_loss:.4f}, SWA={swa:.3f}, CWA={cwa:.3f}, NRGS={nrgs:.3f}\"\n    )\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# FINAL TEST EVALUATION\nmodel.eval()\ntest_y, test_pred, test_seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch_t = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        sym = torch.cat([batch_t[\"shape_var\"], batch_t[\"color_var\"]], dim=1)\n        logits = model(batch_t[\"input_ids\"], sym)\n        preds = logits.argmax(dim=1).cpu().tolist()\n        test_pred.extend(preds)\n        test_y.extend(batch[\"label\"].tolist())\n        test_seqs.extend(batch[\"raw_seq\"])\nswa_test = shape_weighted_accuracy(test_seqs, test_y, test_pred)\ncwa_test = color_weighted_accuracy(test_seqs, test_y, test_pred)\nnrgs_test = compute_NRGS(test_seqs, test_y, test_pred)\nprint(f\"\\nTEST  SWA={swa_test:.4f}, CWA={cwa_test:.4f}, NRGS={nrgs_test:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_y\nexperiment_data[\"SPR_BENCH\"][\"raw_sequences\"] = test_seqs\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, math, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom collections import Counter\n\n# ------------------------------------------------------------------\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# DEVICE\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# -------------- DATA ------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [w_ if t == p else 0 for w_, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [w_ if t == p else 0 for w_, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nDATA_PATH = pathlib.Path(\"SPR_BENCH\")  # adjust if necessary\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------------------------------------------------------------\n# -------------- VOCAB & LABEL MAP ----------------------------------\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab = {tok: i + 2 for i, tok in enumerate(sorted(set(all_tokens)))}  # 0=pad,1=unk\nvocab_size = len(vocab) + 2\npad_id, unk_id = 0, 1\n\n\ndef encode(seq):\n    return [vocab.get(tok, unk_id) for tok in tokenize(seq)]\n\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nnum_classes = len(label_set)\n\n\n# ------------------------------------------------------------------\n# -------------- DATASET OBJ ----------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, split):\n        self.seq = spr[split][\"sequence\"]\n        self.lab = [label2id[l] for l in spr[split][\"label\"]]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = torch.tensor(encode(self.seq[idx]), dtype=torch.long)\n        y = torch.tensor(self.lab[idx], dtype=torch.long)\n        return {\"ids\": s, \"label\": y}\n\n\ndef collate(batch):\n    lens = [len(b[\"ids\"]) for b in batch]\n    max_len = max(lens)\n    ids = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : lens[i]] = b[\"ids\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\n        \"ids\": ids.to(device),\n        \"lengths\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"label\": labels.to(device),\n    }\n\n\ntrain_loader = DataLoader(\n    SPRTorch(\"train\"), batch_size=256, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorch(\"dev\"), batch_size=512, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRTorch(\"test\"), batch_size=512, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------\n# -------------- MODEL ----------------------------------------------\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab, emb=64, hid=128, num_cls=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb, padding_idx=pad_id)\n        self.lstm = nn.LSTM(emb, hid, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(hid * 2, num_cls)\n\n    def forward(self, ids, lengths):\n        x = self.emb(ids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (h, _) = self.lstm(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        return self.lin(h_cat)\n\n\nmodel = LSTMClassifier(vocab_size, num_cls=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ------------------------------------------------------------------\n# -------------- TRAIN ----------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\ndef run_epoch(loader, train=True):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    all_preds, all_y = [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits = model(batch[\"ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            preds = logits.argmax(1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            total += batch[\"label\"].size(0)\n            all_preds.extend(preds.cpu().tolist())\n            all_y.extend(batch[\"label\"].cpu().tolist())\n    return total_loss / total, correct / total, all_preds, all_y\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc, _, _ = run_epoch(train_loader, True)\n    val_loss, val_acc, _, _ = run_epoch(dev_loader, False)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}\")\n\n\n# ------------------------------------------------------------------\n# -------------- EVALUATION on DEV/TEST -----------------------------\ndef evaluate(loader, split_name):\n    model.eval()\n    seqs = spr[split_name][\"sequence\"]\n    all_preds, all_y = [], []\n    for batch in loader:\n        with torch.no_grad():\n            logits = model(batch[\"ids\"], batch[\"lengths\"])\n        preds = logits.argmax(1)\n        all_preds.extend(preds.cpu().tolist())\n        all_y.extend(batch[\"label\"].cpu().tolist())\n    # metrics\n    acc = sum(p == y for p, y in zip(all_preds, all_y)) / len(all_y)\n    swa = shape_weighted_accuracy(seqs, all_y, all_preds)\n    cwa = color_weighted_accuracy(seqs, all_y, all_preds)\n    return acc, swa, cwa, all_preds, all_y\n\n\ndev_acc, dev_swa, dev_cwa, _, _ = evaluate(dev_loader, \"dev\")\nprint(f\"Dev  - Acc:{dev_acc:.4f}  SWA:{dev_swa:.4f}  CWA:{dev_cwa:.4f}\")\n\ntest_acc, test_swa, test_cwa, test_preds, test_ys = evaluate(test_loader, \"test\")\n\n\n# ------------------------------------------------------------------\n# -------------- NRGS -----------------------------------------------\ndef rule_signature(seq: str):\n    return \"\".join(tok[0] for tok in seq.split())  # shapes only\n\n\ntrain_sigs = set(rule_signature(s) for s in spr[\"train\"][\"sequence\"])\ntest_sigs = [rule_signature(s) for s in spr[\"test\"][\"sequence\"]]\n\nunseen_idx = [i for i, sig in enumerate(test_sigs) if sig not in train_sigs]\nif unseen_idx:\n    unseen_correct = sum(int(test_preds[i] == test_ys[i]) for i in unseen_idx)\n    nrg_score = unseen_correct / len(unseen_idx)\nelse:\n    nrg_score = float(\"nan\")\n\nprint(\n    f\"Test - Acc:{test_acc:.4f}  SWA:{test_swa:.4f}  CWA:{test_cwa:.4f}  NRGS:{nrg_score:.4f}\"\n)\n\n# ------------------------------------------------------------------\n# -------------- SAVE RESULTS ---------------------------------------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_ys\nexperiment_data[\"SPR_BENCH\"][\"NRGS\"] = nrg_score\ntimestamp = int(time.time())\nnp.save(os.path.join(working_dir, f\"experiment_data_{timestamp}.npy\"), experiment_data)\nprint(\"Saved experiment data to\", working_dir)\n", "import os, pathlib, math, time, json, random, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# --------------------------- house-keeping ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- metric helpers --------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef rule_signature(sequence: str) -> str:\n    \"\"\"signature used for NRGS: ordered list of shapes\"\"\"\n    return \" \".join(tok[0] for tok in sequence.strip().split() if tok)\n\n\n# --------------------------- data loading ----------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# --------------------------- vocab & encoding ------------------------\nPAD_TOKEN = \"<PAD>\"\nUNK_TOKEN = \"<UNK>\"\n\n\ndef build_vocab(dataset):\n    tokens = set()\n    for seq in dataset[\"sequence\"]:\n        tokens.update(seq.strip().split())\n    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n    for t in sorted(tokens):\n        vocab[t] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_sequence(seq, vocab=vocab):\n    return [vocab.get(tok, vocab[UNK_TOKEN]) for tok in seq.strip().split()]\n\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2idx = {l: i for i, l in enumerate(label_set)}\nidx2label = {i: l for l, i in label2idx.items()}\nprint(f\"Labels: {label_set}\")\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = [label2idx[l] for l in hf_split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq_enc\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate_fn(batch):\n    seqs = [b[\"seq_enc\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(\n        seqs, batch_first=True, padding_value=vocab[PAD_TOKEN]\n    )\n    return {\"input_ids\": padded, \"labels\": labels, \"raw_seq\": raw}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\n\n\n# --------------------------- model -----------------------------------\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_size, emb=32, hidden=64, num_labels=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb, padding_idx=0)\n        self.gru = nn.GRU(emb, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_labels)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, h = self.gru(x)\n        logits = self.fc(h.squeeze(0))\n        return logits\n\n\nmodel = GRUClassifier(len(vocab), emb=32, hidden=64, num_labels=len(label_set)).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------------- experiment log --------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# Precompute train rule signatures for NRGS\ntrain_signatures = set(rule_signature(s) for s in spr[\"train\"][\"sequence\"])\n\n\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_seq, all_true, all_pred = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            inp = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            logits = model(inp)\n            loss = criterion(logits, labels)\n            loss_sum += loss.item() * len(labels)\n            preds = logits.argmax(dim=-1)\n            correct += (preds == labels).sum().item()\n            total += len(labels)\n            all_seq.extend(batch[\"raw_seq\"])\n            all_true.extend(labels.cpu().tolist())\n            all_pred.extend(preds.cpu().tolist())\n    acc = correct / total\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    # NRGS\n    novel_mask = [rule_signature(s) not in train_signatures for s in all_seq]\n    novel_total = sum(novel_mask)\n    novel_correct = sum(\n        int(pred == true)\n        for pred, true, nov in zip(all_pred, all_true, novel_mask)\n        if nov\n    )\n    nrgs = novel_correct / novel_total if novel_total > 0 else 0.0\n    avg_loss = loss_sum / total\n    return avg_loss, acc, swa, cwa, nrgs, all_pred, all_true, all_seq\n\n\n# --------------------------- training loop ---------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_ds)\n\n    val_loss, val_acc, val_swa, val_cwa, val_nrgs, _, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.3f}  \"\n        f\"SWA={val_swa:.3f} CWA={val_cwa:.3f} NRGS={val_nrgs:.3f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append({\"epoch\": epoch})\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\n            \"epoch\": epoch,\n            \"acc\": val_acc,\n            \"swa\": val_swa,\n            \"cwa\": val_cwa,\n            \"nrgs\": val_nrgs,\n        }\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n# --------------------------- final test eval -------------------------\ntest_loss, test_acc, test_swa, test_cwa, test_nrgs, preds, trues, seqs = evaluate(\n    test_loader\n)\nprint(\n    \"\\nTEST RESULTS  \",\n    f\"loss={test_loss:.4f}  acc={test_acc:.3f}  \"\n    f\"SWA={test_swa:.3f} CWA={test_cwa:.3f} NRGS={test_nrgs:.3f}\",\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = trues\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"nrgs\": test_nrgs,\n}\n\n# --------------------------- save artefacts --------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# quick visualisation\nfig, ax = plt.subplots(figsize=(6, 4))\nax.bar(\n    [\"Acc\", \"SWA\", \"CWA\", \"NRGS\"],\n    [test_acc, test_swa, test_cwa, test_nrgs],\n    color=\"skyblue\",\n)\nax.set_ylim(0, 1)\nax.set_title(\"SPR_BENCH Test Metrics\")\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"spr_metrics_bar.png\")\nplt.savefig(plot_path)\nprint(f\"Plot saved to {plot_path}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, random, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# --------------------------- house-keeping ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- metric helpers --------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef rule_signature(sequence: str) -> str:\n    \"\"\"signature used for NRGS: ordered list of shapes\"\"\"\n    return \" \".join(tok[0] for tok in sequence.strip().split() if tok)\n\n\n# --------------------------- data loading ----------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# --------------------------- vocab & encoding ------------------------\nPAD_TOKEN = \"<PAD>\"\nUNK_TOKEN = \"<UNK>\"\n\n\ndef build_vocab(dataset):\n    tokens = set()\n    for seq in dataset[\"sequence\"]:\n        tokens.update(seq.strip().split())\n    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n    for t in sorted(tokens):\n        vocab[t] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_sequence(seq, vocab=vocab):\n    return [vocab.get(tok, vocab[UNK_TOKEN]) for tok in seq.strip().split()]\n\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2idx = {l: i for i, l in enumerate(label_set)}\nidx2label = {i: l for l, i in label2idx.items()}\nprint(f\"Labels: {label_set}\")\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = [label2idx[l] for l in hf_split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq_enc\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate_fn(batch):\n    seqs = [b[\"seq_enc\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(\n        seqs, batch_first=True, padding_value=vocab[PAD_TOKEN]\n    )\n    return {\"input_ids\": padded, \"labels\": labels, \"raw_seq\": raw}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\n\n\n# --------------------------- model -----------------------------------\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_size, emb=32, hidden=64, num_labels=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb, padding_idx=0)\n        self.gru = nn.GRU(emb, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_labels)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, h = self.gru(x)\n        logits = self.fc(h.squeeze(0))\n        return logits\n\n\nmodel = GRUClassifier(len(vocab), emb=32, hidden=64, num_labels=len(label_set)).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------------- experiment log --------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# Precompute train rule signatures for NRGS\ntrain_signatures = set(rule_signature(s) for s in spr[\"train\"][\"sequence\"])\n\n\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_seq, all_true, all_pred = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            inp = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            logits = model(inp)\n            loss = criterion(logits, labels)\n            loss_sum += loss.item() * len(labels)\n            preds = logits.argmax(dim=-1)\n            correct += (preds == labels).sum().item()\n            total += len(labels)\n            all_seq.extend(batch[\"raw_seq\"])\n            all_true.extend(labels.cpu().tolist())\n            all_pred.extend(preds.cpu().tolist())\n    acc = correct / total\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    # NRGS\n    novel_mask = [rule_signature(s) not in train_signatures for s in all_seq]\n    novel_total = sum(novel_mask)\n    novel_correct = sum(\n        int(pred == true)\n        for pred, true, nov in zip(all_pred, all_true, novel_mask)\n        if nov\n    )\n    nrgs = novel_correct / novel_total if novel_total > 0 else 0.0\n    avg_loss = loss_sum / total\n    return avg_loss, acc, swa, cwa, nrgs, all_pred, all_true, all_seq\n\n\n# --------------------------- training loop ---------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_ds)\n\n    val_loss, val_acc, val_swa, val_cwa, val_nrgs, _, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.3f}  \"\n        f\"SWA={val_swa:.3f} CWA={val_cwa:.3f} NRGS={val_nrgs:.3f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append({\"epoch\": epoch})\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\n            \"epoch\": epoch,\n            \"acc\": val_acc,\n            \"swa\": val_swa,\n            \"cwa\": val_cwa,\n            \"nrgs\": val_nrgs,\n        }\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n# --------------------------- final test eval -------------------------\ntest_loss, test_acc, test_swa, test_cwa, test_nrgs, preds, trues, seqs = evaluate(\n    test_loader\n)\nprint(\n    \"\\nTEST RESULTS  \",\n    f\"loss={test_loss:.4f}  acc={test_acc:.3f}  \"\n    f\"SWA={test_swa:.3f} CWA={test_cwa:.3f} NRGS={test_nrgs:.3f}\",\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = trues\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"nrgs\": test_nrgs,\n}\n\n# --------------------------- save artefacts --------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# quick visualisation\nfig, ax = plt.subplots(figsize=(6, 4))\nax.bar(\n    [\"Acc\", \"SWA\", \"CWA\", \"NRGS\"],\n    [test_acc, test_swa, test_cwa, test_nrgs],\n    color=\"skyblue\",\n)\nax.set_ylim(0, 1)\nax.set_title(\"SPR_BENCH Test Metrics\")\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"spr_metrics_bar.png\")\nplt.savefig(plot_path)\nprint(f\"Plot saved to {plot_path}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, random, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# --------------------------- house-keeping ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- metric helpers --------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef rule_signature(sequence: str) -> str:\n    \"\"\"signature used for NRGS: ordered list of shapes\"\"\"\n    return \" \".join(tok[0] for tok in sequence.strip().split() if tok)\n\n\n# --------------------------- data loading ----------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# --------------------------- vocab & encoding ------------------------\nPAD_TOKEN = \"<PAD>\"\nUNK_TOKEN = \"<UNK>\"\n\n\ndef build_vocab(dataset):\n    tokens = set()\n    for seq in dataset[\"sequence\"]:\n        tokens.update(seq.strip().split())\n    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n    for t in sorted(tokens):\n        vocab[t] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_sequence(seq, vocab=vocab):\n    return [vocab.get(tok, vocab[UNK_TOKEN]) for tok in seq.strip().split()]\n\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2idx = {l: i for i, l in enumerate(label_set)}\nidx2label = {i: l for l, i in label2idx.items()}\nprint(f\"Labels: {label_set}\")\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = [label2idx[l] for l in hf_split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq_enc\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate_fn(batch):\n    seqs = [b[\"seq_enc\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(\n        seqs, batch_first=True, padding_value=vocab[PAD_TOKEN]\n    )\n    return {\"input_ids\": padded, \"labels\": labels, \"raw_seq\": raw}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\n\n\n# --------------------------- model -----------------------------------\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_size, emb=32, hidden=64, num_labels=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb, padding_idx=0)\n        self.gru = nn.GRU(emb, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_labels)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, h = self.gru(x)\n        logits = self.fc(h.squeeze(0))\n        return logits\n\n\nmodel = GRUClassifier(len(vocab), emb=32, hidden=64, num_labels=len(label_set)).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------------- experiment log --------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# Precompute train rule signatures for NRGS\ntrain_signatures = set(rule_signature(s) for s in spr[\"train\"][\"sequence\"])\n\n\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_seq, all_true, all_pred = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            inp = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            logits = model(inp)\n            loss = criterion(logits, labels)\n            loss_sum += loss.item() * len(labels)\n            preds = logits.argmax(dim=-1)\n            correct += (preds == labels).sum().item()\n            total += len(labels)\n            all_seq.extend(batch[\"raw_seq\"])\n            all_true.extend(labels.cpu().tolist())\n            all_pred.extend(preds.cpu().tolist())\n    acc = correct / total\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    # NRGS\n    novel_mask = [rule_signature(s) not in train_signatures for s in all_seq]\n    novel_total = sum(novel_mask)\n    novel_correct = sum(\n        int(pred == true)\n        for pred, true, nov in zip(all_pred, all_true, novel_mask)\n        if nov\n    )\n    nrgs = novel_correct / novel_total if novel_total > 0 else 0.0\n    avg_loss = loss_sum / total\n    return avg_loss, acc, swa, cwa, nrgs, all_pred, all_true, all_seq\n\n\n# --------------------------- training loop ---------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_ds)\n\n    val_loss, val_acc, val_swa, val_cwa, val_nrgs, _, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.3f}  \"\n        f\"SWA={val_swa:.3f} CWA={val_cwa:.3f} NRGS={val_nrgs:.3f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append({\"epoch\": epoch})\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\n            \"epoch\": epoch,\n            \"acc\": val_acc,\n            \"swa\": val_swa,\n            \"cwa\": val_cwa,\n            \"nrgs\": val_nrgs,\n        }\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n# --------------------------- final test eval -------------------------\ntest_loss, test_acc, test_swa, test_cwa, test_nrgs, preds, trues, seqs = evaluate(\n    test_loader\n)\nprint(\n    \"\\nTEST RESULTS  \",\n    f\"loss={test_loss:.4f}  acc={test_acc:.3f}  \"\n    f\"SWA={test_swa:.3f} CWA={test_cwa:.3f} NRGS={test_nrgs:.3f}\",\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = trues\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"nrgs\": test_nrgs,\n}\n\n# --------------------------- save artefacts --------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# quick visualisation\nfig, ax = plt.subplots(figsize=(6, 4))\nax.bar(\n    [\"Acc\", \"SWA\", \"CWA\", \"NRGS\"],\n    [test_acc, test_swa, test_cwa, test_nrgs],\n    color=\"skyblue\",\n)\nax.set_ylim(0, 1)\nax.set_title(\"SPR_BENCH Test Metrics\")\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"spr_metrics_bar.png\")\nplt.savefig(plot_path)\nprint(f\"Plot saved to {plot_path}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, random, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# --------------------------- house-keeping ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------- metric helpers --------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef rule_signature(sequence: str) -> str:\n    \"\"\"signature used for NRGS: ordered list of shapes\"\"\"\n    return \" \".join(tok[0] for tok in sequence.strip().split() if tok)\n\n\n# --------------------------- data loading ----------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# --------------------------- vocab & encoding ------------------------\nPAD_TOKEN = \"<PAD>\"\nUNK_TOKEN = \"<UNK>\"\n\n\ndef build_vocab(dataset):\n    tokens = set()\n    for seq in dataset[\"sequence\"]:\n        tokens.update(seq.strip().split())\n    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n    for t in sorted(tokens):\n        vocab[t] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_sequence(seq, vocab=vocab):\n    return [vocab.get(tok, vocab[UNK_TOKEN]) for tok in seq.strip().split()]\n\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2idx = {l: i for i, l in enumerate(label_set)}\nidx2label = {i: l for l, i in label2idx.items()}\nprint(f\"Labels: {label_set}\")\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = [label2idx[l] for l in hf_split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq_enc\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate_fn(batch):\n    seqs = [b[\"seq_enc\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(\n        seqs, batch_first=True, padding_value=vocab[PAD_TOKEN]\n    )\n    return {\"input_ids\": padded, \"labels\": labels, \"raw_seq\": raw}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\n\n\n# --------------------------- model -----------------------------------\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_size, emb=32, hidden=64, num_labels=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb, padding_idx=0)\n        self.gru = nn.GRU(emb, hidden, batch_first=True)\n        self.fc = nn.Linear(hidden, num_labels)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, h = self.gru(x)\n        logits = self.fc(h.squeeze(0))\n        return logits\n\n\nmodel = GRUClassifier(len(vocab), emb=32, hidden=64, num_labels=len(label_set)).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------------- experiment log --------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# Precompute train rule signatures for NRGS\ntrain_signatures = set(rule_signature(s) for s in spr[\"train\"][\"sequence\"])\n\n\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_seq, all_true, all_pred = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            inp = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            logits = model(inp)\n            loss = criterion(logits, labels)\n            loss_sum += loss.item() * len(labels)\n            preds = logits.argmax(dim=-1)\n            correct += (preds == labels).sum().item()\n            total += len(labels)\n            all_seq.extend(batch[\"raw_seq\"])\n            all_true.extend(labels.cpu().tolist())\n            all_pred.extend(preds.cpu().tolist())\n    acc = correct / total\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    # NRGS\n    novel_mask = [rule_signature(s) not in train_signatures for s in all_seq]\n    novel_total = sum(novel_mask)\n    novel_correct = sum(\n        int(pred == true)\n        for pred, true, nov in zip(all_pred, all_true, novel_mask)\n        if nov\n    )\n    nrgs = novel_correct / novel_total if novel_total > 0 else 0.0\n    avg_loss = loss_sum / total\n    return avg_loss, acc, swa, cwa, nrgs, all_pred, all_true, all_seq\n\n\n# --------------------------- training loop ---------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_ds)\n\n    val_loss, val_acc, val_swa, val_cwa, val_nrgs, _, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.3f}  \"\n        f\"SWA={val_swa:.3f} CWA={val_cwa:.3f} NRGS={val_nrgs:.3f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append({\"epoch\": epoch})\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\n            \"epoch\": epoch,\n            \"acc\": val_acc,\n            \"swa\": val_swa,\n            \"cwa\": val_cwa,\n            \"nrgs\": val_nrgs,\n        }\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n# --------------------------- final test eval -------------------------\ntest_loss, test_acc, test_swa, test_cwa, test_nrgs, preds, trues, seqs = evaluate(\n    test_loader\n)\nprint(\n    \"\\nTEST RESULTS  \",\n    f\"loss={test_loss:.4f}  acc={test_acc:.3f}  \"\n    f\"SWA={test_swa:.3f} CWA={test_cwa:.3f} NRGS={test_nrgs:.3f}\",\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = trues\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"nrgs\": test_nrgs,\n}\n\n# --------------------------- save artefacts --------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# quick visualisation\nfig, ax = plt.subplots(figsize=(6, 4))\nax.bar(\n    [\"Acc\", \"SWA\", \"CWA\", \"NRGS\"],\n    [test_acc, test_swa, test_cwa, test_nrgs],\n    color=\"skyblue\",\n)\nax.set_ylim(0, 1)\nax.set_title(\"SPR_BENCH Test Metrics\")\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"spr_metrics_bar.png\")\nplt.savefig(plot_path)\nprint(f\"Plot saved to {plot_path}\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Real SPR_BENCH not found, generating synthetic\ndata \u2026', '\\n', 'Number of labels = 2', '\\n', 'Vocab size = 27', '\\n', 'Epoch 1:\nvalidation_loss = 0.6252 | ACC=0.698 SWA=0.671 CWA=0.747 NRGS=1.000', '\\n',\n'Epoch 2: validation_loss = 0.6163 | ACC=0.702 SWA=0.672 CWA=0.751 NRGS=1.000',\n'\\n', 'Epoch 3: validation_loss = 0.6135 | ACC=0.703 SWA=0.673 CWA=0.752\nNRGS=1.000', '\\n', 'Epoch 4: validation_loss = 0.6127 | ACC=0.703 SWA=0.673\nCWA=0.752 NRGS=1.000', '\\n', 'Epoch 5: validation_loss = 0.6114 | ACC=0.703\nSWA=0.673 CWA=0.752 NRGS=1.000', '\\n', '\\nTest set results:', '\\n', '  ACC :\n0.698', '\\n', '  SWA : 0.661', '\\n', '  CWA : 0.747', '\\n', '  NRG : 0.500',\n'\\n', '\\nAll experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-1/working/experiment_data.npy', '\\n', 'Execution time: 3\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 59, in <module>\\n    raise\nFileNotFoundError(\\nFileNotFoundError: SPR_BENCH data folder not found at\nSPR_BENCH. Set env var SPR_DATA_DIR if located elsewhere.\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 56, in <module>\\n    spr = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 29, in load_spr_bench\\n\nd[\"train\"] = _load(\"train.csv\")\\n                 ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 21, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-14_17-37-\n20_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n3/SPR_BENCH/train.csv\\'\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 604114.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 798702.06\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 950421.25\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', 'Labels: [0, 1]', '\\n', 'Epoch 1: train_loss=0.3444\nval_loss=0.1715  val_acc=0.941  SWA=0.943 CWA=0.942 NRGS=0.907', '\\n', 'Epoch 2:\ntrain_loss=0.1467  val_loss=0.1293  val_acc=0.958  SWA=0.961 CWA=0.959\nNRGS=0.933', '\\n', 'Epoch 3: train_loss=0.1117  val_loss=0.0969  val_acc=0.974\nSWA=0.974 CWA=0.974 NRGS=0.959', '\\n', 'Epoch 4: train_loss=0.0768\nval_loss=0.0596  val_acc=0.982  SWA=0.982 CWA=0.982 NRGS=0.959', '\\n', 'Epoch 5:\ntrain_loss=0.0524  val_loss=0.0426  val_acc=0.990  SWA=0.990 CWA=0.990\nNRGS=0.979', '\\n', '\\nTEST RESULTS  ', ' ', 'loss=1.8976  acc=0.696  SWA=0.650\nCWA=0.696 NRGS=0.750', '\\n', 'Plot saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-4/working/spr_metrics_bar.png', '\\n', 'Execution time: 8\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 355044.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 558064.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 612548.60\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', 'Labels: [0, 1]', '\\n', 'Epoch 1: train_loss=0.3328\nval_loss=0.1637  val_acc=0.950  SWA=0.953 CWA=0.951 NRGS=0.922', '\\n', 'Epoch 2:\ntrain_loss=0.1419  val_loss=0.1284  val_acc=0.962  SWA=0.965 CWA=0.963\nNRGS=0.948', '\\n', 'Epoch 3: train_loss=0.1136  val_loss=0.1046  val_acc=0.968\nSWA=0.969 CWA=0.968 NRGS=0.943', '\\n', 'Epoch 4: train_loss=0.0930\nval_loss=0.0850  val_acc=0.977  SWA=0.976 CWA=0.977 NRGS=0.959', '\\n', 'Epoch 5:\ntrain_loss=0.0776  val_loss=0.0713  val_acc=0.978  SWA=0.978 CWA=0.978\nNRGS=0.969', '\\n', '\\nTEST RESULTS  ', ' ', 'loss=1.8436  acc=0.693  SWA=0.647\nCWA=0.693 NRGS=0.747', '\\n', 'Plot saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-2/working/spr_metrics_bar.png', '\\n', 'Execution time:\n11 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Vocab size: 18', '\\n', 'Labels: [0, 1]', '\\n', 'Epoch 1:\ntrain_loss=0.3399  val_loss=0.1670  val_acc=0.945  SWA=0.948 CWA=0.946\nNRGS=0.912', '\\n', 'Epoch 2: train_loss=0.1530  val_loss=0.1435  val_acc=0.957\nSWA=0.959 CWA=0.958 NRGS=0.927', '\\n', 'Epoch 3: train_loss=0.1329\nval_loss=0.1233  val_acc=0.965  SWA=0.967 CWA=0.966 NRGS=0.938', '\\n', 'Epoch 4:\ntrain_loss=0.1104  val_loss=0.0958  val_acc=0.969  SWA=0.970 CWA=0.969\nNRGS=0.943', '\\n', 'Epoch 5: train_loss=0.0815  val_loss=0.0634  val_acc=0.979\nSWA=0.978 CWA=0.979 NRGS=0.953', '\\n', '\\nTEST RESULTS  ', ' ', 'loss=1.5900\nacc=0.691  SWA=0.646 CWA=0.692 NRGS=0.745', '\\n', 'Plot saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-14_17-37-\n20_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n4/working/spr_metrics_bar.png', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 559669.61\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 695780.50\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 802722.24\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', 'Labels: [0, 1]', '\\n', 'Epoch 1: train_loss=0.3306\nval_loss=0.1636  val_acc=0.944  SWA=0.947 CWA=0.945 NRGS=0.902', '\\n', 'Epoch 2:\ntrain_loss=0.1496  val_loss=0.1431  val_acc=0.956  SWA=0.959 CWA=0.956\nNRGS=0.917', '\\n', 'Epoch 3: train_loss=0.1326  val_loss=0.1341  val_acc=0.960\nSWA=0.963 CWA=0.961 NRGS=0.938', '\\n', 'Epoch 4: train_loss=0.1204\nval_loss=0.1132  val_acc=0.964  SWA=0.966 CWA=0.964 NRGS=0.927', '\\n', 'Epoch 5:\ntrain_loss=0.0936  val_loss=0.0787  val_acc=0.974  SWA=0.974 CWA=0.974\nNRGS=0.953', '\\n', '\\nTEST RESULTS  ', ' ', 'loss=1.5631  acc=0.689  SWA=0.644\nCWA=0.690 NRGS=0.745', '\\n', 'Plot saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-3/working/spr_metrics_bar.png', '\\n', 'Execution time: 5\nseconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "The execution failed because the SPR_BENCH data folder was not found. The script\nexpects the SPR_BENCH dataset to be located at a specific path, but it seems\nthat the data folder is either missing or the path is incorrectly set. To fix\nthis issue, ensure that the SPR_BENCH dataset is correctly placed in the\nexpected directory or set the environment variable SPR_DATA_DIR to the correct\npath where the dataset is located.", "The execution failed due to a FileNotFoundError. The script attempted to load\nthe 'SPR_BENCH/train.csv' file, but it could not find the required file in the\nspecified directory '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-14_17-\n37-20_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n3/SPR_BENCH/'.  To fix this issue, ensure that the 'SPR_BENCH' directory and its\nrequired files ('train.csv', 'dev.csv', 'test.csv') are present in the correct\npath. Verify the directory structure and file paths to confirm their existence\nbefore running the script again. If necessary, update the DATA_PATH variable to\npoint to the correct location of the dataset.", "", "", "", "The execution output indicates a significant drop in performance metrics during\nthe test evaluation phase compared to the validation phase. While the validation\naccuracy (val_acc) reaches 0.974 and other metrics (SWA, CWA, NRGS) are\nsimilarly high, the test accuracy (acc) drops to 0.689, and SWA to 0.644,\nindicating potential overfitting of the model to the training/validation data.\nTo address this, consider implementing regularization techniques such as dropout\nor weight decay, using early stopping based on validation performance, or\nincreasing the diversity of the training dataset to improve generalization.", ""], "exc_type": [null, "FileNotFoundError", "FileNotFoundError", null, null, null, null, null], "exc_info": [null, {"args": ["SPR_BENCH data folder not found at SPR_BENCH. Set env var SPR_DATA_DIR if located elsewhere."]}, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-3/SPR_BENCH/train.csv'"]}, null, null, null, null, null], "exc_stack": [null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 59, "<module>", "raise FileNotFoundError("]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 56, "<module>", "spr = load_spr_bench(DATA_PATH)"], ["runfile.py", 29, "load_spr_bench", "d[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 21, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value indicating the model's performance on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5962, "best_value": 0.5962}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value indicating the model's performance on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6114, "best_value": 0.6114}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.703, "best_value": 0.703}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6729, "best_value": 0.6729}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7517, "best_value": 0.7517}]}, {"metric_name": "validation new-relation generalization score", "lower_is_better": false, "description": "The new-relation generalization score of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6606, "best_value": 0.6606}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7472, "best_value": 0.7472}]}, {"metric_name": "test new-relation generalization score", "lower_is_better": false, "description": "The new-relation generalization score of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5, "best_value": 0.5}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.052, "best_value": 0.052}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation NRGS", "lower_is_better": false, "description": "The NRGS metric during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.979, "best_value": 0.979}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.898, "best_value": 1.898}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.65, "best_value": 0.65}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test NRGS", "lower_is_better": false, "description": "The NRGS metric during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.75, "best_value": 0.75}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.078, "best_value": 0.078}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.071, "best_value": 0.071}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation NRGS", "lower_is_better": false, "description": "The NRGS metric during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.969, "best_value": 0.969}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.844, "best_value": 1.844}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.693, "best_value": 0.693}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.647, "best_value": 0.647}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.693, "best_value": 0.693}]}, {"metric_name": "test NRGS", "lower_is_better": false, "description": "The NRGS metric during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.747, "best_value": 0.747}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model fits the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.082, "best_value": 0.082}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, used to evaluate the model's performance during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.063, "best_value": 0.063}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.979, "best_value": 0.979}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.979, "best_value": 0.979}]}, {"metric_name": "validation NRGS", "lower_is_better": false, "description": "The NRGS metric on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.953, "best_value": 0.953}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset, used to evaluate the model's generalization performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.59, "best_value": 1.59}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.691, "best_value": 0.691}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.646, "best_value": 0.646}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}, {"metric_name": "test NRGS", "lower_is_better": false, "description": "The NRGS metric on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.745, "best_value": 0.745}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.094, "best_value": 0.094}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.079, "best_value": 0.079}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "validation NRGS", "lower_is_better": false, "description": "The NRGS metric during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.953, "best_value": 0.953}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.563, "best_value": 1.563}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.689, "best_value": 0.689}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.644, "best_value": 0.644}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.69, "best_value": 0.69}]}, {"metric_name": "test NRGS", "lower_is_better": false, "description": "The NRGS metric during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.745, "best_value": 0.745}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_SWA.png", "../../logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_CWA.png"], [], [], ["../../logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_val_metrics.png", "../../logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_val_metrics.png", "../../logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_val_metrics.png", "../../logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_loss.png", "../../logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_acc.png", "../../logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_swa.png", "../../logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_cwa.png", "../../logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_nrgs.png", "../../logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_confusion_matrix.png"]], "plot_paths": [["experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_accuracy.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_SWA.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_CWA.png"], [], [], ["experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_metrics_bar.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_loss_curves.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_val_metrics.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_confusion_matrix.png"], ["experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_metrics_bar.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_loss_curves.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_val_metrics.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_confusion_matrix.png"], ["experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_metrics_bar.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_loss_curves.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_val_metrics.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_confusion_matrix.png"], [], ["experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_loss.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_acc.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_swa.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_cwa.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_nrgs.png", "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_72e8bf9867644b7d876c5f362363f859/spr_bench_aggregated_confusion_matrix.png"]], "plot_analyses": [[{"analysis": "The training and validation loss both decrease steadily over the epochs, indicating that the model is learning effectively. The training loss decreases more rapidly than the validation loss, which could suggest slight overfitting, but the gap between the two remains relatively small, indicating good generalization.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation accuracy improves consistently over the first few epochs and plateaus after epoch 3. This suggests that the model is converging and that additional training epochs may not yield significant improvements in accuracy.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_accuracy.png"}, {"analysis": "The shape-weighted accuracy (SWA) improves significantly in the first few epochs and stabilizes after epoch 3. This indicates that the model's ability to handle shape-related variations in the data improves quickly and reaches a saturation point.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_SWA.png"}, {"analysis": "The color-weighted accuracy (CWA) shows a similar trend to SWA, with rapid improvement in the first few epochs followed by stabilization. This demonstrates that the model is effectively learning to generalize across color-related variations in the data and achieves consistent performance after a few epochs.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_184ca956cf3d4cc99cfeda5a990d5550_proc_2699129/SPR_BENCH_val_CWA.png"}], [], [], [{"analysis": "The bar chart shows the performance metrics (Accuracy, Shape-Weighted Accuracy, Color-Weighted Accuracy, and Neural Rule Generalization Score) for the SPR_BENCH test set. All metrics are relatively high, with Neural Rule Generalization Score (NRGS) achieving the highest score. This suggests that the model is performing well across all evaluated dimensions, particularly in generalizing rules.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_metrics_bar.png"}, {"analysis": "The loss curve shows a steady decrease in both training and validation loss over the epochs. The validation loss closely follows the training loss, indicating no significant overfitting. The convergence of the loss curves suggests that the model is learning effectively and the optimization process is stable.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_loss_curves.png"}, {"analysis": "The line chart illustrates the validation metrics (Accuracy, SWA, CWA, and NRGS) over the epochs. All metrics show an upward trend and converge near 1.0, indicating that the model's performance improves consistently during training. This further supports the model's capability to generalize effectively across the validation set.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_val_metrics.png"}, {"analysis": "The confusion matrix provides a detailed view of the model's prediction performance. The true positives and true negatives are higher compared to false positives and false negatives, which indicates good predictive performance. However, there is room for improvement in reducing the number of false positives and false negatives to enhance overall accuracy.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2f00bcaba4ec4a3091522a3cd0f52f28_proc_2699132/spr_bench_confusion_matrix.png"}], [{"analysis": "This bar chart presents the test metrics for the SPR_BENCH dataset. The four metrics shown are Accuracy (Acc), Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and Neural Rule Generalization Score (NRGS). All metrics are relatively high, with NRGS achieving the highest value, indicating strong generalization capabilities of the model. The SWA and CWA metrics are slightly lower than the overall accuracy, suggesting that the model might be slightly less effective at handling the weighted aspects of shape and color diversity.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_metrics_bar.png"}, {"analysis": "This line graph illustrates the training and validation loss curves over five epochs. Both curves show a consistent decrease, with the validation loss closely following the training loss, indicating good generalization and minimal overfitting. The rapid decline in the first epoch suggests effective learning, and the subsequent stabilization demonstrates convergence.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_loss_curves.png"}, {"analysis": "This plot shows the validation metrics (Accuracy, SWA, CWA, and NRGS) across epochs. All metrics exhibit a steady increase, plateauing near their maximum values by the fifth epoch. The close alignment of the metrics indicates balanced performance across different evaluation criteria, with no significant trade-offs between them.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_val_metrics.png"}, {"analysis": "The confusion matrix provides insights into the model's classification performance. The diagonal values represent correct predictions, while off-diagonal values indicate misclassifications. The model shows a higher number of true positives (3247) and true negatives (3678) compared to false positives (1974) and false negatives (1101). This suggests the model performs well but has room for improvement in reducing false positives.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/spr_bench_confusion_matrix.png"}], [{"analysis": "The bar chart displays the test metrics for the SPR_BENCH dataset. Four metrics are shown: Accuracy (Acc), Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and Neural Rule Generalization Score (NRGS). All metrics have relatively high values, with NRGS achieving the highest score, indicating strong generalization capabilities of the model to unseen rules. SWA and CWA are slightly lower than NRGS but still demonstrate satisfactory performance. This suggests the model effectively balances shape and color reasoning in its predictions.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_metrics_bar.png"}, {"analysis": "The line plot shows the training and validation loss curves over 5 epochs. Both losses decrease consistently, with validation loss remaining slightly lower than training loss throughout. This indicates that the model is learning effectively without overfitting. The convergence of the loss curves suggests the model has achieved a stable state by the end of training.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_loss_curves.png"}, {"analysis": "This line plot illustrates the progression of validation metrics (Accuracy, SWA, CWA, and NRGS) across epochs. All metrics show a steady increase, with NRGS consistently achieving the highest values. This indicates the model's ability to generalize and improve its reasoning capabilities as training progresses. The relatively small gap between the metrics suggests balanced performance across different evaluation criteria.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_val_metrics.png"}, {"analysis": "The confusion matrix provides insights into the model's classification performance. The true positive and true negative counts (3243 and 3670 respectively) are significantly higher than false positives and false negatives (1982 and 1105 respectively). This indicates that the model has a strong ability to correctly classify samples, though there is room for improvement in reducing false positives and negatives. The matrix suggests a slightly imbalanced performance in favor of negative samples.", "plot_path": "experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/spr_bench_confusion_matrix.png"}], [], []], "vlm_feedback_summary": ["The plots indicate that the model is learning effectively, with consistent\nimprovements in loss, accuracy, and weighted accuracies across the initial\nepochs. Both SWA and CWA metrics demonstrate good generalization to shape and\ncolor variations, respectively, and the results suggest that the model converges\nefficiently within five epochs.", "[]", "[]", "The provided plots demonstrate a well-performing model with high metrics across\nthe board, stable loss curves with no overfitting, and consistent improvement in\nvalidation metrics. The confusion matrix highlights good predictive performance\nbut also suggests areas for improvement in reducing misclassifications.", "The plots demonstrate strong model performance with high accuracy and weighted\nmetrics, effective learning dynamics, and balanced validation metrics. The\nconfusion matrix highlights areas for improvement in reducing false positives.", "The plots demonstrate that the model performs well across various metrics and\nshows steady improvement during training. The results indicate effective\ngeneralization and balanced performance, with some room for reducing\nclassification errors.", "[]", "[]"], "exec_time": [3.9596056938171387, 0.632056474685669, 0.6900162696838379, 8.714859008789062, 11.793885707855225, 5.448888301849365, 5.963804721832275, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], [], [], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], [], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    losses_tr = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\n    val_metrics = data[\"metrics\"][\"val\"]  # list of dicts\n    epochs = list(range(1, len(losses_tr) + 1))\n\n    # Helper to extract metric series safely\n    def metric_series(key):\n        return [m.get(key, np.nan) for m in val_metrics]\n\n    # 1) Loss curves ------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, losses_tr, label=\"Train Loss\")\n        plt.plot(epochs, losses_val, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2) Accuracy curve --------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metric_series(\"acc\"), marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Accuracy per Epoch\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_accuracy.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # 3) Shape-weighted accuracy -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metric_series(\"swa\"), marker=\"o\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Shape-Weighted Acc\")\n        plt.title(\"SPR_BENCH: Validation SWA per Epoch\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_SWA.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot: {e}\")\n        plt.close()\n\n    # 4) Color-weighted accuracy -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metric_series(\"cwa\"), marker=\"o\", color=\"magenta\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Color-Weighted Acc\")\n        plt.title(\"SPR_BENCH: Validation CWA per Epoch\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_CWA.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------\n    # Print final test metrics\n    test_metrics = data[\"metrics\"][\"test\"]\n    print(\"\\nTest set metrics:\")\n    for k, v in test_metrics.items():\n        print(f\"  {k.upper():4s}: {v:.3f}\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- load experiment data -------------------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to limit epochs plotted\ndef epoch_subsample(x, max_points=5):\n    if len(x) <= max_points:\n        return np.arange(1, len(x) + 1), x\n    idx = np.linspace(0, len(x) - 1, max_points, dtype=int)\n    return idx + 1, [x[i] for i in idx]\n\n\nfor dset, data in experiment_data.items():\n    # ---------- 1. Loss curves ----------\n    try:\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n        if train_losses and val_losses:\n            ep_t, train_plot = epoch_subsample(train_losses)\n            ep_v, val_plot = epoch_subsample(val_losses)\n            plt.figure()\n            plt.plot(ep_t, train_plot, label=\"Train Loss\")\n            plt.plot(ep_v, val_plot, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset} Loss Curves\\nTrain vs. Validation\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 2. Validation metric trends ----------\n    try:\n        val_metrics = data[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            epochs = [m.get(\"epoch\", i + 1) for i, m in enumerate(val_metrics)]\n            metrics_to_plot = {\n                k: [m.get(k) for m in val_metrics]\n                for k in (\"acc\", \"swa\", \"cwa\", \"nrgs\")\n                if val_metrics[0].get(k) is not None\n            }\n            plt.figure()\n            for name, values in metrics_to_plot.items():\n                ep_s, vals_s = epoch_subsample(values)\n                plt.plot(ep_s, vals_s, label=name.upper())\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Value\")\n            plt.ylim(0, 1)\n            plt.title(f\"{dset} Validation Metrics Across Epochs\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_val_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 3. Confusion matrix ----------\n    try:\n        preds = np.array(data.get(\"predictions\", []))\n        trues = np.array(data.get(\"ground_truth\", []))\n        if preds.size and trues.size:\n            labels = np.unique(np.concatenate([preds, trues]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(trues, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{dset} Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- load experiment data -------------------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to limit epochs plotted\ndef epoch_subsample(x, max_points=5):\n    if len(x) <= max_points:\n        return np.arange(1, len(x) + 1), x\n    idx = np.linspace(0, len(x) - 1, max_points, dtype=int)\n    return idx + 1, [x[i] for i in idx]\n\n\nfor dset, data in experiment_data.items():\n    # ---------- 1. Loss curves ----------\n    try:\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n        if train_losses and val_losses:\n            ep_t, train_plot = epoch_subsample(train_losses)\n            ep_v, val_plot = epoch_subsample(val_losses)\n            plt.figure()\n            plt.plot(ep_t, train_plot, label=\"Train Loss\")\n            plt.plot(ep_v, val_plot, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset} Loss Curves\\nTrain vs. Validation\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 2. Validation metric trends ----------\n    try:\n        val_metrics = data[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            epochs = [m.get(\"epoch\", i + 1) for i, m in enumerate(val_metrics)]\n            metrics_to_plot = {\n                k: [m.get(k) for m in val_metrics]\n                for k in (\"acc\", \"swa\", \"cwa\", \"nrgs\")\n                if val_metrics[0].get(k) is not None\n            }\n            plt.figure()\n            for name, values in metrics_to_plot.items():\n                ep_s, vals_s = epoch_subsample(values)\n                plt.plot(ep_s, vals_s, label=name.upper())\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Value\")\n            plt.ylim(0, 1)\n            plt.title(f\"{dset} Validation Metrics Across Epochs\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_val_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 3. Confusion matrix ----------\n    try:\n        preds = np.array(data.get(\"predictions\", []))\n        trues = np.array(data.get(\"ground_truth\", []))\n        if preds.size and trues.size:\n            labels = np.unique(np.concatenate([preds, trues]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(trues, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{dset} Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- load experiment data -------------------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to limit epochs plotted\ndef epoch_subsample(x, max_points=5):\n    if len(x) <= max_points:\n        return np.arange(1, len(x) + 1), x\n    idx = np.linspace(0, len(x) - 1, max_points, dtype=int)\n    return idx + 1, [x[i] for i in idx]\n\n\nfor dset, data in experiment_data.items():\n    # ---------- 1. Loss curves ----------\n    try:\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n        if train_losses and val_losses:\n            ep_t, train_plot = epoch_subsample(train_losses)\n            ep_v, val_plot = epoch_subsample(val_losses)\n            plt.figure()\n            plt.plot(ep_t, train_plot, label=\"Train Loss\")\n            plt.plot(ep_v, val_plot, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset} Loss Curves\\nTrain vs. Validation\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 2. Validation metric trends ----------\n    try:\n        val_metrics = data[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            epochs = [m.get(\"epoch\", i + 1) for i, m in enumerate(val_metrics)]\n            metrics_to_plot = {\n                k: [m.get(k) for m in val_metrics]\n                for k in (\"acc\", \"swa\", \"cwa\", \"nrgs\")\n                if val_metrics[0].get(k) is not None\n            }\n            plt.figure()\n            for name, values in metrics_to_plot.items():\n                ep_s, vals_s = epoch_subsample(values)\n                plt.plot(ep_s, vals_s, label=name.upper())\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Value\")\n            plt.ylim(0, 1)\n            plt.title(f\"{dset} Validation Metrics Across Epochs\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_val_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 3. Confusion matrix ----------\n    try:\n        preds = np.array(data.get(\"predictions\", []))\n        trues = np.array(data.get(\"ground_truth\", []))\n        if preds.size and trues.size:\n            labels = np.unique(np.concatenate([preds, trues]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(trues, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{dset} Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- load experiment data -------------------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to limit epochs plotted\ndef epoch_subsample(x, max_points=5):\n    if len(x) <= max_points:\n        return np.arange(1, len(x) + 1), x\n    idx = np.linspace(0, len(x) - 1, max_points, dtype=int)\n    return idx + 1, [x[i] for i in idx]\n\n\nfor dset, data in experiment_data.items():\n    # ---------- 1. Loss curves ----------\n    try:\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n        if train_losses and val_losses:\n            ep_t, train_plot = epoch_subsample(train_losses)\n            ep_v, val_plot = epoch_subsample(val_losses)\n            plt.figure()\n            plt.plot(ep_t, train_plot, label=\"Train Loss\")\n            plt.plot(ep_v, val_plot, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset} Loss Curves\\nTrain vs. Validation\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 2. Validation metric trends ----------\n    try:\n        val_metrics = data[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            epochs = [m.get(\"epoch\", i + 1) for i, m in enumerate(val_metrics)]\n            metrics_to_plot = {\n                k: [m.get(k) for m in val_metrics]\n                for k in (\"acc\", \"swa\", \"cwa\", \"nrgs\")\n                if val_metrics[0].get(k) is not None\n            }\n            plt.figure()\n            for name, values in metrics_to_plot.items():\n                ep_s, vals_s = epoch_subsample(values)\n                plt.plot(ep_s, vals_s, label=name.upper())\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Value\")\n            plt.ylim(0, 1)\n            plt.title(f\"{dset} Validation Metrics Across Epochs\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_val_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- 3. Confusion matrix ----------\n    try:\n        preds = np.array(data.get(\"predictions\", []))\n        trues = np.array(data.get(\"ground_truth\", []))\n        if preds.size and trues.size:\n            labels = np.unique(np.concatenate([preds, trues]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(trues, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{dset} Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# basic setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load every experiment_data.npy that was supplied\nexperiment_data_path_list = [\n    \"experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_9b9e6880e5b44a73a095908a86e968e4_proc_2699130/experiment_data.npy\",\n    \"experiments/2025-08-14_17-37-20_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_57c4bfbf5b304cb99f20fca8220539e1_proc_2699132/experiment_data.npy\",\n    \"None/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor path in experiment_data_path_list:\n    try:\n        abs_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), path)\n        exp_dict = np.load(abs_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dict)\n    except Exception as e:\n        print(f\"Error loading {path}: {e}\")\n\n\n# ------------------------------------------------------------------\n# helper: gather arrays of equal length\ndef stack_and_crop(list_of_lists):\n    \"\"\"Crop every sequence to the minimum length and stack into 2-D array.\"\"\"\n    if not list_of_lists:\n        return np.array([])\n    min_len = min(len(seq) for seq in list_of_lists)\n    if min_len == 0:\n        return np.array([])\n    return np.vstack([np.array(seq[:min_len]) for seq in list_of_lists])\n\n\ndef epoch_subsample(arr, max_points=100):\n    \"\"\"Downsample long curves for readability while keeping first/last point.\"\"\"\n    if arr.size == 0:\n        return np.array([]), np.array([])\n    if arr.shape[-1] <= max_points:\n        idx = np.arange(arr.shape[-1])\n    else:\n        idx = np.round(np.linspace(0, arr.shape[-1] - 1, max_points)).astype(int)\n    return idx + 1, arr[..., idx]\n\n\n# ------------------------------------------------------------------\n# union of all dataset names appearing in any run\ndataset_names = set()\nfor exp in all_experiment_data:\n    dataset_names.update(exp.keys())\n\nfor dset in dataset_names:\n\n    # ------------------------------------------------------------------\n    # 1) aggregated loss curves\n    try:\n        train_seq, val_seq = [], []\n        for exp in all_experiment_data:\n            if dset not in exp:\n                continue\n            losses = exp[dset].get(\"losses\", {})\n            if \"train\" in losses:\n                train_seq.append(losses[\"train\"])\n            if \"val\" in losses:\n                val_seq.append(losses[\"val\"])\n\n        train_mat = stack_and_crop(train_seq)\n        val_mat = stack_and_crop(val_seq)\n\n        if train_mat.size and val_mat.size:\n            train_mean, train_sem = train_mat.mean(0), train_mat.std(\n                0, ddof=1\n            ) / np.sqrt(train_mat.shape[0])\n            val_mean, val_sem = val_mat.mean(0), val_mat.std(0, ddof=1) / np.sqrt(\n                val_mat.shape[0]\n            )\n\n            ep_train, train_plot = epoch_subsample(train_mean)\n            _, train_sems = epoch_subsample(train_sem)\n            ep_val, val_plot = epoch_subsample(val_mean)\n            _, val_sems = epoch_subsample(val_sem)\n\n            plt.figure()\n            plt.plot(ep_train, train_plot, label=\"Train Mean\", color=\"tab:blue\")\n            plt.fill_between(\n                ep_train,\n                train_plot - train_sems,\n                train_plot + train_sems,\n                color=\"tab:blue\",\n                alpha=0.2,\n                label=\"Train \u00b1 SEM\",\n            )\n            plt.plot(ep_val, val_plot, label=\"Val Mean\", color=\"tab:orange\")\n            plt.fill_between(\n                ep_val,\n                val_plot - val_sems,\n                val_plot + val_sems,\n                color=\"tab:orange\",\n                alpha=0.2,\n                label=\"Val \u00b1 SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\n                f\"{dset} Aggregated Loss Curves\\nMean \u00b1 SEM over {train_mat.shape[0]} runs\"\n            )\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_aggregated_loss.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------\n    # 2) aggregated validation metrics\n    try:\n        metrics_names = (\"acc\", \"swa\", \"cwa\", \"nrgs\")\n        metric_arrays = {m: [] for m in metrics_names}\n\n        for exp in all_experiment_data:\n            if dset not in exp:\n                continue\n            val_metrics = exp[dset].get(\"metrics\", {}).get(\"val\", [])\n            if not val_metrics:\n                continue\n            # build per-metric sequence\n            for m in metrics_names:\n                seq = [ep.get(m) for ep in val_metrics if ep.get(m) is not None]\n                if seq:\n                    metric_arrays[m].append(seq)\n\n        # plot each metric separately so figure isn't overcrowded\n        for m, seqs in metric_arrays.items():\n            mat = stack_and_crop(seqs)\n            if not mat.size:\n                continue\n            mean, sem = mat.mean(0), mat.std(0, ddof=1) / np.sqrt(mat.shape[0])\n            epochs, mean = epoch_subsample(mean)\n            _, sem = epoch_subsample(sem)\n\n            plt.figure()\n            plt.plot(epochs, mean, label=f\"{m.upper()} Mean\", color=\"tab:green\")\n            plt.fill_between(\n                epochs,\n                mean - sem,\n                mean + sem,\n                color=\"tab:green\",\n                alpha=0.2,\n                label=\"\u00b1 SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(m.upper())\n            plt.ylim(0, 1)\n            plt.title(\n                f\"{dset} Validation {m.upper()} (Mean \u00b1 SEM, {mat.shape[0]} runs)\"\n            )\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_aggregated_{m}.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated metric plot for {dset}: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------\n    # 3) aggregated confusion matrix\n    try:\n        agg_cm = None\n        labels_set = set()\n        for exp in all_experiment_data:\n            if dset not in exp:\n                continue\n            preds = np.array(exp[dset].get(\"predictions\", []))\n            trues = np.array(exp[dset].get(\"ground_truth\", []))\n            if preds.size == 0 or trues.size == 0:\n                continue\n            labels_set.update(np.unique(np.concatenate([preds, trues])))\n\n        if labels_set:\n            labels = np.sort(np.array(list(labels_set)))\n            label_index = {lbl: i for i, lbl in enumerate(labels)}\n            agg_cm = np.zeros((labels.size, labels.size), dtype=int)\n\n            for exp in all_experiment_data:\n                if dset not in exp:\n                    continue\n                preds = np.array(exp[dset].get(\"predictions\", []))\n                trues = np.array(exp[dset].get(\"ground_truth\", []))\n                for t, p in zip(trues, preds):\n                    agg_cm[label_index[t], label_index[p]] += 1\n\n            plt.figure()\n            im = plt.imshow(agg_cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.xticks(range(labels.size), labels)\n            plt.yticks(range(labels.size), labels)\n            plt.title(\n                f\"{dset} Aggregated Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            # annotate cells\n            for i in range(labels.size):\n                for j in range(labels.size):\n                    plt.text(\n                        j,\n                        i,\n                        agg_cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            plt.tight_layout()\n            fname = os.path.join(\n                working_dir, f\"{dset.lower()}_aggregated_confusion_matrix.png\"\n            )\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated confusion matrix for {dset}: {e}\")\n        plt.close()\n"], "plot_plan": ["We first load the saved experiment_data.npy dictionary and pull out the SPR_BENCH entry. From it we extract the training/validation loss lists and the validation metrics recorded at every epoch. We then create up to four simple line plots: (1) training vs. validation loss, (2) validation accuracy across epochs, (3) validation shape-weighted accuracy, and (4) validation color-weighted accuracy. Each plot lives in its own try-except block so failures are contained, and every figure is closed after saving. Filenames include the dataset name plus an informative suffix to keep results organized. After plotting, we print the final test-set metrics (ACC, SWA, CWA, NRGS) to stdout for quick inspection. Only data actually present in experiment_data.npy are referenced, with no fabricated values. All plots are stored in the previously created working directory.", null, null, "We load experiment_data.npy, fetch each dataset entry, and create up to three key visualizations: (1) training vs. validation loss curves, (2) validation metric trends (acc / SWA / CWA / NRGS), and (3) a confusion-matrix heat-map of ground-truth vs. predicted labels on the test split.  Each figure is wrapped in its own try-except block to guarantee graceful failure and to ensure all figures are closed.  Filenames embed the dataset name and plot type and are saved to the predefined working_dir.  We only plot values that exist inside experiment_data.npy; if a metric is missing we skip it.  Epoch indices are inferred from list lengths, and we down-sample plots with many epochs to a maximum of five snapshots (though the provided example only has 5 epochs).  All titles clearly state the dataset, plot purpose, and\u2014when relevant\u2014subtitles such as \u201cLeft: Ground Truth, Right: Generated Samples\u201d.  The code adheres strictly to basic Matplotlib, closes figures after saving, and prints the save location for convenience.  The final script is concise and self-contained.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved NumPy dictionary, iterate over every dataset key\n(e.g. \u201cSPR_BENCH\u201d), and collect the stored losses and metric dictionaries.   It\nprints:   \u2022 the final training loss and final validation loss,   \u2022 the best\n(highest-accuracy) validation metrics, and   \u2022 the final test metrics.   Each\nline clearly names the metric so the output is self-explanatory, and the code\nexecutes immediately when run.", "", "", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate over every dataset it contains. For each dataset it will pull out\nthe final (i.e., last-epoch) training loss, the final validation loss and\nmetrics, and the single stored test metrics block. It then prints the dataset\nname followed by clearly labelled, human-readable metric names and their\ncorresponding values, rounded to three decimal places. No plots are produced and\nall code runs immediately at import time.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate over every dataset it contains. For each dataset it will pull out\nthe final (i.e., last-epoch) training loss, the final validation loss and\nmetrics, and the single stored test metrics block. It then prints the dataset\nname followed by clearly labelled, human-readable metric names and their\ncorresponding values, rounded to three decimal places. No plots are produced and\nall code runs immediately at import time.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate over every dataset it contains. For each dataset it will pull out\nthe final (i.e., last-epoch) training loss, the final validation loss and\nmetrics, and the single stored test metrics block. It then prints the dataset\nname followed by clearly labelled, human-readable metric names and their\ncorresponding values, rounded to three decimal places. No plots are produced and\nall code runs immediately at import time.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate over every dataset it contains. For each dataset it will pull out\nthe final (i.e., last-epoch) training loss, the final validation loss and\nmetrics, and the single stored test metrics block. It then prints the dataset\nname followed by clearly labelled, human-readable metric names and their\ncorresponding values, rounded to three decimal places. No plots are produced and\nall code runs immediately at import time.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper to print with uniform formatting\ndef _p(name, value):\n    print(f\"{name}: {value:.4f}\")\n\n\n# ------------------------------------------------------------------\n# iterate through every stored dataset\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ------------- losses -----------------\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        _p(\"final train loss\", train_losses[-1])\n\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        _p(\"final validation loss\", val_losses[-1])\n\n    # ------------- validation metrics -----------------\n    val_metrics_list = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics_list:\n        # choose the entry with the highest validation accuracy\n        best_idx = max(\n            range(len(val_metrics_list)),\n            key=lambda i: val_metrics_list[i].get(\"acc\", float(\"-inf\")),\n        )\n        best_val = val_metrics_list[best_idx]\n        _p(\"best validation accuracy\", best_val.get(\"acc\", float(\"nan\")))\n        _p(\n            \"best validation shape-weighted accuracy\",\n            best_val.get(\"swa\", float(\"nan\")),\n        )\n        _p(\n            \"best validation color-weighted accuracy\",\n            best_val.get(\"cwa\", float(\"nan\")),\n        )\n        _p(\n            \"best validation new-relation generalization score\",\n            best_val.get(\"nrg\", float(\"nan\")),\n        )\n\n    # ------------- test metrics -----------------\n    test_metrics = data.get(\"metrics\", {}).get(\"test\")\n    if test_metrics:\n        _p(\"test accuracy\", test_metrics.get(\"acc\", float(\"nan\")))\n        _p(\"test shape-weighted accuracy\", test_metrics.get(\"swa\", float(\"nan\")))\n        _p(\"test color-weighted accuracy\", test_metrics.get(\"cwa\", float(\"nan\")))\n        _p(\n            \"test new-relation generalization score\",\n            test_metrics.get(\"nrg\", float(\"nan\")),\n        )\n\n    print()  # blank line between datasets\n", "", "", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# locate and load the saved experiment artefacts\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------\n# helper to format floating numbers uniformly\n# --------------------------------------------------\ndef fmt(x):\n    return f\"{x:.3f}\" if isinstance(x, (float, int)) else str(x)\n\n\n# --------------------------------------------------\n# iterate over each dataset and print final metrics\n# --------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset header\n\n    # ---------- training ----------\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(\"training loss:\", fmt(train_losses[-1]))\n\n    # ---------- validation ----------\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        print(\"validation loss:\", fmt(val_losses[-1]))\n\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        last_val = val_metrics[-1]\n        if \"acc\" in last_val:\n            print(\"validation accuracy:\", fmt(last_val[\"acc\"]))\n        if \"swa\" in last_val:\n            print(\"validation shape weighted accuracy:\", fmt(last_val[\"swa\"]))\n        if \"cwa\" in last_val:\n            print(\"validation color weighted accuracy:\", fmt(last_val[\"cwa\"]))\n        if \"nrgs\" in last_val:\n            print(\"validation NRGS:\", fmt(last_val[\"nrgs\"]))\n\n    # ---------- test ----------\n    test_metrics = data.get(\"metrics\", {}).get(\"test\", {})\n    if test_metrics:\n        if \"loss\" in test_metrics:\n            print(\"test loss:\", fmt(test_metrics[\"loss\"]))\n        if \"acc\" in test_metrics:\n            print(\"test accuracy:\", fmt(test_metrics[\"acc\"]))\n        if \"swa\" in test_metrics:\n            print(\"test shape weighted accuracy:\", fmt(test_metrics[\"swa\"]))\n        if \"cwa\" in test_metrics:\n            print(\"test color weighted accuracy:\", fmt(test_metrics[\"cwa\"]))\n        if \"nrgs\" in test_metrics:\n            print(\"test NRGS:\", fmt(test_metrics[\"nrgs\"]))\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# locate and load the saved experiment artefacts\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------\n# helper to format floating numbers uniformly\n# --------------------------------------------------\ndef fmt(x):\n    return f\"{x:.3f}\" if isinstance(x, (float, int)) else str(x)\n\n\n# --------------------------------------------------\n# iterate over each dataset and print final metrics\n# --------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset header\n\n    # ---------- training ----------\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(\"training loss:\", fmt(train_losses[-1]))\n\n    # ---------- validation ----------\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        print(\"validation loss:\", fmt(val_losses[-1]))\n\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        last_val = val_metrics[-1]\n        if \"acc\" in last_val:\n            print(\"validation accuracy:\", fmt(last_val[\"acc\"]))\n        if \"swa\" in last_val:\n            print(\"validation shape weighted accuracy:\", fmt(last_val[\"swa\"]))\n        if \"cwa\" in last_val:\n            print(\"validation color weighted accuracy:\", fmt(last_val[\"cwa\"]))\n        if \"nrgs\" in last_val:\n            print(\"validation NRGS:\", fmt(last_val[\"nrgs\"]))\n\n    # ---------- test ----------\n    test_metrics = data.get(\"metrics\", {}).get(\"test\", {})\n    if test_metrics:\n        if \"loss\" in test_metrics:\n            print(\"test loss:\", fmt(test_metrics[\"loss\"]))\n        if \"acc\" in test_metrics:\n            print(\"test accuracy:\", fmt(test_metrics[\"acc\"]))\n        if \"swa\" in test_metrics:\n            print(\"test shape weighted accuracy:\", fmt(test_metrics[\"swa\"]))\n        if \"cwa\" in test_metrics:\n            print(\"test color weighted accuracy:\", fmt(test_metrics[\"cwa\"]))\n        if \"nrgs\" in test_metrics:\n            print(\"test NRGS:\", fmt(test_metrics[\"nrgs\"]))\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# locate and load the saved experiment artefacts\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------\n# helper to format floating numbers uniformly\n# --------------------------------------------------\ndef fmt(x):\n    return f\"{x:.3f}\" if isinstance(x, (float, int)) else str(x)\n\n\n# --------------------------------------------------\n# iterate over each dataset and print final metrics\n# --------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset header\n\n    # ---------- training ----------\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(\"training loss:\", fmt(train_losses[-1]))\n\n    # ---------- validation ----------\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        print(\"validation loss:\", fmt(val_losses[-1]))\n\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        last_val = val_metrics[-1]\n        if \"acc\" in last_val:\n            print(\"validation accuracy:\", fmt(last_val[\"acc\"]))\n        if \"swa\" in last_val:\n            print(\"validation shape weighted accuracy:\", fmt(last_val[\"swa\"]))\n        if \"cwa\" in last_val:\n            print(\"validation color weighted accuracy:\", fmt(last_val[\"cwa\"]))\n        if \"nrgs\" in last_val:\n            print(\"validation NRGS:\", fmt(last_val[\"nrgs\"]))\n\n    # ---------- test ----------\n    test_metrics = data.get(\"metrics\", {}).get(\"test\", {})\n    if test_metrics:\n        if \"loss\" in test_metrics:\n            print(\"test loss:\", fmt(test_metrics[\"loss\"]))\n        if \"acc\" in test_metrics:\n            print(\"test accuracy:\", fmt(test_metrics[\"acc\"]))\n        if \"swa\" in test_metrics:\n            print(\"test shape weighted accuracy:\", fmt(test_metrics[\"swa\"]))\n        if \"cwa\" in test_metrics:\n            print(\"test color weighted accuracy:\", fmt(test_metrics[\"cwa\"]))\n        if \"nrgs\" in test_metrics:\n            print(\"test NRGS:\", fmt(test_metrics[\"nrgs\"]))\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# locate and load the saved experiment artefacts\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------\n# helper to format floating numbers uniformly\n# --------------------------------------------------\ndef fmt(x):\n    return f\"{x:.3f}\" if isinstance(x, (float, int)) else str(x)\n\n\n# --------------------------------------------------\n# iterate over each dataset and print final metrics\n# --------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset header\n\n    # ---------- training ----------\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(\"training loss:\", fmt(train_losses[-1]))\n\n    # ---------- validation ----------\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        print(\"validation loss:\", fmt(val_losses[-1]))\n\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        last_val = val_metrics[-1]\n        if \"acc\" in last_val:\n            print(\"validation accuracy:\", fmt(last_val[\"acc\"]))\n        if \"swa\" in last_val:\n            print(\"validation shape weighted accuracy:\", fmt(last_val[\"swa\"]))\n        if \"cwa\" in last_val:\n            print(\"validation color weighted accuracy:\", fmt(last_val[\"cwa\"]))\n        if \"nrgs\" in last_val:\n            print(\"validation NRGS:\", fmt(last_val[\"nrgs\"]))\n\n    # ---------- test ----------\n    test_metrics = data.get(\"metrics\", {}).get(\"test\", {})\n    if test_metrics:\n        if \"loss\" in test_metrics:\n            print(\"test loss:\", fmt(test_metrics[\"loss\"]))\n        if \"acc\" in test_metrics:\n            print(\"test accuracy:\", fmt(test_metrics[\"acc\"]))\n        if \"swa\" in test_metrics:\n            print(\"test shape weighted accuracy:\", fmt(test_metrics[\"swa\"]))\n        if \"cwa\" in test_metrics:\n            print(\"test color weighted accuracy:\", fmt(test_metrics[\"cwa\"]))\n        if \"nrgs\" in test_metrics:\n            print(\"test NRGS:\", fmt(test_metrics[\"nrgs\"]))\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'final train loss: 0.5962', '\\n', 'final validation loss:\n0.6114', '\\n', 'best validation accuracy: 0.7030', '\\n', 'best validation shape-\nweighted accuracy: 0.6729', '\\n', 'best validation color-weighted accuracy:\n0.7517', '\\n', 'best validation new-relation generalization score: 1.0000',\n'\\n', 'test accuracy: 0.6980', '\\n', 'test shape-weighted accuracy: 0.6606',\n'\\n', 'test color-weighted accuracy: 0.7472', '\\n', 'test new-relation\ngeneralization score: 0.5000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "", "", "['\\nSPR_BENCH', '\\n', 'training loss:', ' ', '0.052', '\\n', 'validation loss:',\n' ', '0.043', '\\n', 'validation accuracy:', ' ', '0.990', '\\n', 'validation\nshape weighted accuracy:', ' ', '0.990', '\\n', 'validation color weighted\naccuracy:', ' ', '0.990', '\\n', 'validation NRGS:', ' ', '0.979', '\\n', 'test\nloss:', ' ', '1.898', '\\n', 'test accuracy:', ' ', '0.696', '\\n', 'test shape\nweighted accuracy:', ' ', '0.650', '\\n', 'test color weighted accuracy:', ' ',\n'0.696', '\\n', 'test NRGS:', ' ', '0.750', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'training loss:', ' ', '0.078', '\\n', 'validation loss:',\n' ', '0.071', '\\n', 'validation accuracy:', ' ', '0.978', '\\n', 'validation\nshape weighted accuracy:', ' ', '0.978', '\\n', 'validation color weighted\naccuracy:', ' ', '0.978', '\\n', 'validation NRGS:', ' ', '0.969', '\\n', 'test\nloss:', ' ', '1.844', '\\n', 'test accuracy:', ' ', '0.693', '\\n', 'test shape\nweighted accuracy:', ' ', '0.647', '\\n', 'test color weighted accuracy:', ' ',\n'0.693', '\\n', 'test NRGS:', ' ', '0.747', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'training loss:', ' ', '0.082', '\\n', 'validation loss:',\n' ', '0.063', '\\n', 'validation accuracy:', ' ', '0.979', '\\n', 'validation\nshape weighted accuracy:', ' ', '0.978', '\\n', 'validation color weighted\naccuracy:', ' ', '0.979', '\\n', 'validation NRGS:', ' ', '0.953', '\\n', 'test\nloss:', ' ', '1.590', '\\n', 'test accuracy:', ' ', '0.691', '\\n', 'test shape\nweighted accuracy:', ' ', '0.646', '\\n', 'test color weighted accuracy:', ' ',\n'0.692', '\\n', 'test NRGS:', ' ', '0.745', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'training loss:', ' ', '0.094', '\\n', 'validation loss:',\n' ', '0.079', '\\n', 'validation accuracy:', ' ', '0.974', '\\n', 'validation\nshape weighted accuracy:', ' ', '0.974', '\\n', 'validation color weighted\naccuracy:', ' ', '0.974', '\\n', 'validation NRGS:', ' ', '0.953', '\\n', 'test\nloss:', ' ', '1.563', '\\n', 'test accuracy:', ' ', '0.689', '\\n', 'test shape\nweighted accuracy:', ' ', '0.644', '\\n', 'test color weighted accuracy:', ' ',\n'0.690', '\\n', 'test NRGS:', ' ', '0.745', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
