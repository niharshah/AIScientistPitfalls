<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[2, 6], [2, 5], [2, 7], [2, 4]], "layout": [[0.0, 0.0], [0.2857142857142857, 0.0], [0.5714285714285714, 0.0], [0.8571428571428571, 0.0], [0.14285714285714285, 1.0], [0.42857142857142855, 1.0], [0.7142857142857143, 1.0], [1.0, 1.0]], "plan": ["We start with a compact token-average baseline: build a vocabulary from the\ntraining sequences, map every shape-color token to an index (0 = PAD, 1 = UNK),\nand encode every sequence as a padded tensor.  A tiny neural model made of an\nEmbedding layer (padding aware) and an average-pooling read-out feeds a linear\nclassifier.  Labels are mapped to contiguous integers taken from the training\nsplit so the model can output the correct number of classes.   The code loads\nthe benchmark with the helper provided, constructs DataLoaders, trains for a few\nepochs on GPU if available, and evaluates after every epoch using the required\nRule-Complexity-Weighted-Accuracy (RCWA).  Training/validation losses and\nmetrics are stored in a nested experiment_data dict and saved to\n./working/experiment_data.npy; predictions and ground-truth labels for the test\nsplit are also recorded.   All tensors and the model are moved to the detected\ndevice, optimizers are instantiated afterwards, and batches are transferred\ninside the training loop.  Finally, the script prints the validation loss/RCWA\neach epoch and the final test RCWA.  This gives a simple, working end-to-end\nbaseline that can later be enriched with symbolic features or more powerful\narchitectures.", "We start with a minimal neural-symbolic baseline: every SPR sequence is\ntokenised at the symbol level (each shape-colour token is treated as one\nvocabulary item).  A small vocabulary (pad+unk+tokens) is built from the\ntraining split, and sequences are padded for batching.  A lightweight bi-\ndirectional GRU with an embedding layer transforms the input; the final hidden\nstates are pooled and fed to a linear soft-max layer that predicts the label.\nWe train for a handful of epochs with cross-entropy, monitoring validation loss,\nplain accuracy and the proposed Rule-Complexity-Weighted Accuracy (RCWA), whose\nweight is (#unique-shapes \u00d7 #unique-colours) per example.  All metrics, losses,\npredictions and ground-truth are stored in an experiment_data dict and dumped to\n\u201c./working/experiment_data.npy\u201d.  The code automatically moves all\ntensors/models to GPU if available, and gracefully falls back to a tiny\nsynthetic dataset if the real benchmark is missing, ensuring standalone\nexecutability.  After training, the model is evaluated on the test split and\nfinal metrics are printed.  This gives a clean, fully-working reference pipeline\nthat we can later enrich with explicit symbolic components for zero-shot rule\ntransfer.", "We start by loading SPR_BENCH with the supplied helper, build a simple token-\nlevel vocabulary from the training split, and map each sequence to a list of\ninteger ids.  A minimal neural baseline is a small embedding layer followed by\nmean-pooled embeddings and a linear classifier; this lets us test zero-shot\nsymbolic generalisation with negligible computational cost.  Sequences are\npadded inside a custom collate_fn, and all tensors as well as the model reside\non GPU when available, following the mandatory device rules.  We train for a\nhandful of epochs with Adam and cross-entropy, printing validation loss plus\nRule-Complexity-Weighted Accuracy (RCWA) after every epoch.  SWA and CWA are\nalso reported at the end for completeness.  All losses, metrics, predictions and\nground-truth labels are stored in the experiment_data dict and persisted to\n./working/experiment_data.npy.  The code is self-contained, executes on import,\nand finishes within minutes on a typical GPU/CPU.", "We will treat each SPR sequence as a list of two-character symbols\n(shape+color).  A tiny neural baseline embeds every unique symbol, averages the\nembeddings (bag-of-symbols), and feeds the result to a linear classifier; this\nkeeps the model simple yet trainable on GPU.  The training loop iterates for a\nhandful of epochs, reports cross-entropy loss and our custom Rule-Complexity-\nWeighted-Accuracy (RCWA) on the dev split, and stores all metrics/predictions in\nthe prescribed experiment_data structure.  Sequences are tokenised only with\nvocabulary learnt from training data, unknown tokens mapped to <UNK>.  We\nconform to the device-handling rules (moving model and tensors to GPU when\navailable), and we pad batches to equal length in a custom collate_fn.  After\ntraining, the script evaluates on the test split, prints final RCWA, and saves\nmetrics, losses, predictions and ground-truth arrays (plus a simple loss curve\nfigure) into the ./working directory.  This first implementation gives us a\nfully working end-to-end pipeline that can later be replaced with more\nsophisticated neural-symbolic modules.  The code executes immediately\u2014no main\nguard\u2014and respects the single-file, GPU, and data-saving requirements.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----- helper code from proposal -------------------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef rule_complexity_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return float(sum(correct)) / float(sum(weights)) if sum(weights) else 0.0\n\n\n# ------------- load SPR_BENCH ----------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# attempt both provided absolute path and local folder\npossible_paths = [\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\"),\n    pathlib.Path(\"SPR_BENCH/\"),\n]\ndataset_path = None\nfor p in possible_paths:\n    if p.exists():\n        dataset_path = p\n        break\nif dataset_path is None:\n    raise FileNotFoundError(\"Cannot locate SPR_BENCH folder.\")\n\nspr = load_spr_bench(dataset_path)\nprint(\"Loaded splits:\", spr.keys(), {k: len(v) for k, v in spr.items()})\n\n# ------------- vocabulary & label mapping ----------------------------------------\nPAD, UNK = 0, 1\ntok2id = {}\n\n\ndef add_token(t):\n    if t not in tok2id:\n        tok2id[t] = len(tok2id) + 2  # reserve 0,1\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(tok2id) + 2\nprint(f\"Vocabulary size (incl PAD/UNK): {vocab_size}\")\n\ntrain_labels = spr[\"train\"][\"label\"]\nlabel_set = sorted(set(train_labels))\nlabel2id = {lab: i for i, lab in enumerate(label_set)}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"Num classes: {num_labels}\")\n\n\ndef encode_sequence(seq):\n    ids = []\n    for tok in seq.split():\n        ids.append(tok2id.get(tok, UNK))\n    return ids\n\n\n# ------------- Dataset wrappers --------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = [label2id[l] for l in hf_ds[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(\n                encode_sequence(self.seqs[idx]), dtype=torch.long\n            ),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    max_len = max(lengths)\n    input_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, x in enumerate(batch):\n        seq = x[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([x[\"label\"] for x in batch])\n    raw_seqs = [x[\"raw_seq\"] for x in batch]\n    return {\"input_ids\": input_ids, \"labels\": labels, \"raw_seqs\": raw_seqs}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\nval_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(spr[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ------------- Model -------------------------------------------------------------\nclass AvgEmbClassifier(nn.Module):\n    def __init__(self, vocab, emb_dim, num_labels):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab, emb_dim, padding_idx=PAD)\n        self.fc = nn.Linear(emb_dim, num_labels)\n\n    def forward(self, input_ids):\n        emb = self.embedding(input_ids)  # (B,L,D)\n        mask = (input_ids != PAD).unsqueeze(-1)  # (B,L,1)\n        summed = (emb * mask).sum(1)  # (B,D)\n        lengths = mask.sum(1).clamp(min=1)\n        pooled = summed / lengths\n        return self.fc(pooled)\n\n\nmodel = AvgEmbClassifier(vocab_size, 32, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n\n# ------------- experiment data dict ---------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ------------- training ----------------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss, n_batches = 0.0, 0\n    for batch in train_loader:\n        batch_t = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch_t[\"input_ids\"])\n        loss = criterion(logits, batch_t[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        n_batches += 1\n    avg_train_loss = total_loss / n_batches\n\n    def evaluate(dataloader):\n        model.eval()\n        all_true, all_pred, all_seq = [], [], []\n        total_loss, n = 0.0, 0\n        with torch.no_grad():\n            for batch in dataloader:\n                batch_t = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch_t[\"input_ids\"])\n                loss = criterion(logits, batch_t[\"labels\"])\n                total_loss += loss.item()\n                n += 1\n                preds = logits.argmax(-1).cpu().tolist()\n                trues = batch_t[\"labels\"].cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(trues)\n                all_seq.extend(batch[\"raw_seqs\"])\n        avg_loss = total_loss / n\n        rcwa = rule_complexity_weighted_accuracy(all_seq, all_true, all_pred)\n        return avg_loss, rcwa, all_true, all_pred, all_seq\n\n    val_loss, val_rcwa, _, _, _ = evaluate(val_loader)\n\n    print(\n        f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}, \"\n        f\"validation_loss={val_loss:.4f}, val_RCWA={val_rcwa:.4f}\"\n    )\n\n    # store metrics\n    ts = time.time()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((ts, avg_train_loss))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((ts, val_loss))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        (ts, None)\n    )  # no RCWA for train\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((ts, val_rcwa))\n\n# ------------- final test evaluation --------------------------------------------\ntest_loss, test_rcwa, gts, preds, seqs = evaluate(test_loader)\nprint(f\"Final Test   : loss={test_loss:.4f}, RCWA={test_rcwa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nexperiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom typing import List, Dict\nfrom datetime import datetime\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- helper metrics ----------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rcwa(seqs: List[str], y_true: List[int], y_pred: List[int]) -> float:\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\n# ---------- data loading ----------\ndef load_spr(root: pathlib.Path):\n    try:\n        from datasets import load_dataset, DatasetDict\n\n        def _load(csv_name):\n            return load_dataset(\n                \"csv\",\n                data_files=str(root / csv_name),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        dset = {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n        print(\"Loaded real SPR_BENCH dataset\")\n    except Exception as e:\n        print(f\"Could not load real data ({e}); generating synthetic toy data.\")\n\n        def synth_split(n):\n            data = []\n            shapes = list(\"ABCDE\")\n            colors = list(\"abcde\")\n            for i in range(n):\n                seq_len = random.randint(3, 8)\n                seq = \" \".join(\n                    random.choice(shapes) + random.choice(colors)\n                    for _ in range(seq_len)\n                )\n                label = random.randint(0, 1)\n                data.append({\"id\": str(i), \"sequence\": seq, \"label\": label})\n            return data\n\n        dset = {\n            \"train\": synth_split(800),\n            \"dev\": synth_split(200),\n            \"test\": synth_split(200),\n        }\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndatasets_dict = load_spr(DATA_PATH)\n\n# ---------- vocab ----------\nPAD, UNK = 0, 1\n\n\ndef build_vocab(seqs):\n    vocab = {\"<PAD>\": PAD, \"<UNK>\": UNK}\n    idx = 2\n    for s in seqs:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(\n    [\n        ex[\"sequence\"] if isinstance(ex, dict) else ex[\"sequence\"]\n        for ex in datasets_dict[\"train\"]\n    ]\n)\n\n\ndef encode(seq: str, vocab: Dict[str, int]):\n    return [vocab.get(tok, UNK) for tok in seq.strip().split()]\n\n\n# ---------- torch dataset ----------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, data, vocab):\n        self.seqs = [\n            d[\"sequence\"] if isinstance(d, dict) else d[\"sequence\"] for d in data\n        ]\n        self.labels = [int(d[\"label\"]) for d in data]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"ids\": torch.tensor(encode(self.seqs[idx], self.vocab), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate_fn(batch):\n    seqs = [b[\"ids\"] for b in batch]\n    lens = torch.tensor([len(x) for x in seqs], dtype=torch.long)\n    padded = pad_sequence(seqs, batch_first=True, padding_value=PAD)\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw_seq = [b[\"raw_seq\"] for b in batch]\n    return {\"ids\": padded, \"lens\": lens, \"label\": labels, \"raw_seq\": raw_seq}\n\n\nbatch_size = 128\ndl_train = DataLoader(\n    SPRTorchDataset(datasets_dict[\"train\"], vocab),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n)\ndl_val = DataLoader(\n    SPRTorchDataset(datasets_dict[\"dev\"], vocab),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\ndl_test = DataLoader(\n    SPRTorchDataset(datasets_dict[\"test\"], vocab),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n)\n\n\n# ---------- model ----------\nclass SimpleGRUClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, n_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hid_dim * 2, n_classes)\n\n    def forward(self, ids, lens):\n        emb = self.emb(ids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)  # [B, 2*hid]\n        return self.fc(h)\n\n\nn_classes = len(set(int(d[\"label\"]) for d in datasets_dict[\"train\"]))\nmodel = SimpleGRUClassifier(len(vocab), 64, 64, n_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------- training ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss, n_train = 0.0, 0\n    for batch in dl_train:\n        ids = batch[\"ids\"].to(device)\n        lens = batch[\"lens\"].to(device)\n        lab = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(ids, lens)\n        loss = criterion(logits, lab)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * lab.size(0)\n        n_train += lab.size(0)\n    train_loss /= n_train\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ----- validation -----\n    model.eval()\n    val_loss, n_val = 0.0, 0\n    all_preds, all_trues, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dl_val:\n            ids = batch[\"ids\"].to(device)\n            lens = batch[\"lens\"].to(device)\n            lab = batch[\"label\"].to(device)\n            logits = model(ids, lens)\n            loss = criterion(logits, lab)\n            preds = logits.argmax(1).cpu().tolist()\n            val_loss += loss.item() * lab.size(0)\n            n_val += lab.size(0)\n            all_preds.extend(preds)\n            all_trues.extend(lab.cpu().tolist())\n            all_seqs.extend(batch[\"raw_seq\"])\n    val_loss /= n_val\n    rcwa_val = rcwa(all_seqs, all_trues, all_preds)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(rcwa_val)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | RCWA = {rcwa_val:.4f}\")\n\n# ---------- test evaluation ----------\nmodel.eval()\nall_preds, all_trues, all_seqs = [], [], []\nwith torch.no_grad():\n    for batch in dl_test:\n        ids = batch[\"ids\"].to(device)\n        lens = batch[\"lens\"].to(device)\n        lab = batch[\"label\"].to(device)\n        logits = model(ids, lens)\n        preds = logits.argmax(1).cpu().tolist()\n        all_preds.extend(preds)\n        all_trues.extend(lab.cpu().tolist())\n        all_seqs.extend(batch[\"raw_seq\"])\ntest_rcwa = rcwa(all_seqs, all_trues, all_preds)\nprint(f\"\\nTEST RCWA: {test_rcwa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_trues\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_rcwa\"] = test_rcwa\nexperiment_data[\"timestamp\"] = str(datetime.now())\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict\nfrom typing import List, Dict\n\n# ------------------ working dir ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ device -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------ helper from prompt -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef rcwa(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) else 0.0\n\n\n# ------------------ load dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    # fallback: assume folder in current dir\n    DATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------- build vocabulary ------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\n# ------------- dataset class ---------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, seq: str) -> List[int]:\n        return [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.encode(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"sequence_str\": self.seqs[idx],\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab)\n\n\n# ------------- collate fn ------------------------\ndef collate_fn(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    seq_str = [b[\"sequence_str\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=0)\n    mask = (padded != 0).float()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": mask,\n        \"labels\": labels,\n        \"sequence_str\": seq_str,\n    }\n\n\nBATCH_SIZE = 128\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ------------- model -----------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, ids, mask):\n        emb = self.embed(ids)  # B x T x D\n        masked = emb * mask.unsqueeze(-1)  # zero-out pads\n        sum_emb = masked.sum(dim=1)\n        lengths = mask.sum(dim=1).clamp(min=1e-6)\n        mean_emb = sum_emb / lengths.unsqueeze(-1)\n        return self.fc(mean_emb)\n\n\nnum_classes = int(max(train_ds.labels)) + 1\nmodel = MeanPoolClassifier(len(vocab), 64, num_classes).to(device)\n\n# ------------- training setup --------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_rcwa\": [], \"val_rcwa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\ndef evaluate(loader):\n    model.eval()\n    total_loss, preds, gts, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"input_ids\"], batch_t[\"attention_mask\"])\n            loss = criterion(logits, batch_t[\"labels\"])\n            total_loss += loss.item() * batch_t[\"labels\"].size(0)\n            pred = logits.argmax(dim=-1).cpu().tolist()\n            gt = batch_t[\"labels\"].cpu().tolist()\n            preds.extend(pred)\n            gts.extend(gt)\n            seqs.extend(batch[\"sequence_str\"])\n    rcwa_val = rcwa(seqs, gts, preds)\n    return total_loss / len(loader.dataset), rcwa_val, preds, gts, seqs\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_loader.dataset)\n    val_loss, val_rcwa, _, _, _ = evaluate(dev_loader)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_rcwa\"].append(np.nan)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_rcwa\"].append(val_rcwa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}  RCWA = {val_rcwa:.4f}\")\n\n# ------------- final test evaluation -------------\ntest_loss, test_rcwa, test_preds, test_gts, test_seqs = evaluate(test_loader)\nswa = (\n    lambda seqs, y, p: sum(\n        count_shape_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_shape_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\ncwa = (\n    lambda seqs, y, p: sum(\n        count_color_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_color_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\nprint(\n    f\"Test   loss={test_loss:.4f}  RCWA={test_rcwa:.4f}  SWA={swa:.4f}  CWA={cwa:.4f}\"\n)\n\n# save predictions & gt\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = np.array(test_preds)\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = np.array(test_gts)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport pathlib\nfrom typing import List, Dict\nimport matplotlib.pyplot as plt\n\n# ----- device handling -----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- Dataset utilities (provided) ----------\nfrom datasets import load_dataset, DatasetDict\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rcwa(seqs: List[str], y_true: List[int], y_pred: List[int]) -> float:\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / (sum(weights) + 1e-8)\n\n\n# ---------- simple vocabulary ----------\nclass Vocab:\n    def __init__(self, sequences: List[str], min_freq: int = 1):\n        freqs: Dict[str, int] = {}\n        for seq in sequences:\n            for tok in seq.strip().split():\n                freqs[tok] = freqs.get(tok, 0) + 1\n        tokens = [t for t, f in freqs.items() if f >= min_freq]\n        self.specials = [\"<PAD>\", \"<UNK>\"]\n        self.itos = self.specials + sorted(tokens)\n        self.stoi = {t: i for i, t in enumerate(self.itos)}\n\n    def encode(self, sequence: str) -> List[int]:\n        return [self.stoi.get(tok, 1) for tok in sequence.strip().split()]  # 1 is <UNK>\n\n    def __len__(self):\n        return len(self.itos)\n\n\n# ---------- Dataset wrappers ----------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_dataset, vocab: Vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        seq_tensor = torch.tensor(self.vocab.encode(item[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(item[\"label\"], dtype=torch.long)\n        return {\"seq\": seq_tensor, \"label\": label, \"sequence_str\": item[\"sequence\"]}\n\n\ndef collate(batch):\n    max_len = max(x[\"seq\"].size(0) for x in batch)\n    seq_batch = []\n    for x in batch:\n        pad_len = max_len - x[\"seq\"].size(0)\n        seq_batch.append(torch.cat([x[\"seq\"], torch.zeros(pad_len, dtype=torch.long)]))\n    seq_batch = torch.stack(seq_batch)\n    labels = torch.stack([x[\"label\"] for x in batch])\n    seq_strs = [x[\"sequence_str\"] for x in batch]\n    return {\n        \"seq\": seq_batch.to(device),\n        \"label\": labels.to(device),\n        \"sequence_str\": seq_strs,\n    }\n\n\n# ---------- Model ----------\nclass MeanEmbedClassifier(nn.Module):\n    def __init__(self, vocab_size: int, emb_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        emb = self.embed(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(dim=1)\n        counts = mask.sum(dim=1).clamp(min=1)\n        mean = summed / counts\n        return self.fc(mean)\n\n\n# ---------- Training utilities ----------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    total_loss = 0\n    y_true = []\n    y_pred = []\n    seqs = []\n    model.train() if train else model.eval()\n    for batch in loader:\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"label\"].size(0)\n        preds = out.argmax(dim=-1).detach().cpu().tolist()\n        y_pred.extend(preds)\n        y_true.extend(batch[\"label\"].detach().cpu().tolist())\n        seqs.extend(batch[\"sequence_str\"])\n    avg_loss = total_loss / len(loader.dataset)\n    metric = rcwa(seqs, y_true, y_pred)\n    return avg_loss, metric, y_pred, y_true, seqs\n\n\n# ---------- Load dataset ----------\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_PATH\", \"SPR_BENCH\"))\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset folder not found. Set env var SPR_PATH or place folder here.\"\n    )\ndsets = load_spr_bench(DATA_PATH)\nprint(\"Loaded SPR_BENCH splits:\", dsets)\n\n# ---------- Prepare vocab and torch datasets ----------\ntrain_seqs = [x[\"sequence\"] for x in dsets[\"train\"]]\nvocab = Vocab(train_seqs, min_freq=1)\nprint(f\"Vocab size: {len(vocab)}\")\n\ntrain_ds = SPRTorchDataset(dsets[\"train\"], vocab)\ndev_ds = SPRTorchDataset(dsets[\"dev\"], vocab)\ntest_ds = SPRTorchDataset(dsets[\"test\"], vocab)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n# ---------- Initialize model ----------\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\nmodel = MeanEmbedClassifier(len(vocab), emb_dim=64, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------- Experiment data dict ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ---------- Training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    tr_loss, tr_rcwa, _, _, _ = run_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_rcwa, _, _, _ = run_epoch(\n        model, dev_loader, criterion, optimizer=None\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_rcwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_rcwa)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n        f\"train_RCWA={tr_rcwa:.4f}, val_RCWA={val_rcwa:.4f}\"\n    )\n\n# ---------- Test evaluation ----------\ntest_loss, test_rcwa, preds, gts, seqs_test = run_epoch(\n    model, test_loader, criterion, optimizer=None\n)\nprint(f\"Final Test RCWA: {test_rcwa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\n\n# ---------- Save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- Simple loss curve ----------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve_spr.png\"))\nplt.close()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict\nfrom typing import List, Dict\n\n# ------------------ working dir ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ device -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------ helper from prompt -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef rcwa(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) else 0.0\n\n\n# ------------------ load dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    # fallback: assume folder in current dir\n    DATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------- build vocabulary ------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\n# ------------- dataset class ---------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, seq: str) -> List[int]:\n        return [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.encode(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"sequence_str\": self.seqs[idx],\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab)\n\n\n# ------------- collate fn ------------------------\ndef collate_fn(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    seq_str = [b[\"sequence_str\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=0)\n    mask = (padded != 0).float()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": mask,\n        \"labels\": labels,\n        \"sequence_str\": seq_str,\n    }\n\n\nBATCH_SIZE = 128\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ------------- model -----------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, ids, mask):\n        emb = self.embed(ids)  # B x T x D\n        masked = emb * mask.unsqueeze(-1)  # zero-out pads\n        sum_emb = masked.sum(dim=1)\n        lengths = mask.sum(dim=1).clamp(min=1e-6)\n        mean_emb = sum_emb / lengths.unsqueeze(-1)\n        return self.fc(mean_emb)\n\n\nnum_classes = int(max(train_ds.labels)) + 1\nmodel = MeanPoolClassifier(len(vocab), 64, num_classes).to(device)\n\n# ------------- training setup --------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_rcwa\": [], \"val_rcwa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\ndef evaluate(loader):\n    model.eval()\n    total_loss, preds, gts, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"input_ids\"], batch_t[\"attention_mask\"])\n            loss = criterion(logits, batch_t[\"labels\"])\n            total_loss += loss.item() * batch_t[\"labels\"].size(0)\n            pred = logits.argmax(dim=-1).cpu().tolist()\n            gt = batch_t[\"labels\"].cpu().tolist()\n            preds.extend(pred)\n            gts.extend(gt)\n            seqs.extend(batch[\"sequence_str\"])\n    rcwa_val = rcwa(seqs, gts, preds)\n    return total_loss / len(loader.dataset), rcwa_val, preds, gts, seqs\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_loader.dataset)\n    val_loss, val_rcwa, _, _, _ = evaluate(dev_loader)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_rcwa\"].append(np.nan)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_rcwa\"].append(val_rcwa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}  RCWA = {val_rcwa:.4f}\")\n\n# ------------- final test evaluation -------------\ntest_loss, test_rcwa, test_preds, test_gts, test_seqs = evaluate(test_loader)\nswa = (\n    lambda seqs, y, p: sum(\n        count_shape_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_shape_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\ncwa = (\n    lambda seqs, y, p: sum(\n        count_color_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_color_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\nprint(\n    f\"Test   loss={test_loss:.4f}  RCWA={test_rcwa:.4f}  SWA={swa:.4f}  CWA={cwa:.4f}\"\n)\n\n# save predictions & gt\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = np.array(test_preds)\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = np.array(test_gts)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict\nfrom typing import List, Dict\n\n# ------------------ working dir ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ device -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------ helper from prompt -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef rcwa(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) else 0.0\n\n\n# ------------------ load dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    # fallback: assume folder in current dir\n    DATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------- build vocabulary ------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\n# ------------- dataset class ---------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, seq: str) -> List[int]:\n        return [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.encode(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"sequence_str\": self.seqs[idx],\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab)\n\n\n# ------------- collate fn ------------------------\ndef collate_fn(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    seq_str = [b[\"sequence_str\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=0)\n    mask = (padded != 0).float()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": mask,\n        \"labels\": labels,\n        \"sequence_str\": seq_str,\n    }\n\n\nBATCH_SIZE = 128\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ------------- model -----------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, ids, mask):\n        emb = self.embed(ids)  # B x T x D\n        masked = emb * mask.unsqueeze(-1)  # zero-out pads\n        sum_emb = masked.sum(dim=1)\n        lengths = mask.sum(dim=1).clamp(min=1e-6)\n        mean_emb = sum_emb / lengths.unsqueeze(-1)\n        return self.fc(mean_emb)\n\n\nnum_classes = int(max(train_ds.labels)) + 1\nmodel = MeanPoolClassifier(len(vocab), 64, num_classes).to(device)\n\n# ------------- training setup --------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_rcwa\": [], \"val_rcwa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\ndef evaluate(loader):\n    model.eval()\n    total_loss, preds, gts, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"input_ids\"], batch_t[\"attention_mask\"])\n            loss = criterion(logits, batch_t[\"labels\"])\n            total_loss += loss.item() * batch_t[\"labels\"].size(0)\n            pred = logits.argmax(dim=-1).cpu().tolist()\n            gt = batch_t[\"labels\"].cpu().tolist()\n            preds.extend(pred)\n            gts.extend(gt)\n            seqs.extend(batch[\"sequence_str\"])\n    rcwa_val = rcwa(seqs, gts, preds)\n    return total_loss / len(loader.dataset), rcwa_val, preds, gts, seqs\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_loader.dataset)\n    val_loss, val_rcwa, _, _, _ = evaluate(dev_loader)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_rcwa\"].append(np.nan)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_rcwa\"].append(val_rcwa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}  RCWA = {val_rcwa:.4f}\")\n\n# ------------- final test evaluation -------------\ntest_loss, test_rcwa, test_preds, test_gts, test_seqs = evaluate(test_loader)\nswa = (\n    lambda seqs, y, p: sum(\n        count_shape_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_shape_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\ncwa = (\n    lambda seqs, y, p: sum(\n        count_color_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_color_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\nprint(\n    f\"Test   loss={test_loss:.4f}  RCWA={test_rcwa:.4f}  SWA={swa:.4f}  CWA={cwa:.4f}\"\n)\n\n# save predictions & gt\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = np.array(test_preds)\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = np.array(test_gts)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict\nfrom typing import List, Dict\n\n# ------------------ working dir ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ device -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------ helper from prompt -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef rcwa(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) else 0.0\n\n\n# ------------------ load dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    # fallback: assume folder in current dir\n    DATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------- build vocabulary ------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\n# ------------- dataset class ---------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, seq: str) -> List[int]:\n        return [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.encode(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"sequence_str\": self.seqs[idx],\n        }\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab)\n\n\n# ------------- collate fn ------------------------\ndef collate_fn(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    seq_str = [b[\"sequence_str\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=0)\n    mask = (padded != 0).float()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": mask,\n        \"labels\": labels,\n        \"sequence_str\": seq_str,\n    }\n\n\nBATCH_SIZE = 128\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ------------- model -----------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, ids, mask):\n        emb = self.embed(ids)  # B x T x D\n        masked = emb * mask.unsqueeze(-1)  # zero-out pads\n        sum_emb = masked.sum(dim=1)\n        lengths = mask.sum(dim=1).clamp(min=1e-6)\n        mean_emb = sum_emb / lengths.unsqueeze(-1)\n        return self.fc(mean_emb)\n\n\nnum_classes = int(max(train_ds.labels)) + 1\nmodel = MeanPoolClassifier(len(vocab), 64, num_classes).to(device)\n\n# ------------- training setup --------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_rcwa\": [], \"val_rcwa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\ndef evaluate(loader):\n    model.eval()\n    total_loss, preds, gts, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"input_ids\"], batch_t[\"attention_mask\"])\n            loss = criterion(logits, batch_t[\"labels\"])\n            total_loss += loss.item() * batch_t[\"labels\"].size(0)\n            pred = logits.argmax(dim=-1).cpu().tolist()\n            gt = batch_t[\"labels\"].cpu().tolist()\n            preds.extend(pred)\n            gts.extend(gt)\n            seqs.extend(batch[\"sequence_str\"])\n    rcwa_val = rcwa(seqs, gts, preds)\n    return total_loss / len(loader.dataset), rcwa_val, preds, gts, seqs\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = epoch_loss / len(train_loader.dataset)\n    val_loss, val_rcwa, _, _, _ = evaluate(dev_loader)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_rcwa\"].append(np.nan)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_rcwa\"].append(val_rcwa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}  RCWA = {val_rcwa:.4f}\")\n\n# ------------- final test evaluation -------------\ntest_loss, test_rcwa, test_preds, test_gts, test_seqs = evaluate(test_loader)\nswa = (\n    lambda seqs, y, p: sum(\n        count_shape_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_shape_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\ncwa = (\n    lambda seqs, y, p: sum(\n        count_color_variety(s) if yt == pt else 0 for s, yt, pt in zip(seqs, y, p)\n    )\n    / sum(count_color_variety(s) for s in seqs)\n)(test_seqs, test_gts, test_preds)\nprint(\n    f\"Test   loss={test_loss:.4f}  RCWA={test_rcwa:.4f}  SWA={swa:.4f}  CWA={cwa:.4f}\"\n)\n\n# save predictions & gt\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = np.array(test_preds)\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = np.array(test_gts)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 439371.26\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 698143.08\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 788417.83\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train', 'dev',\n'test'])\", ' ', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n',\n'Vocabulary size (incl PAD/UNK): 18', '\\n', 'Num classes: 2', '\\n', 'Epoch 1:\ntrain_loss=0.5686, validation_loss=0.5223, val_RCWA=0.7328', '\\n', 'Epoch 2:\ntrain_loss=0.5202, validation_loss=0.5207, val_RCWA=0.7333', '\\n', 'Epoch 3:\ntrain_loss=0.5202, validation_loss=0.5210, val_RCWA=0.7343', '\\n', 'Epoch 4:\ntrain_loss=0.5203, validation_loss=0.5201, val_RCWA=0.7421', '\\n', 'Epoch 5:\ntrain_loss=0.5210, validation_loss=0.5214, val_RCWA=0.7247', '\\n', 'Final Test\n: loss=0.7133, RCWA=0.5989', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"Could not load real data (Unable to find\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-14_15-47-\n52_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n2/SPR_BENCH/train.csv'); generating synthetic toy data.\", '\\n', 'Epoch 1:\nvalidation_loss = 0.6995 | RCWA = 0.5151', '\\n', 'Epoch 2: validation_loss =\n0.6987 | RCWA = 0.5172', '\\n', 'Epoch 3: validation_loss = 0.7018 | RCWA =\n0.5267', '\\n', 'Epoch 4: validation_loss = 0.7035 | RCWA = 0.5013', '\\n', 'Epoch\n5: validation_loss = 0.7060 | RCWA = 0.5082', '\\n', '\\nTEST RCWA: 0.4950', '\\n',\n'Execution time: 2 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 479247.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 691604.39\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 768314.19\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', 'Epoch 1: validation_loss = 0.5383  RCWA = 0.7295', '\\n',\n'Epoch 2: validation_loss = 0.5226  RCWA = 0.7249', '\\n', 'Epoch 3:\nvalidation_loss = 0.5219  RCWA = 0.7245', '\\n', 'Epoch 4: validation_loss =\n0.5212  RCWA = 0.7356', '\\n', 'Epoch 5: validation_loss = 0.5213  RCWA =\n0.7438', '\\n', 'Test   loss=0.7210  RCWA=0.5960  SWA=0.5943  CWA=0.6210', '\\n',\n'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 147, in <module>\\n    raise\nFileNotFoundError(\\nFileNotFoundError: SPR_BENCH dataset folder not found. Set\nenv var SPR_PATH or place folder here.\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 229078.65\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 359958.12\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 596799.09\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', 'Epoch 1: validation_loss = 0.5456  RCWA = 0.7240', '\\n',\n'Epoch 2: validation_loss = 0.5237  RCWA = 0.7263', '\\n', 'Epoch 3:\nvalidation_loss = 0.5205  RCWA = 0.7294', '\\n', 'Epoch 4: validation_loss =\n0.5214  RCWA = 0.7337', '\\n', 'Epoch 5: validation_loss = 0.5210  RCWA =\n0.7257', '\\n', 'Test   loss=0.7173  RCWA=0.5958  SWA=0.5943  CWA=0.6210', '\\n',\n'Execution time: 5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 456734.16\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 631215.99\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 739449.24\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', 'Epoch 1: validation_loss = 0.5346  RCWA = 0.7377', '\\n',\n'Epoch 2: validation_loss = 0.5220  RCWA = 0.7351', '\\n', 'Epoch 3:\nvalidation_loss = 0.5216  RCWA = 0.7439', '\\n', 'Epoch 4: validation_loss =\n0.5214  RCWA = 0.7282', '\\n', 'Epoch 5: validation_loss = 0.5216  RCWA =\n0.7500', '\\n', 'Test   loss=0.7258  RCWA=0.5955  SWA=0.5936  CWA=0.6197', '\\n',\n'Execution time: 5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Vocab size: 18', '\\n', 'Epoch 1: validation_loss = 0.5297  RCWA =\n0.7384', '\\n', 'Epoch 2: validation_loss = 0.5216  RCWA = 0.7565', '\\n', 'Epoch\n3: validation_loss = 0.5210  RCWA = 0.7334', '\\n', 'Epoch 4: validation_loss =\n0.5209  RCWA = 0.7374', '\\n', 'Epoch 5: validation_loss = 0.5213  RCWA =\n0.7372', '\\n', 'Test   loss=0.7288  RCWA=0.5906  SWA=0.5892  CWA=0.6150', '\\n',\n'Execution time: 5 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "", "The execution failed because the SPR_BENCH dataset folder was not found. The\ncode requires the dataset to be placed in the directory specified by the\nenvironment variable SPR_PATH or in the current working directory. To fix this\nissue, ensure that the SPR_BENCH dataset folder is available and correctly\nplaced. Alternatively, set the SPR_PATH environment variable to point to the\ncorrect location of the dataset.", "", "", "", ""], "exc_type": [null, null, null, "FileNotFoundError", null, null, null, null], "exc_info": [null, null, null, {"args": ["SPR_BENCH dataset folder not found. Set env var SPR_PATH or place folder here."]}, null, null, null, null], "exc_stack": [null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 147, "<module>", "raise FileNotFoundError("]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521033, "best_value": 0.521033}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation set, used to evaluate model performance during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.520096, "best_value": 0.520096}]}, {"metric_name": "validation RCWA", "lower_is_better": false, "description": "The RCWA metric on the validation set, indicating the model's performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.74215, "best_value": 0.74215}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6229, "best_value": 0.6229}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is fitting the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.660541, "best_value": 0.660541}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, used to monitor model performance during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.705997, "best_value": 0.705997}]}, {"metric_name": "validation RCWA", "lower_is_better": false, "description": "The RCWA (presumably a performance metric) on the validation dataset, with higher values indicating better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.526701, "best_value": 0.526701}]}, {"metric_name": "test RCWA", "lower_is_better": false, "description": "The RCWA (presumably a performance metric) on the test dataset, with higher values indicating better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.49501, "best_value": 0.49501}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.519977, "best_value": 0.519977}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521239, "best_value": 0.521239}]}, {"metric_name": "validation RCWA", "lower_is_better": false, "description": "The RCWA (metric unspecified) computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.743799, "best_value": 0.743799}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.519857, "best_value": 0.519857}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation set. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.520457, "best_value": 0.520457}]}, {"metric_name": "validation RCWA", "lower_is_better": false, "description": "RCWA metric on the validation set. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.733675, "best_value": 0.733675}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.519932, "best_value": 0.519932}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521385, "best_value": 0.521385}]}, {"metric_name": "validation RCWA", "lower_is_better": false, "description": "RCWA metric on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.749969, "best_value": 0.749969}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.519651, "best_value": 0.519651}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.520931, "best_value": 0.520931}]}, {"metric_name": "validation RCWA", "lower_is_better": false, "description": "Measures the RCWA during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.756547, "best_value": 0.756547}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_RCWA_curve.png", "../../logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_RCWA_curve.png", "../../logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_train_rcwa_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_val_rcwa_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_accuracy.png"]], "plot_paths": [["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_loss_curve.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_RCWA_curve.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_loss_curve.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_RCWA_curve.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_label_distribution.png"], ["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_metric_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_metric_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_metric_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_metric_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_loss_curves.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_train_rcwa_curve.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_val_rcwa_curve.png", "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_03ac3fbd868e442db83715d94b438569/SPR_BENCH_aggregated_accuracy.png"]], "plot_analyses": [[{"analysis": "This plot compares the training and validation loss over epochs. The training loss decreases sharply in the first epoch and then stabilizes, indicating that the model quickly learns the basic patterns in the data. The validation loss remains relatively stable, suggesting that the model generalizes well to unseen data without overfitting. However, the slight increase in validation loss in later epochs might indicate the beginning of overfitting, which could be addressed by early stopping or regularization techniques.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot tracks the Validation RCWA (Relative Color-Weighted Accuracy) over epochs. The RCWA initially improves, reaching a peak at epoch 4, but then drops sharply at epoch 5. This suggests that the model's ability to generalize to unseen data improves during the initial epochs but deteriorates after epoch 4, possibly due to overfitting or instability in learning. Further investigation into hyperparameter tuning or regularization methods may be needed to maintain consistent performance.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_RCWA_curve.png"}, {"analysis": "The confusion matrix shows the distribution of true vs. predicted labels. The model performs well on both classes, with a higher number of true positives and true negatives compared to false positives and false negatives. However, there is still room for improvement in reducing misclassifications, particularly in the off-diagonal elements of the matrix. This could be achieved by refining the model's architecture or using a more balanced dataset.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3429cc34d52e421e8e55416d033dbd42_proc_2675003/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily across epochs, indicating that the model is learning effectively from the training data. However, the validation loss increases after the first epoch, suggesting potential overfitting. This trend implies the model generalizes poorly to the validation set as training progresses.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_loss_curve.png"}, {"analysis": "The validation RCWA (Relative Color-Weighted Accuracy) fluctuates significantly over epochs. It peaks at epoch 3 but then drops sharply at epoch 4 before recovering slightly in epoch 5. This instability may indicate that the model struggles to maintain consistent performance on the validation set, potentially due to overfitting or insufficient regularization.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_RCWA_curve.png"}, {"analysis": "The confusion matrix shows that the model performs similarly for both classes, with slightly more false positives (55) than true positives (51) for Class 1. The high number of misclassifications (false positives and false negatives) indicates that the model's predictions lack precision and recall, suggesting room for improvement in classification accuracy.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The test label distribution plot reveals that the predictions closely match the ground truth for both classes, with only minor discrepancies. This indicates that the model maintains a balanced prediction distribution, which is a positive sign for its robustness. However, the earlier confusion matrix suggests that the model's accuracy within these distributions is still suboptimal.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b14fad1cc91749c1873a58f2a233a4cd_proc_2675004/SPR_BENCH_label_distribution.png"}], [{"analysis": "This plot shows the training and validation loss over epochs. Both curves decrease initially, indicating that the model is learning effectively. However, after the second epoch, the loss values stabilize, suggesting that the model reaches a plateau in performance. The training and validation loss are closely aligned, which is a good sign of minimal overfitting at this stage.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the metric curves for Shape-Weighted Accuracy (SWA) and Color-Weighted Accuracy (CWA) during training and validation. The validation metric improves steadily after an initial dip, surpassing the training metric by the final epoch. This indicates that the model generalizes well to unseen data and benefits from the neural-symbolic integration approach.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_metric_curves.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's predictions versus the ground truth. The diagonal dominance indicates that the model performs well overall. However, the lighter shades in some off-diagonal regions suggest areas where the model struggles, indicating potential room for improvement in handling specific classes.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_52a71daa1ea644c68917eba797852671_proc_2675005/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The training and validation loss curves show a rapid decrease in loss during the initial epochs, followed by a plateau. This indicates that the model is learning effectively and converging. The close alignment between the training and validation loss curves suggests that the model is not overfitting, as the validation loss does not diverge significantly from the training loss.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_loss_curves.png"}, {"analysis": "The metric curves for train_rcwa and val_rcwa indicate an improvement in the metric value over the initial epochs, with a peak in validation performance at epoch 3. However, there is a slight decline in validation performance by epoch 4, which could suggest overfitting or a need for additional regularization techniques.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_metric_curves.png"}, {"analysis": "The confusion matrix reveals the distribution of predictions compared to the ground truth. The darker diagonal elements indicate correct predictions, while off-diagonal elements represent misclassifications. The matrix shows that while the model performs well, there is room for improvement in reducing the number of misclassifications, particularly in the lower-right quadrant.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over the epochs. The training loss decreases rapidly in the initial epoch and then stabilizes, indicating that the model is learning effectively from the training data. The validation loss follows a similar trend, which suggests that the model is generalizing well to unseen data without overfitting. The convergence of both losses at a low value is a positive sign of a well-trained model.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the metric curves for training and validation relative color-weighted accuracy (RCWA) over the epochs. The training RCWA remains relatively stable, while the validation RCWA fluctuates significantly, with a sharp drop and subsequent increase. This variability in the validation metric could indicate sensitivity to the data or potential overfitting issues. Further investigation into the data distribution or model regularization might be necessary.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_metric_curves.png"}, {"analysis": "The confusion matrix provides a visualization of the model's prediction performance. The diagonal elements represent correct predictions, while the off-diagonal elements indicate misclassifications. The matrix shows a high number of correct predictions for certain classes, but there are also notable misclassifications, as evidenced by the presence of significant values in the off-diagonal cells. This suggests that while the model performs well overall, there is room for improvement in reducing specific types of errors.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over 5 epochs. The training loss decreases rapidly in the initial epochs and stabilizes around epoch 2, indicating that the model is learning effectively during training. The validation loss follows a similar trend, suggesting that the model is not overfitting and generalizes well to unseen data. However, the slight increase in validation loss after epoch 2 may indicate the start of overfitting, which requires monitoring in further training.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the metric curves for training and validation relative color-weighted accuracy (RCWA) over 5 epochs. The validation RCWA shows a peak at epoch 1 followed by a sharp drop and stabilization, indicating potential instability or sensitivity in the metric. The training RCWA remains more consistent, suggesting the model's performance on the training set is stable. The discrepancy between training and validation RCWA highlights a need to investigate potential issues like data distribution differences or model capacity.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_metric_curves.png"}, {"analysis": "This confusion matrix shows the distribution of predictions versus ground truth labels. The diagonal elements represent correct predictions, while off-diagonal elements represent misclassifications. The darker diagonal cells indicate a relatively high number of correct predictions, suggesting the model performs reasonably well. However, the lighter off-diagonal cells indicate areas where the model struggles, possibly due to class imbalance or insufficient feature representation.", "plot_path": "experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model learns quickly and generalizes well initially\nbut may suffer from overfitting or instability in later epochs. The confusion\nmatrix suggests good overall performance but highlights areas for improvement in\nreducing misclassifications. Further steps could include hyperparameter tuning,\nregularization techniques, and refining the model architecture.", "The plots reveal that the model is learning effectively on the training set but\nstruggles with generalization, as evidenced by increasing validation loss and\nfluctuating RCWA. The confusion matrix highlights significant\nmisclassifications, while the label distribution indicates balanced predictions.\nImprovements in regularization and model tuning are needed to enhance\ngeneralization and accuracy.", "The plots indicate that the model is learning effectively and generalizing well.\nThe training and validation loss curves suggest stable learning without\noverfitting. The metric curves highlight improvement in validation performance,\nand the confusion matrix shows strong overall accuracy with some minor\nmisclassification issues.", "[]", "The plots indicate that the model is learning effectively with no significant\noverfitting. Both the training and validation losses converge well, though\nslight overfitting may be inferred from the validation metric's decline. The\nconfusion matrix highlights areas where misclassifications occur, suggesting\npotential for further optimization.", "The plots provide insights into the model's performance, showing effective\nlearning and generalization in terms of loss reduction, but with some\nvariability in validation accuracy and areas for improvement in classification\naccuracy as indicated by the confusion matrix.", "The plots indicate that the model is learning effectively and generalizing\nreasonably well, but there are areas for improvement. The loss curves suggest\ngood initial training dynamics, though the slight increase in validation loss\nafter epoch 2 requires attention. The RCWA metric curves reveal potential\ninstability in validation performance, which warrants further investigation. The\nconfusion matrix shows decent overall performance but highlights specific\nmisclassification patterns that could be addressed.", "[]"], "exec_time": [4.589434623718262, 2.143462657928467, 4.522567510604858, 0.2277355194091797, 5.011489152908325, 5.1113975048065186, 5.17786431312561, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[]"], ["['experiment_data']"], [], ["['experiment_data']"], ["[]"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data.get(\"SPR_BENCH\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\n# ------------------------------------------------------------------\n# 1. Loss curves ----------------------------------------------------\ntry:\n    tr_losses = [x[1] for x in data[\"losses\"][\"train\"]]\n    val_losses = [x[1] for x in data[\"losses\"][\"val\"]]\n    epochs = np.arange(1, len(tr_losses) + 1)\n\n    plt.figure()\n    plt.plot(epochs, tr_losses, label=\"Train Loss\")\n    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2. Validation RCWA ------------------------------------------------\ntry:\n    val_rcwa = [x[1] for x in data[\"metrics\"][\"val\"]]\n    epochs = np.arange(1, len(val_rcwa) + 1)\n\n    plt.figure()\n    plt.plot(epochs, val_rcwa, marker=\"o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RCWA\")\n    plt.title(\"SPR_BENCH: Validation RCWA over Epochs\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_RCWA_curve.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating RCWA curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3. Confusion matrix ----------------------------------------------\ntry:\n    y_true = np.array(data[\"ground_truth\"])\n    y_pred = np.array(data[\"predictions\"])\n    num_classes = max(y_true.max(), y_pred.max()) + 1\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(\"SPR_BENCH: Confusion Matrix (True vs Predicted)\")\n    plt.xticks(np.arange(num_classes))\n    plt.yticks(np.arange(num_classes))\n    fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nname = \"SPR_BENCH\"\ndata = experiment_data.get(name, {})\nepochs = data.get(\"epochs\", [])\ntrain_loss = data.get(\"losses\", {}).get(\"train\", [])\nval_loss = data.get(\"losses\", {}).get(\"val\", [])\nval_rcwa = data.get(\"metrics\", {}).get(\"val\", [])\ntest_preds = np.array(data.get(\"predictions\", []))\ntest_truth = np.array(data.get(\"ground_truth\", []))\ntest_rcwa = data.get(\"metrics\", {}).get(\"test_rcwa\", None)\n\n# 1) Loss curve --------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2) RCWA curve --------------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, val_rcwa, marker=\"o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RCWA\")\n    plt.title(\"SPR_BENCH: Validation RCWA over Epochs\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_RCWA_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating RCWA curve: {e}\")\n    plt.close()\n\n# 3) Confusion matrix --------------------------------------------------------\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    cm = confusion_matrix(test_truth, test_preds, labels=[0, 1])\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar()\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n    plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n    plt.title(\"SPR_BENCH: Test Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# 4) Prediction distribution -------------------------------------------------\ntry:\n    plt.figure()\n    unique, counts_pred = np.unique(test_preds, return_counts=True)\n    unique_t, counts_true = np.unique(test_truth, return_counts=True)\n    width = 0.35\n    idx = np.arange(len(unique))\n    plt.bar(idx - width / 2, counts_true, width, label=\"Ground Truth\")\n    plt.bar(idx + width / 2, counts_pred, width, label=\"Predictions\")\n    plt.xticks(idx, [f\"Class {u}\" for u in unique])\n    plt.ylabel(\"Count\")\n    plt.title(\n        \"SPR_BENCH: Test Label Distribution\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_label_distribution.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating label distribution plot: {e}\")\n    plt.close()\n\n# -------- print metric ----------\nif test_rcwa is not None:\n    print(f\"Loaded Test RCWA: {test_rcwa:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef confusion_matrix(preds, gts, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for p, g in zip(preds, gts):\n        cm[g, p] += 1\n    return cm\n\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor dset, dval in experiment_data.items():\n    losses = dval.get(\"losses\", {})\n    metrics = dval.get(\"metrics\", {})\n    preds = dval.get(\"predictions\")\n    gts = dval.get(\"ground_truth\")\n\n    # 1) loss curves ----------------------------------------------------------\n    try:\n        if losses:\n            plt.figure()\n            if \"train\" in losses and len(losses[\"train\"]):\n                plt.plot(losses[\"train\"], label=\"Train\")\n            if \"val\" in losses and len(losses[\"val\"]):\n                plt.plot(losses[\"val\"], label=\"Validation\")\n            plt.title(f\"{dset}: Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {dset}: {e}\")\n        plt.close()\n\n    # 2) metric curves --------------------------------------------------------\n    try:\n        if metrics:\n            plt.figure()\n            for mname, mvals in metrics.items():\n                if len(mvals):\n                    plt.plot(mvals, label=mname)\n            plt.title(f\"{dset}: Metric Curves\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Metric Value\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_metric_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting metrics for {dset}: {e}\")\n        plt.close()\n\n    # 3) confusion matrix / accuracy -----------------------------------------\n    try:\n        if preds is not None and gts is not None:\n            preds = np.asarray(preds)\n            gts = np.asarray(gts)\n            acc = (preds == gts).mean()\n            print(f\"{dset}: Test accuracy = {acc:.4f}\")\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            if num_classes <= 5:  # keep plot readable\n                cm = confusion_matrix(preds, gts, num_classes)\n                plt.figure()\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.title(f\"{dset}: Confusion Matrix\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.colorbar()\n                fname = os.path.join(working_dir, f\"{dset}_confusion_matrix.png\")\n                plt.savefig(fname)\n                plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {dset}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef confusion_matrix(preds, gts, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for p, g in zip(preds, gts):\n        cm[g, p] += 1\n    return cm\n\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor dset, dval in experiment_data.items():\n    losses = dval.get(\"losses\", {})\n    metrics = dval.get(\"metrics\", {})\n    preds = dval.get(\"predictions\")\n    gts = dval.get(\"ground_truth\")\n\n    # 1) loss curves ----------------------------------------------------------\n    try:\n        if losses:\n            plt.figure()\n            if \"train\" in losses and len(losses[\"train\"]):\n                plt.plot(losses[\"train\"], label=\"Train\")\n            if \"val\" in losses and len(losses[\"val\"]):\n                plt.plot(losses[\"val\"], label=\"Validation\")\n            plt.title(f\"{dset}: Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {dset}: {e}\")\n        plt.close()\n\n    # 2) metric curves --------------------------------------------------------\n    try:\n        if metrics:\n            plt.figure()\n            for mname, mvals in metrics.items():\n                if len(mvals):\n                    plt.plot(mvals, label=mname)\n            plt.title(f\"{dset}: Metric Curves\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Metric Value\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_metric_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting metrics for {dset}: {e}\")\n        plt.close()\n\n    # 3) confusion matrix / accuracy -----------------------------------------\n    try:\n        if preds is not None and gts is not None:\n            preds = np.asarray(preds)\n            gts = np.asarray(gts)\n            acc = (preds == gts).mean()\n            print(f\"{dset}: Test accuracy = {acc:.4f}\")\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            if num_classes <= 5:  # keep plot readable\n                cm = confusion_matrix(preds, gts, num_classes)\n                plt.figure()\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.title(f\"{dset}: Confusion Matrix\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.colorbar()\n                fname = os.path.join(working_dir, f\"{dset}_confusion_matrix.png\")\n                plt.savefig(fname)\n                plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef confusion_matrix(preds, gts, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for p, g in zip(preds, gts):\n        cm[g, p] += 1\n    return cm\n\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor dset, dval in experiment_data.items():\n    losses = dval.get(\"losses\", {})\n    metrics = dval.get(\"metrics\", {})\n    preds = dval.get(\"predictions\")\n    gts = dval.get(\"ground_truth\")\n\n    # 1) loss curves ----------------------------------------------------------\n    try:\n        if losses:\n            plt.figure()\n            if \"train\" in losses and len(losses[\"train\"]):\n                plt.plot(losses[\"train\"], label=\"Train\")\n            if \"val\" in losses and len(losses[\"val\"]):\n                plt.plot(losses[\"val\"], label=\"Validation\")\n            plt.title(f\"{dset}: Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {dset}: {e}\")\n        plt.close()\n\n    # 2) metric curves --------------------------------------------------------\n    try:\n        if metrics:\n            plt.figure()\n            for mname, mvals in metrics.items():\n                if len(mvals):\n                    plt.plot(mvals, label=mname)\n            plt.title(f\"{dset}: Metric Curves\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Metric Value\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_metric_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting metrics for {dset}: {e}\")\n        plt.close()\n\n    # 3) confusion matrix / accuracy -----------------------------------------\n    try:\n        if preds is not None and gts is not None:\n            preds = np.asarray(preds)\n            gts = np.asarray(gts)\n            acc = (preds == gts).mean()\n            print(f\"{dset}: Test accuracy = {acc:.4f}\")\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            if num_classes <= 5:  # keep plot readable\n                cm = confusion_matrix(preds, gts, num_classes)\n                plt.figure()\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.title(f\"{dset}: Confusion Matrix\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.colorbar()\n                fname = os.path.join(working_dir, f\"{dset}_confusion_matrix.png\")\n                plt.savefig(fname)\n                plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef confusion_matrix(preds, gts, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for p, g in zip(preds, gts):\n        cm[g, p] += 1\n    return cm\n\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor dset, dval in experiment_data.items():\n    losses = dval.get(\"losses\", {})\n    metrics = dval.get(\"metrics\", {})\n    preds = dval.get(\"predictions\")\n    gts = dval.get(\"ground_truth\")\n\n    # 1) loss curves ----------------------------------------------------------\n    try:\n        if losses:\n            plt.figure()\n            if \"train\" in losses and len(losses[\"train\"]):\n                plt.plot(losses[\"train\"], label=\"Train\")\n            if \"val\" in losses and len(losses[\"val\"]):\n                plt.plot(losses[\"val\"], label=\"Validation\")\n            plt.title(f\"{dset}: Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {dset}: {e}\")\n        plt.close()\n\n    # 2) metric curves --------------------------------------------------------\n    try:\n        if metrics:\n            plt.figure()\n            for mname, mvals in metrics.items():\n                if len(mvals):\n                    plt.plot(mvals, label=mname)\n            plt.title(f\"{dset}: Metric Curves\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Metric Value\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_metric_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting metrics for {dset}: {e}\")\n        plt.close()\n\n    # 3) confusion matrix / accuracy -----------------------------------------\n    try:\n        if preds is not None and gts is not None:\n            preds = np.asarray(preds)\n            gts = np.asarray(gts)\n            acc = (preds == gts).mean()\n            print(f\"{dset}: Test accuracy = {acc:.4f}\")\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            if num_classes <= 5:  # keep plot readable\n                cm = confusion_matrix(preds, gts, num_classes)\n                plt.figure()\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.title(f\"{dset}: Confusion Matrix\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.colorbar()\n                fname = os.path.join(working_dir, f\"{dset}_confusion_matrix.png\")\n                plt.savefig(fname)\n                plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {dset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# --------------------------------------------------------------------------- #\n# helper functions\n# --------------------------------------------------------------------------- #\ndef pad_and_stack(list_of_1d_arrays, pad_val=np.nan):\n    \"\"\"Pad 1-D arrays to the same length with NaN and stack (runs, time).\"\"\"\n    if not list_of_1d_arrays:\n        return None\n    max_len = max(len(a) for a in list_of_1d_arrays)\n    stacked = np.full((len(list_of_1d_arrays), max_len), pad_val, dtype=float)\n    for i, arr in enumerate(list_of_1d_arrays):\n        stacked[i, : len(arr)] = arr\n    return stacked\n\n\ndef mean_stderr(stacked):\n    \"\"\"Return mean and stderr ignoring NaNs along axis 0.\"\"\"\n    mean = np.nanmean(stacked, axis=0)\n    std = np.nanstd(stacked, axis=0)\n    n = np.sum(~np.isnan(stacked), axis=0)\n    stderr = np.where(n > 0, std / np.sqrt(n), np.nan)\n    return mean, stderr\n\n\n# --------------------------------------------------------------------------- #\n# load all experiment files\n# --------------------------------------------------------------------------- #\nexperiment_data_path_list = [\n    \"experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b4cc84622fc646bab34c2cb750b6820c_proc_2675006/experiment_data.npy\",\n    \"experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_e8422addb0674cee9b10e71ab35d9518_proc_2675004/experiment_data.npy\",\n    \"experiments/2025-08-14_15-47-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_580ea1caf5ce4012afb6de6ceb21f534_proc_2675003/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n        full_p = os.path.join(root, p)\n        data = np.load(full_p, allow_pickle=True).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\n# --------------------------------------------------------------------------- #\n# aggregate by dataset\n# --------------------------------------------------------------------------- #\ndatasets = {}\nfor run_idx, run_data in enumerate(all_experiment_data):\n    for dset_name, dset_val in run_data.items():\n        ds = datasets.setdefault(\n            dset_name,\n            {\"losses\": {\"train\": [], \"val\": []}, \"metrics\": {}, \"accuracies\": []},\n        )\n        # losses\n        losses = dset_val.get(\"losses\", {})\n        for phase in [\"train\", \"val\"]:\n            if phase in losses and len(losses[phase]):\n                ds[\"losses\"][phase].append(np.array(losses[phase], dtype=float))\n        # metrics\n        for mname, mvals in dset_val.get(\"metrics\", {}).items():\n            if len(mvals):\n                ds[\"metrics\"].setdefault(mname, []).append(np.array(mvals, dtype=float))\n        # accuracy\n        preds = dset_val.get(\"predictions\")\n        gts = dset_val.get(\"ground_truth\")\n        if preds is not None and gts is not None and len(preds) == len(gts):\n            acc = (np.asarray(preds) == np.asarray(gts)).mean()\n            ds[\"accuracies\"].append(acc)\n\n# --------------------------------------------------------------------------- #\n# create aggregated plots\n# --------------------------------------------------------------------------- #\nfor dset_name, agg in datasets.items():\n\n    # 1) aggregated loss curves --------------------------------------------- #\n    try:\n        any_loss = any(len(v) for v in agg[\"losses\"].values())\n        if any_loss:\n            plt.figure()\n            for phase, runs in agg[\"losses\"].items():\n                if not runs:\n                    continue\n                stacked = pad_and_stack(runs)\n                mean, stderr = mean_stderr(stacked)\n                epochs = np.arange(len(mean))\n                plt.plot(epochs, mean, label=f\"{phase} mean\")\n                plt.fill_between(\n                    epochs,\n                    mean - stderr,\n                    mean + stderr,\n                    alpha=0.3,\n                    label=f\"{phase} \u00b1 stderr\",\n                )\n            plt.title(f\"{dset_name}: Aggregated Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_aggregated_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated loss for {dset_name}: {e}\")\n        plt.close()\n\n    # 2) aggregated metric curves ------------------------------------------- #\n    try:\n        if agg[\"metrics\"]:\n            for mname, runs in agg[\"metrics\"].items():\n                plt.figure()\n                stacked = pad_and_stack(runs)\n                mean, stderr = mean_stderr(stacked)\n                epochs = np.arange(len(mean))\n                plt.plot(epochs, mean, label=\"mean\")\n                plt.fill_between(\n                    epochs, mean - stderr, mean + stderr, alpha=0.3, label=\"\u00b1 stderr\"\n                )\n                plt.title(f\"{dset_name}: Aggregated {mname} Curve\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(mname)\n                plt.legend()\n                fname = os.path.join(\n                    working_dir, f\"{dset_name}_aggregated_{mname}_curve.png\"\n                )\n                plt.savefig(fname)\n                plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated metrics for {dset_name}: {e}\")\n        plt.close()\n\n    # 3) aggregated accuracy ------------------------------------------------- #\n    try:\n        accs = agg[\"accuracies\"]\n        if accs:\n            accs = np.array(accs, dtype=float)\n            mean_acc = accs.mean()\n            stderr_acc = accs.std(ddof=0) / np.sqrt(len(accs))\n            print(\n                f\"{dset_name}: accuracy mean \u00b1 stderr = {mean_acc:.4f} \u00b1 {stderr_acc:.4f}\"\n            )\n\n            plt.figure()\n            plt.bar(\n                [0],\n                [mean_acc],\n                yerr=[stderr_acc],\n                alpha=0.7,\n                capsize=10,\n                label=f\"mean \u00b1 stderr (n={len(accs)})\",\n            )\n            plt.xticks([0], [dset_name])\n            plt.ylim(0, 1)\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dset_name}: Aggregated Accuracy Across Runs\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_aggregated_accuracy.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated accuracy for {dset_name}: {e}\")\n        plt.close()\n"], "plot_plan": ["The script loads the saved experiment_data, retrieves the SPR_BENCH entry, and builds three plots: (1) training-vs-validation loss curves, (2) validation RCWA across epochs, and (3) a confusion matrix comparing ground-truth and predicted labels. Each plot is wrapped in its own try-except block, saved to the working directory with descriptive names, and the figure is closed afterward. The code guards against absent data, uses only numpy and matplotlib, and never fabricates values\u2014everything comes directly from experiment_data.npy. It also prints out basic statistics and the locations of the saved figures so they can be inspected later.", "The script will first load the saved NumPy dictionary, locate the SPR_BENCH entry, and pull out epochs, train / validation losses, validation RCWA values, and test predictions vs ground-truth labels. Within separate try-except blocks it will then: (1) draw a joint train/validation loss curve, (2) draw a validation-RCWA-over-epochs curve, (3) create and display a 2\u00d72 confusion-matrix heat-map for the test set, and (4) show a bar chart of class counts in the test predictions versus ground truth. All figures are titled, saved under descriptive names to working_dir, closed immediately, and any plotting failure is caught and reported without halting the rest of the script. Finally, the script prints the stored test RCWA score so that downstream CI can verify correct loading.", "We first load experiment_data.npy from the working directory and iterate over all recorded datasets. For each dataset we extract training/validation losses and metrics, then plot (i) the loss curves and (ii) the RCWA (or any other metric stored) across epochs. We wrap every individual figure in a try-except block, save it with a descriptive filename (dataset + plot type), and always close the figure. If prediction and ground-truth arrays are present we additionally compute overall accuracy and plot a simple confusion-matrix\u2010style heatmap (limited to \u22645\u00d75 classes to keep plots readable, otherwise we skip). All plots are stored in working_dir. Finally, we print the computed evaluation metrics so the notebook/log captures them. The code is kept concise, uses only numpy/matplotlib/os, and never fabricates data\u2014everything comes from experiment_data.npy.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["We will load the serialized dictionary from working/experiment_data.npy, iterate\nover every dataset entry, and extract the stored tuples that keep (timestamp,\nvalue) pairs for each metric.  For training we show the last recorded loss, for\nvalidation we report the minimum loss and the maximum RCWA that occurred during\ntraining.  Finally, if test-set predictions and ground-truth labels are\navailable, we compute and print the resulting test accuracy.  All code runs\nimmediately at the top level and produces plain-text outputs \u2013 no plots, no\nspecial entry points.", "The script will directly load the saved NumPy file from the \u201cworking\u201d directory,\nretrieve the single dataset contained in it, and then determine the required\n\u201cbest\u201d or \u201cfinal\u201d figures.   For losses, the final value (last epoch) is\nreported, while for the validation RCWA metric the best (maximum) value across\nepochs is shown.   The test\u2010set metric is stored separately, so it is printed as\nis.   Each piece of information is prefaced with an explicit, descriptive label\nto satisfy the formatting rules.", "The script loads the saved NumPy file from the \u201cworking\u201d directory, retrieves\nthe dictionary for each dataset and then reports the most informative single\nvalue for every stored metric: the minimum value for each loss array (best =\nlowest) and the maximum value for each accuracy\u2013style metric such as RCWA (best\n= highest). If a metric array is empty or contains only NaNs, it is skipped.\nEach dataset name is printed first, followed by clearly-labeled metric/value\npairs.", "", "The script loads the saved NumPy file from the \u201cworking\u201d directory, retrieves\nthe dictionary for each dataset and then reports the most informative single\nvalue for every stored metric: the minimum value for each loss array (best =\nlowest) and the maximum value for each accuracy\u2013style metric such as RCWA (best\n= highest). If a metric array is empty or contains only NaNs, it is skipped.\nEach dataset name is printed first, followed by clearly-labeled metric/value\npairs.", "The script loads the saved NumPy file from the \u201cworking\u201d directory, retrieves\nthe dictionary for each dataset and then reports the most informative single\nvalue for every stored metric: the minimum value for each loss array (best =\nlowest) and the maximum value for each accuracy\u2013style metric such as RCWA (best\n= highest). If a metric array is empty or contains only NaNs, it is skipped.\nEach dataset name is printed first, followed by clearly-labeled metric/value\npairs.", "The script loads the saved NumPy file from the \u201cworking\u201d directory, retrieves\nthe dictionary for each dataset and then reports the most informative single\nvalue for every stored metric: the minimum value for each loss array (best =\nlowest) and the maximum value for each accuracy\u2013style metric such as RCWA (best\n= highest). If a metric array is empty or contains only NaNs, it is skipped.\nEach dataset name is printed first, followed by clearly-labeled metric/value\npairs.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# helper to safely get best / final metric\ndef get_final(metric_list):\n    \"\"\"Return the last recorded value or None.\"\"\"\n    return metric_list[-1][1] if metric_list else None\n\n\ndef get_best(metric_list, higher_is_better=True):\n    \"\"\"Return best value according to the supplied direction.\"\"\"\n    if not metric_list:\n        return None\n    key_fn = (lambda x: x[1]) if higher_is_better else (lambda x: -x[1])\n    best_tuple = max(metric_list, key=key_fn)\n    return best_tuple[1]\n\n\n# ---------------------------------------------------------------------\n# iterate over all datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(ds_name)  # dataset header\n\n    # ----- training loss -----\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    final_train_loss = get_final(train_losses)\n    if final_train_loss is not None:\n        print(f\"final training loss: {final_train_loss:.6f}\")\n\n    # ----- validation loss -----\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n    best_val_loss = get_best(val_losses, higher_is_better=False)\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    # ----- validation RCWA -----\n    val_rcwas = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    # metrics entries may contain None, filter them\n    val_rcwas = [t for t in val_rcwas if t[1] is not None]\n    best_val_rcwa = get_best(val_rcwas, higher_is_better=True)\n    if best_val_rcwa is not None:\n        print(f\"best validation RCWA: {best_val_rcwa:.6f}\")\n\n    # ----- test accuracy -----\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        correct = sum(int(p == g) for p, g in zip(preds, gts))\n        test_accuracy = correct / len(preds)\n        print(f\"test accuracy: {test_accuracy:.6f}\")\n\n    # visual spacing between datasets\n    print()\n", "import os\nimport numpy as np\n\n# ----- locate and load -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----- iterate through datasets -----\nfor dataset_name, content in experiment_data.items():\n    # skip non-dataset keys such as \"timestamp\"\n    if not isinstance(content, dict) or \"metrics\" not in content:\n        continue\n\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = content.get(\"losses\", {}).get(\"train\", [])\n    val_losses = content.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n\n    # ----- validation metric (RCWA over epochs) -----\n    val_rcwa_list = content.get(\"metrics\", {}).get(\"val\", [])\n    if val_rcwa_list:\n        best_val_rcwa = max(val_rcwa_list)\n        print(f\"Best validation RCWA: {best_val_rcwa:.6f}\")\n\n    # ----- test metric -----\n    test_rcwa = content.get(\"metrics\", {}).get(\"test_rcwa\")\n    if test_rcwa is not None:\n        print(f\"Test RCWA: {test_rcwa:.6f}\")\n", "import os\nimport numpy as np\nimport math\n\n# ----------------- locate and load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions ---------------------\ndef _best_value(arr, higher_is_better=True):\n    \"\"\"\n    Return the best (max or min) finite value from a list-like object.\n    If no finite values exist, return None.\n    \"\"\"\n    arr = np.asarray(arr, dtype=float)\n    arr = arr[np.isfinite(arr)]\n    if arr.size == 0:\n        return None\n    return arr.max() if higher_is_better else arr.min()\n\n\n# ----------------- iterate and print --------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ----- losses -----\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n    best_train_loss = _best_value(train_losses, higher_is_better=False)\n    best_val_loss = _best_value(val_losses, higher_is_better=False)\n\n    if best_train_loss is not None:\n        print(f\"Best training loss: {best_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n\n    # ----- accuracy / RCWA metrics -----\n    train_rcwa = data.get(\"metrics\", {}).get(\"train_rcwa\", [])\n    val_rcwa = data.get(\"metrics\", {}).get(\"val_rcwa\", [])\n\n    best_train_rcwa = _best_value(train_rcwa, higher_is_better=True)\n    best_val_rcwa = _best_value(val_rcwa, higher_is_better=True)\n\n    if best_train_rcwa is not None:\n        print(f\"Best training RCWA: {best_train_rcwa:.6f}\")\n    if best_val_rcwa is not None:\n        print(f\"Best validation RCWA: {best_val_rcwa:.6f}\")\n\n    # ----- newline between datasets -----\n    print()\n", "", "import os\nimport numpy as np\nimport math\n\n# ----------------- locate and load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions ---------------------\ndef _best_value(arr, higher_is_better=True):\n    \"\"\"\n    Return the best (max or min) finite value from a list-like object.\n    If no finite values exist, return None.\n    \"\"\"\n    arr = np.asarray(arr, dtype=float)\n    arr = arr[np.isfinite(arr)]\n    if arr.size == 0:\n        return None\n    return arr.max() if higher_is_better else arr.min()\n\n\n# ----------------- iterate and print --------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ----- losses -----\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n    best_train_loss = _best_value(train_losses, higher_is_better=False)\n    best_val_loss = _best_value(val_losses, higher_is_better=False)\n\n    if best_train_loss is not None:\n        print(f\"Best training loss: {best_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n\n    # ----- accuracy / RCWA metrics -----\n    train_rcwa = data.get(\"metrics\", {}).get(\"train_rcwa\", [])\n    val_rcwa = data.get(\"metrics\", {}).get(\"val_rcwa\", [])\n\n    best_train_rcwa = _best_value(train_rcwa, higher_is_better=True)\n    best_val_rcwa = _best_value(val_rcwa, higher_is_better=True)\n\n    if best_train_rcwa is not None:\n        print(f\"Best training RCWA: {best_train_rcwa:.6f}\")\n    if best_val_rcwa is not None:\n        print(f\"Best validation RCWA: {best_val_rcwa:.6f}\")\n\n    # ----- newline between datasets -----\n    print()\n", "import os\nimport numpy as np\nimport math\n\n# ----------------- locate and load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions ---------------------\ndef _best_value(arr, higher_is_better=True):\n    \"\"\"\n    Return the best (max or min) finite value from a list-like object.\n    If no finite values exist, return None.\n    \"\"\"\n    arr = np.asarray(arr, dtype=float)\n    arr = arr[np.isfinite(arr)]\n    if arr.size == 0:\n        return None\n    return arr.max() if higher_is_better else arr.min()\n\n\n# ----------------- iterate and print --------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ----- losses -----\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n    best_train_loss = _best_value(train_losses, higher_is_better=False)\n    best_val_loss = _best_value(val_losses, higher_is_better=False)\n\n    if best_train_loss is not None:\n        print(f\"Best training loss: {best_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n\n    # ----- accuracy / RCWA metrics -----\n    train_rcwa = data.get(\"metrics\", {}).get(\"train_rcwa\", [])\n    val_rcwa = data.get(\"metrics\", {}).get(\"val_rcwa\", [])\n\n    best_train_rcwa = _best_value(train_rcwa, higher_is_better=True)\n    best_val_rcwa = _best_value(val_rcwa, higher_is_better=True)\n\n    if best_train_rcwa is not None:\n        print(f\"Best training RCWA: {best_train_rcwa:.6f}\")\n    if best_val_rcwa is not None:\n        print(f\"Best validation RCWA: {best_val_rcwa:.6f}\")\n\n    # ----- newline between datasets -----\n    print()\n", "import os\nimport numpy as np\nimport math\n\n# ----------------- locate and load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions ---------------------\ndef _best_value(arr, higher_is_better=True):\n    \"\"\"\n    Return the best (max or min) finite value from a list-like object.\n    If no finite values exist, return None.\n    \"\"\"\n    arr = np.asarray(arr, dtype=float)\n    arr = arr[np.isfinite(arr)]\n    if arr.size == 0:\n        return None\n    return arr.max() if higher_is_better else arr.min()\n\n\n# ----------------- iterate and print --------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ----- losses -----\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n    best_train_loss = _best_value(train_losses, higher_is_better=False)\n    best_val_loss = _best_value(val_losses, higher_is_better=False)\n\n    if best_train_loss is not None:\n        print(f\"Best training loss: {best_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n\n    # ----- accuracy / RCWA metrics -----\n    train_rcwa = data.get(\"metrics\", {}).get(\"train_rcwa\", [])\n    val_rcwa = data.get(\"metrics\", {}).get(\"val_rcwa\", [])\n\n    best_train_rcwa = _best_value(train_rcwa, higher_is_better=True)\n    best_val_rcwa = _best_value(val_rcwa, higher_is_better=True)\n\n    if best_train_rcwa is not None:\n        print(f\"Best training RCWA: {best_train_rcwa:.6f}\")\n    if best_val_rcwa is not None:\n        print(f\"Best validation RCWA: {best_val_rcwa:.6f}\")\n\n    # ----- newline between datasets -----\n    print()\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'final training loss: 0.521033', '\\n', 'best validation\nloss: 0.520096', '\\n', 'best validation RCWA: 0.742150', '\\n', 'test accuracy:\n0.622900', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.660541', '\\n', 'Final\nvalidation loss: 0.705997', '\\n', 'Best validation RCWA: 0.526701', '\\n', 'Test\nRCWA: 0.495010', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'Best training loss: 0.519977', '\\n', 'Best validation loss:\n0.521239', '\\n', 'Best validation RCWA: 0.743799', '\\n', '\\n', 'Execution time:\na moment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH', '\\n', 'Best training loss: 0.519857', '\\n', 'Best validation loss:\n0.520457', '\\n', 'Best validation RCWA: 0.733675', '\\n', '\\n', 'Execution time:\na moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Best training loss: 0.519932', '\\n', 'Best validation loss:\n0.521385', '\\n', 'Best validation RCWA: 0.749969', '\\n', '\\n', 'Execution time:\na moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Best training loss: 0.519651', '\\n', 'Best validation loss:\n0.520931', '\\n', 'Best validation RCWA: 0.756547', '\\n', '\\n', 'Execution time:\na moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
