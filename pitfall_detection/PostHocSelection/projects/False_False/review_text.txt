{
    "Summary": "The paper investigates unexpected pitfalls in deep neural sequence models, focusing on issues such as declining performance with increased pretraining data and the impact of architectural modifications. Using a standard sequence encoder-decoder framework, the authors explore how variations like freezing encoder layers or removing positional embeddings impact performance.",
    "Strengths": [
        "The paper addresses an important topic by investigating pitfalls in deep learning models during training and deployment.",
        "The experiments highlight some known issues with pretraining data scaling and architectural modifications."
    ],
    "Weaknesses": [
        "The paper lacks originality, as the findings (e.g., diminishing returns from additional pretraining data) are already well-documented in the literature.",
        "There are no proper citations to support claims, which undermines the paper's credibility.",
        "The experimental design is weak and not convincing. Key details about datasets, model configurations, and metrics are missing.",
        "The ablation studies fail to provide deep insights or meaningful conclusions. They merely reiterate known behaviors without exploring the root causes or offering solutions.",
        "The results are poorly presented, with unclear figures and insufficient explanation of trends or insights.",
        "The paper does not make significant theoretical or practical contributions to the field. It highlights problems but does not propose actionable solutions or methodologies."
    ],
    "Originality": 2,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 2,
    "Questions": [
        "What datasets and tasks were used in the experiments? Please provide details and justify their relevance.",
        "How do the findings differ from prior work on pretraining data scaling and ablation studies? What is novel here?",
        "Can the authors provide concrete examples or case studies where these findings have real-world implications?",
        "What are the specific hyperparameters and configurations used during training? These are missing from the paper."
    ],
    "Limitations": [
        "The paper does not propose any solutions or methodologies to address the identified pitfalls.",
        "The scope of the experiments is limited, making it hard to generalize the findings.",
        "There is a lack of discussion on potential negative societal impacts or ethical considerations of misrepresenting experimental findings."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 3,
    "Confidence": 4,
    "Decision": "Reject"
}