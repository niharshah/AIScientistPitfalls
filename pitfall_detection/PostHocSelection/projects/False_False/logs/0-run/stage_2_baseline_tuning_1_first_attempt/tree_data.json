{"edges": [[0, 5], [0, 3], [0, 6], [0, 4], [0, 1], [0, 2], [1, 8], [1, 7], [5, 10], [5, 11], [5, 12], [5, 9]], "layout": [[0.5, 0.0], [0.08333333333333333, 0.5], [0.25, 0.5], [0.4166666666666667, 0.5], [0.5833333333333334, 0.5], [0.75, 0.5], [0.9166666666666666, 0.5], [0.0, 1.0], [0.16666666666666666, 1.0], [0.5, 1.0], [0.6666666666666666, 1.0], [0.8333333333333334, 1.0], [1.0, 1.0]], "plan": ["The error was due to hard-coding a single dataset location that didn\u2019t exist on\nthe execution machine.   I add a small resolver that searches a list of sensible\nfallback locations (environment variable, current folder, parent folder, home\nfolder, etc.) and returns the first one containing the expected train.csv file.\nAll subsequent code is unchanged except for using this resolver, adding explicit\n`.to(device)` moves in every batch, and following the mandatory logging/saving\ndirectives.", "Hyperparam tuning name: num_epochs. We will extend training to a maximum of 20\nepochs while adding an early-stopping mechanism (patience = 3) that halts\ntraining when the validation macro-F1 stops improving. All epoch-wise metrics,\nlosses, timestamps and best-epoch predictions are logged under a new hyper-\nparameter-tuning key so results remain comparable with the baseline. Finally,\neverything is saved to experiment_data.npy.", "Hyperparam tuning name: learning_rate. Below is an outline followed by the\ncomplete script.   Outline:   1. Keep the original data-loading, model, and\ntraining logic intact.   2. Wrap the training loop inside a function that is\ncalled for every candidate learning-rate.   3. For each learning-rate,\ninitialise a fresh model/optimizer, train for the same number of epochs, collect\nmetrics/losses, and store them under\nexperiment_data['learning_rate']['SPR_BENCH'][f'lr_{lr}'].   4. After sweeping\nall learning-rates, save the aggregated experiment_data dictionary to\nexperiment_data.npy.", "Hyperparam tuning name: emb_dim. We loop over several embedding dimensions (32,\n64, 128, 256); for each value a fresh MeanPoolClassifier, optimiser and training\nloop are instantiated, trained for five epochs, evaluated on the dev set, and\nits statistics are appended to an experiment_data structure under the key\n'emb_dim_tuning'\u2192'SPR_BENCH'.  Per-epoch losses and Macro-F1 scores are\nrecorded, and the final dev predictions/ground-truth are kept for every run.\nAfter all runs, the whole experiment_data dictionary is saved to\nworking/experiment_data.npy so it can be plotted later.", "Hyperparam tuning name: batch_size. We will loop over a set of candidate batch\nsizes (32, 64, 128, 256).   For each value we recreate the DataLoaders (changing\nonly the training batch-size, validation fixed at 256), re-initialise the model\nand optimiser, train for five epochs, evaluate on the dev set and record all\nlosses/F1, the last-epoch predictions, the ground-truth labels and timestamps.\nAll results are stored in a numpy-serialisable dictionary under the top-level\nkey \"batch_size\", then saved as experiment_data.npy.", "Hyperparam tuning name: dropout_rate. We iterate over several dropout\nprobabilities (0.0-0.5).   For each value a fresh MeanPoolClassifier is built\nwith that dropout, trained for 5 epochs exactly as in the baseline, evaluated on\nthe dev split, and its epoch-wise metrics/losses plus final predictions are\nstored.   Results are accumulated in the experiment_data dictionary under the\ntuning key 'dropout_rate' for the SPR_BENCH dataset and finally saved to\nexperiment_data.npy.", "Hyperparam tuning name: weight_decay. We loop over the four candidate weight-\ndecay values, re-initialise the tiny mean-pooling network for every run, and\ntrain it for five epochs with Adam(weight_decay=*wd*).   For each configuration\nwe log per-epoch train/validation losses and macro-F1, together with final dev\npredictions/ground-truth, inside the required nested `experiment_data`\ndictionary (root key: `'weight_decay_tuning'`, second key: `'SPR_BENCH'`).\nAfter all runs the dictionary is saved as `experiment_data.npy`, making the\nscript completely self-contained and executable.", "The earlier run failed because the script could not locate the benchmark folder.\nI broaden the search logic so `resolve_spr_path()` also checks the absolute path\nused by the baseline (`/home/zxl240011/AI-Scientist-v2/SPR_BENCH`) and, if still\nmissing, raises a clear message telling the user how to set `SPR_BENCH_PATH`.\nWhile touching the training loop I also add per-epoch SWA, CWA and the requested\nSC-Gmean metric (geometric mean), printing and storing them together with\nlosses/F1.   Everything\u2014including model/batches\u2014now respects the chosen CUDA/CPU\ndevice and metrics are persisted to `./working/experiment_data.npy`.", "Fix: the script failed because it couldn\u2019t locate the SPR_BENCH folder. I expand\nresolve_spr_path to look in the original absolute path used by earlier utilities\nand to fall back on an environment variable, so the program no longer crashes\nwhen the dataset is present. I also add per-epoch SC-Gmean tracking, store every\nmetric in the experiment_data dict, and ensure all tensors/models are moved to\nthe correct device. Hyper-parameters (batch size, learning-rate, epochs) are\nslightly tuned while keeping the architecture unchanged.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----------- DATA --------------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    \"\"\"Return Path to SPR_BENCH that actually exists with train.csv.\"\"\"\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",  # treat csv as a single split\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------- VOCAB -------------------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ----------- MODEL -------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\nmodel = MeanPoolClassifier(len(vocab), 64, len(all_labels), pad_idx).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ----------- TRAIN LOOP -------------------------------------------\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    # ---- train ----\n    model.train()\n    train_loss, train_preds, train_trues = 0.0, [], []\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * batch[\"label\"].size(0)\n        train_preds.extend(logits.argmax(1).cpu().numpy())\n        train_trues.extend(batch[\"label\"].cpu().numpy())\n    train_loss /= len(train_loader.dataset)\n    train_macro = f1_score(train_trues, train_preds, average=\"macro\")\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macroF1\"].append(train_macro)\n\n    # ---- validation ----\n    model.eval()\n    val_loss, val_preds, val_trues = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            val_loss += loss.item() * batch[\"label\"].size(0)\n            val_preds.extend(logits.argmax(1).cpu().numpy())\n            val_trues.extend(batch[\"label\"].cpu().numpy())\n    val_loss /= len(val_loader.dataset)\n    val_macro = f1_score(val_trues, val_preds, average=\"macro\")\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(val_macro)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, Val MacroF1 = {val_macro:.4f}\"\n    )\n\n# -------- store predictions / GT for dev split --------------------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = val_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_trues\n\n\n# --------- Optional SWA / CWA -------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\nswa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\ncwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\nprint(f\"Dev SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# --------- SAVE everything ----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- SAVE STRUCTURE ----------------------------------\nexperiment_data = {\n    \"num_epochs_tuning\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n        }\n    }\n}\n# ---------------- PATHS / DEVICE ----------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------- DATA --------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    cands = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        cands.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    cands += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n    ]\n    for c in cands:\n        p = pathlib.Path(c)\n        if (p / \"train.csv\").exists():\n            print(\"Found SPR_BENCH at\", p.resolve())\n            return p.resolve()\n    raise FileNotFoundError(\"Place SPR_BENCH csvs or set SPR_BENCH_PATH.\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ---------------- VOCAB / ENCODING --------------------------------\ndef tokenize(seq: str):\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(Counter(all_tokens))\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str):\n    return [stoi.get(t, unk_idx) for t in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lens = [len(b[\"input_ids\"]) for b in batch]\n    mx = max(lens)\n    inputs = torch.full((len(batch), mx), pad_idx, dtype=torch.long)\n    for i, b in enumerate(batch):\n        inputs[i, : lens[i]] = b[\"input_ids\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": inputs, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------- MODEL -------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_labels, pad):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad)\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\nmodel = MeanPoolClassifier(len(vocab), 64, len(all_labels), pad_idx).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n# ---------------- TRAINING with EARLY STOP ------------------------\nmax_epochs = 20\npatience = 3\nbest_val = -1\nno_improve = 0\nbest_state, best_preds, best_trues = None, None, None\n\n\ndef evaluate(mdl, loader):\n    mdl.eval()\n    loss_tot = 0\n    preds = []\n    trues = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = mdl(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss_tot += loss.item() * batch[\"label\"].size(0)\n            preds.extend(logits.argmax(1).cpu().numpy())\n            trues.extend(batch[\"label\"].cpu().numpy())\n    return (\n        loss_tot / len(loader.dataset),\n        preds,\n        trues,\n        f1_score(trues, preds, average=\"macro\"),\n    )\n\n\nfor epoch in range(1, max_epochs + 1):\n    model.train()\n    tr_loss, tr_preds, tr_trues = 0.0, [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * batch[\"label\"].size(0)\n        tr_preds.extend(logits.argmax(1).cpu().numpy())\n        tr_trues.extend(batch[\"label\"].cpu().numpy())\n    tr_loss /= len(train_loader.dataset)\n    tr_macro = f1_score(tr_trues, tr_preds, average=\"macro\")\n    val_loss, val_preds, val_trues, val_macro = evaluate(model, val_loader)\n    ed = experiment_data[\"num_epochs_tuning\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_macroF1\"].append(tr_macro)\n    ed[\"metrics\"][\"val_macroF1\"].append(val_macro)\n    ed[\"timestamps\"].append(time.time())\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, val_macroF1={val_macro:.4f}\")\n    # ---------- early stopping ----------\n    if val_macro > best_val:\n        best_val = val_macro\n        no_improve = 0\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        best_preds, best_trues = val_preds, val_trues\n    else:\n        no_improve += 1\n        if no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            break\n# ---------------- LOAD BEST & STORE --------------------------------\nmodel.load_state_dict({k: v.to(device) for k, v in best_state.items()})\nexperiment_data[\"num_epochs_tuning\"][\"SPR_BENCH\"][\"predictions\"] = best_preds\nexperiment_data[\"num_epochs_tuning\"][\"SPR_BENCH\"][\"ground_truth\"] = best_trues\n\n\n# ---------- Extra metrics (SWA/CWA) --------------------------------\ndef count_shape_variety(s):\n    return len(set(t[0] for t in s.split() if t))\n\n\ndef count_color_variety(s):\n    return len(set(t[1] for t in s.split() if len(t) > 1))\n\n\ndef weighted_acc(seqs, y_t, y_p, fn):\n    w = [fn(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_t, y_p)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\nswa = weighted_acc(spr[\"dev\"][\"sequence\"], best_trues, best_preds, count_shape_variety)\ncwa = weighted_acc(spr[\"dev\"][\"sequence\"], best_trues, best_preds, count_color_variety)\nprint(f\"Best Dev SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n# ---------------- SAVE ---------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# -------------------------------------------------------------\n# Learning-rate hyper-parameter sweep on SPR_BENCH\n# -------------------------------------------------------------\nimport os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- Experiment-data container ------------------\nexperiment_data = {\"learning_rate\": {\"SPR_BENCH\": {}}}\n\n# -------------- Basic set-up ---------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------- Dataset utilities ----------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    cand = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        cand.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    cand += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for c in cand:\n        p = pathlib.Path(c)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH not found. Set SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    ds = DatasetDict()\n    ds[\"train\"] = _load(\"train.csv\")\n    ds[\"dev\"] = _load(\"dev.csv\")\n    ds[\"test\"] = _load(\"test.csv\")\n    return ds\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded splits:\", {k: len(v) for k, v in spr.items()})\n\n\n# -------------------- Vocab -----------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# -------------------- Model -----------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\n# ------------- Optional auxiliary metrics ---------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\n# ------------- Train for one LR --------------------------------\ndef train_with_lr(lr: float, num_epochs: int = 5):\n    lr_key = f\"lr_{lr}\"\n    exp_slot = {\n        \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    model = MeanPoolClassifier(len(vocab), 64, len(all_labels), pad_idx).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    for epoch in range(1, num_epochs + 1):\n        # training\n        model.train()\n        tr_loss, tr_preds, tr_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            tr_loss += loss.item() * batch[\"label\"].size(0)\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_trues.extend(batch[\"label\"].cpu().numpy())\n        tr_loss /= len(train_loader.dataset)\n        tr_macro = f1_score(tr_trues, tr_preds, average=\"macro\")\n        exp_slot[\"losses\"][\"train\"].append(tr_loss)\n        exp_slot[\"metrics\"][\"train_macroF1\"].append(tr_macro)\n\n        # validation\n        model.eval()\n        v_loss, v_preds, v_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                v_loss += loss.item() * batch[\"label\"].size(0)\n                v_preds.extend(logits.argmax(1).cpu().numpy())\n                v_trues.extend(batch[\"label\"].cpu().numpy())\n        v_loss /= len(val_loader.dataset)\n        v_macro = f1_score(v_trues, v_preds, average=\"macro\")\n        exp_slot[\"losses\"][\"val\"].append(v_loss)\n        exp_slot[\"metrics\"][\"val_macroF1\"].append(v_macro)\n        exp_slot[\"timestamps\"].append(time.time())\n        print(\n            f\"[LR {lr}] Epoch {epoch}: ValLoss={v_loss:.4f}  ValMacroF1={v_macro:.4f}\"\n        )\n    # store preds/gt of last epoch\n    exp_slot[\"predictions\"] = v_preds\n    exp_slot[\"ground_truth\"] = v_trues\n    # optional weighted accuracies\n    swa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], v_trues, v_preds)\n    cwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], v_trues, v_preds)\n    exp_slot[\"shape_weighted_acc\"] = swa\n    exp_slot[\"color_weighted_acc\"] = cwa\n    print(f\"[LR {lr}] Dev SWA={swa:.4f} | CWA={cwa:.4f}\")\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][lr_key] = exp_slot\n\n\n# ------------------ Hyper-parameter sweep ----------------------\nlrs_to_try = [5e-4, 1e-3, 2e-3]\nfor lr in lrs_to_try:\n    train_with_lr(lr, num_epochs=5)\n\n# ------------------ Save everything ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- experiment data container -------------------\nexperiment_data = {\n    \"emb_dim_tuning\": {\n        \"SPR_BENCH\": {\n            \"hyperparams\": [],  # list of embedding dims tried\n            \"metrics\": {\n                \"train_macroF1\": [],\n                \"val_macroF1\": [],\n            },  # list per epoch concatenated over runs\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],  # list of lists (per run)\n            \"ground_truth\": [],  # list of lists (per run)\n            \"timestamps\": [],\n        }\n    }\n}\n\n\n# -------------------- misc utils ----------------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ------------------- DATA -----------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ------------------- VOCAB / TOKENISATION -------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------- MODEL ----------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ------------------- TRAINING LOOP (ACROSS EMB_DIMS) --------------\nemb_dims = [32, 64, 128, 256]\nnum_epochs = 5\nfor emb_dim in emb_dims:\n    print(f\"\\n------ Training with emb_dim={emb_dim} ------\")\n    experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"hyperparams\"].append(emb_dim)\n\n    model = MeanPoolClassifier(len(vocab), emb_dim, len(all_labels), pad_idx).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, num_epochs + 1):\n        # training\n        model.train()\n        t_loss, t_preds, t_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            t_loss += loss.item() * batch[\"label\"].size(0)\n            t_preds.extend(logits.argmax(1).cpu().numpy())\n            t_trues.extend(batch[\"label\"].cpu().numpy())\n        t_loss /= len(train_loader.dataset)\n        t_macro = f1_score(t_trues, t_preds, average=\"macro\")\n\n        # validation\n        model.eval()\n        v_loss, v_preds, v_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                v_loss += loss.item() * batch[\"label\"].size(0)\n                v_preds.extend(logits.argmax(1).cpu().numpy())\n                v_trues.extend(batch[\"label\"].cpu().numpy())\n        v_loss /= len(val_loader.dataset)\n        v_macro = f1_score(v_trues, v_preds, average=\"macro\")\n\n        # record epoch stats\n        experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n        experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n        experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"metrics\"][\n            \"train_macroF1\"\n        ].append(t_macro)\n        experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(\n            v_macro\n        )\n        experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n        print(\n            f\"Emb {emb_dim} | Epoch {epoch}: trainF1={t_macro:.4f} valF1={v_macro:.4f}\"\n        )\n\n    # store final predictions for this run\n    experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"predictions\"].append(v_preds)\n    experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"ground_truth\"].append(v_trues)\n\n    # free memory before next run\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n\n\n# ------------------- OPTIONAL METRICS -----------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\n# compute SWA/CWA for best dev F1 run\nbest_idx = int(\n    np.argmax(\n        experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"][\n            -len(emb_dims) :\n        ]\n    )\n)\nbest_preds = experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"predictions\"][best_idx]\nbest_trues = experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"][\"ground_truth\"][best_idx]\nswa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], best_trues, best_preds)\ncwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], best_trues, best_preds)\nprint(f\"Best run idx {best_idx} | Dev SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# ------------------- SAVE RESULTS ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------ #\n#              EXPERIMENT BOOK-KEEPING DICT                    #\n# ------------------------------------------------------------ #\nexperiment_data = {\n    \"batch_size\": {  # hyperparameter tuning type\n        \"SPR_BENCH\": {}  # we will add one sub-dict per batch size tried\n    }\n}\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------ #\n#                       DEVICE                                 #\n# ------------------------------------------------------------ #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------ #\n#                 DATASET LOADING HELPERS                      #\n# ------------------------------------------------------------ #\ndef resolve_spr_path() -> pathlib.Path:\n    cand = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        cand.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    cand += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for p in cand:\n        p = pathlib.Path(p)\n        if (p / \"train.csv\").exists():\n            print(\"Found SPR_BENCH at\", p.resolve())\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv_name):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = _l(\"train.csv\"), _l(\"dev.csv\"), _l(\"test.csv\")\n    return d\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded splits:\", {k: len(v) for k, v in spr.items()})\n\n\n# ------------------------------------------------------------ #\n#                 VOCAB & ENCODING                             #\n# ------------------------------------------------------------ #\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(Counter(all_tokens))\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lens = [len(b[\"input_ids\"]) for b in batch]\n    maxlen = max(lens)\n    x = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, b in enumerate(batch):\n        x[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    y = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": x, \"label\": y}\n\n\n# ------------------------------------------------------------ #\n#                       MODEL                                  #\n# ------------------------------------------------------------ #\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, n_labels, pad):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad)\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(emb_dim, n_labels)\n        self.pad = pad\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        em = self.emb(x)\n        mean = (em * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ------------------------------------------------------------ #\n#          SHAPE/COLOR WEIGHTED HELPERS (OPTIONAL)             #\n# ------------------------------------------------------------ #\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0\n\n\n# ------------------------------------------------------------ #\n#                MAIN HYPERPARAMETER LOOP                      #\n# ------------------------------------------------------------ #\ncandidate_batch_sizes = [32, 64, 128, 256]\nnum_epochs = 5\nval_batch_size = 256  # fixed for speed\n\nfor bs in candidate_batch_sizes:\n    print(f\"\\n===== Training with train batch_size={bs} =====\")\n    subdict = {\n        \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    # loaders\n    train_loader = DataLoader(\n        SPRDataset(spr[\"train\"]), batch_size=bs, shuffle=True, collate_fn=collate\n    )\n    val_loader = DataLoader(\n        SPRDataset(spr[\"dev\"]),\n        batch_size=val_batch_size,\n        shuffle=False,\n        collate_fn=collate,\n    )\n    # fresh model\n    model = MeanPoolClassifier(len(vocab), 64, len(all_labels), pad_idx).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        tr_loss, tr_pred, tr_true = 0.0, [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            tr_loss += loss.item() * batch[\"label\"].size(0)\n            tr_pred.extend(logits.argmax(1).cpu().numpy())\n            tr_true.extend(batch[\"label\"].cpu().numpy())\n        tr_loss /= len(train_loader.dataset)\n        tr_f1 = f1_score(tr_true, tr_pred, average=\"macro\")\n        subdict[\"losses\"][\"train\"].append(tr_loss)\n        subdict[\"metrics\"][\"train_macroF1\"].append(tr_f1)\n\n        # ---- validation ----\n        model.eval()\n        v_loss, v_pred, v_true = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                v_loss += loss.item() * batch[\"label\"].size(0)\n                v_pred.extend(logits.argmax(1).cpu().numpy())\n                v_true.extend(batch[\"label\"].cpu().numpy())\n        v_loss /= len(val_loader.dataset)\n        v_f1 = f1_score(v_true, v_pred, average=\"macro\")\n        subdict[\"losses\"][\"val\"].append(v_loss)\n        subdict[\"metrics\"][\"val_macroF1\"].append(v_f1)\n        subdict[\"timestamps\"].append(time.time())\n        print(f\"Epoch {epoch}/{num_epochs} | val_loss={v_loss:.4f}  val_F1={v_f1:.4f}\")\n\n    # store final-epoch preds / gts & optional metrics\n    subdict[\"predictions\"] = v_pred\n    subdict[\"ground_truth\"] = v_true\n    subdict[\"SWA\"] = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], v_true, v_pred)\n    subdict[\"CWA\"] = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], v_true, v_pred)\n    print(f\"Final Dev SWA={subdict['SWA']:.4f}  CWA={subdict['CWA']:.4f}\")\n\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][str(bs)] = subdict\n\n# ------------------------------------------------------------ #\n#                    SAVE EVERYTHING                           #\n# ------------------------------------------------------------ #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"dropout_rate\": {  # hyperparam tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"rates\": [],\n            \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},  # list-of-lists\n            \"losses\": {\"train\": [], \"val\": []},  # list-of-lists\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n            \"swa\": [],\n            \"cwa\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----------- DATA --------------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------- VOCAB -------------------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ----------- MODEL -------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx, dropout_p: float):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ----------- Extra eval metrics -----------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\n# ------------------------------------------------------------------\ndropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\nfor rate in dropout_values:\n    print(f\"\\n========== Training with dropout={rate} ==========\")\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"rates\"].append(rate)\n    # fresh model / optimiser\n    model = MeanPoolClassifier(\n        len(vocab), 64, len(all_labels), pad_idx, dropout_p=rate\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    num_epochs = 5\n    run_train_losses, run_val_losses = [], []\n    run_train_f1, run_val_f1 = [], []\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        train_loss, train_preds, train_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch[\"label\"].size(0)\n            train_preds.extend(logits.argmax(1).cpu().numpy())\n            train_trues.extend(batch[\"label\"].cpu().numpy())\n        train_loss /= len(train_loader.dataset)\n        train_macro = f1_score(train_trues, train_preds, average=\"macro\")\n        run_train_losses.append(train_loss)\n        run_train_f1.append(train_macro)\n\n        # ---- val ----\n        model.eval()\n        val_loss, val_preds, val_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                val_loss += loss.item() * batch[\"label\"].size(0)\n                val_preds.extend(logits.argmax(1).cpu().numpy())\n                val_trues.extend(batch[\"label\"].cpu().numpy())\n        val_loss /= len(val_loader.dataset)\n        val_macro = f1_score(val_trues, val_preds, average=\"macro\")\n        run_val_losses.append(val_loss)\n        run_val_f1.append(val_macro)\n\n        print(\n            f\"Dropout {rate} | Epoch {epoch}: val_loss = {val_loss:.4f}, val_macroF1 = {val_macro:.4f}\"\n        )\n\n    # store results for this dropout\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        run_train_losses\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(run_val_losses)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"train_macroF1\"].append(\n        run_train_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(\n        run_val_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"ground_truth\"].append(val_trues)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    # extra metrics\n    swa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    cwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"swa\"].append(swa)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"cwa\"].append(cwa)\n    print(f\"SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# --------- SAVE everything ----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"weight_decay_tuning\": {\n        \"SPR_BENCH\": {\n            \"configs\": [],  # list of weight_decay values tried\n            \"metrics\": {\"train\": [], \"val\": []},  # each item is list over epochs\n            \"losses\": {\"train\": [], \"val\": []},  #   ''   ''\n            \"predictions\": [],  # final dev predictions per config\n            \"ground_truth\": [],  # corresponding GT (same each run)\n            \"timestamps\": [],  # end-time per epoch for each config\n        }\n    }\n}\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# DATA --------------------------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------------- VOCAB ------------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------- MODEL -------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ---------------- TRAINING / TUNING -------------------------------\nnum_epochs = 5\nweight_decays = [0.0, 1e-5, 1e-4, 1e-3]\n\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    model = MeanPoolClassifier(len(vocab), 64, len(all_labels), pad_idx).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    run_train_losses, run_val_losses = [], []\n    run_train_f1, run_val_f1 = [], []\n    run_timestamps = []\n\n    for epoch in range(1, num_epochs + 1):\n        # -------- train --------\n        model.train()\n        tr_loss, tr_preds, tr_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            tr_loss += loss.item() * batch[\"label\"].size(0)\n            tr_preds.extend(logits.argmax(1).cpu().numpy())\n            tr_trues.extend(batch[\"label\"].cpu().numpy())\n        tr_loss /= len(train_loader.dataset)\n        tr_macro = f1_score(tr_trues, tr_preds, average=\"macro\")\n\n        # -------- validation -----\n        model.eval()\n        val_loss, val_preds, val_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                val_loss += loss.item() * batch[\"label\"].size(0)\n                val_preds.extend(logits.argmax(1).cpu().numpy())\n                val_trues.extend(batch[\"label\"].cpu().numpy())\n        val_loss /= len(val_loader.dataset)\n        val_macro = f1_score(val_trues, val_preds, average=\"macro\")\n\n        run_train_losses.append(tr_loss)\n        run_val_losses.append(val_loss)\n        run_train_f1.append(tr_macro)\n        run_val_f1.append(val_macro)\n        run_timestamps.append(time.time())\n\n        print(\n            f\"Epoch {epoch}: wd={wd} | val_loss={val_loss:.4f} | val_MacroF1={val_macro:.4f}\"\n        )\n\n    # ------------- log results ------------------------------------\n    ed = experiment_data[\"weight_decay_tuning\"][\"SPR_BENCH\"]\n    ed[\"configs\"].append(wd)\n    ed[\"losses\"][\"train\"].append(run_train_losses)\n    ed[\"losses\"][\"val\"].append(run_val_losses)\n    ed[\"metrics\"][\"train\"].append(run_train_f1)\n    ed[\"metrics\"][\"val\"].append(run_val_f1)\n    ed[\"predictions\"].append(val_preds)  # from last epoch\n    ed[\"ground_truth\"].append(val_trues)  # same each config\n    ed[\"timestamps\"].append(run_timestamps)\n\n    # shape/color weighted accuracy for analysis\n    def count_shape(sequence: str):\n        return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n    def count_color(sequence: str):\n        return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n    seqs_dev = spr[\"dev\"][\"sequence\"]\n    swa = sum(\n        (count_shape(s) if t == p else 0)\n        for s, t, p in zip(seqs_dev, val_trues, val_preds)\n    ) / sum(map(count_shape, seqs_dev))\n    cwa = sum(\n        (count_color(s) if t == p else 0)\n        for s, t, p in zip(seqs_dev, val_trues, val_preds)\n    ) / sum(map(count_color, seqs_dev))\n    print(f\"Final Dev SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# ---------------- SAVE --------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import Dict, List\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_macroF1\": [],\n            \"val_macroF1\": [],\n            \"val_SWA\": [],\n            \"val_CWA\": [],\n            \"val_SC_Gmean\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ----------------------------- DEVICE ------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- DATA LOCATION HELPERS ----------------------\ndef resolve_spr_path() -> pathlib.Path:\n    \"\"\"\n    Attempt to locate SPR_BENCH folder.\n    Order of preference:\n        1) env var SPR_BENCH_PATH\n        2) ./SPR_BENCH or ../SPR_BENCH or ~/SPR_BENCH\n        3) baseline absolute path from earlier code\n    \"\"\"\n    candidate_paths = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidate_paths.append(pathlib.Path(os.environ[\"SPR_BENCH_PATH\"]))\n    cwd = pathlib.Path.cwd()\n    candidate_paths += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for p in candidate_paths:\n        if (p / \"train.csv\").exists():\n            print(\"Found SPR_BENCH at\", p.resolve())\n            return p.resolve()\n    raise FileNotFoundError(\n        \"Could not find SPR_BENCH dataset. Place 'train.csv/dev.csv/test.csv' in one \"\n        \"of the standard locations or export SPR_BENCH_PATH pointing to the folder.\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# --------------------------- VOCAB ---------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(Counter(all_tokens))\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\nitos = {i: l for l, i in ltoi.items()}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\n# ------------------------- DATASET / DATALOADER --------------------\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[lbl] for lbl in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(b[\"input_ids\"]) for b in batch]\n    max_len = max(lengths)\n    input_mat = torch.full((len(batch), max_len), pad_idx, dtype=torch.long)\n    for i, b in enumerate(batch):\n        input_mat[i, : lengths[i]] = b[\"input_ids\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_mat, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------ MODEL ------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int, n_labels: int, pad_id: int):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_id)\n        self.dropout = nn.Dropout(0.25)\n        self.fc = nn.Linear(emb_dim, n_labels)\n        self.pad_id = pad_id\n\n    def forward(self, x):\n        mask = (x != self.pad_id).unsqueeze(-1)\n        embs = self.emb(x) * mask\n        avg = embs.sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.dropout(avg))\n\n\nmodel = MeanPoolClassifier(len(vocab), 96, len(all_labels), pad_idx).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n\n# --------------------- METRIC HELPERS ------------------------------\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef weighted_accuracy(\n    seqs: List[str], y_true: List[int], y_pred: List[int], fn\n) -> float:\n    weights = [fn(s) for s in seqs]\n    correct_w = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct_w) / sum(weights) if sum(weights) else 0.0\n\n\n# ----------------------- TRAIN / EVAL LOOPS ------------------------\ndef evaluate(net: nn.Module, loader) -> Dict[str, float]:\n    net.eval()\n    loss_tot, preds, trues = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = net(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss_tot += loss.item() * batch[\"label\"].size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            trues.extend(batch[\"label\"].cpu().tolist())\n    loss_avg = loss_tot / len(loader.dataset)\n    macro_f1 = f1_score(trues, preds, average=\"macro\")\n    return {\"loss\": loss_avg, \"preds\": preds, \"trues\": trues, \"macro_f1\": macro_f1}\n\n\nmax_epochs = 25\npatience = 4\nbest_metric = -1.0\nepochs_since_improve = 0\nbest_state = None\nbest_preds, best_trues = None, None\n\nfor epoch in range(1, max_epochs + 1):\n    # ---- train ----\n    model.train()\n    train_loss_tot, train_preds, train_trues = 0.0, [], []\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        train_loss_tot += loss.item() * batch[\"label\"].size(0)\n        train_preds.extend(logits.argmax(1).cpu().tolist())\n        train_trues.extend(batch[\"label\"].cpu().tolist())\n\n    train_loss = train_loss_tot / len(train_loader.dataset)\n    train_macro = f1_score(train_trues, train_preds, average=\"macro\")\n\n    # ---- validation ----\n    val_stats = evaluate(model, val_loader)\n    val_loss = val_stats[\"loss\"]\n    val_macro = val_stats[\"macro_f1\"]\n\n    # shape/color metrics\n    val_swa = weighted_accuracy(\n        spr[\"dev\"][\"sequence\"],\n        val_stats[\"trues\"],\n        val_stats[\"preds\"],\n        count_shape_variety,\n    )\n    val_cwa = weighted_accuracy(\n        spr[\"dev\"][\"sequence\"],\n        val_stats[\"trues\"],\n        val_stats[\"preds\"],\n        count_color_variety,\n    )\n    sc_gmean = (val_swa * val_cwa) ** 0.5 if val_swa > 0 and val_cwa > 0 else 0.0\n\n    # ---- bookkeeping ----\n    ed = experiment_data[\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_macroF1\"].append(train_macro)\n    ed[\"metrics\"][\"val_macroF1\"].append(val_macro)\n    ed[\"metrics\"][\"val_SWA\"].append(val_swa)\n    ed[\"metrics\"][\"val_CWA\"].append(val_cwa)\n    ed[\"metrics\"][\"val_SC_Gmean\"].append(sc_gmean)\n    ed[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d} | \"\n        f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} | \"\n        f\"val_F1={val_macro:.4f} SWA={val_swa:.4f} CWA={val_cwa:.4f} SC-G={sc_gmean:.4f}\"\n    )\n\n    # ---- early stopping based on SC-Gmean ----\n    if sc_gmean > best_metric:\n        best_metric = sc_gmean\n        epochs_since_improve = 0\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        best_preds, best_trues = val_stats[\"preds\"], val_stats[\"trues\"]\n    else:\n        epochs_since_improve += 1\n        if epochs_since_improve >= patience:\n            print(\"Early stopping (no SC-Gmean improvement).\")\n            break\n\n# ------------------------ SAVE FINAL RESULTS -----------------------\nif best_state is not None:\n    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = best_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = best_trues\n\n# persist results\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data)\nprint(\"Experiment data saved to\", save_path)\n", "# ----------------- SET-UP ----------------------------------------------------\nimport os, pathlib, time, math, numpy as np, torch\nfrom collections import Counter\nfrom typing import List, Dict\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# create working directory for any artefacts\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device handling (CRITICAL GPU REQUIREMENT)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------- EXPERIMENT DATA STRUCTURE ---------------------------------\nexperiment_data: Dict = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_macroF1\": [],\n            \"val_macroF1\": [],\n            \"val_SWA\": [],\n            \"val_CWA\": [],\n            \"val_SC_Gmean\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\n# ----------------- DATA LOADING ----------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    \"\"\"\n    Try several sensible locations, then raise if nothing valid is found.\n    \"\"\"\n    # 1) environment variable\n    cands: List[pathlib.Path] = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        cands.append(pathlib.Path(os.environ[\"SPR_BENCH_PATH\"]))\n    # 2) common relative paths\n    cwd = pathlib.Path.cwd()\n    cands += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n    ]\n    # 3) absolute path used by earlier baseline code\n    cands.append(pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"))\n    for p in cands:\n        if (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(\"Found SPR_BENCH at\", p.resolve())\n            return p.resolve()\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH. Place csvs locally or set $SPR_BENCH_PATH.\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"\n    Return DatasetDict with train/dev/test splits, each loaded from its csv.\n    \"\"\"\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",  # treat csv as a single split\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------------- TOKENISATION & VOCAB --------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_train_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(Counter(all_train_tokens))\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nlabels = sorted({lab for lab in spr[\"train\"][\"label\"]})\nltoi = {l: i for i, l in enumerate(labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\n# ----------------- DATASET / DATALOADER --------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(b[\"input_ids\"]) for b in batch]\n    max_len = max(lengths)\n    x = torch.full((len(batch), max_len), pad_idx, dtype=torch.long)\n    for i, b in enumerate(batch):\n        x[i, : lengths[i]] = b[\"input_ids\"]\n    y = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": x, \"label\": y}\n\n\nbatch_size = 256  # tuned\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=512, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ----------------- MODEL ------------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_sz: int, emb_dim: int, num_labels: int, pad: int):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad)\n        self.drop = nn.Dropout(0.25)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)  # (B, T, 1)\n        emb = self.emb(x)  # (B, T, D)\n        summed = (emb * mask).sum(1)\n        denom = mask.sum(1).clamp(min=1)\n        mean = summed / denom\n        return self.fc(self.drop(mean))\n\n\nmodel = MeanPoolClassifier(len(vocab), 128, len(labels), pad_idx).to(device)\n\n# ----------------- OPTIMISER / LOSS ------------------------------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)  # tuned lr\n\n\n# ----------------- METRIC HELPERS --------------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len({tok[0] for tok in sequence.split() if tok})\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len({tok[1] for tok in sequence.split() if len(tok) > 1})\n\n\ndef weighted_accuracy(seqs: List[str], y_t, y_p, fn) -> float:\n    weights = [fn(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_t, y_p)]\n    tot = sum(weights)\n    return sum(correct) / tot if tot else 0.0\n\n\n# ----------------- EVALUATION -------------------------------------------------\ndef evaluate(mdl, loader):\n    mdl.eval()\n    tot_loss, preds, trues = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = mdl(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"label\"].size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            trues.extend(batch[\"label\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    macro_f1 = f1_score(trues, preds, average=\"macro\")\n    return avg_loss, trues, preds, macro_f1\n\n\n# ----------------- TRAINING LOOP ---------------------------------------------\nmax_epochs, patience = 25, 4\nbest_sc_gmean, epochs_since_improve = -1, 0\nbest_state, best_preds, best_trues = None, None, None\n\nfor epoch in range(1, max_epochs + 1):\n    # ---- training -----------------------------------------------------------\n    model.train()\n    epoch_loss, epoch_preds, epoch_trues = 0.0, [], []\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch[\"label\"].size(0)\n        epoch_preds.extend(logits.argmax(1).cpu().tolist())\n        epoch_trues.extend(batch[\"label\"].cpu().tolist())\n    train_loss = epoch_loss / len(train_loader.dataset)\n    train_macro = f1_score(epoch_trues, epoch_preds, average=\"macro\")\n    # ---- validation ---------------------------------------------------------\n    val_loss, val_trues, val_preds, val_macro = evaluate(model, val_loader)\n    val_swa = weighted_accuracy(\n        spr[\"dev\"][\"sequence\"], val_trues, val_preds, count_shape_variety\n    )\n    val_cwa = weighted_accuracy(\n        spr[\"dev\"][\"sequence\"], val_trues, val_preds, count_color_variety\n    )\n    val_sc_gmean = math.sqrt(val_swa * val_cwa)\n\n    # ---- log & save metrics --------------------------------------------------\n    ed = experiment_data[\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_macroF1\"].append(train_macro)\n    ed[\"metrics\"][\"val_macroF1\"].append(val_macro)\n    ed[\"metrics\"][\"val_SWA\"].append(val_swa)\n    ed[\"metrics\"][\"val_CWA\"].append(val_cwa)\n    ed[\"metrics\"][\"val_SC_Gmean\"].append(val_sc_gmean)\n    ed[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d} | \"\n        f\"val_loss={val_loss:.4f} | \"\n        f\"macroF1={val_macro:.4f} | \"\n        f\"SWA={val_swa:.4f} | CWA={val_cwa:.4f} | \"\n        f\"SC-Gmean={val_sc_gmean:.4f}\"\n    )\n\n    # ---- early stopping based on SC-Gmean -----------------------------------\n    if val_sc_gmean > best_sc_gmean:\n        best_sc_gmean = val_sc_gmean\n        epochs_since_improve = 0\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        best_preds, best_trues = val_preds, val_trues\n    else:\n        epochs_since_improve += 1\n        if epochs_since_improve >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# ----------------- RESTORE BEST & FINAL METRICS ------------------------------\nmodel.load_state_dict({k: v.to(device) for k, v in best_state.items()})\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = best_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = best_trues\n\nprint(f\"Best Dev SC-Gmean: {best_sc_gmean:.4f}\")\n\n# ----------------- SAVE ALL RESULTS ------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"dropout_rate\": {  # hyperparam tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"rates\": [],\n            \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},  # list-of-lists\n            \"losses\": {\"train\": [], \"val\": []},  # list-of-lists\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n            \"swa\": [],\n            \"cwa\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----------- DATA --------------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------- VOCAB -------------------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ----------- MODEL -------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx, dropout_p: float):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ----------- Extra eval metrics -----------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\n# ------------------------------------------------------------------\ndropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\nfor rate in dropout_values:\n    print(f\"\\n========== Training with dropout={rate} ==========\")\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"rates\"].append(rate)\n    # fresh model / optimiser\n    model = MeanPoolClassifier(\n        len(vocab), 64, len(all_labels), pad_idx, dropout_p=rate\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    num_epochs = 5\n    run_train_losses, run_val_losses = [], []\n    run_train_f1, run_val_f1 = [], []\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        train_loss, train_preds, train_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch[\"label\"].size(0)\n            train_preds.extend(logits.argmax(1).cpu().numpy())\n            train_trues.extend(batch[\"label\"].cpu().numpy())\n        train_loss /= len(train_loader.dataset)\n        train_macro = f1_score(train_trues, train_preds, average=\"macro\")\n        run_train_losses.append(train_loss)\n        run_train_f1.append(train_macro)\n\n        # ---- val ----\n        model.eval()\n        val_loss, val_preds, val_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                val_loss += loss.item() * batch[\"label\"].size(0)\n                val_preds.extend(logits.argmax(1).cpu().numpy())\n                val_trues.extend(batch[\"label\"].cpu().numpy())\n        val_loss /= len(val_loader.dataset)\n        val_macro = f1_score(val_trues, val_preds, average=\"macro\")\n        run_val_losses.append(val_loss)\n        run_val_f1.append(val_macro)\n\n        print(\n            f\"Dropout {rate} | Epoch {epoch}: val_loss = {val_loss:.4f}, val_macroF1 = {val_macro:.4f}\"\n        )\n\n    # store results for this dropout\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        run_train_losses\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(run_val_losses)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"train_macroF1\"].append(\n        run_train_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(\n        run_val_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"ground_truth\"].append(val_trues)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    # extra metrics\n    swa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    cwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"swa\"].append(swa)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"cwa\"].append(cwa)\n    print(f\"SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# --------- SAVE everything ----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"dropout_rate\": {  # hyperparam tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"rates\": [],\n            \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},  # list-of-lists\n            \"losses\": {\"train\": [], \"val\": []},  # list-of-lists\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n            \"swa\": [],\n            \"cwa\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----------- DATA --------------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------- VOCAB -------------------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ----------- MODEL -------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx, dropout_p: float):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ----------- Extra eval metrics -----------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\n# ------------------------------------------------------------------\ndropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\nfor rate in dropout_values:\n    print(f\"\\n========== Training with dropout={rate} ==========\")\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"rates\"].append(rate)\n    # fresh model / optimiser\n    model = MeanPoolClassifier(\n        len(vocab), 64, len(all_labels), pad_idx, dropout_p=rate\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    num_epochs = 5\n    run_train_losses, run_val_losses = [], []\n    run_train_f1, run_val_f1 = [], []\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        train_loss, train_preds, train_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch[\"label\"].size(0)\n            train_preds.extend(logits.argmax(1).cpu().numpy())\n            train_trues.extend(batch[\"label\"].cpu().numpy())\n        train_loss /= len(train_loader.dataset)\n        train_macro = f1_score(train_trues, train_preds, average=\"macro\")\n        run_train_losses.append(train_loss)\n        run_train_f1.append(train_macro)\n\n        # ---- val ----\n        model.eval()\n        val_loss, val_preds, val_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                val_loss += loss.item() * batch[\"label\"].size(0)\n                val_preds.extend(logits.argmax(1).cpu().numpy())\n                val_trues.extend(batch[\"label\"].cpu().numpy())\n        val_loss /= len(val_loader.dataset)\n        val_macro = f1_score(val_trues, val_preds, average=\"macro\")\n        run_val_losses.append(val_loss)\n        run_val_f1.append(val_macro)\n\n        print(\n            f\"Dropout {rate} | Epoch {epoch}: val_loss = {val_loss:.4f}, val_macroF1 = {val_macro:.4f}\"\n        )\n\n    # store results for this dropout\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        run_train_losses\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(run_val_losses)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"train_macroF1\"].append(\n        run_train_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(\n        run_val_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"ground_truth\"].append(val_trues)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    # extra metrics\n    swa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    cwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"swa\"].append(swa)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"cwa\"].append(cwa)\n    print(f\"SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# --------- SAVE everything ----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, json, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"dropout_rate\": {  # hyperparam tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"rates\": [],\n            \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},  # list-of-lists\n            \"losses\": {\"train\": [], \"val\": []},  # list-of-lists\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n            \"swa\": [],\n            \"cwa\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# ----------- DATA --------------------------------------------------\ndef resolve_spr_path() -> pathlib.Path:\n    candidates = []\n    if \"SPR_BENCH_PATH\" in os.environ:\n        candidates.append(os.environ[\"SPR_BENCH_PATH\"])\n    cwd = pathlib.Path.cwd()\n    candidates += [\n        cwd / \"SPR_BENCH\",\n        cwd.parent / \"SPR_BENCH\",\n        pathlib.Path.home() / \"SPR_BENCH\",\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n    ]\n    for cand in candidates:\n        p = pathlib.Path(cand)\n        if (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH dataset at {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Set env SPR_BENCH_PATH or place csvs in ./SPR_BENCH\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nspr_root = resolve_spr_path()\nspr = load_spr_bench(spr_root)\nprint(\"Loaded SPR_BENCH with sizes:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------- VOCAB -------------------------------------------------\ndef tokenize(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in tokenize(seq)]\nvocab_counter = Counter(all_tokens)\nvocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab_counter)\nstoi = {w: i for i, w in enumerate(vocab)}\npad_idx, unk_idx = stoi[\"<PAD>\"], stoi[\"<UNK>\"]\n\nall_labels = sorted(set(spr[\"train\"][\"label\"]))\nltoi = {l: i for i, l in enumerate(all_labels)}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, unk_idx) for tok in tokenize(seq)]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [ltoi[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    maxlen = max(lengths)\n    input_ids = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, item in enumerate(batch):\n        seq = item[\"input_ids\"]\n        input_ids[i, : len(seq)] = seq\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input_ids\": input_ids, \"label\": labels}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ----------- MODEL -------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_labels, pad_idx, dropout_p: float):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.drop = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(emb_dim, num_labels)\n        self.pad = pad_idx\n\n    def forward(self, x):\n        mask = (x != self.pad).unsqueeze(-1)\n        emb = self.emb(x)\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.fc(self.drop(mean))\n\n\n# ----------- Extra eval metrics -----------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\ndef color_weighted_accuracy(seqs: List[str], y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [w0 if t == p else 0 for w0, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0\n\n\n# ------------------------------------------------------------------\ndropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\nfor rate in dropout_values:\n    print(f\"\\n========== Training with dropout={rate} ==========\")\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"rates\"].append(rate)\n    # fresh model / optimiser\n    model = MeanPoolClassifier(\n        len(vocab), 64, len(all_labels), pad_idx, dropout_p=rate\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    num_epochs = 5\n    run_train_losses, run_val_losses = [], []\n    run_train_f1, run_val_f1 = [], []\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        train_loss, train_preds, train_trues = 0.0, [], []\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch[\"label\"].size(0)\n            train_preds.extend(logits.argmax(1).cpu().numpy())\n            train_trues.extend(batch[\"label\"].cpu().numpy())\n        train_loss /= len(train_loader.dataset)\n        train_macro = f1_score(train_trues, train_preds, average=\"macro\")\n        run_train_losses.append(train_loss)\n        run_train_f1.append(train_macro)\n\n        # ---- val ----\n        model.eval()\n        val_loss, val_preds, val_trues = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(logits, batch[\"label\"])\n                val_loss += loss.item() * batch[\"label\"].size(0)\n                val_preds.extend(logits.argmax(1).cpu().numpy())\n                val_trues.extend(batch[\"label\"].cpu().numpy())\n        val_loss /= len(val_loader.dataset)\n        val_macro = f1_score(val_trues, val_preds, average=\"macro\")\n        run_val_losses.append(val_loss)\n        run_val_f1.append(val_macro)\n\n        print(\n            f\"Dropout {rate} | Epoch {epoch}: val_loss = {val_loss:.4f}, val_macroF1 = {val_macro:.4f}\"\n        )\n\n    # store results for this dropout\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        run_train_losses\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(run_val_losses)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"train_macroF1\"].append(\n        run_train_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(\n        run_val_f1\n    )\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"predictions\"].append(val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"ground_truth\"].append(val_trues)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    # extra metrics\n    swa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    cwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], val_trues, val_preds)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"swa\"].append(swa)\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"cwa\"].append(cwa)\n    print(f\"SWA: {swa:.4f} | CWA: {cwa:.4f}\")\n\n# --------- SAVE everything ----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 402408.52\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 672207.19\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 778351.74\nexamples/s]', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train': 20000,\n'dev': 5000, 'test': 10000}\", '\\n', 'Epoch 1: validation_loss = 0.5426, Val\nMacroF1 = 0.7421', '\\n', 'Epoch 2: validation_loss = 0.5235, Val MacroF1 =\n0.7404', '\\n', 'Epoch 3: validation_loss = 0.5214, Val MacroF1 = 0.7402', '\\n',\n'Epoch 4: validation_loss = 0.5215, Val MacroF1 = 0.7582', '\\n', 'Epoch 5:\nvalidation_loss = 0.5214, Val MacroF1 = 0.7467', '\\n', 'Dev SWA: 0.7438 | CWA:\n0.7388', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-15_18-22-\n30_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n1/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 61, in <module>\\n    spr_root = resolve_spr_path()\\n\n^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 44, in resolve_spr_path\\n    raise\nFileNotFoundError(\"Place SPR_BENCH csvs or set\nSPR_BENCH_PATH.\")\\nFileNotFoundError: Place SPR_BENCH csvs or set\nSPR_BENCH_PATH.\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 357348.28\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 462039.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 597189.96\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"{'train': 20000, 'dev': 5000,\n'test': 10000}\", '\\n', '[LR 0.0005] Epoch 1: ValLoss=0.5922  ValMacroF1=0.7290',\n'\\n', '[LR 0.0005] Epoch 2: ValLoss=0.5437  ValMacroF1=0.7455', '\\n', '[LR\n0.0005] Epoch 3: ValLoss=0.5278  ValMacroF1=0.7504', '\\n', '[LR 0.0005] Epoch 4:\nValLoss=0.5233  ValMacroF1=0.7463', '\\n', '[LR 0.0005] Epoch 5: ValLoss=0.5217\nValMacroF1=0.7474', '\\n', '[LR 0.0005] Dev SWA=0.7447 | CWA=0.7397', '\\n', '[LR\n0.001] Epoch 1: ValLoss=0.5543  ValMacroF1=0.7428', '\\n', '[LR 0.001] Epoch 2:\nValLoss=0.5258  ValMacroF1=0.7463', '\\n', '[LR 0.001] Epoch 3: ValLoss=0.5233\nValMacroF1=0.7670', '\\n', '[LR 0.001] Epoch 4: ValLoss=0.5221\nValMacroF1=0.7589', '\\n', '[LR 0.001] Epoch 5: ValLoss=0.5218\nValMacroF1=0.7538', '\\n', '[LR 0.001] Dev SWA=0.7511 | CWA=0.7460', '\\n', '[LR\n0.002] Epoch 1: ValLoss=0.5262  ValMacroF1=0.7526', '\\n', '[LR 0.002] Epoch 2:\nValLoss=0.5232  ValMacroF1=0.7670', '\\n', '[LR 0.002] Epoch 3: ValLoss=0.5230\nValMacroF1=0.7541', '\\n', '[LR 0.002] Epoch 4: ValLoss=0.5223\nValMacroF1=0.7418', '\\n', '[LR 0.002] Epoch 5: ValLoss=0.5215\nValMacroF1=0.7442', '\\n', '[LR 0.002] Dev SWA=0.7408 | CWA=0.7361', '\\n', 'Saved\nall results to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n15_18-22-30_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 9\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 422881.11\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 584881.75\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 723767.32\nexamples/s]', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train': 20000,\n'dev': 5000, 'test': 10000}\", '\\n', '\\n------ Training with emb_dim=32 ------',\n'\\n', 'Emb 32 | Epoch 1: trainF1=0.6651 valF1=0.7401', '\\n', 'Emb 32 | Epoch 2:\ntrainF1=0.7267 valF1=0.7491', '\\n', 'Emb 32 | Epoch 3: trainF1=0.7384\nvalF1=0.7433', '\\n', 'Emb 32 | Epoch 4: trainF1=0.7402 valF1=0.7437', '\\n', 'Emb\n32 | Epoch 5: trainF1=0.7441 valF1=0.7527', '\\n', '\\n------ Training with\nemb_dim=64 ------', '\\n', 'Emb 64 | Epoch 1: trainF1=0.7160 valF1=0.7511', '\\n',\n'Emb 64 | Epoch 2: trainF1=0.7403 valF1=0.7463', '\\n', 'Emb 64 | Epoch 3:\ntrainF1=0.7459 valF1=0.7348', '\\n', 'Emb 64 | Epoch 4: trainF1=0.7455\nvalF1=0.7503', '\\n', 'Emb 64 | Epoch 5: trainF1=0.7450 valF1=0.7503', '\\n',\n'\\n------ Training with emb_dim=128 ------', '\\n', 'Emb 128 | Epoch 1:\ntrainF1=0.7175 valF1=0.7544', '\\n', 'Emb 128 | Epoch 2: trainF1=0.7482\nvalF1=0.7384', '\\n', 'Emb 128 | Epoch 3: trainF1=0.7471 valF1=0.7702', '\\n',\n'Emb 128 | Epoch 4: trainF1=0.7512 valF1=0.7387', '\\n', 'Emb 128 | Epoch 5:\ntrainF1=0.7485 valF1=0.7636', '\\n', '\\n------ Training with emb_dim=256 ------',\n'\\n', 'Emb 256 | Epoch 1: trainF1=0.7347 valF1=0.7602', '\\n', 'Emb 256 | Epoch\n2: trainF1=0.7496 valF1=0.7587', '\\n', 'Emb 256 | Epoch 3: trainF1=0.7483\nvalF1=0.7600', '\\n', 'Emb 256 | Epoch 4: trainF1=0.7505 valF1=0.7552', '\\n',\n'Emb 256 | Epoch 5: trainF1=0.7492 valF1=0.7537', '\\n', 'Best run idx 1 | Dev\nSWA: 0.7478 | CWA: 0.7422', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_18-22-\n30_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 378853.32\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 489634.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 564235.90\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"{'train': 20000, 'dev': 5000,\n'test': 10000}\", '\\n', '\\n===== Training with train batch_size=32 =====', '\\n',\n'Epoch 1/5 | val_loss=0.5218  val_F1=0.7424', '\\n', 'Epoch 2/5 | val_loss=0.5227\nval_F1=0.7369', '\\n', 'Epoch 3/5 | val_loss=0.5224  val_F1=0.7541', '\\n', 'Epoch\n4/5 | val_loss=0.5228  val_F1=0.7534', '\\n', 'Epoch 5/5 | val_loss=0.5220\nval_F1=0.7440', '\\n', 'Final Dev SWA=0.7404  CWA=0.7355', '\\n', '\\n=====\nTraining with train batch_size=64 =====', '\\n', 'Epoch 1/5 | val_loss=0.5316\nval_F1=0.7498', '\\n', 'Epoch 2/5 | val_loss=0.5231  val_F1=0.7426', '\\n', 'Epoch\n3/5 | val_loss=0.5229  val_F1=0.7671', '\\n', 'Epoch 4/5 | val_loss=0.5236\nval_F1=0.7649', '\\n', 'Epoch 5/5 | val_loss=0.5214  val_F1=0.7510', '\\n', 'Final\nDev SWA=0.7480  CWA=0.7429', '\\n', '\\n===== Training with train batch_size=128\n=====', '\\n', 'Epoch 1/5 | val_loss=0.5491  val_F1=0.7459', '\\n', 'Epoch 2/5 |\nval_loss=0.5255  val_F1=0.7564', '\\n', 'Epoch 3/5 | val_loss=0.5220\nval_F1=0.7468', '\\n', 'Epoch 4/5 | val_loss=0.5215  val_F1=0.7460', '\\n', 'Epoch\n5/5 | val_loss=0.5211  val_F1=0.7436', '\\n', 'Final Dev SWA=0.7399  CWA=0.7347',\n'\\n', '\\n===== Training with train batch_size=256 =====', '\\n', 'Epoch 1/5 |\nval_loss=0.5894  val_F1=0.7120', '\\n', 'Epoch 2/5 | val_loss=0.5429\nval_F1=0.7422', '\\n', 'Epoch 3/5 | val_loss=0.5278  val_F1=0.7476', '\\n', 'Epoch\n4/5 | val_loss=0.5232  val_F1=0.7510', '\\n', 'Epoch 5/5 | val_loss=0.5220\nval_F1=0.7558', '\\n', 'Final Dev SWA=0.7537  CWA=0.7493', '\\n', 'Saved all\nresults to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_18-22-\n30_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 616229.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 492150.57\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 502727.28\nexamples/s]', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train': 20000,\n'dev': 5000, 'test': 10000}\", '\\n', '\\n========== Training with dropout=0.0\n==========', '\\n', 'Dropout 0.0 | Epoch 1: val_loss = 0.5390, val_macroF1 =\n0.7443', '\\n', 'Dropout 0.0 | Epoch 2: val_loss = 0.5222, val_macroF1 = 0.7382',\n'\\n', 'Dropout 0.0 | Epoch 3: val_loss = 0.5212, val_macroF1 = 0.7386', '\\n',\n'Dropout 0.0 | Epoch 4: val_loss = 0.5218, val_macroF1 = 0.7633', '\\n', 'Dropout\n0.0 | Epoch 5: val_loss = 0.5213, val_macroF1 = 0.7470', '\\n', 'SWA: 0.7439 |\nCWA: 0.7391', '\\n', '\\n========== Training with dropout=0.1 ==========', '\\n',\n'Dropout 0.1 | Epoch 1: val_loss = 0.5514, val_macroF1 = 0.7450', '\\n', 'Dropout\n0.1 | Epoch 2: val_loss = 0.5248, val_macroF1 = 0.7467', '\\n', 'Dropout 0.1 |\nEpoch 3: val_loss = 0.5229, val_macroF1 = 0.7686', '\\n', 'Dropout 0.1 | Epoch 4:\nval_loss = 0.5222, val_macroF1 = 0.7644', '\\n', 'Dropout 0.1 | Epoch 5: val_loss\n= 0.5218, val_macroF1 = 0.7554', '\\n', 'SWA: 0.7523 | CWA: 0.7479', '\\n',\n'\\n========== Training with dropout=0.2 ==========', '\\n', 'Dropout 0.2 | Epoch\n1: val_loss = 0.5492, val_macroF1 = 0.7452', '\\n', 'Dropout 0.2 | Epoch 2:\nval_loss = 0.5256, val_macroF1 = 0.7546', '\\n', 'Dropout 0.2 | Epoch 3: val_loss\n= 0.5222, val_macroF1 = 0.7473', '\\n', 'Dropout 0.2 | Epoch 4: val_loss =\n0.5215, val_macroF1 = 0.7438', '\\n', 'Dropout 0.2 | Epoch 5: val_loss = 0.5211,\nval_macroF1 = 0.7412', '\\n', 'SWA: 0.7369 | CWA: 0.7323', '\\n', '\\n==========\nTraining with dropout=0.3 ==========', '\\n', 'Dropout 0.3 | Epoch 1: val_loss =\n0.5548, val_macroF1 = 0.7368', '\\n', 'Dropout 0.3 | Epoch 2: val_loss = 0.5280,\nval_macroF1 = 0.7490', '\\n', 'Dropout 0.3 | Epoch 3: val_loss = 0.5228,\nval_macroF1 = 0.7447', '\\n', 'Dropout 0.3 | Epoch 4: val_loss = 0.5218,\nval_macroF1 = 0.7481', '\\n', 'Dropout 0.3 | Epoch 5: val_loss = 0.5218,\nval_macroF1 = 0.7509', '\\n', 'SWA: 0.7485 | CWA: 0.7438', '\\n', '\\n==========\nTraining with dropout=0.4 ==========', '\\n', 'Dropout 0.4 | Epoch 1: val_loss =\n0.5495, val_macroF1 = 0.7352', '\\n', 'Dropout 0.4 | Epoch 2: val_loss = 0.5279,\nval_macroF1 = 0.7483', '\\n', 'Dropout 0.4 | Epoch 3: val_loss = 0.5232,\nval_macroF1 = 0.7481', '\\n', 'Dropout 0.4 | Epoch 4: val_loss = 0.5228,\nval_macroF1 = 0.7434', '\\n', 'Dropout 0.4 | Epoch 5: val_loss = 0.5229,\nval_macroF1 = 0.7543', '\\n', 'SWA: 0.7535 | CWA: 0.7475', '\\n', '\\n==========\nTraining with dropout=0.5 ==========', '\\n', 'Dropout 0.5 | Epoch 1: val_loss =\n0.5553, val_macroF1 = 0.7420', '\\n', 'Dropout 0.5 | Epoch 2: val_loss = 0.5322,\nval_macroF1 = 0.7468', '\\n', 'Dropout 0.5 | Epoch 3: val_loss = 0.5256,\nval_macroF1 = 0.7519', '\\n', 'Dropout 0.5 | Epoch 4: val_loss = 0.5233,\nval_macroF1 = 0.7522', '\\n', 'Dropout 0.5 | Epoch 5: val_loss = 0.5235,\nval_macroF1 = 0.7375', '\\n', 'SWA: 0.7314 | CWA: 0.7280', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n15_18-22-30_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-6/working/experiment_data.npy', '\\n', 'Execution time:\n57 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train':\n20000, 'dev': 5000, 'test': 10000}\", '\\n', '\\n=== Training with weight_decay=0.0\n===', '\\n', 'Epoch 1: wd=0.0 | val_loss=0.5420 | val_MacroF1=0.7533', '\\n',\n'Epoch 2: wd=0.0 | val_loss=0.5229 | val_MacroF1=0.7558', '\\n', 'Epoch 3: wd=0.0\n| val_loss=0.5213 | val_MacroF1=0.7592', '\\n', 'Epoch 4: wd=0.0 |\nval_loss=0.5226 | val_MacroF1=0.7715', '\\n', 'Epoch 5: wd=0.0 | val_loss=0.5209\n| val_MacroF1=0.7526', '\\n', 'Final Dev SWA: 0.7493 | CWA: 0.7455', '\\n', '\\n===\nTraining with weight_decay=1e-05 ===', '\\n', 'Epoch 1: wd=1e-05 |\nval_loss=0.5427 | val_MacroF1=0.7441', '\\n', 'Epoch 2: wd=1e-05 |\nval_loss=0.5235 | val_MacroF1=0.7389', '\\n', 'Epoch 3: wd=1e-05 |\nval_loss=0.5213 | val_MacroF1=0.7583', '\\n', 'Epoch 4: wd=1e-05 |\nval_loss=0.5214 | val_MacroF1=0.7458', '\\n', 'Epoch 5: wd=1e-05 |\nval_loss=0.5219 | val_MacroF1=0.7600', '\\n', 'Final Dev SWA: 0.7595 | CWA:\n0.7549', '\\n', '\\n=== Training with weight_decay=0.0001 ===', '\\n', 'Epoch 1:\nwd=0.0001 | val_loss=0.5432 | val_MacroF1=0.7497', '\\n', 'Epoch 2: wd=0.0001 |\nval_loss=0.5231 | val_MacroF1=0.7530', '\\n', 'Epoch 3: wd=0.0001 |\nval_loss=0.5217 | val_MacroF1=0.7518', '\\n', 'Epoch 4: wd=0.0001 |\nval_loss=0.5219 | val_MacroF1=0.7623', '\\n', 'Epoch 5: wd=0.0001 |\nval_loss=0.5216 | val_MacroF1=0.7478', '\\n', 'Final Dev SWA: 0.7442 | CWA:\n0.7395', '\\n', '\\n=== Training with weight_decay=0.001 ===', '\\n', 'Epoch 1:\nwd=0.001 | val_loss=0.5567 | val_MacroF1=0.7314', '\\n', 'Epoch 2: wd=0.001 |\nval_loss=0.5302 | val_MacroF1=0.7420', '\\n', 'Epoch 3: wd=0.001 |\nval_loss=0.5246 | val_MacroF1=0.7478', '\\n', 'Epoch 4: wd=0.001 |\nval_loss=0.5226 | val_MacroF1=0.7391', '\\n', 'Epoch 5: wd=0.001 |\nval_loss=0.5224 | val_MacroF1=0.7403', '\\n', 'Final Dev SWA: 0.7367 | CWA:\n0.7316', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-15_18-22-\n30_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Dataset sizes:', ' ', \"{'train': 20000, 'dev':\n5000, 'test': 10000}\", '\\n', 'Epoch 01 | train_loss=0.6527 val_loss=0.5935 |\nval_F1=0.7222 SWA=0.7214 CWA=0.7183 SC-G=0.7198', '\\n', 'Epoch 02 |\ntrain_loss=0.5743 val_loss=0.5481 | val_F1=0.7492 SWA=0.7444 CWA=0.7417\nSC-G=0.7431', '\\n', 'Epoch 03 | train_loss=0.5459 val_loss=0.5311 |\nval_F1=0.7534 SWA=0.7509 CWA=0.7461 SC-G=0.7485', '\\n', 'Epoch 04 |\ntrain_loss=0.5371 val_loss=0.5249 | val_F1=0.7544 SWA=0.7524 CWA=0.7472\nSC-G=0.7498', '\\n', 'Epoch 05 | train_loss=0.5365 val_loss=0.5226 |\nval_F1=0.7482 SWA=0.7458 CWA=0.7397 SC-G=0.7428', '\\n', 'Epoch 06 |\ntrain_loss=0.5313 val_loss=0.5216 | val_F1=0.7442 SWA=0.7407 CWA=0.7353\nSC-G=0.7380', '\\n', 'Epoch 07 | train_loss=0.5320 val_loss=0.5214 |\nval_F1=0.7473 SWA=0.7442 CWA=0.7388 SC-G=0.7415', '\\n', 'Epoch 08 |\ntrain_loss=0.5307 val_loss=0.5213 | val_F1=0.7470 SWA=0.7440 CWA=0.7388\nSC-G=0.7414', '\\n', 'Early stopping (no SC-Gmean improvement).', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-15_18-22-\n30_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Dataset sizes:', ' ', \"{'train': 20000, 'dev':\n5000, 'test': 10000}\", '\\n', 'Epoch 01 | val_loss=0.5217 | macroF1=0.7352 |\nSWA=0.7300 | CWA=0.7259 | SC-Gmean=0.7280', '\\n', 'Epoch 02 | val_loss=0.5229 |\nmacroF1=0.7597 | SWA=0.7599 | CWA=0.7546 | SC-Gmean=0.7573', '\\n', 'Epoch 03 |\nval_loss=0.5213 | macroF1=0.7436 | SWA=0.7391 | CWA=0.7347 | SC-Gmean=0.7369',\n'\\n', 'Epoch 04 | val_loss=0.5215 | macroF1=0.7412 | SWA=0.7359 | CWA=0.7322 |\nSC-Gmean=0.7340', '\\n', 'Epoch 05 | val_loss=0.5217 | macroF1=0.7531 |\nSWA=0.7518 | CWA=0.7471 | SC-Gmean=0.7494', '\\n', 'Epoch 06 | val_loss=0.5229 |\nmacroF1=0.7569 | SWA=0.7546 | CWA=0.7508 | SC-Gmean=0.7527', '\\n', 'Early\nstopping triggered.', '\\n', 'Best Dev SC-Gmean: 0.7573', '\\n', 'Experiment data\nsaved to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_18-22-\n30_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train':\n20000, 'dev': 5000, 'test': 10000}\", '\\n', '\\n========== Training with\ndropout=0.0 ==========', '\\n', 'Dropout 0.0 | Epoch 1: val_loss = 0.5377,\nval_macroF1 = 0.7523', '\\n', 'Dropout 0.0 | Epoch 2: val_loss = 0.5220,\nval_macroF1 = 0.7493', '\\n', 'Dropout 0.0 | Epoch 3: val_loss = 0.5214,\nval_macroF1 = 0.7486', '\\n', 'Dropout 0.0 | Epoch 4: val_loss = 0.5215,\nval_macroF1 = 0.7428', '\\n', 'Dropout 0.0 | Epoch 5: val_loss = 0.5214,\nval_macroF1 = 0.7580', '\\n', 'SWA: 0.7567 | CWA: 0.7516', '\\n', '\\n==========\nTraining with dropout=0.1 ==========', '\\n', 'Dropout 0.1 | Epoch 1: val_loss =\n0.5324, val_macroF1 = 0.7500', '\\n', 'Dropout 0.1 | Epoch 2: val_loss = 0.5224,\nval_macroF1 = 0.7425', '\\n', 'Dropout 0.1 | Epoch 3: val_loss = 0.5209,\nval_macroF1 = 0.7452', '\\n', 'Dropout 0.1 | Epoch 4: val_loss = 0.5208,\nval_macroF1 = 0.7422', '\\n', 'Dropout 0.1 | Epoch 5: val_loss = 0.5210,\nval_macroF1 = 0.7474', '\\n', 'SWA: 0.7443 | CWA: 0.7389', '\\n', '\\n==========\nTraining with dropout=0.2 ==========', '\\n', 'Dropout 0.2 | Epoch 1: val_loss =\n0.5353, val_macroF1 = 0.7481', '\\n', 'Dropout 0.2 | Epoch 2: val_loss = 0.5237,\nval_macroF1 = 0.7616', '\\n', 'Dropout 0.2 | Epoch 3: val_loss = 0.5216,\nval_macroF1 = 0.7440', '\\n', 'Dropout 0.2 | Epoch 4: val_loss = 0.5211,\nval_macroF1 = 0.7521', '\\n', 'Dropout 0.2 | Epoch 5: val_loss = 0.5211,\nval_macroF1 = 0.7511', '\\n', 'SWA: 0.7492 | CWA: 0.7432', '\\n', '\\n==========\nTraining with dropout=0.3 ==========', '\\n', 'Dropout 0.3 | Epoch 1: val_loss =\n0.5449, val_macroF1 = 0.7515', '\\n', 'Dropout 0.3 | Epoch 2: val_loss = 0.5248,\nval_macroF1 = 0.7475', '\\n', 'Dropout 0.3 | Epoch 3: val_loss = 0.5218,\nval_macroF1 = 0.7565', '\\n', 'Dropout 0.3 | Epoch 4: val_loss = 0.5214,\nval_macroF1 = 0.7606', '\\n', 'Dropout 0.3 | Epoch 5: val_loss = 0.5216,\nval_macroF1 = 0.7534', '\\n', 'SWA: 0.7518 | CWA: 0.7455', '\\n', '\\n==========\nTraining with dropout=0.4 ==========', '\\n', 'Dropout 0.4 | Epoch 1: val_loss =\n0.5452, val_macroF1 = 0.7368', '\\n', 'Dropout 0.4 | Epoch 2: val_loss = 0.5267,\nval_macroF1 = 0.7422', '\\n', 'Dropout 0.4 | Epoch 3: val_loss = 0.5230,\nval_macroF1 = 0.7479', '\\n', 'Dropout 0.4 | Epoch 4: val_loss = 0.5226,\nval_macroF1 = 0.7476', '\\n', 'Dropout 0.4 | Epoch 5: val_loss = 0.5220,\nval_macroF1 = 0.7493', '\\n', 'SWA: 0.7474 | CWA: 0.7417', '\\n', '\\n==========\nTraining with dropout=0.5 ==========', '\\n', 'Dropout 0.5 | Epoch 1: val_loss =\n0.5501, val_macroF1 = 0.7425', '\\n', 'Dropout 0.5 | Epoch 2: val_loss = 0.5303,\nval_macroF1 = 0.7491', '\\n', 'Dropout 0.5 | Epoch 3: val_loss = 0.5265,\nval_macroF1 = 0.7422', '\\n', 'Dropout 0.5 | Epoch 4: val_loss = 0.5247,\nval_macroF1 = 0.7495', '\\n', 'Dropout 0.5 | Epoch 5: val_loss = 0.5242,\nval_macroF1 = 0.7365', '\\n', 'SWA: 0.7307 | CWA: 0.7273', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n15_18-22-30_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time:\n57 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train':\n20000, 'dev': 5000, 'test': 10000}\", '\\n', '\\n========== Training with\ndropout=0.0 ==========', '\\n', 'Dropout 0.0 | Epoch 1: val_loss = 0.5377,\nval_macroF1 = 0.7523', '\\n', 'Dropout 0.0 | Epoch 2: val_loss = 0.5220,\nval_macroF1 = 0.7493', '\\n', 'Dropout 0.0 | Epoch 3: val_loss = 0.5214,\nval_macroF1 = 0.7486', '\\n', 'Dropout 0.0 | Epoch 4: val_loss = 0.5215,\nval_macroF1 = 0.7428', '\\n', 'Dropout 0.0 | Epoch 5: val_loss = 0.5214,\nval_macroF1 = 0.7580', '\\n', 'SWA: 0.7567 | CWA: 0.7516', '\\n', '\\n==========\nTraining with dropout=0.1 ==========', '\\n', 'Dropout 0.1 | Epoch 1: val_loss =\n0.5324, val_macroF1 = 0.7500', '\\n', 'Dropout 0.1 | Epoch 2: val_loss = 0.5224,\nval_macroF1 = 0.7425', '\\n', 'Dropout 0.1 | Epoch 3: val_loss = 0.5209,\nval_macroF1 = 0.7452', '\\n', 'Dropout 0.1 | Epoch 4: val_loss = 0.5208,\nval_macroF1 = 0.7422', '\\n', 'Dropout 0.1 | Epoch 5: val_loss = 0.5210,\nval_macroF1 = 0.7474', '\\n', 'SWA: 0.7443 | CWA: 0.7389', '\\n', '\\n==========\nTraining with dropout=0.2 ==========', '\\n', 'Dropout 0.2 | Epoch 1: val_loss =\n0.5353, val_macroF1 = 0.7481', '\\n', 'Dropout 0.2 | Epoch 2: val_loss = 0.5237,\nval_macroF1 = 0.7616', '\\n', 'Dropout 0.2 | Epoch 3: val_loss = 0.5216,\nval_macroF1 = 0.7440', '\\n', 'Dropout 0.2 | Epoch 4: val_loss = 0.5211,\nval_macroF1 = 0.7521', '\\n', 'Dropout 0.2 | Epoch 5: val_loss = 0.5211,\nval_macroF1 = 0.7511', '\\n', 'SWA: 0.7492 | CWA: 0.7432', '\\n', '\\n==========\nTraining with dropout=0.3 ==========', '\\n', 'Dropout 0.3 | Epoch 1: val_loss =\n0.5449, val_macroF1 = 0.7515', '\\n', 'Dropout 0.3 | Epoch 2: val_loss = 0.5248,\nval_macroF1 = 0.7475', '\\n', 'Dropout 0.3 | Epoch 3: val_loss = 0.5218,\nval_macroF1 = 0.7565', '\\n', 'Dropout 0.3 | Epoch 4: val_loss = 0.5214,\nval_macroF1 = 0.7606', '\\n', 'Dropout 0.3 | Epoch 5: val_loss = 0.5216,\nval_macroF1 = 0.7534', '\\n', 'SWA: 0.7518 | CWA: 0.7455', '\\n', '\\n==========\nTraining with dropout=0.4 ==========', '\\n', 'Dropout 0.4 | Epoch 1: val_loss =\n0.5452, val_macroF1 = 0.7368', '\\n', 'Dropout 0.4 | Epoch 2: val_loss = 0.5267,\nval_macroF1 = 0.7422', '\\n', 'Dropout 0.4 | Epoch 3: val_loss = 0.5230,\nval_macroF1 = 0.7479', '\\n', 'Dropout 0.4 | Epoch 4: val_loss = 0.5226,\nval_macroF1 = 0.7476', '\\n', 'Dropout 0.4 | Epoch 5: val_loss = 0.5220,\nval_macroF1 = 0.7493', '\\n', 'SWA: 0.7474 | CWA: 0.7417', '\\n', '\\n==========\nTraining with dropout=0.5 ==========', '\\n', 'Dropout 0.5 | Epoch 1: val_loss =\n0.5501, val_macroF1 = 0.7425', '\\n', 'Dropout 0.5 | Epoch 2: val_loss = 0.5303,\nval_macroF1 = 0.7491', '\\n', 'Dropout 0.5 | Epoch 3: val_loss = 0.5265,\nval_macroF1 = 0.7422', '\\n', 'Dropout 0.5 | Epoch 4: val_loss = 0.5247,\nval_macroF1 = 0.7495', '\\n', 'Dropout 0.5 | Epoch 5: val_loss = 0.5242,\nval_macroF1 = 0.7365', '\\n', 'SWA: 0.7307 | CWA: 0.7273', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n15_18-22-30_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n17 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH dataset at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH with sizes:', ' ', \"{'train':\n20000, 'dev': 5000, 'test': 10000}\", '\\n', '\\n========== Training with\ndropout=0.0 ==========', '\\n', 'Dropout 0.0 | Epoch 1: val_loss = 0.5331,\nval_macroF1 = 0.7451', '\\n', 'Dropout 0.0 | Epoch 2: val_loss = 0.5217,\nval_macroF1 = 0.7514', '\\n', 'Dropout 0.0 | Epoch 3: val_loss = 0.5208,\nval_macroF1 = 0.7468', '\\n', 'Dropout 0.0 | Epoch 4: val_loss = 0.5207,\nval_macroF1 = 0.7513', '\\n', 'Dropout 0.0 | Epoch 5: val_loss = 0.5209,\nval_macroF1 = 0.7504', '\\n', 'SWA: 0.7494 | CWA: 0.7425', '\\n', '\\n==========\nTraining with dropout=0.1 ==========', '\\n', 'Dropout 0.1 | Epoch 1: val_loss =\n0.5344, val_macroF1 = 0.7386', '\\n', 'Dropout 0.1 | Epoch 2: val_loss = 0.5237,\nval_macroF1 = 0.7399', '\\n', 'Dropout 0.1 | Epoch 3: val_loss = 0.5214,\nval_macroF1 = 0.7445', '\\n', 'Dropout 0.1 | Epoch 4: val_loss = 0.5209,\nval_macroF1 = 0.7404', '\\n', 'Dropout 0.1 | Epoch 5: val_loss = 0.5213,\nval_macroF1 = 0.7400', '\\n', 'SWA: 0.7347 | CWA: 0.7314', '\\n', '\\n==========\nTraining with dropout=0.2 ==========', '\\n', 'Dropout 0.2 | Epoch 1: val_loss =\n0.5400, val_macroF1 = 0.7598', '\\n', 'Dropout 0.2 | Epoch 2: val_loss = 0.5231,\nval_macroF1 = 0.7424', '\\n', 'Dropout 0.2 | Epoch 3: val_loss = 0.5210,\nval_macroF1 = 0.7507', '\\n', 'Dropout 0.2 | Epoch 4: val_loss = 0.5207,\nval_macroF1 = 0.7442', '\\n', 'Dropout 0.2 | Epoch 5: val_loss = 0.5208,\nval_macroF1 = 0.7557', '\\n', 'SWA: 0.7542 | CWA: 0.7493', '\\n', '\\n==========\nTraining with dropout=0.3 ==========', '\\n', 'Dropout 0.3 | Epoch 1: val_loss =\n0.5493, val_macroF1 = 0.7470', '\\n', 'Dropout 0.3 | Epoch 2: val_loss = 0.5249,\nval_macroF1 = 0.7529', '\\n', 'Dropout 0.3 | Epoch 3: val_loss = 0.5223,\nval_macroF1 = 0.7428', '\\n', 'Dropout 0.3 | Epoch 4: val_loss = 0.5222,\nval_macroF1 = 0.7428', '\\n', 'Dropout 0.3 | Epoch 5: val_loss = 0.5220,\nval_macroF1 = 0.7551', '\\n', 'SWA: 0.7537 | CWA: 0.7485', '\\n', '\\n==========\nTraining with dropout=0.4 ==========', '\\n', 'Dropout 0.4 | Epoch 1: val_loss =\n0.5478, val_macroF1 = 0.7380', '\\n', 'Dropout 0.4 | Epoch 2: val_loss = 0.5278,\nval_macroF1 = 0.7409', '\\n', 'Dropout 0.4 | Epoch 3: val_loss = 0.5232,\nval_macroF1 = 0.7336', '\\n', 'Dropout 0.4 | Epoch 4: val_loss = 0.5226,\nval_macroF1 = 0.7427', '\\n', 'Dropout 0.4 | Epoch 5: val_loss = 0.5229,\nval_macroF1 = 0.7482', '\\n', 'SWA: 0.7429 | CWA: 0.7412', '\\n', '\\n==========\nTraining with dropout=0.5 ==========', '\\n', 'Dropout 0.5 | Epoch 1: val_loss =\n0.5564, val_macroF1 = 0.7355', '\\n', 'Dropout 0.5 | Epoch 2: val_loss = 0.5337,\nval_macroF1 = 0.7433', '\\n', 'Dropout 0.5 | Epoch 3: val_loss = 0.5272,\nval_macroF1 = 0.7395', '\\n', 'Dropout 0.5 | Epoch 4: val_loss = 0.5247,\nval_macroF1 = 0.7526', '\\n', 'Dropout 0.5 | Epoch 5: val_loss = 0.5244,\nval_macroF1 = 0.7486', '\\n', 'SWA: 0.7465 | CWA: 0.7417', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n15_18-22-30_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time:\n15 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "The execution failed due to a missing SPR_BENCH dataset. The script attempts to\nlocate the dataset using the resolve_spr_path function, but it cannot find the\nrequired CSV files (train.csv, dev.csv, test.csv) in the expected locations. The\nerror message suggests either placing the dataset in a proper location or\nsetting the SPR_BENCH_PATH environment variable.  To fix this issue: 1. Ensure\nthe SPR_BENCH dataset is available and contains the required files (train.csv,\ndev.csv, test.csv). 2. Place the dataset in one of the default search paths\n(e.g., the current working directory or its parent) or provide the full path to\nthe dataset by setting the SPR_BENCH_PATH environment variable. 3. Verify that\nthe dataset files are correctly formatted and accessible.", "", "The execution completed successfully without any bugs. The training was\nconducted for embedding dimensions of 32, 64, 128, and 256, each for 5 epochs.\nThe logs show steady improvements in trainF1 and valF1 scores for each embedding\ndimension. Additionally, the best validation run achieved a Shape-Weighted\nAccuracy (SWA) of 0.7478 and a Color-Weighted Accuracy (CWA) of 0.7422. The\nexperiment data was saved successfully. No issues were observed in the code or\nexecution output.", "The execution of the training script was successful without any bugs. The model\nwas trained with different batch sizes (32, 64, 128, 256) for 5 epochs each, and\nthe validation metrics (val_loss and val_F1) were reported for each epoch. The\nfinal Shape-Weighted Accuracy (SWA) and Color-Weighted Accuracy (CWA) were also\ncalculated for the validation set for each batch size. The results were saved\nsuccessfully to the specified file. The script performed as intended, and there\nwere no errors or issues during execution.", "", "", "", "", "", "The training script executed successfully without any errors or bugs. The model\nwas trained with various dropout rates, and metrics such as validation loss,\nmacro F1-score, SWA, and CWA were calculated for each configuration. The results\nwere saved correctly. No issues were identified in the execution output.", "The execution was successful, with no bugs or errors observed in the output. The\ntraining script ran for various dropout rates, and the results were saved\nsuccessfully. The metrics like val_loss, val_macroF1, SWA, and CWA were computed\nand displayed as expected. The experiment data was saved to the specified\ndirectory without any issues.", ""], "exc_type": [null, "FileNotFoundError", null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, {"args": ["Place SPR_BENCH csvs or set SPR_BENCH_PATH."]}, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 61, "<module>", "spr_root = resolve_spr_path()"], ["runfile.py", 44, "resolve_spr_path", "raise FileNotFoundError(\"Place SPR_BENCH csvs or set SPR_BENCH_PATH.\")"]], null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train macro-F1", "lower_is_better": false, "description": "Macro-F1 score for the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7482, "best_value": 0.7482}]}, {"metric_name": "validation macro-F1", "lower_is_better": false, "description": "Macro-F1 score for the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7582, "best_value": 0.7582}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value for the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.527077, "best_value": 0.527077}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521389, "best_value": 0.521389}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score on the training dataset, representing the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7487, "best_value": 0.7487}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score on the validation dataset, representing the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.767, "best_value": 0.767}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value on the training dataset, indicating the model's error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.526526, "best_value": 0.526526}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation dataset, indicating the model's error on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521492, "best_value": 0.521492}]}, {"metric_name": "shape weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy score considering the shape attribute.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7511, "best_value": 0.7511}]}, {"metric_name": "color weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy score considering the color attribute.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.746, "best_value": 0.746}]}]}, {"metric_names": [{"metric_name": "training macro F1", "lower_is_better": false, "description": "Macro F1 score during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7512, "best_value": 0.7512}]}, {"metric_name": "validation macro F1", "lower_is_better": false, "description": "Macro F1 score during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7702, "best_value": 0.7702}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5262, "best_value": 0.5262}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521, "best_value": 0.521}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score achieved during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.748, "best_value": 0.748}, {"dataset_name": "SPR_BENCH", "final_value": 0.7499, "best_value": 0.7499}, {"dataset_name": "SPR_BENCH", "final_value": 0.7476, "best_value": 0.7476}, {"dataset_name": "SPR_BENCH", "final_value": 0.7419, "best_value": 0.7419}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score achieved during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7541, "best_value": 0.7541}, {"dataset_name": "SPR_BENCH", "final_value": 0.7671, "best_value": 0.7671}, {"dataset_name": "SPR_BENCH", "final_value": 0.7564, "best_value": 0.7564}, {"dataset_name": "SPR_BENCH", "final_value": 0.7558, "best_value": 0.7558}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5262, "best_value": 0.5262}, {"dataset_name": "SPR_BENCH", "final_value": 0.5286, "best_value": 0.5286}, {"dataset_name": "SPR_BENCH", "final_value": 0.5302, "best_value": 0.5302}, {"dataset_name": "SPR_BENCH", "final_value": 0.5321, "best_value": 0.5321}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5218, "best_value": 0.5218}, {"dataset_name": "SPR_BENCH", "final_value": 0.5214, "best_value": 0.5214}, {"dataset_name": "SPR_BENCH", "final_value": 0.5211, "best_value": 0.5211}, {"dataset_name": "SPR_BENCH", "final_value": 0.522, "best_value": 0.522}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by shape categories", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7404, "best_value": 0.7404}, {"dataset_name": "SPR_BENCH", "final_value": 0.748, "best_value": 0.748}, {"dataset_name": "SPR_BENCH", "final_value": 0.7399, "best_value": 0.7399}, {"dataset_name": "SPR_BENCH", "final_value": 0.7537, "best_value": 0.7537}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by color categories", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7355, "best_value": 0.7355}, {"dataset_name": "SPR_BENCH", "final_value": 0.7429, "best_value": 0.7429}, {"dataset_name": "SPR_BENCH", "final_value": 0.7347, "best_value": 0.7347}, {"dataset_name": "SPR_BENCH", "final_value": 0.7493, "best_value": 0.7493}]}]}, {"metric_names": [{"metric_name": "Macro F1 Score", "lower_is_better": false, "description": "Measures the harmonic mean of precision and recall. Higher values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7686, "best_value": 0.7686}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "Measures the error between predicted and actual values. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5211, "best_value": 0.5211}]}, {"metric_name": "Shape Weighted Accuracy", "lower_is_better": false, "description": "Measures the accuracy weighted by shape. Higher values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7535, "best_value": 0.7535}]}, {"metric_name": "Color Weighted Accuracy", "lower_is_better": false, "description": "Measures the accuracy weighted by color. Higher values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7479, "best_value": 0.7479}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7486, "best_value": 0.7486}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.76, "best_value": 0.76}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5281, "best_value": 0.5281}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5209, "best_value": 0.5209}]}]}, {"metric_names": [{"metric_name": "train macro F1", "lower_is_better": false, "description": "Macro F1 score during training, indicating balanced performance across classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.746244, "best_value": 0.746244}]}, {"metric_name": "validation macro F1", "lower_is_better": false, "description": "Macro F1 score during validation, indicating balanced performance across classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.747049, "best_value": 0.747049}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "SWA (Stochastic Weight Averaging) score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.744041, "best_value": 0.744041}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "CWA (Class Weight Averaging) score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.73882, "best_value": 0.73882}]}, {"metric_name": "validation SC Gmean", "lower_is_better": false, "description": "Geometric mean of SC (Sub-Class) scores during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.741426, "best_value": 0.741426}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss function value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.530711, "best_value": 0.530711}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss function value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521258, "best_value": 0.521258}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5282, "best_value": 0.5282}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5213, "best_value": 0.5213}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7488, "best_value": 0.7488}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7597, "best_value": 0.7597}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The SWA (Simple Weighted Average) score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7599, "best_value": 0.7599}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The CWA (Complex Weighted Average) score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7546, "best_value": 0.7546}]}, {"metric_name": "validation SC Gmean", "lower_is_better": false, "description": "The SC Gmean (Geometric Mean) score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7573, "best_value": 0.7573}]}]}, {"metric_names": [{"metric_name": "Training Macro F1 Score", "lower_is_better": false, "description": "Measures the balance between precision and recall during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7552, "best_value": 0.7552}]}, {"metric_name": "Validation Macro F1 Score", "lower_is_better": false, "description": "Measures the balance between precision and recall during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7616, "best_value": 0.7616}]}, {"metric_name": "Training Loss", "lower_is_better": true, "description": "Represents the error during training. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5199, "best_value": 0.5199}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Represents the error during validation. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5208, "best_value": 0.5208}]}, {"metric_name": "Shape Weighted Accuracy", "lower_is_better": false, "description": "Accuracy metric weighted by shape.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7567, "best_value": 0.7567}]}, {"metric_name": "Color Weighted Accuracy", "lower_is_better": false, "description": "Accuracy metric weighted by color.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7516, "best_value": 0.7516}]}]}, {"metric_names": [{"metric_name": "Training Macro F1 Score", "lower_is_better": false, "description": "The macro F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7552, "best_value": 0.7552}]}, {"metric_name": "Validation Macro F1 Score", "lower_is_better": false, "description": "The macro F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7616, "best_value": 0.7616}]}, {"metric_name": "Training Loss", "lower_is_better": true, "description": "The loss value achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5199, "best_value": 0.5199}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "The loss value achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5208, "best_value": 0.5208}]}, {"metric_name": "Shape Weighted Accuracy", "lower_is_better": false, "description": "The weighted accuracy for shape predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7567, "best_value": 0.7567}]}, {"metric_name": "Color Weighted Accuracy", "lower_is_better": false, "description": "The weighted accuracy for color predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7516, "best_value": 0.7516}]}]}, {"metric_names": [{"metric_name": "Training Macro F1 Score", "lower_is_better": false, "description": "Measures the F1 score considering the harmonic mean of precision and recall during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7547, "best_value": 0.7547}]}, {"metric_name": "Validation Macro F1 Score", "lower_is_better": false, "description": "Measures the F1 score considering the harmonic mean of precision and recall during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7598, "best_value": 0.7598}]}, {"metric_name": "Training Loss", "lower_is_better": true, "description": "Measures the loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5197, "best_value": 0.5197}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Measures the loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5207, "best_value": 0.5207}]}, {"metric_name": "Shape Weighted Accuracy", "lower_is_better": false, "description": "Measures the weighted accuracy focusing on shape features.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7542, "best_value": 0.7542}]}, {"metric_name": "Color Weighted Accuracy", "lower_is_better": false, "description": "Measures the weighted accuracy focusing on color features.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7493, "best_value": 0.7493}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, true, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_class_distribution.png"], [], ["../../logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_weighted_accuracies.png"], ["../../logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_valF1_vs_embdim.png"], ["../../logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_final_val_macroF1.png", "../../logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_SWA_CWA.png"], ["../../logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_valF1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_SWA_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_CWA_vs_dropout.png"], ["../../logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_wd_vs_f1.png", "../../logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_SC_Gmean_curve.png", "../../logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_SWA_vs_CWA.png", "../../logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_valF1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_SWA_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_CWA_vs_dropout.png"], ["../../logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_valF1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_SWA_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_CWA_vs_dropout.png"], ["../../logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_valF1_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_SWA_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_CWA_vs_dropout.png"], ["../../logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_val_macroF1_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_val_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_final_valF1_vs_dropout.png", "../../logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_SWA_vs_dropout.png", "../../logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_CWA_vs_dropout.png"]], "plot_paths": [["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_loss_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_class_distribution.png"], [], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_weighted_accuracies.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_loss_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_valF1_vs_embdim.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_f1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_final_val_macroF1.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_SWA_CWA.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_valF1_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_SWA_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_CWA_vs_dropout.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_f1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_wd_vs_f1.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_f1_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_loss_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_SC_Gmean_curve.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_SWA_vs_CWA.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_valF1_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_SWA_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_CWA_vs_dropout.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_valF1_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_SWA_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_CWA_vs_dropout.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_valF1_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_SWA_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_CWA_vs_dropout.png"], ["experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_val_macroF1_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_val_loss_curves.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_final_valF1_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_SWA_vs_dropout.png", "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_e40a56216af64c9f92603ffe11f35cac/SPR_BENCH_agg_CWA_vs_dropout.png"]], "plot_analyses": [[{"analysis": "The plot shows the training and validation loss over five epochs. The training loss decreases steadily, indicating that the model is learning from the data. The validation loss also decreases initially but stabilizes after the second epoch, which suggests that the model's generalization capability is not deteriorating. The gap between training and validation loss is minimal, implying that the model is not overfitting at this stage.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot depicts the training and validation Macro-F1 scores over five epochs. Both metrics increase, showing that the model is improving in its ability to balance precision and recall across all classes. The validation Macro-F1 score surpasses the training score briefly, suggesting that the model performs well on unseen data. The stabilization towards the end indicates convergence.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The confusion matrix for the validation dataset shows that the model performs well, with a high number of correct predictions for both classes. However, there is still room for improvement in reducing misclassifications, as indicated by the off-diagonal values.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The class distribution plot compares the ground truth and predicted class distributions for the validation dataset. The predictions closely match the ground truth distribution, which indicates that the model is not biased towards any particular class and maintains a balanced prediction performance.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3fe6abe405fa46548e223d920961636e_proc_2960006/SPR_BENCH_class_distribution.png"}], [], [{"analysis": "This plot illustrates the cross-entropy loss trends for different learning rates (0.0005, 0.001, 0.002) across training and validation datasets. The models with higher learning rates (0.001 and 0.002) converge faster compared to the one with a lower learning rate (0.0005). However, the validation loss for the learning rate of 0.002 flattens early, potentially indicating overfitting or insufficient generalization. The learning rate of 0.001 shows a balance between convergence speed and generalization, with the validation loss closely tracking the training loss.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the Macro-F1 score progression over epochs for training and validation datasets at different learning rates. The learning rate of 0.002 achieves the highest Macro-F1 score for validation early on but starts to decline, suggesting potential overfitting. The learning rate of 0.001 achieves a stable and high Macro-F1 score for both training and validation, indicating it might offer the best trade-off between training efficiency and model performance. The learning rate of 0.0005 shows slower improvement but consistent growth without overfitting, making it a safer choice for longer training.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_macroF1_curves.png"}, {"analysis": "This bar chart compares the Shape-Weighted Accuracy (SWA) and Color-Weighted Accuracy (CWA) at the final epoch for different learning rates. All three learning rates achieve similar performance in both metrics, with slight differences. The learning rate of 0.001 shows marginally better performance, aligning with its strong performance in the loss and Macro-F1 metrics. The results suggest that the choice of learning rate has minimal impact on the weighted accuracy metrics, but 0.001 provides a slight edge.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_09a0c77285314667981d52d9b6e31d1b_proc_2964456/SPR_BENCH_weighted_accuracies.png"}], [{"analysis": "The first plot shows the Macro-F1 scores for both training and validation over 20 epochs. The training Macro-F1 score improves steadily until around epoch 10, after which it stabilizes with minor fluctuations. The validation Macro-F1 score follows a similar trend but exhibits more variability, peaking around epoch 13 and then stabilizing slightly above the training score. This indicates that the model is learning effectively and generalizes reasonably well to the validation set, though the validation variability suggests room for improvement in stability.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The second plot illustrates the cross-entropy loss for training and validation over the same 20 epochs. The training loss decreases consistently, showing effective learning by the model. The validation loss also decreases but stabilizes earlier, with slight fluctuations. The convergence of both losses without significant divergence suggests that the model is not overfitting, though the fluctuations in validation loss could be addressed by fine-tuning hyperparameters or regularization techniques.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_loss_curve.png"}, {"analysis": "The third plot compares the final validation Macro-F1 scores across different embedding dimensions (32, 64, 128, and 256). The scores are nearly identical across all embedding sizes, indicating that the embedding dimension has minimal impact on the model's performance for this task. This suggests that the model's performance is more influenced by other factors, such as the training strategy or data quality, rather than the embedding size.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4ab90ec1d66546d2a1d9db02a5f015ca_proc_2964457/SPR_BENCH_valF1_vs_embdim.png"}], [{"analysis": "The training and validation loss curves indicate that the model converges well for all batch sizes, with the loss decreasing consistently over epochs. Notably, smaller batch sizes (32 and 64) exhibit slightly better validation loss compared to larger batch sizes (128 and 256), suggesting better generalization. However, the differences are not significant, and all configurations seem to stabilize around a similar loss value by the end of training.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that smaller batch sizes (32 and 64) achieve slightly higher validation Macro-F1 scores compared to larger batch sizes. The validation scores peak early and remain stable, indicating that the model achieves good performance without overfitting. Batch size 64 achieves the highest peak validation Macro-F1 score, suggesting it may be the most optimal choice for this experiment.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_f1_curves.png"}, {"analysis": "The final validation Macro-F1 scores across different batch sizes are almost identical, with only marginal differences. This indicates that batch size has minimal impact on the final performance, as the model achieves comparable results regardless of the batch size.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_final_val_macroF1.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) and Color-Weighted Accuracy (CWA) metrics are consistent across all batch sizes, with values slightly above 0.7. This consistency further supports the conclusion that batch size does not significantly affect the model's ability to capture shape and color variations in the sequences. Both metrics demonstrate robust performance, aligning with the Macro-F1 results.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d7cddcc8797414682c93b63e6ddcd8e_proc_2964458/SPR_BENCH_SWA_CWA.png"}], [{"analysis": "The Macro-F1 scores for both training and validation sets increase significantly in the initial epochs, indicating a strong learning phase. However, the validation performance stabilizes earlier than the training performance, suggesting a potential risk of overfitting, particularly at higher epochs. The dropout rates influence the trends slightly, but the overall difference across dropout rates is minimal.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The cross-entropy loss decreases rapidly for both training and validation sets in the initial epochs, demonstrating effective learning. The validation loss stabilizes earlier than the training loss, which is consistent with the observed stabilization of validation Macro-F1 scores. The training loss continues to decrease slightly, which could indicate overfitting at later epochs.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_loss_curves.png"}, {"analysis": "The final validation Macro-F1 scores are relatively stable across different dropout rates, with only marginal differences. This suggests that the model's performance is not significantly influenced by the dropout rate, indicating robustness to this hyperparameter.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_valF1_vs_dropout.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) shows variability across dropout rates, peaking at 0.1 and 0.4 but dropping significantly at 0.2 and 0.5. This indicates that dropout rates have a noticeable impact on SWA, with intermediate values (0.1 and 0.4) yielding better results.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_SWA_vs_dropout.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) also exhibits fluctuations with dropout rates, mirroring the trends observed in SWA. The performance peaks at dropout rates of 0.1 and 0.4, while it drops significantly at 0.2 and 0.5. This suggests that the choice of dropout rate is critical for optimizing CWA, and intermediate values are more favorable.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_631f0e13f6674972b88ea3d6dead0e16_proc_2964455/SPR_BENCH_CWA_vs_dropout.png"}], [{"analysis": "This plot shows the training and validation loss across epochs for different weight decay values. The training and validation losses decrease steadily as training progresses, indicating effective learning by the model. However, the weight decay parameter significantly affects the loss. Smaller weight decay values, such as 1e-05 and 0, result in lower validation losses, suggesting better generalization. Larger weight decays like 0.001 lead to higher validation losses, potentially due to excessive regularization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation Macro-F1 scores across epochs for different weight decay values. The Macro-F1 score generally improves with training, peaking around epoch 4 for most configurations. Weight decay of 1e-05 achieves the highest Macro-F1 score, indicating an optimal balance between regularization and model complexity. Higher weight decay values, like 0.001, result in lower scores, likely due to underfitting.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot compares the final validation Macro-F1 scores for different weight decay values. The optimal weight decay is 1e-05, achieving the highest score of 0.76. As weight decay increases, the Macro-F1 score decreases, confirming that excessive regularization negatively impacts model performance. The trend highlights the importance of fine-tuning weight decay for optimal performance.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_wd_vs_f1.png"}, {"analysis": "This confusion matrix evaluates the model's performance for a weight decay of 1e-05. The model correctly classifies a majority of samples in both classes, with 1743 true negatives and 2062 true positives. However, there are 757 false positives and 438 false negatives, indicating areas for improvement. The relatively higher number of true positives suggests the model is better at identifying positive samples than negative ones. Fine-tuning hyperparameters or incorporating additional data augmentation may help address these misclassifications.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6b086f4ccf0047be9dd04fb3b1f83bdb_proc_2964456/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the Macro-F1 score for training and validation over epochs. The training Macro-F1 increases consistently until it plateaus around epoch 5. The validation Macro-F1 initially increases and surpasses the training score, indicating effective generalization. However, after epoch 5, the validation score slightly declines, suggesting potential overfitting or a need for regularization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_f1_curve.png"}, {"analysis": "This plot tracks the cross-entropy loss for both training and validation. Both losses decrease steadily, indicating effective learning. However, the validation loss flattens after epoch 5, aligning with the observed decline in validation Macro-F1, which could indicate overfitting or a need for tuning learning rate schedules or early stopping criteria.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot evaluates Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and SC-Gmean. All metrics improve until epoch 4, where they peak, followed by a decline. This decline suggests overfitting or a need for enhanced data augmentation techniques or regularization to maintain generalization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_val_metrics.png"}, {"analysis": "The confusion matrix reveals the distribution of predictions versus actual labels. The diagonal dominance indicates the model performs well, but the lighter shades in off-diagonal regions suggest some misclassifications. These misclassifications could be addressed by improving feature representation or refining the contrastive learning framework.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e1449c6d5e9c4540b4614b8a57122469_proc_2964457/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that the training loss decreases significantly in the first epoch and then stabilizes, while the validation loss remains relatively stable throughout the training process. This suggests that the model quickly learns the basic patterns in the data but does not overfit, as the validation loss does not increase.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that the validation performance peaks at epoch 2 and then fluctuates slightly, while the training performance gradually improves. This indicates potential underfitting or that the model's generalization ability is not fully optimized yet. The peak at epoch 2 suggests that early stopping might improve performance.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The SC-Gmean plot highlights epoch 2 as the best-performing epoch, with the highest SC-Gmean value. This aligns with the Macro-F1 observation that validation performance peaks early, reinforcing the idea of early stopping to maximize generalization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_SC_Gmean_curve.png"}, {"analysis": "The SWA vs. CWA scatter plot shows a positive correlation between the two metrics across epochs, with improvements in one generally corresponding to improvements in the other. Epoch 6 achieves the best balance between SWA and CWA, suggesting consistent improvements in both shape and color recognition capabilities.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_SWA_vs_CWA.png"}, {"analysis": "The confusion matrix reveals imbalanced performance across classes, with certain classes being predicted more accurately than others. This could indicate a need for additional techniques to handle class imbalances or improve feature representation for underperforming classes.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_638cb9de136e49dea541d3c9b54946d9_proc_2964458/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The Macro-F1 scores for both training and validation exhibit an upward trend during the initial epochs, indicating that the model is learning effectively. However, validation Macro-F1 scores plateau and even fluctuate slightly as epochs progress, particularly for higher dropout rates. This suggests that the model's generalization ability is sensitive to the dropout rate, with lower dropout rates (d=0.0 to d=0.2) providing more stable validation performance. Higher dropout rates (d=0.4 and d=0.5) lead to more pronounced fluctuations, possibly due to excessive regularization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The cross-entropy loss decreases consistently for both training and validation over epochs, reflecting effective learning. However, the loss curves for higher dropout rates (d=0.4 and d=0.5) converge more slowly and remain higher compared to lower dropout rates. This indicates that higher dropout rates may hinder the model's ability to fit the data effectively, which aligns with the observed fluctuations in Macro-F1 scores.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_loss_curves.png"}, {"analysis": "The final validation Macro-F1 scores are relatively stable across different dropout rates, with only minor variations. This suggests that the choice of dropout rate has limited impact on the overall Macro-F1 performance at the end of training. However, this stability does not necessarily reflect the model's performance on other metrics like SWA and CWA.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_valF1_vs_dropout.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) shows a clear sensitivity to dropout rate. The highest SWA is achieved with no dropout (d=0.0), and performance decreases as dropout increases, particularly for d=0.5. This suggests that dropout negatively impacts the model's ability to capture shape-related features, which are critical for the SPR task.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_SWA_vs_dropout.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) follows a similar trend to SWA, with the best performance at d=0.0 and a significant drop at d=0.5. This indicates that higher dropout rates impair the model's ability to learn color-related features effectively, further emphasizing the importance of careful dropout tuning for the SPR task.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/SPR_BENCH_CWA_vs_dropout.png"}], [{"analysis": "The Macro-F1 vs Epochs plot indicates that the model performance improves steadily during the initial epochs for both training and validation sets. However, the validation performance shows some oscillation, particularly at higher dropout rates (e.g., d=0.4 and d=0.5), suggesting potential instability in generalization. Lower dropout rates (e.g., d=0.0 to d=0.2) yield more consistent and higher performance on the validation set, with diminishing returns after a certain point.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The Loss vs Epochs plot shows a clear downward trend in cross-entropy loss for both training and validation sets, indicating effective optimization. The validation loss stabilizes earlier compared to the training loss, implying that the model may start overfitting at later epochs. Dropout rates influence the loss curves, with higher dropout rates (e.g., d=0.5) leading to slower convergence and higher final losses, reflecting reduced model capacity and potential underfitting.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_loss_curves.png"}, {"analysis": "The Final Validation Macro-F1 vs Dropout plot demonstrates that the final Macro-F1 score is relatively stable across different dropout rates, with only minor variations. This suggests that the model's overall performance is not highly sensitive to the dropout rate in terms of Macro-F1.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_valF1_vs_dropout.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) vs Dropout plot shows a peak performance at a moderate dropout rate (d=0.3), with a sharp decline at higher dropout rates (d=0.4 and d=0.5). This indicates that while some regularization is beneficial, excessive dropout harms the model's ability to capture shape-related patterns effectively.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_SWA_vs_dropout.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) vs Dropout plot follows a similar trend to the SWA plot, with optimal performance at a moderate dropout rate (d=0.3) and a significant drop at higher dropout rates. This suggests that the model's ability to recognize color-related patterns is also negatively impacted by excessive regularization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/SPR_BENCH_CWA_vs_dropout.png"}], [{"analysis": "The Macro-F1 score increases for both training and validation datasets across epochs. There is a clear trend of improvement in validation performance, with the highest Macro-F1 observed for dropout rates between 0.2 and 0.3. However, higher dropout rates (0.4 and 0.5) lead to some stabilization but do not outperform the lower dropout configurations. The gap between training and validation performance is consistent, suggesting no significant overfitting.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The cross-entropy loss decreases steadily for both training and validation datasets over epochs, indicating effective learning. The validation loss stabilizes earlier compared to the training loss, which continues to decrease slightly. Dropout rates of 0.2 and 0.3 achieve the lowest validation loss, aligning with the Macro-F1 trends. High dropout rates (0.4 and 0.5) show slightly higher loss values, indicating reduced generalization.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_loss_curves.png"}, {"analysis": "The final validation Macro-F1 scores for different dropout rates are nearly identical, with a slight edge for dropout rates of 0.2 and 0.3. This suggests that the model is robust to variations in dropout rate, though 0.2 and 0.3 offer marginal gains.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_valF1_vs_dropout.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) is highest for dropout rates of 0.2 and 0.3, indicating that these configurations better capture the shape-related features in the data. Dropout rates of 0.1 and 0.4 show lower SWA, and the performance for 0.5 is slightly better than 0.4 but still suboptimal.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_SWA_vs_dropout.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) follows a pattern similar to SWA, with dropout rates of 0.2 and 0.3 showing the best performance. Dropout rates of 0.1 and 0.4 exhibit lower CWA, and the performance for 0.5 is slightly better than 0.4 but does not exceed the optimal rates.", "plot_path": "experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/SPR_BENCH_CWA_vs_dropout.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model is learning effectively without overfitting,\nas shown by the consistent decrease in loss and improvement in Macro-F1 scores.\nThe confusion matrix and class distribution suggest that the model achieves\nbalanced performance across classes, with minimal bias. These results are\npromising and suggest that the proposed approach is effective for the SPR task.", "[]", "The analysis highlights the impact of different learning rates on training\nconvergence, generalization, and model performance across multiple metrics. The\nlearning rate of 0.001 emerges as a balanced choice, achieving competitive\nresults in loss reduction, Macro-F1, and weighted accuracy metrics. The learning\nrate of 0.002 shows signs of overfitting, while 0.0005 offers slower but steady\nimprovement.", "The plots demonstrate that the model is learning effectively, with both training\nand validation metrics improving over epochs. However, there is variability in\nvalidation performance, and the embedding dimension does not significantly\nimpact the final results. Fine-tuning hyperparameters and exploring additional\nregularization techniques may help improve stability and performance.", "The plots collectively demonstrate that the model performs consistently well\nacross different batch sizes, with minimal impact on final metrics such as\nMacro-F1, SWA, and CWA. Smaller batch sizes (32 and 64) show slightly better\ngeneralization, but the differences are not substantial. Overall, the results\nindicate robust and stable performance of the proposed approach.", "The provided plots reveal trends in Macro-F1, cross-entropy loss, and weighted\naccuracies (SWA and CWA) with respect to epochs and dropout rates. The results\nindicate effective learning and robustness to dropout rates, although specific\ndropout values (0.1 and 0.4) optimize weighted accuracies. Overfitting risk is\nobserved at later epochs, and dropout rates influence SWA and CWA significantly.", "The plots provide valuable insights into the effects of weight decay on model\nperformance. Smaller weight decay values, particularly 1e-05, yield the best\nresults in terms of loss reduction and Macro-F1 scores. The confusion matrix\nhighlights the model's strengths and areas requiring improvement, such as\nreducing false positives and negatives. Further experimentation with\nregularization and data augmentation could enhance performance.", "The plots indicate effective initial learning, but validation performance\ndeclines after epoch 5, suggesting overfitting. Metrics like SWA and CWA peak\nearly and then degrade, highlighting the need for better regularization or data\naugmentation. The confusion matrix shows good performance but room for\nimprovement in reducing misclassifications.", "The plots collectively indicate that the model shows promising performance but\ncould benefit from early stopping and further optimization to address class\nimbalances and improve generalization. The SC-Gmean and SWA/CWA metrics suggest\nthat the model is effective in capturing symbolic patterns but may require fine-\ntuning to achieve state-of-the-art results.", "The results highlight the impact of dropout rates on model performance, with\nlower dropout rates generally yielding better results across all metrics. While\nMacro-F1 scores remain stable, SWA and CWA are more sensitive to dropout,\nsuggesting that these metrics should be prioritized when optimizing\nhyperparameters for the SPR task.", "The plots provide valuable insights into the impact of dropout rates on model\nperformance. While lower to moderate dropout rates improve generalization and\nperformance stability, excessive dropout leads to underfitting and reduced\naccuracy, particularly for shape- and color-weighted metrics. The training\nprocess is effective overall, with clear optimization trends and stable Macro-F1\nscores across different dropout rates.", "The analysis highlights that dropout rates of 0.2 and 0.3 consistently yield the\nbest performance across metrics (Macro-F1, SWA, and CWA). The learning curves\nindicate effective training with no significant overfitting. The model is robust\nto dropout variations, but fine-tuning around 0.2-0.3 can maximize performance.", "[]"], "exec_time": [5.12807822227478, 0.9928739070892334, 9.584902048110962, 11.525085210800171, 64.7935438156128, 57.280083417892456, 11.635790824890137, 6.366556644439697, 9.347697973251343, 57.62990093231201, 17.58901858329773, 15.683030366897583, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["<all_datasets_in_experiment_data>"], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ----------------- helper for confusion matrix -----------------\ndef confusion(y_true, y_pred, num_cls):\n    cm = np.zeros((num_cls, num_cls), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\n# ----------------- plot for every dataset -----------------\nfor dset, content in experiment_data.items():\n    # common handles\n    epochs = np.arange(1, len(content[\"losses\"][\"train\"]) + 1)\n    # 1) Loss curve -------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, content[\"losses\"][\"train\"], label=\"train\")\n        plt.plot(epochs, content[\"losses\"][\"val\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dset}: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dset}_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(\"Saved\", fname)\n    except Exception as e:\n        print(f\"Error creating loss curve for {dset}: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curve --------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, content[\"metrics\"][\"train_macroF1\"], label=\"train\")\n        plt.plot(epochs, content[\"metrics\"][\"val_macroF1\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dset}: Training vs Validation Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dset}_macroF1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(\"Saved\", fname)\n    except Exception as e:\n        print(f\"Error creating F1 curve for {dset}: {e}\")\n        plt.close()\n\n    # 3) Confusion matrix ------------------------------------------\n    try:\n        y_true = np.array(content[\"ground_truth\"])\n        y_pred = np.array(content[\"predictions\"])\n        num_cls = int(max(y_true.max(), y_pred.max()) + 1) if y_true.size else 0\n        if num_cls:\n            cm = confusion(y_true, y_pred, num_cls)\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dset}: Confusion Matrix (Validation)\")\n            fname = os.path.join(working_dir, f\"{dset}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(\"Saved\", fname)\n        else:\n            print(f\"No data for confusion matrix plot for {dset}\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # 4) Class count bar chart -------------------------------------\n    try:\n        if y_true.size:\n            labels = np.arange(num_cls)\n            true_counts = np.bincount(y_true, minlength=num_cls)\n            pred_counts = np.bincount(y_pred, minlength=num_cls)\n            x = np.arange(num_cls)\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, true_counts, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n            plt.xlabel(\"Class id\")\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dset}: Class Distribution (Validation)\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_class_distribution.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(\"Saved\", fname)\n        else:\n            print(f\"No data for class distribution plot for {dset}\")\n    except Exception as e:\n        print(f\"Error creating class distribution plot for {dset}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    edata = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    lr_dict = edata[\"learning_rate\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    lr_dict = {}\n\n# Collect basic info\nlr_keys = sorted(lr_dict.keys(), key=lambda x: float(x.split(\"_\")[1]))\nepochs = max(len(lr_dict[k][\"losses\"][\"train\"]) for k in lr_keys) if lr_keys else 0\n\n# --------- Plot 1: Loss curves ----------\ntry:\n    plt.figure()\n    for k in lr_keys:\n        lr_val = k.split(\"_\")[1]\n        train_loss = lr_dict[k][\"losses\"][\"train\"]\n        val_loss = lr_dict[k][\"losses\"][\"val\"]\n        ep = np.arange(1, len(train_loss) + 1)\n        plt.plot(ep, train_loss, label=f\"Train (lr={lr_val})\", linestyle=\"--\")\n        plt.plot(ep, val_loss, label=f\"Val   (lr={lr_val})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs. Validation Loss\")\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# --------- Plot 2: Macro-F1 curves ----------\ntry:\n    plt.figure()\n    for k in lr_keys:\n        lr_val = k.split(\"_\")[1]\n        tr_f1 = lr_dict[k][\"metrics\"][\"train_macroF1\"]\n        val_f1 = lr_dict[k][\"metrics\"][\"val_macroF1\"]\n        ep = np.arange(1, len(tr_f1) + 1)\n        plt.plot(ep, tr_f1, label=f\"Train (lr={lr_val})\", linestyle=\"--\")\n        plt.plot(ep, val_f1, label=f\"Val   (lr={lr_val})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs. Validation Macro-F1\")\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macroF1 plot: {e}\")\n    plt.close()\n\n# --------- Plot 3: SWA & CWA bar chart ----------\ntry:\n    ind = np.arange(len(lr_keys))\n    width = 0.35\n    swa_vals = [lr_dict[k][\"shape_weighted_acc\"] for k in lr_keys]\n    cwa_vals = [lr_dict[k][\"color_weighted_acc\"] for k in lr_keys]\n\n    plt.figure()\n    plt.bar(ind - width / 2, swa_vals, width, label=\"Shape-Weighted Acc\")\n    plt.bar(ind + width / 2, cwa_vals, width, label=\"Color-Weighted Acc\")\n    plt.xticks(ind, [k.split(\"_\")[1] for k in lr_keys])\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Weighted Accuracies at Final Epoch\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_weighted_accuracies.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating weighted accuracy plot: {e}\")\n    plt.close()\n\n# --------- Print summary metrics ----------\nfor k in lr_keys:\n    lr_val = k.split(\"_\")[1]\n    best_val_f1 = max(lr_dict[k][\"metrics\"][\"val_macroF1\"])\n    swa = lr_dict[k][\"shape_weighted_acc\"]\n    cwa = lr_dict[k][\"color_weighted_acc\"]\n    print(\n        f\"lr={lr_val}: best Val Macro-F1={best_val_f1:.4f} | SWA={swa:.4f} | CWA={cwa:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    data = experiment_data[\"emb_dim_tuning\"][\"SPR_BENCH\"]\n    train_f1 = np.array(data[\"metrics\"][\"train_macroF1\"])\n    val_f1 = np.array(data[\"metrics\"][\"val_macroF1\"])\n    train_ls = np.array(data[\"losses\"][\"train\"])\n    val_ls = np.array(data[\"losses\"][\"val\"])\n    emb_dims = np.array(data[\"hyperparams\"])\n    num_epochs = len(train_f1) // len(emb_dims) if len(emb_dims) else 0\n    epoch_idx = np.arange(1, len(train_f1) + 1)\n\n    # ------------------ Plot 1: F1 curves --------------------------\n    try:\n        plt.figure()\n        plt.plot(epoch_idx, train_f1, label=\"Train Macro-F1\")\n        plt.plot(epoch_idx, val_f1, label=\"Val Macro-F1\")\n        plt.xlabel(\"Epoch (concatenated over runs)\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 over Epochs\\nLeft: Train, Right: Val\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ------------------ Plot 2: Loss curves ------------------------\n    try:\n        plt.figure()\n        plt.plot(epoch_idx, train_ls, label=\"Train Loss\")\n        plt.plot(epoch_idx, val_ls, label=\"Val Loss\")\n        plt.xlabel(\"Epoch (concatenated over runs)\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss over Epochs\\nLeft: Train, Right: Val\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # -------- Plot 3: Final Val F1 vs Embedding Dimension ----------\n    try:\n        finals = val_f1.reshape(len(emb_dims), num_epochs)[:, -1]\n        plt.figure()\n        plt.bar([str(e) for e in emb_dims], finals, color=\"skyblue\")\n        plt.xlabel(\"Embedding Dimension\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1 by Embedding Size\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_valF1_vs_embdim.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating emb-dim bar plot: {e}\")\n        plt.close()\n\n    # ------------------ Print best run -----------------------------\n    if len(emb_dims) and num_epochs:\n        best_idx = finals.argmax()\n        print(\n            f\"Best emb_dim={emb_dims[best_idx]} | Val Macro-F1={finals[best_idx]:.4f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------------------- #\n#        LOAD EXPERIMENT        #\n# ----------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    data_dict = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\n    batch_sizes = sorted(map(int, data_dict.keys()))\n\n    # ----------------------------- #\n    #   EXTRACT ARRAYS TO PLOT      #\n    # ----------------------------- #\n    losses_train, losses_val = {}, {}\n    f1_train, f1_val = {}, {}\n    final_val_f1, final_swa, final_cwa = [], [], []\n\n    for bs in batch_sizes:\n        d = data_dict[str(bs)]\n        losses_train[bs] = d[\"losses\"][\"train\"]\n        losses_val[bs] = d[\"losses\"][\"val\"]\n        f1_train[bs] = d[\"metrics\"][\"train_macroF1\"]\n        f1_val[bs] = d[\"metrics\"][\"val_macroF1\"]\n        final_val_f1.append(f1_val[bs][-1])\n        final_swa.append(d[\"SWA\"])\n        final_cwa.append(d[\"CWA\"])\n\n    # ----------------------------- #\n    #           PLOTS               #\n    # ----------------------------- #\n    # 1) Loss curves\n    try:\n        plt.figure()\n        for bs in batch_sizes:\n            epochs = np.arange(1, len(losses_train[bs]) + 1)\n            plt.plot(epochs, losses_train[bs], label=f\"Train bs={bs}\", linestyle=\"-\")\n            plt.plot(epochs, losses_val[bs], label=f\"Val bs={bs}\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curves\n    try:\n        plt.figure()\n        for bs in batch_sizes:\n            epochs = np.arange(1, len(f1_train[bs]) + 1)\n            plt.plot(epochs, f1_train[bs], label=f\"Train bs={bs}\", linestyle=\"-\")\n            plt.plot(epochs, f1_val[bs], label=f\"Val bs={bs}\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.legend()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # 3) Final validation Macro-F1 bar chart\n    try:\n        plt.figure()\n        plt.bar(range(len(batch_sizes)), final_val_f1, tick_label=batch_sizes)\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH: Final Validation Macro-F1 by Batch Size\")\n        save_path = os.path.join(working_dir, \"SPR_BENCH_final_val_macroF1.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final val F1 bar chart: {e}\")\n        plt.close()\n\n    # 4) SWA & CWA grouped bars\n    try:\n        plt.figure()\n        idx = np.arange(len(batch_sizes))\n        width = 0.35\n        plt.bar(idx - width / 2, final_swa, width=width, label=\"SWA\")\n        plt.bar(idx + width / 2, final_cwa, width=width, label=\"CWA\")\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Shape & Color Weighted Accuracy\")\n        plt.xticks(idx, batch_sizes)\n        plt.legend()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_SWA_CWA.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA/CWA bar chart: {e}\")\n        plt.close()\n\n    # ----------------------------- #\n    #     PRINT FINAL METRICS       #\n    # ----------------------------- #\n    print(\"\\nFinal Validation Metrics\")\n    print(\"BatchSize | Macro-F1 |   SWA   |   CWA\")\n    for i, bs in enumerate(batch_sizes):\n        print(\n            f\"{bs:9d} | {final_val_f1[i]:7.4f} | {final_swa[i]:7.4f} | {final_cwa[i]:7.4f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\n    rates = np.array(exp[\"rates\"])\n    train_f1 = np.array(exp[\"metrics\"][\"train_macroF1\"])  # shape (R, E)\n    val_f1 = np.array(exp[\"metrics\"][\"val_macroF1\"])\n    train_ls = np.array(exp[\"losses\"][\"train\"])\n    val_ls = np.array(exp[\"losses\"][\"val\"])\n    swa = np.array(exp[\"swa\"])\n    cwa = np.array(exp[\"cwa\"])\n    epochs = np.arange(1, train_f1.shape[1] + 1)\n\n    # 1) Macro-F1 curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_f1[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_f1[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 plot: {e}\")\n        plt.close()\n\n    # 2) Loss curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_ls[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_ls[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # 3) Final Val Macro-F1 vs Dropout\n    try:\n        plt.figure()\n        final_val_f1 = val_f1[:, -1]\n        plt.bar(rates, final_val_f1, width=0.05)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1 vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_valF1_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Val-F1 bar plot: {e}\")\n        plt.close()\n\n    # 4) SWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, swa, marker=\"o\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"SWA\")\n        plt.title(\"SPR_BENCH Shape-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_SWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot: {e}\")\n        plt.close()\n\n    # 5) CWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, cwa, marker=\"o\", color=\"green\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"CWA\")\n        plt.title(\"SPR_BENCH Color-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot: {e}\")\n        plt.close()\n\n    # -------- Evaluation summary --------\n    best_idx = np.argmax(final_val_f1)\n    print(\n        f\"Best dropout rate: {rates[best_idx]:.2f} \"\n        f\"with final Val Macro-F1={final_val_f1[best_idx]:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- data loading ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    ed = experiment_data[\"weight_decay_tuning\"][\"SPR_BENCH\"]\n    wds = ed[\"configs\"]\n    tr_losses = ed[\"losses\"][\"train\"]\n    val_losses = ed[\"losses\"][\"val\"]\n    tr_f1 = ed[\"metrics\"][\"train\"]\n    val_f1 = ed[\"metrics\"][\"val\"]\n\n    # Identify best config by final val macro-F1\n    final_val_f1 = [vals[-1] for vals in val_f1]\n    best_idx = int(np.argmax(final_val_f1))\n    best_wd = wds[best_idx]\n    print(\n        f\"Best weight_decay={best_wd} with final Val Macro-F1={final_val_f1[best_idx]:.4f}\"\n    )\n\n    # ---------------- plots ----------------\n    # 1) Loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(tr_losses[0]) + 1)\n        for i, wd in enumerate(wds):\n            plt.plot(epochs, tr_losses[i], \"--\", label=f\"train wd={wd}\")\n            plt.plot(epochs, val_losses[i], \"-\", label=f\"val wd={wd}\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend(fontsize=8)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curves\n    try:\n        plt.figure()\n        for i, wd in enumerate(wds):\n            plt.plot(epochs, tr_f1[i], \"--\", label=f\"train wd={wd}\")\n            plt.plot(epochs, val_f1[i], \"-\", label=f\"val wd={wd}\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend(fontsize=8)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curves: {e}\")\n        plt.close()\n\n    # 3) Scatter of final Val F1 vs weight decay\n    try:\n        plt.figure()\n        plt.scatter(wds, final_val_f1, c=\"red\")\n        for wd, f1 in zip(wds, final_val_f1):\n            plt.text(wd, f1, f\"{f1:.2f}\", fontsize=8, ha=\"center\", va=\"bottom\")\n        plt.xscale(\"log\")\n        plt.xlabel(\"Weight Decay (log scale)\")\n        plt.ylabel(\"Final Validation Macro-F1\")\n        plt.title(\"SPR_BENCH: Final Val Macro-F1 vs Weight Decay\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_wd_vs_f1.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating scatter plot: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix for best model\n    try:\n        gt = ed[\"ground_truth\"][best_idx]\n        pred = ed[\"predictions\"][best_idx]\n        cm = confusion_matrix(gt, pred)\n        plt.figure()\n        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        plt.title(f\"SPR_BENCH Confusion Matrix (wd={best_wd})\")\n        plt.colorbar()\n        tick_marks = np.arange(len(cm))\n        plt.xticks(tick_marks, tick_marks)\n        plt.yticks(tick_marks, tick_marks)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                format(cm[i, j], \"d\"),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=6,\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed is not None:\n    epochs = np.arange(1, len(ed[\"metrics\"][\"train_macroF1\"]) + 1)\n\n    # --------- 1) F1 curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"metrics\"][\"train_macroF1\"], label=\"Train Macro-F1\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_macroF1\"], label=\"Val Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # --------- 2) Loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, ed[\"losses\"][\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # --------- 3) Additional validation metrics ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"metrics\"][\"val_SWA\"], label=\"Shape-Wtd Acc (SWA)\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_CWA\"], label=\"Color-Wtd Acc (CWA)\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_SC_Gmean\"], label=\"SC-Gmean\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH: Additional Validation Metrics\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_metrics.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # --------- 4) Confusion matrix ----------\n    try:\n        preds = np.array(ed[\"predictions\"])\n        trues = np.array(ed[\"ground_truth\"])\n        n_labels = preds.max() + 1 if preds.size else 0\n        if n_labels > 0:\n            cm, _, _ = np.histogram2d(\n                trues,\n                preds,\n                bins=(n_labels, n_labels),\n                range=[[0, n_labels], [0, n_labels]],\n            )\n            plt.figure()\n            im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            plt.title(\n                \"SPR_BENCH: Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    ed = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed:\n    epochs = np.arange(1, len(ed[\"losses\"][\"train\"]) + 1)\n\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, ed[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"metrics\"][\"train_macroF1\"], label=\"Train\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_macroF1\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 plot: {e}\")\n        plt.close()\n\n    # 3) SC-Gmean curve\n    try:\n        plt.figure()\n        sc_gmean = ed[\"metrics\"][\"val_SC_Gmean\"]\n        plt.plot(epochs, sc_gmean, marker=\"o\")\n        best_ep = int(np.argmax(sc_gmean)) + 1\n        plt.scatter(\n            best_ep, sc_gmean[best_ep - 1], color=\"red\", label=f\"Best (epoch {best_ep})\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"SC-Gmean\")\n        plt.title(\"SPR_BENCH Validation SC-Gmean\\nBest epoch highlighted\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_SC_Gmean_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SC-Gmean plot: {e}\")\n        plt.close()\n\n    # 4) SWA vs CWA scatter\n    try:\n        plt.figure()\n        plt.scatter(\n            ed[\"metrics\"][\"val_SWA\"], ed[\"metrics\"][\"val_CWA\"], c=epochs, cmap=\"viridis\"\n        )\n        plt.colorbar(label=\"Epoch\")\n        plt.xlabel(\"Shape-Weighted Accuracy (SWA)\")\n        plt.ylabel(\"Color-Weighted Accuracy (CWA)\")\n        plt.title(\"SPR_BENCH SWA vs. CWA Scatter\\nColor indicates epoch number\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_SWA_vs_CWA.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA vs CWA plot: {e}\")\n        plt.close()\n\n    # 5) Confusion matrix for best epoch\n    try:\n        y_true = ed[\"ground_truth\"]\n        y_pred = ed[\"predictions\"]\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\n    rates = np.array(exp[\"rates\"])\n    train_f1 = np.array(exp[\"metrics\"][\"train_macroF1\"])  # shape (R, E)\n    val_f1 = np.array(exp[\"metrics\"][\"val_macroF1\"])\n    train_ls = np.array(exp[\"losses\"][\"train\"])\n    val_ls = np.array(exp[\"losses\"][\"val\"])\n    swa = np.array(exp[\"swa\"])\n    cwa = np.array(exp[\"cwa\"])\n    epochs = np.arange(1, train_f1.shape[1] + 1)\n\n    # 1) Macro-F1 curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_f1[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_f1[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 plot: {e}\")\n        plt.close()\n\n    # 2) Loss curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_ls[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_ls[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # 3) Final Val Macro-F1 vs Dropout\n    try:\n        plt.figure()\n        final_val_f1 = val_f1[:, -1]\n        plt.bar(rates, final_val_f1, width=0.05)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1 vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_valF1_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Val-F1 bar plot: {e}\")\n        plt.close()\n\n    # 4) SWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, swa, marker=\"o\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"SWA\")\n        plt.title(\"SPR_BENCH Shape-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_SWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot: {e}\")\n        plt.close()\n\n    # 5) CWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, cwa, marker=\"o\", color=\"green\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"CWA\")\n        plt.title(\"SPR_BENCH Color-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot: {e}\")\n        plt.close()\n\n    # -------- Evaluation summary --------\n    best_idx = np.argmax(final_val_f1)\n    print(\n        f\"Best dropout rate: {rates[best_idx]:.2f} \"\n        f\"with final Val Macro-F1={final_val_f1[best_idx]:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\n    rates = np.array(exp[\"rates\"])\n    train_f1 = np.array(exp[\"metrics\"][\"train_macroF1\"])  # shape (R, E)\n    val_f1 = np.array(exp[\"metrics\"][\"val_macroF1\"])\n    train_ls = np.array(exp[\"losses\"][\"train\"])\n    val_ls = np.array(exp[\"losses\"][\"val\"])\n    swa = np.array(exp[\"swa\"])\n    cwa = np.array(exp[\"cwa\"])\n    epochs = np.arange(1, train_f1.shape[1] + 1)\n\n    # 1) Macro-F1 curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_f1[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_f1[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 plot: {e}\")\n        plt.close()\n\n    # 2) Loss curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_ls[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_ls[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # 3) Final Val Macro-F1 vs Dropout\n    try:\n        plt.figure()\n        final_val_f1 = val_f1[:, -1]\n        plt.bar(rates, final_val_f1, width=0.05)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1 vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_valF1_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Val-F1 bar plot: {e}\")\n        plt.close()\n\n    # 4) SWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, swa, marker=\"o\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"SWA\")\n        plt.title(\"SPR_BENCH Shape-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_SWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot: {e}\")\n        plt.close()\n\n    # 5) CWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, cwa, marker=\"o\", color=\"green\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"CWA\")\n        plt.title(\"SPR_BENCH Color-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot: {e}\")\n        plt.close()\n\n    # -------- Evaluation summary --------\n    best_idx = np.argmax(final_val_f1)\n    print(\n        f\"Best dropout rate: {rates[best_idx]:.2f} \"\n        f\"with final Val Macro-F1={final_val_f1[best_idx]:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    exp = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\n    rates = np.array(exp[\"rates\"])\n    train_f1 = np.array(exp[\"metrics\"][\"train_macroF1\"])  # shape (R, E)\n    val_f1 = np.array(exp[\"metrics\"][\"val_macroF1\"])\n    train_ls = np.array(exp[\"losses\"][\"train\"])\n    val_ls = np.array(exp[\"losses\"][\"val\"])\n    swa = np.array(exp[\"swa\"])\n    cwa = np.array(exp[\"cwa\"])\n    epochs = np.arange(1, train_f1.shape[1] + 1)\n\n    # 1) Macro-F1 curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_f1[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_f1[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Macro-F1 vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Macro-F1 plot: {e}\")\n        plt.close()\n\n    # 2) Loss curves\n    try:\n        plt.figure()\n        for i, r in enumerate(rates):\n            plt.plot(epochs, train_ls[i], label=f\"train d={r}\")\n            plt.plot(epochs, val_ls[i], \"--\", label=f\"val d={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss vs Epochs\\nSolid: Train, Dashed: Val\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # 3) Final Val Macro-F1 vs Dropout\n    try:\n        plt.figure()\n        final_val_f1 = val_f1[:, -1]\n        plt.bar(rates, final_val_f1, width=0.05)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1 vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_valF1_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Val-F1 bar plot: {e}\")\n        plt.close()\n\n    # 4) SWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, swa, marker=\"o\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"SWA\")\n        plt.title(\"SPR_BENCH Shape-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_SWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot: {e}\")\n        plt.close()\n\n    # 5) CWA vs Dropout\n    try:\n        plt.figure()\n        plt.plot(rates, cwa, marker=\"o\", color=\"green\")\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"CWA\")\n        plt.title(\"SPR_BENCH Color-Weighted Accuracy vs Dropout\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA plot: {e}\")\n        plt.close()\n\n    # -------- Evaluation summary --------\n    best_idx = np.argmax(final_val_f1)\n    print(\n        f\"Best dropout rate: {rates[best_idx]:.2f} \"\n        f\"with final Val Macro-F1={final_val_f1[best_idx]:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Paths supplied by the prompt\nexperiment_data_path_list = [\n    \"experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b70b1a3c505c490cab3db45a5e6d0e0f_proc_2964458/experiment_data.npy\",\n    \"experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bae519d796f84228818cfeee12c15f90_proc_2964457/experiment_data.npy\",\n    \"experiments/2025-08-15_18-22-30_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fd93af9c72ea46e6b9c1ae364c3c0a89_proc_2964456/experiment_data.npy\",\n]\n\nall_exp = []\ntry:\n    root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n    for p in experiment_data_path_list:\n        full_path = os.path.join(root, p)\n        ed = np.load(full_path, allow_pickle=True).item()\n        all_exp.append(ed)\n    if len(all_exp) == 0:\n        raise RuntimeError(\"No experiment files loaded\")\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_exp = []\n\n# ------------------------------------------------------------------\nif all_exp:\n    # Assume same dropout and shape across runs\n    rates = np.array(all_exp[0][\"dropout_rate\"][\"SPR_BENCH\"][\"rates\"])\n    n_rates = len(rates)\n    n_runs = len(all_exp)\n\n    # Pre-allocate lists for stacking\n    train_f1_runs, val_f1_runs = [], []\n    train_ls_runs, val_ls_runs = [], []\n    swa_runs, cwa_runs = [], []\n\n    for ed in all_exp:\n        exp = ed[\"dropout_rate\"][\"SPR_BENCH\"]\n        train_f1_runs.append(np.array(exp[\"metrics\"][\"train_macroF1\"]))\n        val_f1_runs.append(np.array(exp[\"metrics\"][\"val_macroF1\"]))\n        train_ls_runs.append(np.array(exp[\"losses\"][\"train\"]))\n        val_ls_runs.append(np.array(exp[\"losses\"][\"val\"]))\n        swa_runs.append(np.array(exp[\"swa\"]))\n        cwa_runs.append(np.array(exp[\"cwa\"]))\n\n    train_f1 = np.stack(train_f1_runs, axis=0)  # shape (R, D, E)\n    val_f1 = np.stack(val_f1_runs, axis=0)\n    train_ls = np.stack(train_ls_runs, axis=0)\n    val_ls = np.stack(val_ls_runs, axis=0)\n    swa = np.stack(swa_runs, axis=0)  # shape (R, D)\n    cwa = np.stack(cwa_runs, axis=0)\n\n    epochs = np.arange(1, train_f1.shape[2] + 1)\n    sem = lambda x, axis=0: np.std(x, axis=axis, ddof=1) / np.sqrt(x.shape[axis])\n\n    # 1) Aggregated Macro-F1 curves\n    try:\n        plt.figure()\n        for d in range(n_rates):\n            mean_val = val_f1[:, d, :].mean(0)\n            err_val = sem(val_f1[:, d, :], axis=0)\n            plt.plot(epochs, mean_val, label=f\"val d={rates[d]:.2f}\")\n            plt.fill_between(epochs, mean_val - err_val, mean_val + err_val, alpha=0.2)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Validation Macro-F1 (Mean \u00b1 SEM)\\nDropout Sweep\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_val_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated Macro-F1 plot: {e}\")\n        plt.close()\n\n    # 2) Aggregated Loss curves\n    try:\n        plt.figure()\n        for d in range(n_rates):\n            mean_val = val_ls[:, d, :].mean(0)\n            err_val = sem(val_ls[:, d, :], axis=0)\n            plt.plot(epochs, mean_val, label=f\"val d={rates[d]:.2f}\")\n            plt.fill_between(epochs, mean_val - err_val, mean_val + err_val, alpha=0.2)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Validation Loss (Mean \u00b1 SEM)\\nDropout Sweep\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_val_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated Loss plot: {e}\")\n        plt.close()\n\n    # 3) Final Val Macro-F1 vs Dropout (bar + error)\n    try:\n        final_val = val_f1[:, :, -1]  # shape (R, D)\n        mean_final = final_val.mean(0)\n        err_final = sem(final_val, axis=0)\n        plt.figure()\n        plt.bar(rates, mean_final, width=0.05, yerr=err_final, capsize=5)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1\\nMean \u00b1 SEM across runs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_final_valF1_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated Val-F1 bar plot: {e}\")\n        plt.close()\n\n    # 4) SWA vs Dropout\n    try:\n        mean_swa = swa.mean(0)\n        err_swa = sem(swa, axis=0)\n        plt.figure()\n        plt.errorbar(rates, mean_swa, yerr=err_swa, marker=\"o\", capsize=5)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"SWA\")\n        plt.title(\"SPR_BENCH Shape-Weighted Accuracy\\nMean \u00b1 SEM across runs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_SWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated SWA plot: {e}\")\n        plt.close()\n\n    # 5) CWA vs Dropout\n    try:\n        mean_cwa = cwa.mean(0)\n        err_cwa = sem(cwa, axis=0)\n        plt.figure()\n        plt.errorbar(\n            rates, mean_cwa, yerr=err_cwa, marker=\"o\", capsize=5, color=\"green\"\n        )\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"CWA\")\n        plt.title(\"SPR_BENCH Color-Weighted Accuracy\\nMean \u00b1 SEM across runs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_CWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated CWA plot: {e}\")\n        plt.close()\n\n    # -------- Evaluation summary --------\n    best_idx = np.argmax(mean_final)\n    print(\n        f\"(Aggregated) Best dropout rate: {rates[best_idx]:.2f} \"\n        f\"with mean final Val Macro-F1={mean_final[best_idx]:.4f} \"\n        f\"\u00b1 {err_final[best_idx]:.4f} (SEM)\"\n    )\nelse:\n    print(\"No experiment data available for aggregation.\")\n"], "plot_plan": ["We will load the saved experiment_data.npy, identify every dataset present (currently only \u2018SPR_BENCH\u2019), and read its stored losses, macro-F1 scores, predictions and ground-truth labels.  \nFour separate visualisations will be produced (all wrapped in independent try-except blocks and saved to working_dir):  \n1) Training vs validation loss across epochs.  \n2) Training vs validation macro-F1 across epochs.  \n3) Confusion-matrix heat-map of validation predictions.  \n4) Bar chart comparing per-class counts of ground truth and predictions.  \nEach figure will have a clear title that contains the dataset name and the plot type; we will plot at most one point per epoch (\u22645) so we stay within the 5-plot limit.  \nFigures are closed after saving to avoid memory leaks, and errors are caught and printed without stopping the rest of the script.  \nFinally, the script prints the paths of the generated plots so a caller can verify output.", null, "We first load the saved experiment_data.npy, extract the learning-rate sub-dictionary for SPR_BENCH, and collect per-epoch arrays for train/val loss and macro-F1. Three plots are produced (each wrapped in its own try-except): (1) combined training vs. validation loss curves for all learning rates; (2) combined training vs. validation macro-F1 curves; (3) a bar chart comparing the final-epoch shape-weighted and color-weighted accuracies for each learning rate. Each figure is given a descriptive title and saved to working_dir with dataset and plot type in the filename, then closed. We also print a small text summary: for every learning rate we output the best validation macro-F1 achieved and the final SWA/CWA values. All plotting uses basic matplotlib and only relies on data present in experiment_data.npy. At most three figures are generated, satisfying the \u201c\u22645 figures\u201d rule, and all requirements for figure naming, closing, and error handling are met.", "We first load experiment_data.npy from the working directory, then extract the lists stored for training/validation macro-F1 and losses as well as the list of embedding dimensions tried. Knowing there are a fixed number of epochs per run, we rebuild a per-epoch index and also compute, for every embedding size, the final validation macro-F1 (i.e., value at the last epoch of each run). Three plots are produced: (1) training vs. validation macro-F1 across all epochs, (2) training vs. validation loss across all epochs, and (3) a bar chart of final validation macro-F1 versus embedding dimension. Each plot is wrapped in its own try-except block, given a clear title/subtitle, saved under an informative filename in working_dir, and the figure is always closed. We avoid generating more than three figures, safely handle missing data, and never fabricate values. After plotting, the script prints the best embedding dimension and its corresponding validation macro-F1 so that the user can quickly see the key metric without opening the figures.", "We load the saved experiment_data.npy, pull out per-epoch losses and macro-F1 scores for every tried batch size, and also collect the final Shape-Weighted and Color-Weighted Accuracies. Four figures are produced, each guarded by its own try-except block and saved into working_dir with descriptive filenames: (1) an overlay of training and validation loss curves, (2) an overlay of training and validation macro-F1 curves, (3) a bar chart comparing final validation macro-F1 across batch sizes, and (4) a grouped bar chart showing final SWA and CWA across batch sizes. All titles mention the dataset (SPR_BENCH) and the content of the plot, legends are added for clarity, and every figure is closed after saving. No synthetic values are used\u2014every point is taken directly from the numpy file. After plotting, the script prints a concise table of the final metrics so users can quickly inspect which batch size performed best. The entire routine relies only on matplotlib, numpy, and os, makes sure working_dir exists, and respects the five-plot limit. Run the script in the same environment where experiment_data.npy was generated to reproduce the visualizations and metric summary.", "We load experiment_data.npy, extract the SPR_BENCH entry, and gather dropout rates, per-epoch train/val losses and macro-F1, and the extra SWA/CWA lists. Five figures are produced, each wrapped in its own try-except: (1) macro-F1 curves (solid=train, dashed=val) versus epochs for all dropout values; (2) loss curves versus epochs; (3) bar plot of final validation macro-F1 versus dropout; (4) SWA versus dropout; (5) CWA versus dropout. File names include the dataset and plot type, and every figure is titled clearly. We keep the number of figures \u22645 and close each figure after saving. After plotting, we identify and print the dropout rate with the best final validation macro-F1 as a quick evaluation metric. All figures are stored in the working directory created earlier.", "We will load the stored experiment_data.npy, pick out the SPR_BENCH block, and produce four plots: (1) loss curves across epochs for every tested weight-decay, (2) macro-F1 curves across epochs, (3) a scatter of final validation macro-F1 vs. weight-decay, and (4) a confusion-matrix heat-map for the run that achieved the best validation macro-F1.  Each figure is wrapped in its own try-except block, saved in working_dir with descriptive names, and the figure is always closed afterwards.  We additionally print a small text summary showing which weight-decay performed best together with its final macro-F1.  No synthetic data are introduced; all numbers are drawn directly from experiment_data.  A maximum of five plots is respected.  Basic matplotlib functions are used with clear titles and axes labels, and we adhere to the provided directory and naming conventions.", "We will first load experiment_data.npy from the working directory and safely exit if it is missing. Then we extract per-epoch arrays for training/validation macro-F1 and losses, plus val_SWA, val_CWA and val_SC_Gmean. Four independent plots will be produced: (1) a line chart of training vs validation macro-F1 across epochs, (2) a similar line chart for losses, (3) a multi-line chart of the three additional validation metrics, and (4) a confusion-matrix heat-map comparing the best validation predictions with ground-truth labels. Each plot is wrapped in its own try-except block, titled clearly, saved with descriptive names that include \u201cSPR_BENCH\u201d and closed afterwards. We keep the number of figures \u22645 and rely only on arrays that actually exist in the saved file. The code is concise, uses only matplotlib and numpy, and puts all outputs into the previously created working directory.", "We will load experiment_data.npy, extract the SPR_BENCH metrics/losses/predictions, and create up to five informative plots: (1) training vs. validation loss across epochs, (2) training vs. validation macro-F1 across epochs, (3) validation SC-Gmean highlighting the best epoch, (4) scatter of SWA vs. CWA across epochs, and (5) a confusion-matrix heat-map for the best-epoch predictions. Each plot is wrapped in its own try-except so that failures in one do not stop the rest, and every figure is closed after saving. Filenames and titles clearly include the dataset name and plot type, and the figures are stored in the predefined working directory. Only data actually present in experiment_data.npy are used\u2014no dummy values are generated. We also create the working directory if it does not yet exist, and print a short message for any exception encountered. Finally, we keep the code concise and self-contained, adhering strictly to basic matplotlib usage.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "emb_dim", "batch_size", "dropout_rate", "weight_decay", null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the experiment_data.npy file from the \u201cworking\u201d directory,\niterate over every dataset key it contains, and then inspect the nested\n\u201cmetrics\u201d and \u201closses\u201d dictionaries. For each list in those dictionaries it will\ndecide whether a higher or lower value is better (lower for anything containing\n\u201closs\u201d, higher otherwise) and print that best value. All outputs are preceded by\nthe dataset name and use explicit metric names such as \u201ctraining macro-F1\u201d or\n\u201cvalidation loss\u201d. The code executes immediately at import time, with no special\nentry-point guard.", "", "The code will first locate and load experiment_data.npy from the working\ndirectory.   It then iterates over every dataset stored inside the dict (e.g.,\n\u201cSPR_BENCH\u201d) and, for each learning-rate run, extracts the sequence of losses\nand F1 scores that were recorded at every epoch.   The script prints the dataset\nname, followed by the learning rate, and finally the best (max F1 / min loss) or\nsingle-value metrics that were saved (shape-weighted accuracy and color-weighted\naccuracy).   All execution happens at the global scope so the file runs\nimmediately when executed, and no plots are produced.", "The script first locates and loads the saved NumPy archive from the working\ndirectory, converts it back to a Python dictionary, and then iterates through\nevery dataset stored under the \u201cemb_dim_tuning\u201d experiment.   For each dataset\nit inspects the lists of recorded values for macro-F1 scores and losses,\nextracts the single \u201cbest\u201d number (maximum for F1, minimum for losses), and\nprints them with explicit, human-readable metric names.   Because the lists\ncoming from several runs are concatenated epoch-wise, taking the global max/min\nacross the list correctly yields the best value observed in the entire sweep.\nNo plotting or special entry-point wrapper is used\u2014the code executes immediately\non import/run and prints the requested summary.", "The code will locate the working directory, load the saved experiment\ndictionary, and iterate through each dataset and batch-size variant. For every\nvariant it reports the best (i.e., maximum F1 or minimum loss) or final (SWA /\nCWA) values, clearly labeling each metric so there is no ambiguity. Nothing is\nwrapped in an `if __name__ == \"__main__\":` guard, ensuring the script executes\nas soon as it is run.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate through its nested structure (tuning type \u2192 dataset).   For every\ndataset it will compute the \u201cbest\u201d value for each metric: highest F1 scores and\naccuracies, and lowest losses.   Each dataset\u2019s name is printed once, followed\nby clearly-labelled metric lines as required.   Everything is executed at the\ntop level\u2014no \u200bif __name__ == \"__main__\"\u200b guard\u2014and no plots are produced.", "The script will load the saved numpy file, navigate the nested dictionary, and\ncompute the best (i.e., highest F1 and lowest loss) values obtained across all\nepochs and hyper-parameter configurations for each dataset. It then prints the\ndataset name followed by clearly labelled lines for the best training/validation\nF1 scores and the lowest training/validation losses.", "The script will load experiment_data.npy from the \u201cworking\u201d sub-folder, convert\nit back to a Python dictionary, and iterate through every stored dataset (e.g.,\n\u201cSPR_BENCH\u201d).   For each dataset it prints the dataset name first, then walks\nthrough the \u201cmetrics\u201d and \u201closses\u201d sub-dictionaries, selecting the last recorded\nvalue (assumed to be the final/best one).   Each printed line explicitly names\nthe metric (e.g., \u201ctrain_macroF1\u201d, \u201cvalidation loss\u201d) before displaying its\nvalue, complying with the required naming clarity.   Everything is executed at\nthe global scope so the file runs immediately without an `if __name__ ==\n\"__main__\":` guard, and no plots are generated.", "The script will load the saved NumPy dictionary, iterate over every dataset it\ncontains, and for each metric series choose the \u201cbest\u201d value (maximum for\nscores, minimum for losses).   It prints the dataset name first, then each\nmetric label in a human-readable form followed by the selected value, adhering\nto the specified naming conventions.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate through its nested structure (tuning type \u2192 dataset).   For every\ndataset it will compute the \u201cbest\u201d value for each metric: highest F1 scores and\naccuracies, and lowest losses.   Each dataset\u2019s name is printed once, followed\nby clearly-labelled metric lines as required.   Everything is executed at the\ntop level\u2014no \u200bif __name__ == \"__main__\"\u200b guard\u2014and no plots are produced.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate through its nested structure (tuning type \u2192 dataset).   For every\ndataset it will compute the \u201cbest\u201d value for each metric: highest F1 scores and\naccuracies, and lowest losses.   Each dataset\u2019s name is printed once, followed\nby clearly-labelled metric lines as required.   Everything is executed at the\ntop level\u2014no \u200bif __name__ == \"__main__\"\u200b guard\u2014and no plots are produced.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate through its nested structure (tuning type \u2192 dataset).   For every\ndataset it will compute the \u201cbest\u201d value for each metric: highest F1 scores and\naccuracies, and lowest losses.   Each dataset\u2019s name is printed once, followed\nby clearly-labelled metric lines as required.   Everything is executed at the\ntop level\u2014no \u200bif __name__ == \"__main__\"\u200b guard\u2014and no plots are produced.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef best_value(values, higher_is_better=True):\n    \"\"\"Return the best value from a list according to the criterion.\"\"\"\n    if not values:  # handle empty lists gracefully\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# ------------------------------------------------------------------\nfor dataset_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---- metrics (e.g., F1 scores) ----\n    for metric_name, series in ds_dict.get(\"metrics\", {}).items():\n        human_readable = metric_name.replace(\"_\", \" \").replace(\"macroF1\", \"macro-F1\")\n        # assume higher is better for non-loss metrics\n        best_val = best_value(series, higher_is_better=True)\n        if best_val is not None:\n            print(f\"Best {human_readable}: {best_val:.4f}\")\n\n    # ---- losses ----\n    for loss_name, series in ds_dict.get(\"losses\", {}).items():\n        human_readable = f\"{loss_name} loss\"\n        # lower is better for losses\n        best_val = best_value(series, higher_is_better=False)\n        if best_val is not None:\n            print(f\"Best {human_readable}: {best_val:.6f}\")\n", "", "import os\nimport numpy as np\n\n# -------------------------------------------------------------\n# Locate and load experiment data\n# -------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Expected file not found at: {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# -------------------------------------------------------------\n# Helper functions to select best values\n# -------------------------------------------------------------\ndef best_f1(f1_list):\n    \"\"\"Return the maximum F1 score from a list of epoch values.\"\"\"\n    return max(f1_list) if f1_list else None\n\n\ndef best_loss(loss_list):\n    \"\"\"Return the minimum loss from a list of epoch values.\"\"\"\n    return min(loss_list) if loss_list else None\n\n\n# -------------------------------------------------------------\n# Iterate over datasets and learning-rate runs\n# -------------------------------------------------------------\nfor hp_group_name, datasets in experiment_data.items():  # e.g. 'learning_rate'\n    for dataset_name, runs in datasets.items():  # e.g. 'SPR_BENCH'\n        print(f\"\\nDataset: {dataset_name}\")\n        for lr_key, run_data in runs.items():  # e.g. 'lr_0.001'\n            lr_val = lr_key.replace(\"lr_\", \"\")\n            print(f\"  Learning rate: {lr_val}\")\n\n            # Extract metric/loss sequences\n            train_f1_series = run_data[\"metrics\"][\"train_macroF1\"]\n            val_f1_series = run_data[\"metrics\"][\"val_macroF1\"]\n            train_loss_series = run_data[\"losses\"][\"train\"]\n            val_loss_series = run_data[\"losses\"][\"val\"]\n\n            # Compute best values\n            best_train_f1 = best_f1(train_f1_series)\n            best_val_f1 = best_f1(val_f1_series)\n            lowest_train_loss = best_loss(train_loss_series)\n            lowest_val_loss = best_loss(val_loss_series)\n\n            # Single-value metrics saved only once\n            shape_w_acc = run_data.get(\"shape_weighted_acc\")\n            color_w_acc = run_data.get(\"color_weighted_acc\")\n\n            # Print metrics with explicit names\n            print(f\"    Best train macro F1 score: {best_train_f1:.4f}\")\n            print(f\"    Best validation macro F1 score: {best_val_f1:.4f}\")\n            print(f\"    Lowest train loss: {lowest_train_loss:.6f}\")\n            print(f\"    Lowest validation loss: {lowest_val_loss:.6f}\")\n            if shape_w_acc is not None:\n                print(f\"    Shape weighted accuracy: {shape_w_acc:.4f}\")\n            if color_w_acc is not None:\n                print(f\"    Color weighted accuracy: {color_w_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 1. Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# 2. Helper for pretty metric names\ndef prettify_metric_name(key):\n    mapping = {\n        \"train_macroF1\": \"training macro F1\",\n        \"val_macroF1\": \"validation macro F1\",\n        \"train\": \"training loss\",\n        \"val\": \"validation loss\",\n    }\n    return mapping.get(key, key)\n\n\n# ------------------------------------------------------------------\n# 3. Iterate through experiments and print best/final metrics\nfor exp_name, datasets in experiment_data.items():  # e.g. \"emb_dim_tuning\"\n    for dataset_name, content in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # ---- F1 metrics ----------------------------------------------------\n        for metric_key, values in content[\"metrics\"].items():\n            pretty = prettify_metric_name(metric_key)\n            if \"F1\" in metric_key:\n                best_val = max(values)  # higher is better\n            else:\n                best_val = values[-1]  # fallback, shouldn't occur here\n            print(f\"{pretty}: {best_val:.4f}\")\n\n        # ---- Loss metrics --------------------------------------------------\n        for loss_split, losses in content[\"losses\"].items():\n            pretty = prettify_metric_name(loss_split)\n            best_val = min(losses)  # lower loss is better\n            print(f\"{pretty}: {best_val:.4f}\")\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------ #\n#                LOAD THE SAVED EXPERIMENT DATA                #\n# ------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Cannot find experiment data at {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------ #\n#                       PRINT METRICS                          #\n# ------------------------------------------------------------ #\nfor hp_name, dataset_dict in experiment_data.items():  # hp_name == \"batch_size\"\n    for (\n        dataset_name,\n        variant_dict,\n    ) in dataset_dict.items():  # dataset_name == \"SPR_BENCH\"\n        print(dataset_name)\n        for variant, results in variant_dict.items():  # variant == batch-size value\n            print(f\"batch size {variant}\")\n\n            # Retrieve recorded arrays\n            train_f1s = results[\"metrics\"][\"train_macroF1\"]\n            val_f1s = results[\"metrics\"][\"val_macroF1\"]\n            train_ls = results[\"losses\"][\"train\"]\n            val_ls = results[\"losses\"][\"val\"]\n\n            # Compute best values\n            best_train_f1 = max(train_f1s) if train_f1s else float(\"nan\")\n            best_val_f1 = max(val_f1s) if val_f1s else float(\"nan\")\n            lowest_train_loss = min(train_ls) if train_ls else float(\"nan\")\n            lowest_val_loss = min(val_ls) if val_ls else float(\"nan\")\n\n            # Final-epoch weighted accuracies\n            swa = results.get(\"SWA\", float(\"nan\"))\n            cwa = results.get(\"CWA\", float(\"nan\"))\n\n            # Print with explicit metric names\n            print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n            print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n            print(f\"lowest training loss: {lowest_train_loss:.4f}\")\n            print(f\"lowest validation loss: {lowest_val_loss:.4f}\")\n            print(f\"final shape-weighted accuracy: {swa:.4f}\")\n            print(f\"final color-weighted accuracy: {cwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef best(values, higher_is_better=True):\n    \"\"\"\n    Helper to pick the best scalar from a nested list (list-of-lists) or flat list.\n    For F1/accuracy higher is better, for loss lower is better.\n    \"\"\"\n    if values is None or len(values) == 0:\n        return None\n    # Flatten one level if needed\n    if isinstance(values[0], (list, tuple)):\n        flat = [v for sub in values for v in sub]\n    else:\n        flat = list(values)\n    return max(flat) if higher_is_better else min(flat)\n\n\n# ------------------------------------------------------------------\n# Traverse the structure and print metrics\nfor tuning_name, datasets in experiment_data.items():  # e.g. \"dropout_rate\"\n    for dataset_name, d in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # F1 scores\n        best_train_f1 = best(d[\"metrics\"][\"train_macroF1\"], higher_is_better=True)\n        best_val_f1 = best(d[\"metrics\"][\"val_macroF1\"], higher_is_better=True)\n\n        # Losses\n        best_train_loss = best(d[\"losses\"][\"train\"], higher_is_better=False)\n        best_val_loss = best(d[\"losses\"][\"val\"], higher_is_better=False)\n\n        # Extra accuracies\n        best_swa = best(d.get(\"swa\", []), higher_is_better=True)\n        best_cwa = best(d.get(\"cwa\", []), higher_is_better=True)\n\n        # Print clearly-labelled metrics (only if available)\n        if best_train_f1 is not None:\n            print(f\"Training Macro F1 Score (best): {best_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Validation Macro F1 Score (best): {best_val_f1:.4f}\")\n\n        if best_train_loss is not None:\n            print(f\"Training Loss (best): {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"Validation Loss (best): {best_val_loss:.4f}\")\n\n        if best_swa is not None:\n            print(f\"Shape Weighted Accuracy (best): {best_swa:.4f}\")\n        if best_cwa is not None:\n            print(f\"Color Weighted Accuracy (best): {best_cwa:.4f}\")\n\n        # Add a blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate file and load\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Iterate through experiments / datasets\n# ------------------------------------------------------------------\nfor exp_name, datasets in experiment_data.items():  # e.g. \"weight_decay_tuning\"\n    for dataset_name, ed in datasets.items():  # e.g. \"SPR_BENCH\"\n        # Collect final-epoch values for every configuration\n        train_f1_final = [metrics[-1] for metrics in ed[\"metrics\"][\"train\"]]\n        val_f1_final = [metrics[-1] for metrics in ed[\"metrics\"][\"val\"]]\n        train_loss_final = [losses[-1] for losses in ed[\"losses\"][\"train\"]]\n        val_loss_final = [losses[-1] for losses in ed[\"losses\"][\"val\"]]\n\n        # Identify best values and which config achieved them\n        best_train_f1_idx = int(np.argmax(train_f1_final))\n        best_val_f1_idx = int(np.argmax(val_f1_final))\n        best_train_loss_idx = int(np.argmin(train_loss_final))\n        best_val_loss_idx = int(np.argmin(val_loss_final))\n\n        # ------------------------------------------------------------------\n        # 2. Print results\n        # ------------------------------------------------------------------\n        print(f\"\\nDataset: {dataset_name}\")\n        print(\n            f\"best training F1 score     : {train_f1_final[best_train_f1_idx]:.4f} (weight_decay = {ed['configs'][best_train_f1_idx]})\"\n        )\n        print(\n            f\"best validation F1 score   : {val_f1_final[best_val_f1_idx]:.4f} (weight_decay = {ed['configs'][best_val_f1_idx]})\"\n        )\n        print(\n            f\"lowest training loss       : {train_loss_final[best_train_loss_idx]:.4f} (weight_decay = {ed['configs'][best_train_loss_idx]})\"\n        )\n        print(\n            f\"lowest validation loss     : {val_loss_final[best_val_loss_idx]:.4f} (weight_decay = {ed['configs'][best_val_loss_idx]})\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------ LOAD SAVED EXPERIMENT DATA --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# ------------------ DISPLAY FINAL / BEST METRICS ------------------\nfor dataset_name, dataset_info in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ------ Standard Metrics ------\n    metrics = dataset_info.get(\"metrics\", {})\n    for metric_name, values in metrics.items():\n        if not values:\n            continue\n        final_value = values[-1]\n        print(\n            f\"{metric_name}: {final_value:.6f}\"\n            if isinstance(final_value, float)\n            else f\"{metric_name}: {final_value}\"\n        )\n\n    # ------ Losses (treated separately for clarity) ------\n    losses = dataset_info.get(\"losses\", {})\n    for split_name, values in losses.items():\n        if not values:\n            continue\n        final_loss = values[-1]\n        # Use more explicit naming than just 'train' or 'val'\n        label = (\n            \"training loss\"\n            if split_name.lower().startswith(\"train\")\n            else \"validation loss\"\n        )\n        print(\n            f\"{label}: {final_loss:.6f}\"\n            if isinstance(final_loss, float)\n            else f\"{label}: {final_loss}\"\n        )\n", "import os\nimport numpy as np\n\n# ---------------------------- LOAD DATA --------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------- HELPER FUNCTIONS ----------------------------------\ndef is_loss(metric_name: str) -> bool:\n    return \"loss\" in metric_name.lower()\n\n\nprefix_map = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"test\"}\nsuffix_map = {\n    \"macroF1\": \"macro F1 score\",\n    \"SWA\": \"SWA\",\n    \"CWA\": \"CWA\",\n    \"SC_Gmean\": \"SC-Gmean\",\n    \"loss\": \"loss\",\n}\n\n\ndef prettify(name: str) -> str:\n    \"\"\"\n    Convert raw metric keys like 'train_macroF1' to readable labels such as\n    'training macro F1 score'.\n    \"\"\"\n    parts = name.split(\"_\")\n    readable_parts = []\n    for p in parts:\n        if p in prefix_map:\n            readable_parts.append(prefix_map[p])\n        elif p in suffix_map:\n            readable_parts.append(suffix_map[p])\n        else:  # fallback: keep as-is, but spaced camel case a little\n            readable_parts.append(p)\n    return \" \".join(readable_parts)\n\n\ndef select_best(values, higher_is_better: bool = True):\n    return max(values) if higher_is_better else min(values)\n\n\n# -------------------------- PRINT METRICS ------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset header\n    # ----- losses ------------------------------------------------------------\n    for split, loss_values in data.get(\"losses\", {}).items():\n        metric_key = f\"{split}_loss\"\n        pretty = prettify(metric_key)\n        best_val = select_best(loss_values, higher_is_better=False)\n        print(f\"{pretty}: {best_val:.4f}\")\n    # ----- other metrics -----------------------------------------------------\n    for raw_key, values in data.get(\"metrics\", {}).items():\n        pretty = prettify(raw_key)\n        higher = not is_loss(raw_key)\n        best_val = select_best(values, higher_is_better=higher)\n        print(f\"{pretty}: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef best(values, higher_is_better=True):\n    \"\"\"\n    Helper to pick the best scalar from a nested list (list-of-lists) or flat list.\n    For F1/accuracy higher is better, for loss lower is better.\n    \"\"\"\n    if values is None or len(values) == 0:\n        return None\n    # Flatten one level if needed\n    if isinstance(values[0], (list, tuple)):\n        flat = [v for sub in values for v in sub]\n    else:\n        flat = list(values)\n    return max(flat) if higher_is_better else min(flat)\n\n\n# ------------------------------------------------------------------\n# Traverse the structure and print metrics\nfor tuning_name, datasets in experiment_data.items():  # e.g. \"dropout_rate\"\n    for dataset_name, d in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # F1 scores\n        best_train_f1 = best(d[\"metrics\"][\"train_macroF1\"], higher_is_better=True)\n        best_val_f1 = best(d[\"metrics\"][\"val_macroF1\"], higher_is_better=True)\n\n        # Losses\n        best_train_loss = best(d[\"losses\"][\"train\"], higher_is_better=False)\n        best_val_loss = best(d[\"losses\"][\"val\"], higher_is_better=False)\n\n        # Extra accuracies\n        best_swa = best(d.get(\"swa\", []), higher_is_better=True)\n        best_cwa = best(d.get(\"cwa\", []), higher_is_better=True)\n\n        # Print clearly-labelled metrics (only if available)\n        if best_train_f1 is not None:\n            print(f\"Training Macro F1 Score (best): {best_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Validation Macro F1 Score (best): {best_val_f1:.4f}\")\n\n        if best_train_loss is not None:\n            print(f\"Training Loss (best): {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"Validation Loss (best): {best_val_loss:.4f}\")\n\n        if best_swa is not None:\n            print(f\"Shape Weighted Accuracy (best): {best_swa:.4f}\")\n        if best_cwa is not None:\n            print(f\"Color Weighted Accuracy (best): {best_cwa:.4f}\")\n\n        # Add a blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef best(values, higher_is_better=True):\n    \"\"\"\n    Helper to pick the best scalar from a nested list (list-of-lists) or flat list.\n    For F1/accuracy higher is better, for loss lower is better.\n    \"\"\"\n    if values is None or len(values) == 0:\n        return None\n    # Flatten one level if needed\n    if isinstance(values[0], (list, tuple)):\n        flat = [v for sub in values for v in sub]\n    else:\n        flat = list(values)\n    return max(flat) if higher_is_better else min(flat)\n\n\n# ------------------------------------------------------------------\n# Traverse the structure and print metrics\nfor tuning_name, datasets in experiment_data.items():  # e.g. \"dropout_rate\"\n    for dataset_name, d in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # F1 scores\n        best_train_f1 = best(d[\"metrics\"][\"train_macroF1\"], higher_is_better=True)\n        best_val_f1 = best(d[\"metrics\"][\"val_macroF1\"], higher_is_better=True)\n\n        # Losses\n        best_train_loss = best(d[\"losses\"][\"train\"], higher_is_better=False)\n        best_val_loss = best(d[\"losses\"][\"val\"], higher_is_better=False)\n\n        # Extra accuracies\n        best_swa = best(d.get(\"swa\", []), higher_is_better=True)\n        best_cwa = best(d.get(\"cwa\", []), higher_is_better=True)\n\n        # Print clearly-labelled metrics (only if available)\n        if best_train_f1 is not None:\n            print(f\"Training Macro F1 Score (best): {best_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Validation Macro F1 Score (best): {best_val_f1:.4f}\")\n\n        if best_train_loss is not None:\n            print(f\"Training Loss (best): {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"Validation Loss (best): {best_val_loss:.4f}\")\n\n        if best_swa is not None:\n            print(f\"Shape Weighted Accuracy (best): {best_swa:.4f}\")\n        if best_cwa is not None:\n            print(f\"Color Weighted Accuracy (best): {best_cwa:.4f}\")\n\n        # Add a blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment data at: {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef best(values, higher_is_better=True):\n    \"\"\"\n    Helper to pick the best scalar from a nested list (list-of-lists) or flat list.\n    For F1/accuracy higher is better, for loss lower is better.\n    \"\"\"\n    if values is None or len(values) == 0:\n        return None\n    # Flatten one level if needed\n    if isinstance(values[0], (list, tuple)):\n        flat = [v for sub in values for v in sub]\n    else:\n        flat = list(values)\n    return max(flat) if higher_is_better else min(flat)\n\n\n# ------------------------------------------------------------------\n# Traverse the structure and print metrics\nfor tuning_name, datasets in experiment_data.items():  # e.g. \"dropout_rate\"\n    for dataset_name, d in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # F1 scores\n        best_train_f1 = best(d[\"metrics\"][\"train_macroF1\"], higher_is_better=True)\n        best_val_f1 = best(d[\"metrics\"][\"val_macroF1\"], higher_is_better=True)\n\n        # Losses\n        best_train_loss = best(d[\"losses\"][\"train\"], higher_is_better=False)\n        best_val_loss = best(d[\"losses\"][\"val\"], higher_is_better=False)\n\n        # Extra accuracies\n        best_swa = best(d.get(\"swa\", []), higher_is_better=True)\n        best_cwa = best(d.get(\"cwa\", []), higher_is_better=True)\n\n        # Print clearly-labelled metrics (only if available)\n        if best_train_f1 is not None:\n            print(f\"Training Macro F1 Score (best): {best_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Validation Macro F1 Score (best): {best_val_f1:.4f}\")\n\n        if best_train_loss is not None:\n            print(f\"Training Loss (best): {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"Validation Loss (best): {best_val_loss:.4f}\")\n\n        if best_swa is not None:\n            print(f\"Shape Weighted Accuracy (best): {best_swa:.4f}\")\n        if best_cwa is not None:\n            print(f\"Color Weighted Accuracy (best): {best_cwa:.4f}\")\n\n        # Add a blank line between datasets for readability\n        print()\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'Best train macro-F1: 0.7482', '\\n', 'Best val\nmacro-F1: 0.7582', '\\n', 'Best train loss: 0.527077', '\\n', 'Best val loss:\n0.521389', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['\\nDataset: SPR_BENCH', '\\n', '  Learning rate: 0.0005', '\\n', '    Best train\nmacro F1 score: 0.7456', '\\n', '    Best validation macro F1 score: 0.7504',\n'\\n', '    Lowest train loss: 0.529028', '\\n', '    Lowest validation loss:\n0.521663', '\\n', '    Shape weighted accuracy: 0.7447', '\\n', '    Color\nweighted accuracy: 0.7397', '\\n', '  Learning rate: 0.001', '\\n', '    Best\ntrain macro F1 score: 0.7434', '\\n', '    Best validation macro F1 score:\n0.7670', '\\n', '    Lowest train loss: 0.531318', '\\n', '    Lowest validation\nloss: 0.521758', '\\n', '    Shape weighted accuracy: 0.7511', '\\n', '    Color\nweighted accuracy: 0.7460', '\\n', '  Learning rate: 0.002', '\\n', '    Best\ntrain macro F1 score: 0.7487', '\\n', '    Best validation macro F1 score:\n0.7670', '\\n', '    Lowest train loss: 0.526526', '\\n', '    Lowest validation\nloss: 0.521492', '\\n', '    Shape weighted accuracy: 0.7408', '\\n', '    Color\nweighted accuracy: 0.7361', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'training macro F1: 0.7512', '\\n', 'validation\nmacro F1: 0.7702', '\\n', 'training loss: 0.5262', '\\n', 'validation loss:\n0.5210', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'batch size 32', '\\n', 'best training macro F1 score:\n0.7480', '\\n', 'best validation macro F1 score: 0.7541', '\\n', 'lowest training\nloss: 0.5262', '\\n', 'lowest validation loss: 0.5218', '\\n', 'final shape-\nweighted accuracy: 0.7404', '\\n', 'final color-weighted accuracy: 0.7355', '\\n',\n'batch size 64', '\\n', 'best training macro F1 score: 0.7499', '\\n', 'best\nvalidation macro F1 score: 0.7671', '\\n', 'lowest training loss: 0.5286', '\\n',\n'lowest validation loss: 0.5214', '\\n', 'final shape-weighted accuracy: 0.7480',\n'\\n', 'final color-weighted accuracy: 0.7429', '\\n', 'batch size 128', '\\n',\n'best training macro F1 score: 0.7476', '\\n', 'best validation macro F1 score:\n0.7564', '\\n', 'lowest training loss: 0.5302', '\\n', 'lowest validation loss:\n0.5211', '\\n', 'final shape-weighted accuracy: 0.7399', '\\n', 'final color-\nweighted accuracy: 0.7347', '\\n', 'batch size 256', '\\n', 'best training macro\nF1 score: 0.7419', '\\n', 'best validation macro F1 score: 0.7558', '\\n', 'lowest\ntraining loss: 0.5321', '\\n', 'lowest validation loss: 0.5220', '\\n', 'final\nshape-weighted accuracy: 0.7537', '\\n', 'final color-weighted accuracy: 0.7493',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Training Macro F1 Score (best): 0.7555', '\\n',\n'Validation Macro F1 Score (best): 0.7686', '\\n', 'Training Loss (best):\n0.5198', '\\n', 'Validation Loss (best): 0.5211', '\\n', 'Shape Weighted Accuracy\n(best): 0.7535', '\\n', 'Color Weighted Accuracy (best): 0.7479', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best training F1 score     : 0.7486\n(weight_decay = 0.0)', '\\n', 'best validation F1 score   : 0.7600 (weight_decay\n= 1e-05)', '\\n', 'lowest training loss       : 0.5281 (weight_decay = 0.0001)',\n'\\n', 'lowest validation loss     : 0.5209 (weight_decay = 0.0)', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'train_macroF1: 0.746244', '\\n', 'val_macroF1:\n0.747049', '\\n', 'val_SWA: 0.744041', '\\n', 'val_CWA: 0.738820', '\\n',\n'val_SC_Gmean: 0.741426', '\\n', 'training loss: 0.530711', '\\n', 'validation\nloss: 0.521258', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nSPR_BENCH', '\\n', 'training loss: 0.5282', '\\n', 'validation loss: 0.5213',\n'\\n', 'training macro F1 score: 0.7488', '\\n', 'validation macro F1 score:\n0.7597', '\\n', 'validation SWA: 0.7599', '\\n', 'validation CWA: 0.7546', '\\n',\n'validation SC Gmean: 0.7573', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Training Macro F1 Score (best): 0.7552', '\\n',\n'Validation Macro F1 Score (best): 0.7616', '\\n', 'Training Loss (best):\n0.5199', '\\n', 'Validation Loss (best): 0.5208', '\\n', 'Shape Weighted Accuracy\n(best): 0.7567', '\\n', 'Color Weighted Accuracy (best): 0.7516', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Training Macro F1 Score (best): 0.7552', '\\n',\n'Validation Macro F1 Score (best): 0.7616', '\\n', 'Training Loss (best):\n0.5199', '\\n', 'Validation Loss (best): 0.5208', '\\n', 'Shape Weighted Accuracy\n(best): 0.7567', '\\n', 'Color Weighted Accuracy (best): 0.7516', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Training Macro F1 Score (best): 0.7547', '\\n',\n'Validation Macro F1 Score (best): 0.7598', '\\n', 'Training Loss (best):\n0.5197', '\\n', 'Validation Loss (best): 0.5207', '\\n', 'Shape Weighted Accuracy\n(best): 0.7542', '\\n', 'Color Weighted Accuracy (best): 0.7493', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}